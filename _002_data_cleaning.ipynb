{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing  a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Type_of_ownership</th>\n",
       "      <th>...</th>\n",
       "      <th>CEO_approval</th>\n",
       "      <th>Career_opportunities</th>\n",
       "      <th>Comp_&amp;_benefits</th>\n",
       "      <th>Culture_&amp;_values</th>\n",
       "      <th>Senior_management</th>\n",
       "      <th>Work/Life_balance</th>\n",
       "      <th>Pros</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Benefits_rating</th>\n",
       "      <th>Benefits_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infoway solutions LLC</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Need min 10+ Years exp\\nData Engineer\\nBay Are...</td>\n",
       "      <td>Employer Provided Salary:$68.00 Per Hour</td>\n",
       "      <td>3d</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['\"Nice and friendly work environment\" (in 1 r...</td>\n",
       "      <td>['No Cons have been reported by the Glassdoor ...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optimal Inc.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>Dearborn, MI</td>\n",
       "      <td>Data Engineer - Terraform</td>\n",
       "      <td>Position Description:\\nThe GDIA Data Factory P...</td>\n",
       "      <td>$63K - $90K (Glassdoor est.)</td>\n",
       "      <td>12d</td>\n",
       "      <td>True</td>\n",
       "      <td>1 to 50</td>\n",
       "      <td>Nonprofit Organization</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['No Pros have been reported by the Glassdoor ...</td>\n",
       "      <td>['\"Antisocial and downright rude CEO, callous ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strivernet RPO Services Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>(W2 CANDIDATES ONLY) (SANTA CLARA, CA)\\nPLEASE...</td>\n",
       "      <td>Employer Provided Salary:$90.00 - $95.00 Per Hour</td>\n",
       "      <td>5d</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Futuretech Consultants LLC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Newton, MS</td>\n",
       "      <td>Snowflake Data Engineer</td>\n",
       "      <td>My name is Dileep and I am a recruiter at Futu...</td>\n",
       "      <td>Employer Provided Salary:$40.00 - $45.00 Per Hour</td>\n",
       "      <td>30d+</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clairvoyant</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Data Engineer (MDM)</td>\n",
       "      <td>Required Skills:\\nMust have 5-8+ Years of expe...</td>\n",
       "      <td>Employer Provided Salary:$65.00 - $70.00 Per Hour</td>\n",
       "      <td>12d</td>\n",
       "      <td>True</td>\n",
       "      <td>51 to 200</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['\"Benefits, compensation, clean work environm...</td>\n",
       "      <td>['No Cons have been reported by the Glassdoor ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Company_name  Rating         Location  \\\n",
       "0        Infoway solutions LLC     3.9  Santa Clara, CA   \n",
       "1                 Optimal Inc.     3.6     Dearborn, MI   \n",
       "2  Strivernet RPO Services Ltd     NaN  Santa Clara, CA   \n",
       "3   Futuretech Consultants LLC     NaN       Newton, MS   \n",
       "4                  Clairvoyant     4.4           Remote   \n",
       "\n",
       "                   Job_title  \\\n",
       "0              Data Engineer   \n",
       "1  Data Engineer - Terraform   \n",
       "2              Data Engineer   \n",
       "3    Snowflake Data Engineer   \n",
       "4        Data Engineer (MDM)   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Need min 10+ Years exp\\nData Engineer\\nBay Are...   \n",
       "1  Position Description:\\nThe GDIA Data Factory P...   \n",
       "2  (W2 CANDIDATES ONLY) (SANTA CLARA, CA)\\nPLEASE...   \n",
       "3  My name is Dileep and I am a recruiter at Futu...   \n",
       "4  Required Skills:\\nMust have 5-8+ Years of expe...   \n",
       "\n",
       "                                              Salary Job_age  Easy_apply  \\\n",
       "0           Employer Provided Salary:$68.00 Per Hour      3d        True   \n",
       "1                       $63K - $90K (Glassdoor est.)     12d        True   \n",
       "2  Employer Provided Salary:$90.00 - $95.00 Per Hour      5d        True   \n",
       "3  Employer Provided Salary:$40.00 - $45.00 Per Hour    30d+        True   \n",
       "4  Employer Provided Salary:$65.00 - $70.00 Per Hour     12d        True   \n",
       "\n",
       "   Employees       Type_of_ownership  ... CEO_approval  Career_opportunities  \\\n",
       "0        NaN       Company - Private  ...         0.84                   4.0   \n",
       "1    1 to 50  Nonprofit Organization  ...         0.78                   3.2   \n",
       "2        NaN        Company - Public  ...          NaN                   NaN   \n",
       "3        NaN                     NaN  ...          NaN                   NaN   \n",
       "4  51 to 200       Company - Private  ...         0.87                   4.1   \n",
       "\n",
       "  Comp_&_benefits Culture_&_values  Senior_management  Work/Life_balance  \\\n",
       "0             3.9              4.0                3.7                3.9   \n",
       "1             3.7              3.3                2.6                4.5   \n",
       "2             NaN              NaN                NaN                NaN   \n",
       "3             NaN              NaN                NaN                NaN   \n",
       "4             4.2              3.9                4.1                4.0   \n",
       "\n",
       "                                                Pros  \\\n",
       "0  ['\"Nice and friendly work environment\" (in 1 r...   \n",
       "1  ['No Pros have been reported by the Glassdoor ...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  ['\"Benefits, compensation, clean work environm...   \n",
       "\n",
       "                                                Cons  Benefits_rating  \\\n",
       "0  ['No Cons have been reported by the Glassdoor ...              2.2   \n",
       "1  ['\"Antisocial and downright rude CEO, callous ...              5.0   \n",
       "2                                                NaN              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       "\n",
       "   Benefits_reviews  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/RAW/Data_Engineer_06-03-2023_23-41.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company_name', 'Rating', 'Location', 'Job_title', 'Description',\n",
       "       'Salary', 'Job_age', 'Easy_apply', 'Employees', 'Type_of_ownership',\n",
       "       'Sector', 'Founded', 'Industry', 'Revenue_USD', 'Friend_recommend',\n",
       "       'CEO_approval', 'Career_opportunities', 'Comp_&_benefits',\n",
       "       'Culture_&_values', 'Senior_management', 'Work/Life_balance', 'Pros',\n",
       "       'Cons', 'Benefits_rating', 'Benefits_reviews'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove rows only with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how='all')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no empty rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is huge amount of duplicates. But this is the feature of glassdoor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1, how='all')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no empty columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Now we will split `Location` column into `State` and `City`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Santa Clara, CA\n",
       "1       Dearborn, MI\n",
       "2    Santa Clara, CA\n",
       "3         Newton, MS\n",
       "4             Remote\n",
       "Name: Location, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Location'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Santa Clara\n",
       "1       Dearborn\n",
       "2    Santa Clara\n",
       "3         Newton\n",
       "4         Remote\n",
       "Name: City, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['City'] = df['Location'].apply(lambda x: x.split(',')[0] if \",\" in x else x)\n",
    "df['City'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        CA\n",
       "1        MI\n",
       "2        CA\n",
       "3        MS\n",
       "4    Remote\n",
       "Name: State, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['State'] = df['Location'].apply(lambda x: x.split(',')[1] if \",\" in x else x)\n",
    "df['State'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del df['Location']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Add job title seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data Engineer', 'Data Engineer - Terraform',\n",
       "       'Snowflake Data Engineer', 'Data Engineer (MDM)',\n",
       "       'AWS Data Engineer', 'DATA ENGINEER', 'Big Data Engineer',\n",
       "       'Sr. Data Engineer', 'Data Engineer - Flink', 'Jr. Data Engineer',\n",
       "       'Data Engineer - Remote', 'Data Engineer (L5)',\n",
       "       'Software Data Engineer', 'GCP Data Engineer',\n",
       "       'Senior Data Engineer', 'Azure Cloud Data Engineer',\n",
       "       'GCP DATA ENGINEER', 'Data Test Engineer', 'Azure Data Engineer',\n",
       "       'Senior Azure Data Bricks Engineer', 'Data Analytics Engineer',\n",
       "       'Data Engineer (W2 and onsite)', 'Senior Big Data Engineer',\n",
       "       'Data Engineer- Google Cloud',\n",
       "       'Data Engineer (ETL & System Administration concentration)',\n",
       "       'Data Engineer/Data Analyst', 'Data Engineer/Data Scientist',\n",
       "       'ETL Data Engineer', 'Lead Data Engineer',\n",
       "       'Sr. Data Engineer with Snowflake', 'Junior Data Engineer',\n",
       "       'Senior Data Engineer - Remote', 'Data Engineer Level 3',\n",
       "       'Cloud Data Engineer (Azure)', 'Senior Software Engineer, Data',\n",
       "       'Technical Support Engineer (L5) - Data Platform, Big Data / Analytics',\n",
       "       'Full Stack Data Engineer', 'Data Engineer IC4 - US ONLY',\n",
       "       'Data Engineer I',\n",
       "       'Senior Health Data and Interoperability Engineer',\n",
       "       'Azure Data Warehouse Engineer', 'Data Logging Engineer',\n",
       "       'ETL Engineer/ Data Analyst - Software Engineer III',\n",
       "       'Software Engineer III (AI, Data, Python)',\n",
       "       'Software/GCP Data Engineer (W2 only)', 'Software Engineer (Data)',\n",
       "       'Junior Big Data Engineer - C11', 'Anaplan Data Engineer',\n",
       "       'Data Software Engineer',\n",
       "       'Data Engineer (ETL & Data Catalogue Support)',\n",
       "       'Data Scientist / Data Engineer',\n",
       "       'Big Data Sytems Engineer(Hadoop)',\n",
       "       'Data Engineer - Need locals to CA', 'Data Engineer (ETL)',\n",
       "       'Python Data Engineer', 'Data engineer',\n",
       "       'Data Engineer - SnowFlake, Azure Data, Terraform',\n",
       "       'Senior Data Engineer [ Remote ]',\n",
       "       'Senior Data Engineer, Analytics', 'Cloud Data Engineer',\n",
       "       'Software Engineer, Data', 'Data Engineer 925',\n",
       "       'Senior Azure Data Engineer', 'Data Engineer ETL',\n",
       "       'Big Data Engineer, Senior'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Job_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Senior    45\n",
       "Junior     4\n",
       "Lead       4\n",
       "Name: Seniority, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_seniority(job_title:str):\n",
    "\n",
    "    seniority = {\n",
    "        'Junior' : [\"Jr.\", \"Junior\"],\n",
    "        'Mid' : [\"Mid\", \"Middle\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\"],\n",
    "        'Lead': \"Lead\",\n",
    "        'Principle' : \"Principle\"\n",
    "    }\n",
    "    \n",
    "    if seniority['Junior'][0] in job_title or seniority['Junior'][1] in job_title :\n",
    "        return \"Junior\"\n",
    "    elif seniority['Mid'][0] in job_title or seniority['Mid'][1] in job_title :\n",
    "        return \"Mid\"\n",
    "    elif seniority['Senior'][0] in job_title or seniority['Senior'][1] in job_title :\n",
    "        return \"Senior\"\n",
    "    elif seniority['Lead'] in job_title:\n",
    "        return \"Lead\"\n",
    "    elif seniority['Principle'] in job_title:\n",
    "        return \"Principle\"\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "df['Seniority'] = df['Job_title'].apply(get_seniority)\n",
    "\n",
    "del get_seniority\n",
    "\n",
    "df['Seniority'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add non-standard seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Senior    50\n",
       "Lead       5\n",
       "Junior     4\n",
       "Mid        1\n",
       "Name: Seniority, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_seniority_level(df, job_title, company_name, seniority_level):\n",
    "    df['Seniority'] = df.apply(\n",
    "        lambda row: seniority_level if row['Job_title'] == job_title and row['Company_name'] == company_name else row['Seniority'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "apply_seniority_level(df, \"Data Engineer (L5)\", \"Netflix\", \"Senior\")\n",
    "apply_seniority_level(df, \"Technical Support Engineer (L5) - Data Platform, Big Data / Analytics\", \"Netflix\", \"Senior\")\n",
    "apply_seniority_level(df, \"Data Engineer Level 3\", \"Infoorigin Inc\", \"Mid\")\n",
    "apply_seniority_level(df, \"Data Engineer IC4 - US ONLY\", \"Braintrust\", \"Lead\")\n",
    "apply_seniority_level(df, \"ETL Engineer/ Data Analyst - Software Engineer III\", \"JPMorgan Chase Bank, N.A.\", \"Senior\")\n",
    "apply_seniority_level(df, \"Software Engineer III (AI, Data, Python)\", \"JPMorgan Chase Bank, N.A.\", \"Senior\")\n",
    "apply_seniority_level(df, \"Data Engineer 925\", \"Certec Consulting\", \"Senior\")\n",
    "\n",
    "del apply_seniority_level\n",
    "\n",
    "df['Seniority'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Parse salary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Employer provided salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     128\n",
       "False     92\n",
       "Name: Salary_employer_provided, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Salary_employer_provided'] = df['Salary'].apply(lambda salary : True if isinstance(salary, str) and \"Employer Provided Salary\" in salary else False)\n",
    "df['Salary_employer_provided'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Salary per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    139\n",
       "True      81\n",
       "Name: Salary_hourly, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Salary_hourly'] = df['Salary'].apply(lambda salary : True if isinstance(salary, str) and \"Per Hour\" in salary else False)\n",
    "df['Salary_hourly'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Salary min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      141440.0\n",
       "1       63000.0\n",
       "2      187200.0\n",
       "3       83200.0\n",
       "4      135200.0\n",
       "         ...   \n",
       "624         NaN\n",
       "755         NaN\n",
       "778     81000.0\n",
       "825    120640.0\n",
       "895         NaN\n",
       "Name: Salary_min, Length: 220, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_salary_min(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?K?)\"\n",
    "        match_min: str = re.findall(pattern_salary, salary)[0][0]\n",
    "\n",
    "        if \"K\" in match_min:\n",
    "            match_min = float(match_min.replace(\"K\", \"\"))\n",
    "            match_min *= 1000\n",
    "\n",
    "        return float(match_min)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "    \n",
    "def calculate_yearly_income(hourly_rate):\n",
    "\n",
    "    hours_per_week = 40\n",
    "    WEEKS_PER_YEAR = 52\n",
    "    HOURS_PER_YEAR = WEEKS_PER_YEAR * hours_per_week\n",
    "    gross_income = hourly_rate * HOURS_PER_YEAR\n",
    "    return gross_income\n",
    "\n",
    "df['Salary_min'] = df['Salary'].apply(get_salary_min)\n",
    "df['Salary_min'] = df.apply(\n",
    "        lambda row: calculate_yearly_income(row['Salary_min']) if row['Salary_hourly'] == True else row['Salary_min'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "del get_salary_min\n",
    "\n",
    "df['Salary_min']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Salary max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      141440.0\n",
       "1       90000.0\n",
       "2      197600.0\n",
       "3       93600.0\n",
       "4      145600.0\n",
       "         ...   \n",
       "624         NaN\n",
       "755         NaN\n",
       "778    115000.0\n",
       "825    131040.0\n",
       "895         NaN\n",
       "Name: Salary_max, Length: 220, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_salary_max(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?K?)\"\n",
    "        match_max: str = re.findall(pattern_salary, salary)[-1][0]\n",
    "\n",
    "        if \"K\" in match_max:\n",
    "            match_max = float(match_max.replace(\"K\", \"\"))\n",
    "            match_max *= 1000\n",
    "\n",
    "        return float(match_max)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "\n",
    "df['Salary_max'] = df['Salary'].apply(get_salary_max)\n",
    "df['Salary_max'] = df.apply(\n",
    "        lambda row: calculate_yearly_income(row['Salary_max']) if row['Salary_hourly'] == True else row['Salary_max'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "del get_salary_max\n",
    "\n",
    "df['Salary_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "del calculate_yearly_income"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Salary currency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$    199\n",
       "Name: Salary_currency, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_currency(salary: str):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_currency = r\"(.+?(?=\\d))\"\n",
    "\n",
    "        if \"Employer Provided Salary\" in salary:\n",
    "            pattern_currency = r\"(\\:.+?(?=\\d))\"\n",
    "\n",
    "        matched = re.search(pattern_currency, salary)\n",
    "\n",
    "        currency = matched.group(1).strip().replace(\":\", \"\")\n",
    "\n",
    "        return currency\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "    \n",
    "df['Salary_currency'] = df['Salary'].apply(get_currency)\n",
    "    \n",
    "del get_currency\n",
    "    \n",
    "df['Salary_currency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Salary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Salary average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      141440.0\n",
       "1       76500.0\n",
       "2      192400.0\n",
       "3       88400.0\n",
       "4      140400.0\n",
       "         ...   \n",
       "624         NaN\n",
       "755         NaN\n",
       "778     98000.0\n",
       "825    125840.0\n",
       "895         NaN\n",
       "Name: Salary_avg, Length: 220, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Salary_avg'] = (df['Salary_max']+df['Salary_min'])/2\n",
    "df['Salary_avg']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 to 50          56\n",
       "51 to 200        50\n",
       "10000+           23\n",
       "1001 to 5000     20\n",
       "201 to 500       15\n",
       "501 to 1000      10\n",
       "5001 to 10000     4\n",
       "Name: Employees, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Employees'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Type of ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company - Private                 119\n",
       "Company - Public                   57\n",
       "Nonprofit Organization              7\n",
       "Contract                            5\n",
       "Subsidiary or Business Segment      4\n",
       "Self-employed                       2\n",
       "Private Practice / Firm             2\n",
       "Name: Type_of_ownership, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type_of_ownership'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information Technology                         77\n",
       "Financial Services                             16\n",
       "Human Resources & Staffing                      7\n",
       "Management & Consulting                         7\n",
       "Insurance                                       5\n",
       "Manufacturing                                   5\n",
       "Education                                       4\n",
       "Healthcare                                      4\n",
       "Energy, Mining & Utilities                      4\n",
       "Media & Communication                           3\n",
       "Pharmaceutical & Biotechnology                  3\n",
       "Retail & Wholesale                              2\n",
       "Nonprofit & NGO                                 1\n",
       "Agriculture                                     1\n",
       "Transportation & Logistics                      1\n",
       "Arts, Entertainment & Recreation                1\n",
       "Personal Consumer Services                      1\n",
       "Aerospace & Defense                             1\n",
       "Construction, Repair & Maintenance Services     1\n",
       "Name: Sector, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sector'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information Technology Support Services    49\n",
       "Computer Hardware Development              10\n",
       "Enterprise Software & Network Solutions     9\n",
       "Business Consulting                         7\n",
       "Banking & Lending                           6\n",
       "Internet & Web Services                     6\n",
       "Investment & Asset Management               5\n",
       "Insurance Carriers                          5\n",
       "HR Consulting                               5\n",
       "Health Care Services & Hospitals            4\n",
       "Energy & Utilities                          4\n",
       "Financial Transaction Processing            3\n",
       "Software Development                        3\n",
       "Biotech & Pharmaceuticals                   3\n",
       "Education & Training Services               3\n",
       "Accounting & Tax                            2\n",
       "Advertising & Public Relations              2\n",
       "Staffing & Subcontracting                   2\n",
       "Commercial Printing                         2\n",
       "Wholesale                                   1\n",
       "Construction                                1\n",
       "Aerospace & Defense                         1\n",
       "Beauty & Wellness                           1\n",
       "Sports & Recreation                         1\n",
       "Consumer Product Manufacturing              1\n",
       "Video Game Publishing                       1\n",
       "Car & Truck Rental                          1\n",
       "Food & Beverage Manufacturing               1\n",
       "Crop Production                             1\n",
       "Civic & Social Services                     1\n",
       "Automotive Parts & Accessories Stores       1\n",
       "Primary & Secondary Schools                 1\n",
       "Machinery Manufacturing                     1\n",
       "Name: Industry, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Industry'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0     8\n",
       "15.0     8\n",
       "8.0      7\n",
       "10.0     6\n",
       "5.0      5\n",
       "39.0     5\n",
       "9.0      5\n",
       "7.0      5\n",
       "24.0     5\n",
       "27.0     4\n",
       "12.0     4\n",
       "19.0     4\n",
       "20.0     4\n",
       "4.0      3\n",
       "17.0     3\n",
       "25.0     3\n",
       "85.0     3\n",
       "26.0     3\n",
       "6.0      2\n",
       "11.0     2\n",
       "224.0    2\n",
       "14.0     2\n",
       "18.0     2\n",
       "41.0     2\n",
       "13.0     2\n",
       "53.0     1\n",
       "239.0    1\n",
       "76.0     1\n",
       "160.0    1\n",
       "50.0     1\n",
       "122.0    1\n",
       "47.0     1\n",
       "35.0     1\n",
       "81.0     1\n",
       "22.0     1\n",
       "128.0    1\n",
       "211.0    1\n",
       "170.0    1\n",
       "23.0     1\n",
       "171.0    1\n",
       "158.0    1\n",
       "52.0     1\n",
       "97.0     1\n",
       "89.0     1\n",
       "54.0     1\n",
       "77.0     1\n",
       "37.0     1\n",
       "28.0     1\n",
       "106.0    1\n",
       "173.0    1\n",
       "29.0     1\n",
       "3.0      1\n",
       "Name: Company_age, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "year = datetime.date.today().year\n",
    "\n",
    "df['Company_age'] = df['Founded'].apply(lambda x: x if np.isnan(x) else int(year - x))\n",
    "df['Company_age'] = df['Company_age']\n",
    "\n",
    "del df['Founded'], year\n",
    "\n",
    "df['Company_age'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Job age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10d', '11d', '12d', '13d', '14d', '16d', '17d', '18d', '19d',\n",
       "       '20d', '22d', '24d', '24h', '25d', '26d', '28d', '2d', '30d+',\n",
       "       '3d', '4d', '5d', '6d', '7d', '9d'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(df['Job_age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31    91\n",
       "1     18\n",
       "3     14\n",
       "12    14\n",
       "6     13\n",
       "13     9\n",
       "2      9\n",
       "5      8\n",
       "4      7\n",
       "18     6\n",
       "10     5\n",
       "24     4\n",
       "17     4\n",
       "20     3\n",
       "19     2\n",
       "9      2\n",
       "7      2\n",
       "25     2\n",
       "14     2\n",
       "16     1\n",
       "28     1\n",
       "11     1\n",
       "22     1\n",
       "26     1\n",
       "Name: Job_age, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_job_age(job_age):\n",
    "\n",
    "    if job_age == \"24h\":\n",
    "        job_age = \"1d\"\n",
    "    elif job_age == \"30d+\":\n",
    "        job_age = \"31d\"\n",
    "\n",
    "    return int(job_age.replace(\"d\", \"\"))\n",
    "\n",
    "df['Job_age'] = df['Job_age'].apply(clean_job_age)\n",
    "\n",
    "del clean_job_age\n",
    "df['Job_age'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$5 to $25 million             23\n",
       "$10+ billion                  17\n",
       "$25 to $100 million           16\n",
       "$1 to $5 million              15\n",
       "Less than $1 million           9\n",
       "$100 to $500 million           8\n",
       "$1 to $5 billion               7\n",
       "$5 to $10 billion              5\n",
       "$500 million to $1 billion     4\n",
       "Name: Revenue_USD, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Revenue_USD'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Preview columns so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                        object\n",
       "Seniority                    object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_currency              object\n",
       "Salary_avg                  float64\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Change columns order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.1 move salary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                        object\n",
       "Seniority                    object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_column__to_index(column_name: str, index: int):\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "\n",
    "\n",
    "def move_columns_to_index(column_names: list[str], index: int):\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "move_columns_to_index([\n",
    "    'Salary_min', \n",
    "    'Salary_max', \n",
    "    'Salary_avg', \n",
    "    'Salary_currency',\n",
    "    'Salary_employer_provided', \n",
    "    'Salary_hourly'\n",
    "    ], 3\n",
    "    )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.2 Move Seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                        object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_column__to_index('Seniority', 3)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.3 Move City, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Description                  object\n",
       "City                         object\n",
       "State                        object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_columns_to_index(['City', 'State'], 11)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.4 Move Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Description                  object\n",
       "City                         object\n",
       "State                        object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Company_age                 float64\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_column__to_index('Company_age', 19)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.5 Move Work/Life_balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                  bool\n",
       "Description                  object\n",
       "City                         object\n",
       "State                        object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Company_age                 float64\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Culture_&_values            float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_columns_to_index(['Senior_management', 'Work/Life_balance'], 25)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Technology requirements - parsing the job description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19 Git and code repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Git       4\n",
       "GitLab    1\n",
       "Name: Git, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_repo(job_description: str):\n",
    "\n",
    "    git_platforms = [\n",
    "        r\"Github\", \n",
    "        r\"GitLab\", \n",
    "        r\"Bitbucket\", \n",
    "        r\"SourceForge\", \n",
    "        r\"Launchpad\", \n",
    "        r\"Google Cloud Source Repositories\",\n",
    "        r\"AWS CodeCommit\",\n",
    "        r\"GitBucket\",\n",
    "        r\"Gogs\",\n",
    "        r\"Gitea\",\n",
    "        r\"Apache Allura\",\n",
    "        r\"RhodeCode\",\n",
    "        r\"ONEDEV\",\n",
    "        r\"Codeberg\",\n",
    "        r\"Git\" # IMPORTANT, it has to be last!\n",
    "        ]\n",
    "    \n",
    "    for platform in git_platforms:\n",
    "        if re.search((r\"\\b\" + platform + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "            return platform\n",
    "        \n",
    "    return np.nan\n",
    "        \n",
    "df['Git'] = df['Description'].apply(check_repo)\n",
    "\n",
    "del check_repo\n",
    "\n",
    "df['Git'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_tech(cloud_names: list[str]):\n",
    "\n",
    "    def is_tech(job_description: str):\n",
    "\n",
    "        \n",
    "        for cloud in cloud_names:\n",
    "            if re.search((r\"\\b\" + cloud + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    return is_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_needed_column_to_df(column_name: str, tech_names: list[str]):\n",
    "\n",
    "    df[column_name] = df['Description'].apply(make_is_tech(tech_names))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Cloud Platforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.1 AWS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Often times, clients will use this in combination with autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    196\n",
       "True      24\n",
       "Name: AWS, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Amazon Web Services\", \n",
    "    r\"AWS\",\n",
    "    ]\n",
    "\n",
    "column_name = 'AWS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.2 Microsoft Azure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud computing platform operated by Microsoft that provides access, management, and development of applications and services via around the world-distributed data centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    196\n",
       "True      24\n",
       "Name: Microsoft_Azure, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Microsoft Azure\", \n",
    "    r\"Azure\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_Azure'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.3 GCP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    204\n",
       "True      16\n",
       "Name: GPC, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Google Cloud Platform\", \n",
    "    r\"GCP\",\n",
    "    ]\n",
    "\n",
    "column_name = 'GPC'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Alibaba Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibaba Cloud provides cloud computing services to online businesses and Alibaba's own e-commerce ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Alibaba_Cloud, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Alibaba Cloud\", \n",
    "    r\"Aliyun\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Alibaba_Cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Oracle Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Oracle_Cloud, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Oracle Cloud\", \n",
    "    r\"OCI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.5 IBM Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of cloud computing services for business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: IBM_cloud, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"IBM Cloud\", \n",
    "    r\"Kyndryl\",\n",
    "    r\"Bluemix\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.6 Tencent Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tencent Cloud provides businesses across the globe with stable and secure industry-leading cloud products and services, leveraging technological advancements such as cloud computing, Big Data, AI, IoT and network security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Tencent_cloud, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Tencent Cloud\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Tencent_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.8 OVHcloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A French cloud computing company which offers VPS, dedicated servers and other web services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: OVHcloud, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"OVHcloud\",\n",
    "    r\"OVH\"\n",
    "    ]\n",
    "\n",
    "column_name = 'OVHcloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.9 DigitalOcean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud hosting provider that offers cloud computing services and Infrastructure as a Service (IaaS). Known for pricing and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: DigitalOcean_cloud, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"DigitalOcean\"\n",
    "    ]\n",
    "\n",
    "column_name = 'DigitalOcean_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.10 Linode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An American cloud hosting provider that focused on providing Linux-based virtual machines, cloud infrastructure, and managed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Lincode_cloud, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Linode\",\n",
    "    r\"Akamai\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Lincode_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cloud_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Relational Database Management Systems (RDBMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.1 PostgreSQL\n",
    "Can be used as a data store for big data solutions.\n",
    "Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. <br>\n",
    "PostgreSQL features transactions with Atomicity, Consistency, Isolation, Durability (ACID) properties, automatically updatable views, materialized views, triggers, foreign keys, and stored procedures. <br> It is designed to handle a range of workloads, from single machines to data warehouses or Web services with many concurrent users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: PostgreSQL, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PostgreSQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.2 Microsoft SQL Server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A software product with the primary function of storing and retrieving data as requested by other software applicationsâ€”which may run either on the same computer or on another computer across a network (including the Internet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    213\n",
       "True       7\n",
       "Name: Microsoft_SQL_Server, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Microsoft SQL\",\n",
    "    r\"SQL Server\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_SQL_Server'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.3 MySQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source relational database management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: MySQL, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MySQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MySQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.4 IBM Db2 warehouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A family of data management products, including database servers, developed by IBM. It initially supported the relational model, but was extended to support objectâ€“relational features and non-relational structures like JSON and XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    217\n",
       "True       3\n",
       "Name: IBM_Db2, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Db2\",\n",
    "    r\"IBMDb2\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_Db2'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.5. Oracle PL/SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A procedural language designed specifically to embrace SQL statements within its syntax. PL/SQL program units are compiled by the Oracle Database server and stored inside the database. And at run-time, both PL/SQL and SQL run within the same server process, bringing optimal efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: Oracle_PL_SQL, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PL/SQL\",\n",
    "    r\"PL / SQL\",\n",
    "    r\"Procedural Language for SQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_PL_SQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. NoSQL Database Management Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: MongoDB, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MongoDB\",\n",
    "    r\"Mongo DB\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MongoDB'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Cassandra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    217\n",
       "True       3\n",
       "Name: Cassandra, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Cassandra\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cassandra'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Amazon DynamoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proprietary NoSQL database service that supports keyâ€“value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Amazon_DynamoDB, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"DynamoDB\",\n",
    "    r\"Dynamo DB\",\n",
    "    r\"SimpleDB\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_DynamoDB'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22.4 Neo4j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Neo4j, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Neo4j\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Neo4j'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 Apache Solr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features[2] and rich document (e.g., Word, PDF) handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Apache_Solr, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Solr\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Solr'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. Data warehousing and Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 Amazon Redshift"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    215\n",
       "True       5\n",
       "Name: Amazon_Redshift, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Redshift\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_Redshift'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Google BigQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    213\n",
       "True       7\n",
       "Name: Google_BigQuery, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"BigQuery\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_BigQuery'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Snowflake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowflake enables data storage, processing, and analytic solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    207\n",
       "True      13\n",
       "Name: Snowflake, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Snowflake\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Snowflake'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.4 Oracle Exadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed to run Oracle Database workloads, such as an OLTP application running simultaneously with Analytics processing. Historically, specialized database computing platforms were designed for a particular workload, such as Data Warehousing, and poor or unusable for other workloads, such as OLTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Oracle_Exadata, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Exadata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Exadata'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 SAP HANA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-model database that stores data in its memory instead of keeping it on a disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: SAP_HANA, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"HANA\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SAP_HANA'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.6 Teradata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mainly suitable for building large scale data warehousing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    216\n",
       "True       4\n",
       "Name: Teradata, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Teradata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Teradata'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23. Data Integration and Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.1 Informatica PowerCenter - Data integration tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used extensively for ETL operations, data quality, data masking, data replication, data virtualization, and master data management services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Informatica_PowerCenter, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PowerCenter\",\n",
    "    r\"Power Center\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Informatica_PowerCenter'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.2 DataBricks - Data processing and analytics platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    206\n",
       "True      14\n",
       "Name: Databricks, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Data Bricks\",\n",
    "    r\"Databricks\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Databricks'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.3 Presto - Query engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A distributed query engine for big data using the SQL query language. Its architecture allows users to query data sources such as Hadoop, Cassandra, Kafka, AWS S3, Alluxio, MySQL, MongoDB and Teradata, and allows use of multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Presto, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Presto\",\n",
    "    r\"PrestoDB\",\n",
    "    r\"PrestoSQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Presto'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24. Stream processing tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Kafka"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source system, distributed event store and stream-processing platform. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    209\n",
       "True      11\n",
       "Name: Apache_Kafka, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Kafka\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Kafka'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data streams at a large scale and to deliver real-time analytical insights about your processed data with your streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Apache_Flink, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Flink\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Flink'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Dataflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow is a managed service provided by Google Cloud for building and executing data processing pipelines. It enables developers to create scalable and efficient batch and streaming data pipelines using a simple programming model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    216\n",
       "True       4\n",
       "Name: Dataflow, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Dataflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dataflow'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 Workflow orchestration tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.1 Apache Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is an open-source platform used for programmatically creating, scheduling, and monitoring complex workflows or data pipelines. It allows users to define and execute a sequence of tasks or operations, while providing tools for tracking and troubleshooting workflow executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    211\n",
       "True       9\n",
       "Name: Apache_Airflow, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Airflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Airflow'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.2 Luigi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luigi is a Python-based open-source workflow management system that helps to build complex pipelines of batch jobs. It provides a flexible and extensible architecture to create and manage complex data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Luigi, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Luigi\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Luigi'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.3 SSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Server Integration Services (SSIS) is a Microsoft tool used for building data integration and ETL (extract, transform, load) workflows. It allows users to perform a range of tasks such as data extraction, transformation, and loading from various sources to different destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    215\n",
       "True       5\n",
       "Name: SSIS, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"SSIS\",\n",
    "    r\"SQL Server Integration Services\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SSIS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Big Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Hadoop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hadoop is an open-source framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It provides a distributed file system and supports various distributed computing models, such as MapReduce and Spark, for processing and analyzing large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    202\n",
       "True      18\n",
       "Name: Apache_Hadoop, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Hadoop\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hadoop'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Hive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hive is a data warehouse software that facilitates querying and managing large datasets stored in Hadoop file systems using a SQL-like language called HiveQL. It provides a high-level interface for data analysts and developers to analyze, transform, and summarize data stored in Hadoop Distributed File System (HDFS) and other compatible storage systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    205\n",
       "True      15\n",
       "Name: Apache_Hive, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Hive\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hive'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a distributed computing framework designed to process large-scale data processing and analysis workloads in parallel. It can be used for batch processing, real-time stream processing, machine learning, and graph processing, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    185\n",
       "True      35\n",
       "Name: Apache_Spark, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Spark\",\n",
    "    r\"PySpark\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Spark'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name      object\n",
       "Rating           float64\n",
       "Job_title         object\n",
       "Seniority         object\n",
       "Salary_min       float64\n",
       "                  ...   \n",
       "Luigi               bool\n",
       "SSIS                bool\n",
       "Apache_Hadoop       bool\n",
       "Apache_Hive         bool\n",
       "Apache_Spark        bool\n",
       "Length: 71, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25. Linux"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family of Unix-like operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    216\n",
       "True       4\n",
       "Name: Linux, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Linux\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Linux'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Programming languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.1 Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a high-level, interpreted programming language used for various purposes such as web development, data analysis, artificial intelligence, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    162\n",
       "True      58\n",
       "Name: Python, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Python\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Python'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.2 R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language and environment for statistical graphics and computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    216\n",
       "True       4\n",
       "Name: R, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"R\",\n",
    "    r\"RStudio\"\n",
    "    ]\n",
    "\n",
    "column_name = 'R'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.3 Scala"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala is a high-level, statically typed programming language designed for functional programming and scalable, concurrent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    204\n",
       "True      16\n",
       "Name: Scala, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Scala\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Scala'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.4 SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language used to manage and manipulate relational databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    169\n",
       "True      51\n",
       "Name: SQL, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"SQL\",\n",
    "    r\"MySQL\",\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\",\n",
    "    r\"SQLite\",\n",
    "    r\"MariaDB\",\n",
    "    r\"IBM DB2\",\n",
    "    r\"Oracle Database\",\n",
    "    r\"Db2\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.5 Java"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Java is a high-level, object-oriented programming language widely used for developing robust and scalable enterprise applications.\n",
    "\n",
    "In Data Science, Java can be used for developing machine learning models, data analysis, and data processing applications, as well as for building large-scale distributed systems for big data processing and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    200\n",
       "True      20\n",
       "Name: Java, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Java\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Java'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.6 C++"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general-purpose programming language designed for systems and application programming, and it is used in Data Science for building high-performance libraries and applications that require intensive computational tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: C++, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"C\\+\\+\",\n",
    "    ]\n",
    "\n",
    "column_name = 'C++'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.7 Go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statically typed programming language designed for building simple, efficient, and reliable software, and it can be used in data engineering for building scalable, distributed systems for data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Go, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Go language\", # Go as separate word is too common in English\n",
    "    r\"Golang\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Go'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.8 Bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shell scripting language used for automating repetitive tasks and managing the operating system, including data processing tasks, in the command-line interface (CLI) on Unix and Unix-like systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Bash, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Bash\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Bash'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.9 Powershell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task automation and configuration management framework from Microsoft, which can be used in Data Science for automating various data processing tasks on Windows machines in the command-line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: PowerShell, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PowerShell\",\n",
    "    r\"DOS Shell\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PowerShell'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.10 CLI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLI stands for Command Line Interface, which is a way to interact with a computer program through text commands, and it is commonly used in Data Science for running scripts, automating tasks, and managing software packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: CLI, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"CLI\",\n",
    "    r\"Command Line Interface\"\n",
    "    ]\n",
    "\n",
    "column_name = 'CLI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27. Virtualization Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business intelligence and data visualization tools used for analyzing and visualizing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.1 Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Tableau, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Tableau\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Tableau'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.2 Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Power_BI, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Power BI\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Power_BI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.3 Google Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Google_Analytics, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Google Analytics\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_Analytics'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.4 QlikView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: QlikView, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"QlikView\",\n",
    "    r\"Qlik\"\n",
    "    ]\n",
    "\n",
    "column_name = 'QlikView'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.5 Oracle BI server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Oracle_BI_server, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Oracle Business Intelligence Enterprise Edition\",\n",
    "    r\"OBIEE\",\n",
    "    r\"Oracle BI server\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_BI_server'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.6 SAS Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: SAS_Analytics, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"SAS Analytics\",\n",
    "    r\"Statistical Analysis System\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SAS_Analytics'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.7 Lumira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Lumira, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Lumira\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Lumira'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.8 IBM Cognos Impromptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Cognos_Impromptu, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Cognos Impromptu\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cognos_Impromptu'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.9 MicroStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: MicroStrategy, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MicroStrategy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MicroStrategy'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.10 InsightSquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: InsightSquared, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"InsightSquared\",\n",
    "    ]\n",
    "\n",
    "column_name = 'InsightSquared'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.11 Sisense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Sisense, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Sisense\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Sisense'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.12 Dundas BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Dundas_BI, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Dundas BI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dundas_BI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.13 Domo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Domo, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Domo\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Domo'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.14 Looker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    218\n",
       "True       2\n",
       "Name: Looker, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Looker\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Looker'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28. Microsoft Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    217\n",
       "True       3\n",
       "Name: Excel, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Excel\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Excel'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 29. Certifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is a need for any certification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    220\n",
       "Name: Is_certificate, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coursera, Udemy, Datacamp etc. list\n",
    "tool_names = [\n",
    "    r\"Certificates\",\n",
    "    r\"Certificate\",\n",
    "    r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "    r\"Google Professional Data Engineer\",\n",
    "    r\"Microsoft Azure Data Engineering\",\n",
    "    r\"Data Engineer.+Nanodegree\",\n",
    "    r\"DataCamp\",\n",
    "    r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "    r\"Python, Bash and SQL Essentials for Data Engineering Specialization\",\n",
    "    r\"Data Engineering ETL, Web Scraping, and Automation\",\n",
    "    r\"Big Data Engineering with Hadoop and Spark\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Is_certificate'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. Needed education level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.1 BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    204\n",
       "True      16\n",
       "Name: BA, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"BA\",\n",
    "    r\"Bachelor\",\n",
    "    r\"BSc\",\n",
    "    r\"Bachelors\"\n",
    "    ]\n",
    "\n",
    "column_name = 'BA'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.2 MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    207\n",
       "True      13\n",
       "Name: MS, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MS\",\n",
    "    r\"MSc\",\n",
    "    r\"Master\",\n",
    "    r\"Masters\",\n",
    "    r\"master\\'s\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.3 Phd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    219\n",
       "True       1\n",
       "Name: Phd, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Phd\",\n",
    "    r\"Ph\\.D\",\n",
    "    r\"DPhil\",\n",
    "    r\"Doctor of Philosophy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Phd'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 101)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company_name', 'Rating', 'Job_title', 'Seniority', 'Salary_min',\n",
       "       'Salary_max', 'Salary_avg', 'Salary_currency',\n",
       "       'Salary_employer_provided', 'Salary_hourly',\n",
       "       ...\n",
       "       'InsightSquared', 'Sisense', 'Dundas_BI', 'Domo', 'Looker', 'Excel',\n",
       "       'Is_certificate', 'BA', 'MS', 'Phd'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_names = df.columns\n",
    "columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "del columns_names, column_name, tool_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Final cleanup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.1 Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\n",
    "    'Company_name': 'Name',\n",
    "    'Job_title': 'Title',\n",
    "    'Salary_min': 'Min',\n",
    "    'Salary_max': 'Max',\n",
    "    'Salary_avg': 'Avg',\n",
    "    'Salary_currency': 'Currency',\n",
    "    'Salary_employer_provided': 'Employer_provided',\n",
    "    'Salary_hourly': 'Is_hourly',\n",
    "    'Alibaba_Cloud': 'Alibaba',\n",
    "    'Oracle_Cloud': 'Oracle',\n",
    "    'IBM_cloud': 'IBM',\n",
    "    'Tencent_cloud': 'Tencent',\n",
    "    'DigitalOcean_cloud': 'DigitalOcean',\n",
    "    'Lincode_cloud': 'Lincode'\n",
    "    }, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.2 Change columns order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_6932\\3253590854.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Title          object\n",
       "Description    object\n",
       "Seniority      object\n",
       "City           object\n",
       "State          object\n",
       "                ...  \n",
       "Sisense          bool\n",
       "Dundas_BI        bool\n",
       "Domo             bool\n",
       "Looker           bool\n",
       "Excel            bool\n",
       "Length: 101, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_column__to_index(column_name: str, index: int):\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "\n",
    "\n",
    "def move_columns_to_index(column_names: list[str], index: int):\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "move_columns_to_index([                       \n",
    "                    'Title',\n",
    "                    'Description',\n",
    "                    'Seniority',\n",
    "                    'City',\n",
    "                    'State',\n",
    "                    'Job_age',\n",
    "                    'Easy_apply',\n",
    "                    'Min',\n",
    "                    'Max',\n",
    "                    'Avg',\n",
    "                    'Currency',\n",
    "                    'Employer_provided',\n",
    "                    'Is_hourly',\n",
    "                    'Name',\n",
    "                    'Rating',\n",
    "                    'Employees',\n",
    "                    'Type_of_ownership',\n",
    "                    'Sector',\n",
    "                    'Industry',\n",
    "                    'Company_age',\n",
    "                    'Revenue_USD',\n",
    "                    'Friend_recommend',\n",
    "                    'CEO_approval',\n",
    "                    'Career_opportunities',\n",
    "                    'Comp_&_benefits',\n",
    "                    'Senior_management',\n",
    "                    'Work/Life_balance',\n",
    "                    'Culture_&_values',\n",
    "                    'Pros',\n",
    "                    'Cons',\n",
    "                    'Benefits_rating',\n",
    "                    'Benefits_reviews',\n",
    "                    'BA',\n",
    "                    'MS',\n",
    "                    'Phd',\n",
    "                    'Is_certificate',\n",
    "                    'Git',\n",
    "                    'AWS',\n",
    "                    'Microsoft_Azure',\n",
    "                    'GPC',\n",
    "                    'Alibaba',\n",
    "                    'Oracle',\n",
    "                    'IBM',\n",
    "                    'Tencent',\n",
    "                    'OVHcloud',\n",
    "                    'DigitalOcean',\n",
    "                    'Lincode',\n",
    "                    'PostgreSQL',\n",
    "                    'Microsoft_SQL_Server',\n",
    "                    'IBM_Db2',\n",
    "                    'MySQL',\n",
    "                    'Oracle_PL_SQL',\n",
    "                    'MongoDB',\n",
    "                    'Cassandra',\n",
    "                    'Amazon_DynamoDB',\n",
    "                    'Neo4j',\n",
    "                    'Apache_Solr',\n",
    "                    'Amazon_Redshift',\n",
    "                    'Google_BigQuery',\n",
    "                    'Snowflake',\n",
    "                    'Oracle_Exadata',\n",
    "                    'SAP_HANA',\n",
    "                    'Teradata',\n",
    "                    'Informatica_PowerCenter',\n",
    "                    'Databricks',\n",
    "                    'Presto',\n",
    "                    'Apache_Kafka',\n",
    "                    'Apache_Flink',\n",
    "                    'Dataflow',\n",
    "                    'Apache_Airflow',\n",
    "                    'Luigi',\n",
    "                    'SSIS',\n",
    "                    'Apache_Hadoop',\n",
    "                    'Apache_Hive',\n",
    "                    'Apache_Spark',\n",
    "                    'Linux',\n",
    "                    'Python',\n",
    "                    'R',\n",
    "                    'Scala',\n",
    "                    'SQL',\n",
    "                    'Java',\n",
    "                    'C++',\n",
    "                    'Go',\n",
    "                    'Bash',\n",
    "                    'PowerShell',\n",
    "                    'CLI',\n",
    "                    'Tableau',\n",
    "                    'Power_BI',\n",
    "                    'Google_Analytics',\n",
    "                    'QlikView',\n",
    "                    'Oracle_BI_server',\n",
    "                    'SAS_Analytics',\n",
    "                    'Lumira',\n",
    "                    'Cognos_Impromptu',\n",
    "                    'MicroStrategy',\n",
    "                    'InsightSquared', \n",
    "                    'Sisense', \n",
    "                    'Dundas_BI',\n",
    "                    'Domo', \n",
    "                    'Looker', \n",
    "                    'Excel'\n",
    "                    ],0\n",
    "    )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.3 Add multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = pd.MultiIndex.from_tuples([\n",
    "                                        ('Job_details', 'Title'),\n",
    "                                        ('Job_details', 'Description'),\n",
    "                                        ('Job_details', 'Seniority'),\n",
    "                                        ('Job_details', 'City'),\n",
    "                                        ('Job_details', 'State'),\n",
    "                                        ('Job_details', 'Job_age'),\n",
    "                                        ('Job_details', 'Easy_apply'),\n",
    "                                        ('Salary', 'Min'),\n",
    "                                        ('Salary', 'Max'),\n",
    "                                        ('Salary', 'Avg'),\n",
    "                                        ('Salary', 'Currency'),\n",
    "                                        ('Salary', 'Employer_provided'),\n",
    "                                        ('Salary', 'Is_hourly'),\n",
    "                                        ('Company_info', 'Name'),\n",
    "                                        ('Company_info', 'Rating'),\n",
    "                                        ('Company_info', 'Employees'),\n",
    "                                        ('Company_info', 'Type_of_ownership'),\n",
    "                                        ('Company_info', 'Sector'),\n",
    "                                        ('Company_info', 'Industry'),\n",
    "                                        ('Company_info', 'Company_age'),\n",
    "                                        ('Company_info', 'Revenue_USD'),\n",
    "                                        ('Company_info', 'Friend_recommend'),\n",
    "                                        ('Company_info', 'CEO_approval'),\n",
    "                                        ('Company_info', 'Career_opportunities'),\n",
    "                                        ('Company_info', 'Comp_&_benefits'),\n",
    "                                        ('Company_info', 'Senior_management'),\n",
    "                                        ('Company_info', 'Work/Life_balance'),\n",
    "                                        ('Company_info', 'Culture_&_values'),\n",
    "                                        ('Company_info', 'Pros'),\n",
    "                                        ('Company_info', 'Cons'),\n",
    "                                        ('Company_info', 'Benefits_rating'),\n",
    "                                        ('Company_info', 'Benefits_reviews'),\n",
    "                                        ('Education', 'BA'),\n",
    "                                        ('Education', 'MS'),\n",
    "                                        ('Education', 'Phd'),\n",
    "                                        ('Education', 'Is_certificate'),\n",
    "                                        ('Version_control', 'Git'),\n",
    "                                        ('Cloud_platforms', 'AWS'),\n",
    "                                        ('Cloud_platforms', 'Microsoft_Azure'),\n",
    "                                        ('Cloud_platforms', 'GPC'),\n",
    "                                        ('Cloud_platforms', 'Alibaba'),\n",
    "                                        ('Cloud_platforms', 'Oracle'),\n",
    "                                        ('Cloud_platforms', 'IBM'),\n",
    "                                        ('Cloud_platforms', 'Tencent'),\n",
    "                                        ('Cloud_platforms', 'OVHcloud'),\n",
    "                                        ('Cloud_platforms', 'DigitalOcean'),\n",
    "                                        ('Cloud_platforms', 'Lincode'),\n",
    "                                        ('RDBMS', 'PostgreSQL'),\n",
    "                                        ('RDBMS', 'Microsoft_SQL_Server'),\n",
    "                                        ('RDBMS', 'IBM_Db2'),\n",
    "                                        ('RDBMS', 'MySQL'),\n",
    "                                        ('RDBMS', 'Oracle_PL_SQL'),\n",
    "                                        ('NOSQL', 'MongoDB'),\n",
    "                                        ('NOSQL', 'Cassandra'),\n",
    "                                        ('NOSQL', 'Amazon_DynamoDB'),\n",
    "                                        ('NOSQL', 'Neo4j'),\n",
    "                                        ('Search_&_Analytics', 'Apache_Solr'),\n",
    "                                        ('Search_&_Analytics', 'Amazon_Redshift'),\n",
    "                                        ('Search_&_Analytics', 'Google_BigQuery'),\n",
    "                                        ('Search_&_Analytics', 'Snowflake'),\n",
    "                                        ('Search_&_Analytics', 'Oracle_Exadata'),\n",
    "                                        ('Search_&_Analytics', 'SAP_HANA'),\n",
    "                                        ('Search_&_Analytics', 'Teradata'),\n",
    "                                        ('Data_integration_and_processing', 'Informatica_PowerCenter'),\n",
    "                                        ('Data_integration_and_processing', 'Databricks'),\n",
    "                                        ('Data_integration_and_processing', 'Presto'),\n",
    "                                        ('Stream_processing_tools', 'Apache_Kafka'),\n",
    "                                        ('Stream_processing_tools', 'Apache_Flink'),\n",
    "                                        ('Stream_processing_tools', 'Dataflow'),\n",
    "                                        ('Workflow_orchestration_tools', 'Apache_Airflow'),\n",
    "                                        ('Workflow_orchestration_tools', 'Luigi'),\n",
    "                                        ('Workflow_orchestration_tools', 'SSIS'),\n",
    "                                        ('Big_Data_processing', 'Apache_Hadoop'),\n",
    "                                        ('Big_Data_processing', 'Apache_Hive'),\n",
    "                                        ('Big_Data_processing', 'Apache_Spark'),\n",
    "                                        ('OS', 'Linux'),\n",
    "                                        ('Programming_languages', 'Python'),\n",
    "                                        ('Programming_languages', 'R'),\n",
    "                                        ('Programming_languages', 'Scala'),\n",
    "                                        ('Programming_languages', 'SQL'),\n",
    "                                        ('Programming_languages', 'Java'),\n",
    "                                        ('Programming_languages', 'C++'),\n",
    "                                        ('Programming_languages', 'Go'),\n",
    "                                        ('Programming_languages', 'Bash'),\n",
    "                                        ('Programming_languages', 'PowerShell'),\n",
    "                                        ('Programming_languages', 'CLI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Tableau'),\n",
    "                                        ('Business_Intelligence_Tools', 'Power_BI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Google_Analytics'),\n",
    "                                        ('Business_Intelligence_Tools', 'QlikView'),\n",
    "                                        ('Business_Intelligence_Tools', 'Oracle_BI_server'),\n",
    "                                        ('Business_Intelligence_Tools', 'SAS_Analytics'),\n",
    "                                        ('Business_Intelligence_Tools', 'Lumira'),\n",
    "                                        ('Business_Intelligence_Tools', 'Cognos_Impromptu'),\n",
    "                                        ('Business_Intelligence_Tools', 'MicroStrategy'),\n",
    "                                        ('Business_Intelligence_Tools', 'InsightSquared'), \n",
    "                                        ('Business_Intelligence_Tools', 'Sisense'), \n",
    "                                        ('Business_Intelligence_Tools', 'Dundas_BI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Domo'), \n",
    "                                        ('Business_Intelligence_Tools', 'Looker'), \n",
    "                                        ('Business_Intelligence_Tools', 'Excel'),                   \n",
    "                                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Infoway solutions LLC\n",
       "1                   Optimal Inc.\n",
       "2    Strivernet RPO Services Ltd\n",
       "3     Futuretech Consultants LLC\n",
       "4                    Clairvoyant\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Company_info']['Name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: Excel, dtype: bool"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Business_Intelligence_Tools']['Excel'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. Save CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32.1 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from scraper.config.get import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "local_path = f\"{config['output_path']['main']}/{config['output_path']['clean']}\"\n",
    "file_name = \"Data_Engineer_06-03-2023_23-41.csv\"\n",
    "file_path = Path(f\"{local_path}/{file_name}\")\n",
    "\n",
    "folder = os.path.dirname(file_path)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "\n",
    "df.to_csv(file_path, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32.2 Check save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">Job_details</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Salary</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Business_Intelligence_Tools</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Avg</th>\n",
       "      <th>...</th>\n",
       "      <th>SAS_Analytics</th>\n",
       "      <th>Lumira</th>\n",
       "      <th>Cognos_Impromptu</th>\n",
       "      <th>MicroStrategy</th>\n",
       "      <th>InsightSquared</th>\n",
       "      <th>Sisense</th>\n",
       "      <th>Dundas_BI</th>\n",
       "      <th>Domo</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Excel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Need min 10+ Years exp\\nData Engineer\\nBay Are...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>141440.0</td>\n",
       "      <td>141440.0</td>\n",
       "      <td>141440.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer - Terraform</td>\n",
       "      <td>Position Description:\\nThe GDIA Data Factory P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dearborn</td>\n",
       "      <td>MI</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>76500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>(W2 CANDIDATES ONLY) (SANTA CLARA, CA)\\nPLEASE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>CA</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>187200.0</td>\n",
       "      <td>197600.0</td>\n",
       "      <td>192400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snowflake Data Engineer</td>\n",
       "      <td>My name is Dileep and I am a recruiter at Futu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Newton</td>\n",
       "      <td>MS</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "      <td>83200.0</td>\n",
       "      <td>93600.0</td>\n",
       "      <td>88400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer (MDM)</td>\n",
       "      <td>Required Skills:\\nMust have 5-8+ Years of expe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Remote</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>135200.0</td>\n",
       "      <td>145600.0</td>\n",
       "      <td>140400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Job_details  \\\n",
       "                       Title   \n",
       "0              Data Engineer   \n",
       "1  Data Engineer - Terraform   \n",
       "2              Data Engineer   \n",
       "3    Snowflake Data Engineer   \n",
       "4        Data Engineer (MDM)   \n",
       "\n",
       "                                                                             \\\n",
       "                                         Description Seniority         City   \n",
       "0  Need min 10+ Years exp\\nData Engineer\\nBay Are...       NaN  Santa Clara   \n",
       "1  Position Description:\\nThe GDIA Data Factory P...       NaN     Dearborn   \n",
       "2  (W2 CANDIDATES ONLY) (SANTA CLARA, CA)\\nPLEASE...       NaN  Santa Clara   \n",
       "3  My name is Dileep and I am a recruiter at Futu...       NaN       Newton   \n",
       "4  Required Skills:\\nMust have 5-8+ Years of expe...       NaN       Remote   \n",
       "\n",
       "                                Salary                      ...  \\\n",
       "    State Job_age Easy_apply       Min       Max       Avg  ...   \n",
       "0      CA       3       True  141440.0  141440.0  141440.0  ...   \n",
       "1      MI      12       True   63000.0   90000.0   76500.0  ...   \n",
       "2      CA       5       True  187200.0  197600.0  192400.0  ...   \n",
       "3      MS      31       True   83200.0   93600.0   88400.0  ...   \n",
       "4  Remote      12       True  135200.0  145600.0  140400.0  ...   \n",
       "\n",
       "  Business_Intelligence_Tools                                        \\\n",
       "                SAS_Analytics Lumira Cognos_Impromptu MicroStrategy   \n",
       "0                       False  False            False         False   \n",
       "1                       False  False            False         False   \n",
       "2                       False  False            False         False   \n",
       "3                       False  False            False         False   \n",
       "4                       False  False            False         False   \n",
       "\n",
       "                                                         \n",
       "  InsightSquared Sisense Dundas_BI   Domo Looker  Excel  \n",
       "0          False   False     False  False  False  False  \n",
       "1          False   False     False  False  False  False  \n",
       "2          False   False     False  False  False  False  \n",
       "3          False   False     False  False  False  False  \n",
       "4          False   False     False  False  False  False  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check = pd.read_csv(file_path, index_col=0, header=[0, 1])\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.shape == df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
