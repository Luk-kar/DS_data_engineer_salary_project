{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Callable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing  a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs_from_CSVs_in_folder(directory: str) -> dict[str, pd.DataFrame]:\n",
    "\n",
    "    dfs = {}\n",
    "\n",
    "    # https://regex101.com/r/QYuVDf/1\n",
    "    pattern = r\"Data_Engineer_([a-zA-Z_]+)_\\d{2}-\\d{2}-\\d{4}_\\d{2}-\\d{2}.csv\"\n",
    "\n",
    "    for __, _, files in os.walk(directory):\n",
    "\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                match = re.search(pattern, file)\n",
    "                if match:\n",
    "                    country = match.group(1)\n",
    "                    file_path = os.path.join(directory, file)\n",
    "                    dfs[country] = pd.read_csv(file_path)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australia':                                  Company_name  Rating         Location  \\\n",
       " 0                                 ONNEC Group     3.2           Sydney   \n",
       " 1                              Octopus Energy     4.2        Melbourne   \n",
       " 2                                 TPG Telecom     3.7     North Sydney   \n",
       " 3                                          EY     3.9           Sydney   \n",
       " 4                                    7-Eleven     3.3  New South Wales   \n",
       " ..                                        ...     ...              ...   \n",
       " 739                           Publicis Groupe     3.9           Sydney   \n",
       " 740                                   Equinix     4.2        Unanderra   \n",
       " 741                                    TikTok     3.6           Sydney   \n",
       " 742  Australian Government Services Australia     3.6         Canberra   \n",
       " 743                                 Airwallex     3.4  New South Wales   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0      Data Centre Cabling & Hardware Support Engineer   \n",
       " 1                                        Data Engineer   \n",
       " 2                                        Data Engineer   \n",
       " 3    Expressions of Interest - Data Engineer, Data ...   \n",
       " 4                                        Data Engineer   \n",
       " ..                                                 ...   \n",
       " 739                                Data Engineer Roles   \n",
       " 740        Data Center Customer Operations Engineer IV   \n",
       " 741  Backend Software Engineer (TikTok Live) - 2023...   \n",
       " 742                 Power BI Data Engineer and Manager   \n",
       " 743                      2023 Software Engineer Intern   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    ONNEC builds, supports and optimises the IT in...     15d        True   \n",
       " 1    Help us make a big green dent in the universe....      7d        True   \n",
       " 2    We’ve only just begun, but what a beginning. I...      2d       False   \n",
       " 3    At EY, you’ll have the chance to build a caree...     14d       False   \n",
       " 4    Why 7-Eleven?\\r\\nWe're an agile, human centred...      2d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 739  Data Engineer\\r\\nTrading under Publicis Commun...     29d        True   \n",
       " 740  Data Center Customer Operations Engineer IV\\r\\...     28d       False   \n",
       " 741  Responsibilities\\r\\nTikTok is the leading dest...    30d+       False   \n",
       " 742  Executive Level 1\\r\\nACT - Various,Brisbane, Q...     24h       False   \n",
       " 743  Airwallex is the leading financial technology ...    30d+       False   \n",
       " \n",
       "                                Salary     Employees  Type_of_ownership  ...  \\\n",
       " 0       A$80K - A$85K (Employer est.)   501 to 1000  Company - Private  ...   \n",
       " 1                                 NaN     51 to 200  Company - Private  ...   \n",
       " 2    A$112K - A$130K (Glassdoor est.)   501 to 1000   Company - Public  ...   \n",
       " 3               A$90K (Employer est.)           NaN                NaN  ...   \n",
       " 4                                 NaN        10000+  Company - Private  ...   \n",
       " ..                                ...           ...                ...  ...   \n",
       " 739   A$96K - A$120K (Glassdoor est.)        10000+   Company - Public  ...   \n",
       " 740                               NaN        10000+   Company - Public  ...   \n",
       " 741   A$88K - A$118K (Glassdoor est.)  1001 to 5000  Company - Private  ...   \n",
       " 742   A$110K - A$122K (Employer est.)        10000+         Government  ...   \n",
       " 743                               NaN  1001 to 5000  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   2.4             3.2              2.8   \n",
       " 1            NaN                   4.1             3.9              4.5   \n",
       " 2           0.84                   3.3             3.4              3.7   \n",
       " 3            NaN                   NaN             NaN              NaN   \n",
       " 4           0.62                   3.1             3.0              3.1   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 739         0.93                   3.8             3.4              3.8   \n",
       " 740         0.91                   3.9             4.0              4.3   \n",
       " 741         0.71                   3.5             3.6              3.5   \n",
       " 742          NaN                   3.6             3.4              3.4   \n",
       " 743         0.56                   3.6             3.6              3.1   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.3                3.3   \n",
       " 1                  4.2                4.0   \n",
       " 2                  3.4                3.8   \n",
       " 3                  NaN                NaN   \n",
       " 4                  2.9                2.9   \n",
       " ..                 ...                ...   \n",
       " 739                3.6                3.6   \n",
       " 740                3.9                4.0   \n",
       " 741                3.1                3.1   \n",
       " 742                3.2                3.7   \n",
       " 743                3.1                3.3   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Some of the staff were nice that were NOT o...   \n",
       " 1    ['\"Some good people to learn from.\" (in 18 rev...   \n",
       " 2    ['\"people are friendly and supportive\" (in 10 ...   \n",
       " 3                                                  NaN   \n",
       " 4    ['\". good salary\" (in 15 reviews)', '\"Great wo...   \n",
       " ..                                                 ...   \n",
       " 739  ['\"Good management\" (in 10 reviews)', '\"Employ...   \n",
       " 740  ['\"Great Office Environment, Work life balance...   \n",
       " 741  ['\"salary is good not very tough\" (in 103 revi...   \n",
       " 742  ['\"good work culture.\" (in 7 reviews)', '\"Coll...   \n",
       " 743  ['\"Engineer culture and advocate innovation\" (...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"managers no care for staff, staff not happy...              NaN   \n",
       " 1    ['\"This doesn\\'t help with learning team conve...              NaN   \n",
       " 2    ['\"Culture needs to be improved.\" (in 8 review...              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"Management isn\\'t that good.\" (in 8 reviews...              2.6   \n",
       " ..                                                 ...              ...   \n",
       " 739  ['\"Bad Management\" (in 10 reviews)', '\"Low Sal...              4.0   \n",
       " 740  ['\"No work life balance .\" (in 29 reviews)', '...              4.4   \n",
       " 741  ['\"Low salary not a competitive one either\" (i...              4.2   \n",
       " 742  ['\"Work culture is not great\" (in 7 reviews)',...              NaN   \n",
       " 743  ['\"Toxic culture that\\'s largely dominated by ...              4.0   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4    ['Job Training (82 comments)\\n\"It is enough an...  \n",
       " ..                                                 ...  \n",
       " 739  ['Vacation & Paid Time Off (34 comments)\\n\"it\\...  \n",
       " 740  ['Health Insurance (53 comments)\\n\"The health ...  \n",
       " 741  ['Health Insurance (26 comments)\\n\"Great cover...  \n",
       " 742                                                NaN  \n",
       " 743                                                NaN  \n",
       " \n",
       " [744 rows x 25 columns],\n",
       " 'Austria':                  Company_name  Rating Location  \\\n",
       " 0    Infineon Technologies AG     4.2  Villach   \n",
       " 1               B-612 UK Ltd.     NaN   Vienna   \n",
       " 2    Infineon Technologies AG     4.2  Villach   \n",
       " 3    Infineon Technologies AG     4.2  Villach   \n",
       " 4    Infineon Technologies AG     4.2  Villach   \n",
       " ..                        ...     ...      ...   \n",
       " 835             Tractive GmbH     4.9     Linz   \n",
       " 836             GlobalBlue_IT     NaN   Vienna   \n",
       " 837     Capgemini Engineering     3.7     Graz   \n",
       " 838                       AVL     3.9     Graz   \n",
       " 839         EUROPEAN DYNAMICS     3.4   Vienna   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0    Component Verification and Product Characteriz...   \n",
       " 1       Freelance Hardware/ Data Centre Field Engineer   \n",
       " 2    Senior Staff Engineer Digital Verification (f/...   \n",
       " 3    Senior Staff Engineer Product Development for ...   \n",
       " 4              Product Application Engineer (f/m/div)*   \n",
       " ..                                                 ...   \n",
       " 835                   DevOps/Site Reliability Engineer   \n",
       " 836                Senior Automation QA Engineer - REX   \n",
       " 837                     Service Standard Pumps (m/w/d)   \n",
       " 838  Pflichtpraktikant:in FH im Bereich Cost Engine...   \n",
       " 839                      Senior .NET Software Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    You are looking for a new challenge to bring i...    30d+       False   \n",
       " 1    REQUIREMENTS\\r\\nWe will consider self-employed...     17d        True   \n",
       " 2    You are looking for a new challenge to bring i...    30d+       False   \n",
       " 3    Do you want to get to know the development of ...     27d       False   \n",
       " 4    You enjoy working in an international team, in...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 835  About us\\r\\n\\r\\nWe're the world's most-trusted...     17d       False   \n",
       " 836  Nature and aim of the position\\r\\nWe are looki...    30d+       False   \n",
       " 837  Capgemini Engineering combines, under one bran...     26d       False   \n",
       " 838  Standort: Graz, AT\\r\\nFirma: AVL List GmbH\\r\\n...    30d+       False   \n",
       " 839  We currently have vacancy for a Senior .Net De...    30d+        True   \n",
       " \n",
       "                                       Salary    Employees  Type_of_ownership  \\\n",
       " 0                                        NaN       10000+   Company - Public   \n",
       " 1    €41.65 - €50.90 Per Hour(Employer est.)          NaN                NaN   \n",
       " 2                                        NaN       10000+   Company - Public   \n",
       " 3                                        NaN       10000+   Company - Public   \n",
       " 4                                        NaN       10000+   Company - Public   \n",
       " ..                                       ...          ...                ...   \n",
       " 835                                      NaN    51 to 200  Company - Private   \n",
       " 836                                      NaN          NaN                NaN   \n",
       " 837                                      NaN       10000+   Company - Public   \n",
       " 838                                      NaN       10000+  Company - Private   \n",
       " 839                                      NaN  501 to 1000  Company - Private   \n",
       " \n",
       "      ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0    ...         0.95                   3.9             3.8              4.1   \n",
       " 1    ...          NaN                   NaN             NaN              NaN   \n",
       " 2    ...         0.95                   3.9             3.8              4.1   \n",
       " 3    ...         0.95                   3.9             3.8              4.1   \n",
       " 4    ...         0.95                   3.9             3.8              4.1   \n",
       " ..   ...          ...                   ...             ...              ...   \n",
       " 835  ...         1.00                   4.7             4.5              4.9   \n",
       " 836  ...          NaN                   NaN             NaN              NaN   \n",
       " 837  ...         0.78                   3.5             3.0              3.4   \n",
       " 838  ...         0.93                   3.6             3.3              3.8   \n",
       " 839  ...          NaN                   3.5             3.4              3.2   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.7                4.1   \n",
       " 1                  NaN                NaN   \n",
       " 2                  3.7                4.1   \n",
       " 3                  3.7                4.1   \n",
       " 4                  3.7                4.1   \n",
       " ..                 ...                ...   \n",
       " 835                4.9                5.0   \n",
       " 836                NaN                NaN   \n",
       " 837                3.5                3.4   \n",
       " 838                3.3                3.9   \n",
       " 839                3.2                2.9   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Good work life balance and learning\" (in 15...   \n",
       " 1                                                  NaN   \n",
       " 2    ['\"Good work life balance and learning\" (in 15...   \n",
       " 3    ['\"Good work life balance and learning\" (in 15...   \n",
       " 4    ['\"Good work life balance and learning\" (in 15...   \n",
       " ..                                                 ...   \n",
       " 835  ['\"Friendly atmosphere in the team and the com...   \n",
       " 836                                                NaN   \n",
       " 837  ['\"Good salary and technically sound employees...   \n",
       " 838  ['\"Salary is good.\" (in 108 reviews)', '\"The e...   \n",
       " 839  ['\"Good technologies that company uses\" (in 11...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       " 1                                                  NaN              NaN   \n",
       " 2    ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       " 3    ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       " 4    ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       " ..                                                 ...              ...   \n",
       " 835  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 836                                                NaN              NaN   \n",
       " 837  ['\"Salary low and all is good\" (in 887 reviews...              3.3   \n",
       " 838  ['\"Low salary compared to the competitors\" (in...              5.0   \n",
       " 839  ['\"Old technologies used in projects\" (in 11 r...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 1                                                  NaN  \n",
       " 2    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 3    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 4    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " ..                                                 ...  \n",
       " 835                                                NaN  \n",
       " 836                                                NaN  \n",
       " 837  ['Health Insurance (16 comments)\\n\"All around ...  \n",
       " 838                                                NaN  \n",
       " 839                                                NaN  \n",
       " \n",
       " [840 rows x 25 columns],\n",
       " 'Belgium':                    Company_name  Rating             Location  \\\n",
       " 0                         Torfs     3.8         Sint-Niklaas   \n",
       " 1      Stichting Kankerregister     NaN  Sint-Joost-Ten-Node   \n",
       " 2                          VITO     4.1              Belgium   \n",
       " 3    Traxial (a Magnax company)     NaN             Kortrijk   \n",
       " 4                       Telenet     3.9             Mechelen   \n",
       " ..                          ...     ...                  ...   \n",
       " 895                      Ordina     3.9             Mechelen   \n",
       " 896                      Ordina     3.9             Mechelen   \n",
       " 897                       Smals     3.9             Brussels   \n",
       " 898                      Ordina     3.9             Mechelen   \n",
       " 899             BENOIT SECURITY     4.0         Knokke-Heist   \n",
       " \n",
       "                                             Job_title  \\\n",
       " 0    Data Engineer (met affiniteit voor Data Science)   \n",
       " 1                                        data manager   \n",
       " 2                        Python Engineer on FAIR Data   \n",
       " 3                Electromagnetic Simulations Engineer   \n",
       " 4                            Senior Big Data Engineer   \n",
       " ..                                                ...   \n",
       " 895                            DevOps Engineer (Data)   \n",
       " 896                               Azure Data Engineer   \n",
       " 897                           Data Governance Manager   \n",
       " 898                         Data Migration Consultant   \n",
       " 899          Service technieker regio Knokke/Maldegem   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Wie zijn wij?\\r\\nVoor het continu verbeteren v...    30d+       False   \n",
       " 1    De Stichting Kankerregister, gelegen in het ha...    30d+        True   \n",
       " 2    As an internationally recognized research cent...      7d       False   \n",
       " 3    Traxial is looking for an experienced Electrom...     26d        True   \n",
       " 4    Telenet is always looking to improve their bus...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Your Impact\\r\\nWithin our Data Driven Unit, we...     18d       False   \n",
       " 896  Your Impact\\r\\nWithin our Data Driven Unit, we...     18d       False   \n",
       " 897  Chez Smals, plus de 2000 professionnels façonn...    30d+       False   \n",
       " 898  Your Impact\\r\\nWithin our Data Driven Unit, we...     18d       False   \n",
       " 899  BENOIT SECURITY is een familiebedrijf met meer...    30d+        True   \n",
       " \n",
       "     Salary     Employees       Type_of_ownership  ... CEO_approval  \\\n",
       " 0      NaN           NaN                     NaN  ...         0.87   \n",
       " 1      NaN           NaN                     NaN  ...          NaN   \n",
       " 2      NaN   501 to 1000       Company - Private  ...         0.77   \n",
       " 3      NaN           NaN                     NaN  ...          NaN   \n",
       " 4      NaN  1001 to 5000        Company - Public  ...         0.83   \n",
       " ..     ...           ...                     ...  ...          ...   \n",
       " 895    NaN  1001 to 5000        Company - Public  ...         0.86   \n",
       " 896    NaN  1001 to 5000        Company - Public  ...         0.86   \n",
       " 897    NaN  1001 to 5000  Nonprofit Organization  ...         0.53   \n",
       " 898    NaN  1001 to 5000        Company - Public  ...         0.86   \n",
       " 899    NaN       1 to 50       Company - Private  ...          NaN   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     3.5             4.0              4.1                3.6   \n",
       " 1                     NaN             NaN              NaN                NaN   \n",
       " 2                     3.5             4.2              3.9                3.3   \n",
       " 3                     NaN             NaN              NaN                NaN   \n",
       " 4                     3.1             3.8              3.9                3.3   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 895                   3.7             3.4              3.9                3.5   \n",
       " 896                   3.7             3.4              3.9                3.5   \n",
       " 897                   3.1             3.6              3.6                3.3   \n",
       " 898                   3.7             3.4              3.9                3.5   \n",
       " 899                   2.0             2.0              2.0                2.0   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  3.1  ['\"Veel appreciatie van je manager\" (in 2 revi...   \n",
       " 1                  NaN                                                NaN   \n",
       " 2                  4.4              ['\"Friendly people.\" (in 1 reviews)']   \n",
       " 3                  NaN                                                NaN   \n",
       " 4                  3.9  ['\"Ultra nice perks from salary, 13th, 14th mo...   \n",
       " ..                 ...                                                ...   \n",
       " 895                4.0  ['\"Great colleagues, good atmosphere, fair pay...   \n",
       " 896                4.0  ['\"Great colleagues, good atmosphere, fair pay...   \n",
       " 897                4.6  ['No Pros have been reported by the Glassdoor ...   \n",
       " 898                4.0  ['\"Very cool company, everyone is friendly and...   \n",
       " 899                2.0                                                NaN   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 1                                                  NaN              NaN   \n",
       " 2    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"bad culture not appropriate\" (in 1 reviews)...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Low chances of promotions and salaries are ...              NaN   \n",
       " 896  ['\"Low chances of promotions and salaries are ...              NaN   \n",
       " 897  ['\"Salaire pour un senior Aucune promotion ou ...              NaN   \n",
       " 898  ['\"Salaries can be a bit low\" (in 4 reviews)',...              NaN   \n",
       " 899                                                NaN              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 895               NaN  \n",
       " 896               NaN  \n",
       " 897               NaN  \n",
       " 898               NaN  \n",
       " 899               NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Canada':               Company_name  Rating          Location  \\\n",
       " 0                  Trolley     5.0           Toronto   \n",
       " 1                   ALLUDO     3.7           Toronto   \n",
       " 2                 Best Buy     3.8         Vancouver   \n",
       " 3        War Room Holdings     4.4         Vancouver   \n",
       " 4        Algoma Steel Inc.     4.0  Sault Ste. Marie   \n",
       " 5                     SS&C     3.2           Toronto   \n",
       " 6   Fidelity International     4.3            Canada   \n",
       " 7                 Deloitte     4.0           Toronto   \n",
       " 8          York University     4.1           Toronto   \n",
       " 9               Bombardier     3.8            Dorval   \n",
       " 10              Scotiabank     3.9           Toronto   \n",
       " \n",
       "                                             Job_title  \\\n",
       " 0                                  Analytics Engineer   \n",
       " 1                                         BI Engineer   \n",
       " 2                   Analytics Implementation Engineer   \n",
       " 3                   Analytics Implementation Engineer   \n",
       " 4              Process Analyst Engineer - Steel Plant   \n",
       " 5             Financial Engineer/ACM Business Analyst   \n",
       " 6                   Lead Analytics Solutions Engineer   \n",
       " 7   Analyst/Senior Associate, AML - Financial Engi...   \n",
       " 8                 Business Analyst & Process Engineer   \n",
       " 9        Business Analyst Engineering Budget Planning   \n",
       " 10  Senior Manager, Project Delivery Business Anal...   \n",
       " \n",
       "                                           Description Job_age  Easy_apply  \\\n",
       " 0   Trolley is the payouts platform for the intern...    30d+        True   \n",
       " 1   BI Engineer\\r\\nPush the boundaries of tech. In...    30d+        True   \n",
       " 2   Analytics Implementation Engineer\\r\\nDo you ha...    30d+       False   \n",
       " 3   Vancouver, BC\\r\\nFull Time, Hybrid\\r\\n\\r\\nWho ...    30d+       False   \n",
       " 4   Process Analyst Engineer – Steel Plant\\r\\nAlgo...      5d        True   \n",
       " 5   SS&C is a global provider of investment and fi...     10d       False   \n",
       " 6   About the opportunity\\r\\nDepartment Descriptio...    30d+       False   \n",
       " 7   Job Type: Permanent\\r\\nReference code: 122995\\...    30d+       False   \n",
       " 8   Purpose:\\r\\nAccountable to the Manager, Operat...    30d+       False   \n",
       " 9   Business Analyst Engineering Budget Planning -...    30d+       False   \n",
       " 10  Requisition ID: 176293\\r\\n\\r\\nJoin a purpose d...     14d       False   \n",
       " \n",
       "                                Salary      Employees     Type_of_ownership  \\\n",
       " 0    CA$65K - CA$75K (Glassdoor est.)      51 to 200     Company - Private   \n",
       " 1                                 NaN    501 to 1000     Company - Private   \n",
       " 2    CA$88K - CA$94K (Glassdoor est.)         10000+      Company - Public   \n",
       " 3    CA$57K - CA$71K (Glassdoor est.)        1 to 50     Company - Private   \n",
       " 4    CA$71K - CA$92K (Glassdoor est.)   1001 to 5000     Company - Private   \n",
       " 5   CA$97K - CA$102K (Glassdoor est.)         10000+      Company - Public   \n",
       " 6                                 NaN  5001 to 10000     Company - Private   \n",
       " 7                                 NaN         10000+     Company - Private   \n",
       " 8    CA$94K - CA$103K (Employer est.)  5001 to 10000  College / University   \n",
       " 9                                 NaN         10000+      Company - Public   \n",
       " 10                                NaN         10000+      Company - Public   \n",
       " \n",
       "     ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0   ...         1.00                   4.8             4.5              5.0   \n",
       " 1   ...         0.79                   3.4             3.4              3.7   \n",
       " 2   ...         0.45                   3.3             3.4              3.6   \n",
       " 3   ...         0.83                   4.3             4.3              4.3   \n",
       " 4   ...         0.69                   3.7             4.2              3.3   \n",
       " 5   ...         0.50                   2.8             2.8              2.7   \n",
       " 6   ...         0.96                   4.0             4.1              4.4   \n",
       " 7   ...         0.94                   4.3             3.8              4.0   \n",
       " 8   ...         0.77                   3.7             3.8              3.8   \n",
       " 9   ...         0.88                   3.5             3.6              3.4   \n",
       " 10  ...         0.84                   3.8             3.7              3.9   \n",
       " \n",
       "     Senior_management  Work/Life_balance  \\\n",
       " 0                 5.0                5.0   \n",
       " 1                 3.5                3.7   \n",
       " 2                 3.2                3.4   \n",
       " 3                 4.3                4.2   \n",
       " 4                 3.2                3.7   \n",
       " 5                 2.6                3.2   \n",
       " 6                 3.9                4.3   \n",
       " 7                 3.8                3.3   \n",
       " 8                 3.6                3.9   \n",
       " 9                 3.1                3.6   \n",
       " 10                3.5                3.7   \n",
       " \n",
       "                                                  Pros  \\\n",
       " 0   ['\"They have invested in the right tools, trai...   \n",
       " 1   ['\"Good Team to work with\" (in 1 reviews)', '\"...   \n",
       " 2   ['\"Good pay for new employees\" (in 136 reviews...   \n",
       " 3   ['\"The people here are amazing\" (in 14 reviews...   \n",
       " 4   ['\"Great learning opportunity to understand in...   \n",
       " 5   ['\"Flexible workplace, remote working, good pa...   \n",
       " 6   ['\"Management good\" (in 15 reviews)', '\"Pay is...   \n",
       " 7   ['\"A good work life balance when you compare w...   \n",
       " 8   ['\"Benefits are great!\" (in 4 reviews)', '\"Gre...   \n",
       " 9   ['\"Salary is good\" (in 21 reviews)', '\"Benefit...   \n",
       " 10  ['\"Good for work life balance\" (in 104 reviews...   \n",
       " \n",
       "                                                  Cons  Benefits_rating  \\\n",
       " 0   ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 1   ['\"Pay is very low compared to similiar sized ...              3.0   \n",
       " 2   ['\"low pay not worth the stress\" (in 136 revie...              3.7   \n",
       " 3   ['\"The team is open for communication if there...              NaN   \n",
       " 4   ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 5   ['\"Long hours, Saturday working, low salary\" (...              3.7   \n",
       " 6   ['\"3. Very bias and bureaucratic management\" (...              4.4   \n",
       " 7   ['\"No work/life balance and not easy to grow i...              4.2   \n",
       " 8   ['\"boring, low compensation, and uncertainty\" ...              4.0   \n",
       " 9   ['\"As mentioned in the ‘Pros’, high salaries e...              3.6   \n",
       " 10  ['\"no work life balance； heavy load\" (in 104 r...              3.8   \n",
       " \n",
       "                                      Benefits_reviews  \n",
       " 0                                                 NaN  \n",
       " 1   ['Work From Home (4 comments)\\n\"Overall positi...  \n",
       " 2   ['Employee Discount (1348 comments)\\n\"Cheap ac...  \n",
       " 3                                                 NaN  \n",
       " 4                                                 NaN  \n",
       " 5   ['401K Plan (69 comments)\\n\"It\\'s Match and im...  \n",
       " 6                                                 NaN  \n",
       " 7   ['Health Insurance (765 comments)\\n\"A lot of o...  \n",
       " 8                                                 NaN  \n",
       " 9   ['Health Insurance (25 comments)\\n\"excellent b...  \n",
       " 10  ['Health Insurance (5 comments)\\n\"Mid medical ...  \n",
       " \n",
       " [11 rows x 25 columns],\n",
       " 'Czech_Republic':                           Company_name  Rating        Location  \\\n",
       " 0                             EUMETSAT     3.4          Prague   \n",
       " 1                 Hill's Pet Nutrition     4.1       Hustopeče   \n",
       " 2                                Mondi     4.0           Štětí   \n",
       " 3                                  ABB     4.0          Mošnov   \n",
       " 4    ScanmarQED Marketing.Illuminated.     NaN  Czech Republic   \n",
       " ..                                 ...     ...             ...   \n",
       " 895                       Azul Systems     4.6          Prague   \n",
       " 896                             Merkle     3.7          Prague   \n",
       " 897                           Brainlab     3.6          Prague   \n",
       " 898                   Canonical - Jobs     3.1          Prague   \n",
       " 899                           Roivenue     4.1          Prague   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0           Cloud Services Engineer (Based in Germany)   \n",
       " 1                            Technical Systems Manager   \n",
       " 2                               PLC Engineer ( F/M/X )   \n",
       " 3        Application Engineer - suitable for graduates   \n",
       " 4                                        Data Engineer   \n",
       " ..                                                 ...   \n",
       " 895                  Intern - Junior Software Engineer   \n",
       " 896                                   Data QA Engineer   \n",
       " 897    Support Specialist / Engineer - Imaging Systems   \n",
       " 898  Software Engineer - Micro/Private/Bare-Metal C...   \n",
       " 899                                      Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Cloud Services Engineer\\r\\nEUMETSAT is Europe’...     20d       False   \n",
       " 1    Relocation Assistance Offered Within Region\\r\\...     19d       False   \n",
       " 2    Mondi Štětí a.s.\\r\\nNejlepší zaměstnavatel v Č...    30d+       False   \n",
       " 3    Application Engineer - suitable for graduates\\...    30d+       False   \n",
       " 4    We’re looking for an energetic, skilled data e...      8d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Azul Careers\\r\\nIntern - Junior Software Engin...     13d       False   \n",
       " 896  Company Description:\\r\\nWe Dream. We Do. We De...     11d       False   \n",
       " 897  Company Description\\r\\n\\r\\nFounded in Munich, ...    30d+        True   \n",
       " 898  This is an exciting opportunity for a software...     22d       False   \n",
       " 899  We’re looking for an energetic, skilled data e...    30d+       False   \n",
       " \n",
       "                      Salary      Employees  Type_of_ownership  ...  \\\n",
       " 0    CZK 8K (Employer est.)     201 to 500      Self-employed  ...   \n",
       " 1                       NaN   1001 to 5000   Company - Public  ...   \n",
       " 2                       NaN         10000+   Company - Public  ...   \n",
       " 3                       NaN         10000+   Company - Public  ...   \n",
       " 4                       NaN            NaN                NaN  ...   \n",
       " ..                      ...            ...                ...  ...   \n",
       " 895                     NaN     201 to 500  Company - Private  ...   \n",
       " 896                     NaN  5001 to 10000  Company - Private  ...   \n",
       " 897                     NaN   1001 to 5000  Company - Private  ...   \n",
       " 898                     NaN    501 to 1000  Company - Private  ...   \n",
       " 899                     NaN        1 to 50  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.49                   2.4             3.7              2.9   \n",
       " 1           0.76                   3.8             4.4              4.0   \n",
       " 2           0.87                   3.6             3.4              3.8   \n",
       " 3           0.89                   3.7             3.6              4.0   \n",
       " 4            NaN                   NaN             NaN              NaN   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895         1.00                   4.3             4.5              4.5   \n",
       " 896         0.76                   3.6             3.3              3.8   \n",
       " 897         0.62                   3.8             2.8              4.1   \n",
       " 898         0.53                   3.3             3.4              3.1   \n",
       " 899         0.65                   4.5             3.7              4.5   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  2.6                3.1   \n",
       " 1                  3.6                3.6   \n",
       " 2                  3.8                3.5   \n",
       " 3                  3.5                3.8   \n",
       " 4                  NaN                NaN   \n",
       " ..                 ...                ...   \n",
       " 895                4.5                4.3   \n",
       " 896                3.4                3.8   \n",
       " 897                4.1                3.7   \n",
       " 898                2.7                3.5   \n",
       " 899                4.2                3.8   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"this company can offer good salary, on the ...   \n",
       " 1    ['\"Great benefits with the Colgate\" (in 36 rev...   \n",
       " 2    ['\"Pay and benefits are good\" (in 12 reviews)'...   \n",
       " 3    ['\"processes are well defined and work life ba...   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 895  ['\"Great team\" (in 6 reviews)', '\"Great leader...   \n",
       " 896  ['\"They haven\\'t demanded a return to office a...   \n",
       " 897  ['\"Great technology portfolio\" (in 34 reviews)...   \n",
       " 898  ['\"Remote work Open source work Great people\" ...   \n",
       " 899  ['\"A few good people at best\" (in 10 reviews)'...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Some heads are creating a negative culture ...              NaN   \n",
       " 1    ['\"No work/ life balance\" (in 17 reviews)', '\"...              4.5   \n",
       " 2    ['\"Compensation is pretty good, as is the 401K...              3.4   \n",
       " 3    ['\"No work life balance and no decent hike\" (i...              3.7   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['No Cons have been reported by the Glassdoor ...              3.0   \n",
       " 896  ['\"Sometime no work life balance\" (in 194 revi...              3.9   \n",
       " 897  ['\"Low salary Not well\" (in 32 reviews)', '\"Th...              4.4   \n",
       " 898  ['\"men surrounding CEO and CTO\" (in 41 reviews...              3.3   \n",
       " 899  ['\"Still, me and at least 4 other employees wh...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1    ['Maternity & Paternity Leave (6 comments)\\n\"T...  \n",
       " 2    ['Vacation & Paid Time Off (3 comments)\\n\"it i...  \n",
       " 3    ['Health Insurance (87 comments)\\n\"Lack of cov...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896  ['Health Insurance (38 comments)\\n\"Good heath ...  \n",
       " 897  ['Health Insurance (4 comments)\\n\"Pretty good,...  \n",
       " 898  ['Work From Home (10 comments)\\n\"Fully remote ...  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Denmark':                   Company_name  Rating    Location  \\\n",
       " 0                      Custimy     NaN     Denmark   \n",
       " 1                       Quanta     4.3  Copenhagen   \n",
       " 2                      ViaBill     3.8  Copenhagen   \n",
       " 3                         LEGO     4.5     Denmark   \n",
       " 4                  Weatherford     3.7     Esbjerg   \n",
       " ..                         ...     ...         ...   \n",
       " 565                       COWI     4.2      Odense   \n",
       " 566  Dansk Ingeniørservice A/S     3.0      Aarhus   \n",
       " 567                       LEGO     4.5     Denmark   \n",
       " 568                  Trackunit     3.8     Denmark   \n",
       " 569                      Teton     NaN  Copenhagen   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                        Data Engineer   \n",
       " 1                             Lead Instrument Engineer   \n",
       " 2    Data Scientist / Machine Learning Engineer, Fi...   \n",
       " 3                                        Data engineer   \n",
       " 4            Field Engineer - NextGen Graduate Program   \n",
       " ..                                                 ...   \n",
       " 565  Student Intern - Road Engineer for COWI in Ode...   \n",
       " 566             Interns for Digital Solutions - Aarhus   \n",
       " 567                             Data Platform Engineer   \n",
       " 568  Embedded Software Engineer - IoT Solutions - D...   \n",
       " 569                           Computer Vision Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Are you a Data Engineer who is ready to challe...     24h       False   \n",
       " 1    Lead Instrument Engineer – Renewable Energy – ...     19d       False   \n",
       " 2    ViaBill operates at the intersection of the eC...     25d        True   \n",
       " 3    Job Description\\r\\n#LI-EB1\\r\\nAre you excited ...     18d       False   \n",
       " 4    Overview:\\r\\nWeatherford is a leading global e...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 565  Application deadline: 09 May 2023 - Denmark - ...     15d       False   \n",
       " 566  Are you a passionate about technology and moti...    30d+       False   \n",
       " 567  Job Description\\r\\nAre you ready to take your ...     18d       False   \n",
       " 568  Through the past two decades, Trackunit has be...    30d+       False   \n",
       " 569  Teton is building technology that lets us all ...      5d       False   \n",
       " \n",
       "     Salary      Employees  Type_of_ownership  ... CEO_approval  \\\n",
       " 0      NaN        1 to 50  Company - Private  ...          NaN   \n",
       " 1      NaN      51 to 200  Company - Private  ...         0.98   \n",
       " 2      NaN      51 to 200                NaN  ...          NaN   \n",
       " 3      NaN         10000+  Company - Private  ...         0.95   \n",
       " 4      NaN         10000+                NaN  ...         0.79   \n",
       " ..     ...            ...                ...  ...          ...   \n",
       " 565    NaN  5001 to 10000  Company - Private  ...         0.91   \n",
       " 566    NaN    501 to 1000  Company - Private  ...          NaN   \n",
       " 567    NaN         10000+  Company - Private  ...         0.95   \n",
       " 568    NaN     201 to 500  Company - Private  ...          NaN   \n",
       " 569    NaN        1 to 50  Company - Private  ...          NaN   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     NaN             NaN              NaN                NaN   \n",
       " 1                     4.5             4.7              4.6                4.5   \n",
       " 2                     3.5             3.1              3.2                2.9   \n",
       " 3                     3.6             4.2              4.6                4.1   \n",
       " 4                     3.3             3.4              3.4                3.2   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 565                   3.9             3.5              4.2                3.7   \n",
       " 566                   NaN             NaN              NaN                NaN   \n",
       " 567                   3.6             4.2              4.6                4.1   \n",
       " 568                   3.6             3.5              3.8                3.6   \n",
       " 569                   NaN             NaN              NaN                NaN   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  NaN                                                NaN   \n",
       " 1                  4.3  ['\"Clearly defined career path, training and t...   \n",
       " 2                  3.5  ['\"Great team and a friendly colleagues, alway...   \n",
       " 3                  4.3  ['\"Good benefits and company values.\" (in 153 ...   \n",
       " 4                  3.3  ['\"People were great and I made life long frie...   \n",
       " ..                 ...                                                ...   \n",
       " 565                4.0  ['\"Good Salary\" (in 31 reviews)', '\"Good Work ...   \n",
       " 566                NaN  ['No Pros have been reported by the Glassdoor ...   \n",
       " 567                4.3  ['\"Good benefits and company values.\" (in 153 ...   \n",
       " 568                4.2  ['\"Great colleagues, always willing to help.\" ...   \n",
       " 569                NaN                                                NaN   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['\"If you are a \\'yes man\\' you\\'ll be fine an...              NaN   \n",
       " 2    ['\"On the inside, it is an absolute mess with ...              NaN   \n",
       " 3    ['\"No Benefits (Unless you are ASM and M) Limi...              4.4   \n",
       " 4    ['\"It is just a standard and legal setup to le...              3.8   \n",
       " ..                                                 ...              ...   \n",
       " 565  ['\"Minimum increment in annual salary\" (in 31 ...              3.7   \n",
       " 566  ['\"the Salary and the changing strategy\" (in 1...              NaN   \n",
       " 567  ['\"No Benefits (Unless you are ASM and M) Limi...              4.4   \n",
       " 568  ['\"It might not be for everyone with the flexi...              NaN   \n",
       " 569                                                NaN              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Employee Discount (39 comments)\\n\"Half off p...  \n",
       " 4    ['Health Insurance (75 comments)\\n\"The best is...  \n",
       " ..                                                 ...  \n",
       " 565  ['Health Insurance (3 comments)\\n\"COWI is cons...  \n",
       " 566                                                NaN  \n",
       " 567  ['Employee Discount (39 comments)\\n\"Half off p...  \n",
       " 568                                                NaN  \n",
       " 569                                                NaN  \n",
       " \n",
       " [570 rows x 25 columns],\n",
       " 'Finland':                           Company_name  Rating   Location  \\\n",
       " 0           Topcon Positioning Systems     4.4      Espoo   \n",
       " 1                              Nouryon     3.6  Äänekoski   \n",
       " 2                             Meeshkan     NaN    Finland   \n",
       " 3                            Elisa Oyj     4.3    Finland   \n",
       " 4                      ALM Partners Oy     4.6   Helsinki   \n",
       " ..                                 ...     ...        ...   \n",
       " 295                               AFRY     3.9   Helsinki   \n",
       " 296                  Texas Instruments     4.2   Helsinki   \n",
       " 297                          Snowflake     4.1    Finland   \n",
       " 298  Nigel Frank International Limited     3.0    Finland   \n",
       " 299                     murata finland     3.3     Vantaa   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                    SOFTWARE ENGINEER   \n",
       " 1                          Research Engineer/Scientist   \n",
       " 2             Backend engineer (Data science interest)   \n",
       " 3                                        Data Engineer   \n",
       " 4                                        Data Engineer   \n",
       " ..                                                 ...   \n",
       " 295                                      Data Engineer   \n",
       " 296  FAST Rotation Program-Field Applications Engin...   \n",
       " 297                                     Sales Engineer   \n",
       " 298      Data Scientist, Data Engineer, Data Architect   \n",
       " 299                          Data Integration Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    For our Product Development Center in Espoo, F...    30d+        True   \n",
       " 1    Why join us?\\r\\nAre you passionate about susta...     24h       False   \n",
       " 2    The ideal candidate for this opening is someon...     21d        True   \n",
       " 3    Elisa Data & Analytics department is responsib...     22d       False   \n",
       " 4    Data Engineer\\r\\nTehtävässä olet osana tiivist...     14d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 295  Työpaikan kuvaus\\r\\nHaluaisitko rakentaa aikaa...     19d       False   \n",
       " 296  We can't predict what the future holds, but we...    30d+       False   \n",
       " 297  Build the future of data. Join the Snowflake t...      4d       False   \n",
       " 298  Data Scientist, Data Engineer, Data Architect\\...    30d+       False   \n",
       " 299  Our team is comprised of four professionals. E...     19d       False   \n",
       " \n",
       "     Salary      Employees  Type_of_ownership  ... CEO_approval  \\\n",
       " 0      NaN   1001 to 5000  Company - Private  ...         0.95   \n",
       " 1      NaN  5001 to 10000  Company - Private  ...         0.58   \n",
       " 2      NaN        1 to 50  Company - Private  ...          NaN   \n",
       " 3      NaN  5001 to 10000   Company - Public  ...         0.87   \n",
       " 4      NaN      51 to 200  Company - Private  ...          NaN   \n",
       " ..     ...            ...                ...  ...          ...   \n",
       " 295    NaN         10000+  Company - Private  ...         0.87   \n",
       " 296    NaN         10000+   Company - Public  ...         0.93   \n",
       " 297    NaN   1001 to 5000   Company - Public  ...         0.90   \n",
       " 298    NaN   1001 to 5000                NaN  ...          NaN   \n",
       " 299    NaN   1001 to 5000  Company - Private  ...          NaN   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     4.1             4.2              4.3                4.2   \n",
       " 1                     3.1             3.6              3.2                3.1   \n",
       " 2                     NaN             NaN              NaN                NaN   \n",
       " 3                     4.1             4.1              4.4                4.2   \n",
       " 4                     4.0             3.4              4.8                3.8   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 295                   3.8             3.2              3.8                3.5   \n",
       " 296                   4.0             3.9              4.1                3.7   \n",
       " 297                   4.2             4.2              4.0                4.0   \n",
       " 298                   3.1             2.5              2.7                2.6   \n",
       " 299                   3.4             3.4              2.7                3.1   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  4.5  ['\"A few good people are still here that still...   \n",
       " 1                  3.3  ['\"great people to work with\" (in 37 reviews)'...   \n",
       " 2                  NaN                                                NaN   \n",
       " 3                  4.3  ['\"Good salary and great team spirit.\" (in 18 ...   \n",
       " 4                  4.2  ['\"Salary is alright, not the best available.\"...   \n",
       " ..                 ...                                                ...   \n",
       " 295                3.9  ['\"decent to good salary, but not comparable w...   \n",
       " 296                3.6  ['\"It is good and good work life balance\" (in ...   \n",
       " 297                3.6  ['\"Great culture and focus.\" (in 72 reviews)',...   \n",
       " 298                2.5  ['\"Good Training and support system\" (in 23 re...   \n",
       " 299                3.0  ['\"Great management overall and great company\"...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Those people are really rude.\" (in 31 revie...              2.8   \n",
       " 1    ['\"Benefits are a joke!\" (in 19 reviews)', '\"S...              3.6   \n",
       " 2                                                  NaN              NaN   \n",
       " 3    ['\"low salary, hard to recruit to\" (in 18 revi...              5.0   \n",
       " 4    ['\"Salary is a little low but compensated for ...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 295  ['\"Relatively low salary Low bonus\" (in 53 rev...              4.0   \n",
       " 296  ['\"No work life balance eventhough the working...              4.2   \n",
       " 297  ['\"After the IPO people with \"God Complex\" cam...              4.2   \n",
       " 298  ['\"No training!\" (in 23 reviews)', '\"Bad manag...              5.0   \n",
       " 299  ['\"Low pay compared to industry\" (in 2 reviews)']              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['401K Plan (3 comments)\\n\"They match 401k pla...  \n",
       " 1    ['Health Insurance (4 comments)\\n\"UMR is middl...  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 295                                                NaN  \n",
       " 296  ['Health Insurance (122 comments)\\n\"Good offer...  \n",
       " 297  ['Health Insurance (11 comments)\\n\"Plenty of o...  \n",
       " 298                                                NaN  \n",
       " 299                                                NaN  \n",
       " \n",
       " [300 rows x 25 columns],\n",
       " 'France':                         Company_name  Rating           Location  \\\n",
       " 0                      Valtech Group     NaN              Paris   \n",
       " 1                     Valtech France     4.2              Paris   \n",
       " 2                           MICHELIN     4.1   Clermont-Ferrand   \n",
       " 3                   Publicis Sapient     3.7              Paris   \n",
       " 4                      Cobbleweb LTD     NaN        Télétravail   \n",
       " ..                               ...     ...                ...   \n",
       " 895                        La Relève     4.6              Paris   \n",
       " 896                             MAIF     3.9              Niort   \n",
       " 897                APPLIED MATERIALS     4.1             Bernin   \n",
       " 898                              EDF     4.0         Courbevoie   \n",
       " 899  EURO-INFORMATION DEVELOPPEMENTS     4.2  Villeneuve-d'Ascq   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                            Data Engineer h/f - Paris   \n",
       " 1                            Data Engineer h/f - Paris   \n",
       " 2    Software Engineer F/H (Dev.se Full-stack, DevO...   \n",
       " 3                    Manager QA Testing Automation H/F   \n",
       " 4    Senior Data Engineer (online marketplace devel...   \n",
       " ..                                                 ...   \n",
       " 895              CDI - Cloud Data Engineer (Média) F/H   \n",
       " 896                Développeur / Software Engineer F/H   \n",
       " 897  Junior Process Engineer in Semiconductor Indus...   \n",
       " 898                            Lead tech Data engineer   \n",
       " 899                                Data Engineer (H/F)   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Contexte\\r\\nRejoignez le département dédié à l...     18d       False   \n",
       " 1    Contexte\\r\\nRejoignez le département dédié à l...     18d        True   \n",
       " 2    Rejoindre une entreprise qui place sa transfor...      7d       False   \n",
       " 3    Description de l'entreprise\\r\\n\\r\\nPublicis Sa...    30d+       False   \n",
       " 4    This position is 100% remote\\r\\nWhat are we lo...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Nous recrutons pour l’un de nos clients, un Cl...    30d+       False   \n",
       " 896  Intitulé du poste\\r\\nDéveloppeur / Software En...    30d+       False   \n",
       " 897  #LI\\r\\nJunior Process Engineer in Semiconducto...     20d       False   \n",
       " 898  Mise en ligne le 29/03/2023\\r\\nPrincipales car...     13d       False   \n",
       " 899  Qui sommes nous\\r\\nEuro-Information, filiale t...    30d+       False   \n",
       " \n",
       "                           Salary      Employees  Type_of_ownership  ...  \\\n",
       " 0                            NaN            NaN   Company - Public  ...   \n",
       " 1                            NaN   1001 to 5000  Company - Private  ...   \n",
       " 2    €35K - €65K (Employer est.)         10000+   Company - Public  ...   \n",
       " 3                            NaN         10000+   Company - Public  ...   \n",
       " 4                            NaN            NaN                NaN  ...   \n",
       " ..                           ...            ...                ...  ...   \n",
       " 895  €55K - €60K (Employer est.)        1 to 50  Company - Private  ...   \n",
       " 896                          NaN  5001 to 10000  Company - Private  ...   \n",
       " 897                          NaN         10000+   Company - Public  ...   \n",
       " 898                          NaN         10000+   Company - Public  ...   \n",
       " 899  €34K - €60K (Employer est.)         10000+   Company - Public  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1           0.88                   4.0             3.7              4.3   \n",
       " 2           0.93                   3.7             3.6              4.1   \n",
       " 3           0.91                   3.8             3.8              3.9   \n",
       " 4            NaN                   NaN             NaN              NaN   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895         1.00                   4.3             4.4              4.3   \n",
       " 896         0.87                   3.2             4.1              3.9   \n",
       " 897         0.91                   3.9             3.8              3.9   \n",
       " 898         0.82                   3.6             3.6              3.9   \n",
       " 899         0.94                   3.3             3.7              4.1   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  3.9                4.2   \n",
       " 2                  3.6                3.9   \n",
       " 3                  3.6                3.6   \n",
       " 4                  NaN                NaN   \n",
       " ..                 ...                ...   \n",
       " 895                4.4                4.5   \n",
       " 896                3.4                3.8   \n",
       " 897                3.6                3.6   \n",
       " 898                3.5                4.0   \n",
       " 899                3.7                4.1   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1    ['\"Management cool et travail sérieux\" (in 1 r...   \n",
       " 2    ['No Pros have been reported by the Glassdoor ...   \n",
       " 3    ['\"good work life balance and learning curve i...   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 895                                                NaN   \n",
       " 896  ['\"Salaire, prime, avantage CE, management, éq...   \n",
       " 897  ['\"Good work life balance and able to work rem...   \n",
       " 898  ['\"équipe agréable, autonomie, bon management ...   \n",
       " 899  ['\"la relation avec les collègues et le manage...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['No Cons have been reported by the Glassdoor ...              4.1   \n",
       " 2            ['\"Management top down.\" (in 1 reviews)']              4.2   \n",
       " 3    ['\"my previous project was not that hectic but...              4.0   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895                                                NaN              NaN   \n",
       " 896  ['\"Le Management est plutôt limitant\" (in 20 r...              NaN   \n",
       " 897  ['\"Hours were long and no work/life balance\" (...              3.9   \n",
       " 898  ['No Cons have been reported by the Glassdoor ...              4.4   \n",
       " 899  ['\"Aucun management, manager absent du coup c’...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1    ['Work From Home (4 comments)\\n\"Very flexible ...  \n",
       " 2    ['Health Insurance (6 comments)\\n\"All is reall...  \n",
       " 3    ['Vacation & Paid Time Off (47 comments)\\n\"yes...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897  ['Health Insurance (83 comments)\\n\"Offers both...  \n",
       " 898  ['Health Insurance (6 comments)\\n\"The price yo...  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Germany':                                        Company_name  Rating  \\\n",
       " 0    Universitätsklinikum Carl Gustav Carus Dresden     4.3   \n",
       " 1                  The Boston Consulting Group GmbH     4.4   \n",
       " 2                                  KNF Service GmbH     4.2   \n",
       " 3                                     POLYTEC Group     2.7   \n",
       " 4                                    Hoffmann Group     3.8   \n",
       " ..                                              ...     ...   \n",
       " 895                             Arvato Systems GmbH     3.2   \n",
       " 896                     DAHMEN Personalservice GmbH     4.2   \n",
       " 897                                       ebm-papst     3.9   \n",
       " 898                                atlantis dx GmbH     NaN   \n",
       " 899                        Drägerwerk AG & Co. KGaA     3.8   \n",
       " \n",
       "                  Location                                          Job_title  \\\n",
       " 0                 Dresden                              Data Engineer (w/m/d)   \n",
       " 1                  Munich  (Senior) Data Engineer (w/m/d) – Marketing & C...   \n",
       " 2    Freiburg im Breisgau  Cloud Solution Engineer - BI/IoT on Azure (m/w/d)   \n",
       " 3               Heilbronn  SAP Data Engineer in Kraichtal-Gochsheim mit H...   \n",
       " 4                  Munich                       Senior Data Engineer (m/f/d)   \n",
       " ..                    ...                                                ...   \n",
       " 895            Heimarbeit                    System Engineer / Linux (m/w/d)   \n",
       " 896                Munich                           BI Data Engineer (m/w/d)   \n",
       " 897     Baden-Wurttemberg  IT System Engineer - Network Services (m/w/d) ...   \n",
       " 898               Hamburg                              Data Engineer (m/w/d)   \n",
       " 899                Lübeck        Software Engineer (m/w/d) Systementwicklung   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Die Digitalisierungsprozesse im Gesundheitswes...     28d       False   \n",
       " 1    (Senior) Data Engineer (w/m/d)\\r\\nMarketing & ...     15d       False   \n",
       " 2    WIR BEWEGEN WELTWEIT. MACHEN SIE MIT?\\r\\nDie K...     11d       False   \n",
       " 3    Die POLYTEC GROUP ist ein führender Entwickler...     13d        True   \n",
       " 4    More than 4,000 highly motivated employees in ...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Als international agierender IT-Spezialist, su...    30d+       False   \n",
       " 896  Wir suchen für ein IT Beratungsunternehmen ein...     21d        True   \n",
       " 897  Entdecken Sie neue Perspektiven in einem inter...    30d+       False   \n",
       " 898  Wir sind ein dynamisches, wachsendes Team mit ...    30d+        True   \n",
       " 899  - mit Python in die Cloud für Hospital Data An...    30d+       False   \n",
       " \n",
       "                           Salary     Employees  \\\n",
       " 0                            NaN  1001 to 5000   \n",
       " 1                            NaN        10000+   \n",
       " 2                            NaN     51 to 200   \n",
       " 3                            NaN  1001 to 5000   \n",
       " 4                            NaN  1001 to 5000   \n",
       " ..                           ...           ...   \n",
       " 895                          NaN        10000+   \n",
       " 896  €65K - €85K (Employer est.)   501 to 1000   \n",
       " 897                          NaN  1001 to 5000   \n",
       " 898                          NaN           NaN   \n",
       " 899                          NaN        10000+   \n",
       " \n",
       "                   Type_of_ownership  ... CEO_approval  Career_opportunities  \\\n",
       " 0                               NaN  ...         1.00                   4.0   \n",
       " 1                 Company - Private  ...         0.95                   4.5   \n",
       " 2                 Company - Private  ...          NaN                   3.6   \n",
       " 3                               NaN  ...         0.45                   2.4   \n",
       " 4                 Company - Private  ...         1.00                   3.1   \n",
       " ..                              ...  ...          ...                   ...   \n",
       " 895  Subsidiary or Business Segment  ...         0.60                   2.8   \n",
       " 896               Company - Private  ...         1.00                   3.5   \n",
       " 897                             NaN  ...         0.87                   3.4   \n",
       " 898               Company - Private  ...          NaN                   NaN   \n",
       " 899                Company - Public  ...         0.86                   3.3   \n",
       " \n",
       "     Comp_&_benefits Culture_&_values  Senior_management  Work/Life_balance  \\\n",
       " 0               4.1              3.8                3.5                4.0   \n",
       " 1               4.5              4.3                4.1                3.2   \n",
       " 2               4.0              4.1                3.6                4.0   \n",
       " 3               2.6              2.2                2.0                2.5   \n",
       " 4               3.5              3.6                3.2                3.3   \n",
       " ..              ...              ...                ...                ...   \n",
       " 895             2.8              3.0                2.9                3.1   \n",
       " 896             3.4              3.7                4.0                2.8   \n",
       " 897             3.7              3.4                3.1                3.9   \n",
       " 898             NaN              NaN                NaN                NaN   \n",
       " 899             3.6              3.7                3.2                3.7   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Nice and competent team Well equipped\" (in ...   \n",
       " 1    ['\"Name good on CV and good work/life balance....   \n",
       " 2    ['\"Company culture is very healthy.\" (in 1 rev...   \n",
       " 3                                                  NaN   \n",
       " 4    ['\"colleagues is good but 95% are retrenched\" ...   \n",
       " ..                                                 ...   \n",
       " 895  ['\"Good salary and bonus scheme\" (in 108 revie...   \n",
       " 896                                                NaN   \n",
       " 897  ['No Pros have been reported by the Glassdoor ...   \n",
       " 898                                                NaN   \n",
       " 899  ['\"Good benefits and PTO.\" (in 30 reviews)', '...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"management, some of colleagues, some old bu...              NaN   \n",
       " 1    ['\"no work life balance and alot of stress in ...              4.8   \n",
       " 2    ['No Cons have been reported by the Glassdoor ...              4.0   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"&gt; Culture is SO TOXIC that one can not s...              3.0   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"low salary and not friendly for promotion\" ...              3.3   \n",
       " 896                                                NaN              NaN   \n",
       " 897  ['\"chances are low for English speakers\" (in 1...              4.5   \n",
       " 898                                                NaN              NaN   \n",
       " 899  ['\"Cheap pay and benefits.\" (in 30 reviews)', ...              3.7   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1    ['Health Insurance (130 comments)\\n\"Fully cove...  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4    ['Sick Days (4 comments)\\n\"yes they offer sick...  \n",
       " ..                                                 ...  \n",
       " 895  ['Health Insurance (3 comments)\\n\"Health insur...  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899  ['Health Insurance (16 comments)\\n\"Allianz is ...  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Greece':                               Company_name  Rating  \\\n",
       " 0                       Emphasis DigiWorld     NaN   \n",
       " 1                 SDENG ENGINEERING BUREAU     NaN   \n",
       " 2    Sunlight Group Energy Storage Systems     NaN   \n",
       " 3                                 Deloitte     4.0   \n",
       " 4                                 Deloitte     4.0   \n",
       " ..                                     ...     ...   \n",
       " 685                          Cisco Systems     4.3   \n",
       " 686                            ARHS Hellas     3.7   \n",
       " 687                Ispass Technologies Ltd     NaN   \n",
       " 688                       Canonical - Jobs     3.1   \n",
       " 689                       Canonical - Jobs     3.1   \n",
       " \n",
       "                        Location  \\\n",
       " 0                        Athens   \n",
       " 1                        Greece   \n",
       " 2                        Xánthi   \n",
       " 3    Heraklion Airport Terminal   \n",
       " 4                        Patras   \n",
       " ..                          ...   \n",
       " 685                      Athens   \n",
       " 686                      Athens   \n",
       " 687                      Cyprus   \n",
       " 688                      Athens   \n",
       " 689                      Athens   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                Software Engineer C#, .NET (code_MSW)   \n",
       " 1    Structural Engineer - Πολιτικός Μηχανικός Δομο...   \n",
       " 2            Junior Production Engineer (Data Analyst)   \n",
       " 3                   Data Engineer / BI Engineer @Crete   \n",
       " 4                  Data Engineer / BI Engineer @Patras   \n",
       " ..                                                 ...   \n",
       " 685  Associate Solutions Engineer - Bachelor/Master...   \n",
       " 686                                      Data Engineer   \n",
       " 687                                      Data Engineer   \n",
       " 688  Golang System Software Engineer - Containers /...   \n",
       " 689                  Software Engineer - Data Platform   \n",
       " \n",
       "                                            Description    Job_age  Easy_apply  \\\n",
       " 0    Job description\\r\\nAt Emphasis DigiWorld, Athe...         5d        True   \n",
       " 1    SDENG Engineering Bureau is the leading engine...       30d+        True   \n",
       " 2    Basic Information\\r\\nSunlight Group Energy Sto...  1 day ago       False   \n",
       " 3    Basic Information\\r\\nDeloitte’s professionals ...        14d       False   \n",
       " 4    Basic Information\\r\\nDeloitte’s professionals ...        14d       False   \n",
       " ..                                                 ...        ...         ...   \n",
       " 685  Start date: 31st July 2023\\r\\nTraining locatio...       30d+       False   \n",
       " 686  Company Description\\r\\n\\r\\nArηs is a fully ind...         6d        True   \n",
       " 687  We are looking for a Data Engineer to join our...        15d        True   \n",
       " 688  We are hiring a Golang software engineer to wo...        13d       False   \n",
       " 689  Canonical is building a comprehensive automati...       30d+       False   \n",
       " \n",
       "                         Salary     Employees  Type_of_ownership  ...  \\\n",
       " 0                          NaN           NaN                NaN  ...   \n",
       " 1    €2K - €2K (Employer est.)           NaN                NaN  ...   \n",
       " 2                          NaN           NaN                NaN  ...   \n",
       " 3                          NaN        10000+  Company - Private  ...   \n",
       " 4                          NaN        10000+  Company - Private  ...   \n",
       " ..                         ...           ...                ...  ...   \n",
       " 685                        NaN        10000+   Company - Public  ...   \n",
       " 686                        NaN  1001 to 5000  Company - Private  ...   \n",
       " 687                        NaN     51 to 200  Company - Private  ...   \n",
       " 688                        NaN   501 to 1000  Company - Private  ...   \n",
       " 689                        NaN   501 to 1000  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1            NaN                   NaN             NaN              NaN   \n",
       " 2            NaN                   NaN             NaN              NaN   \n",
       " 3           0.94                   4.3             3.8              4.0   \n",
       " 4           0.94                   4.3             3.8              4.0   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 685         0.93                   4.1             4.1              4.4   \n",
       " 686         0.62                   3.2             3.6              3.4   \n",
       " 687          NaN                   NaN             NaN              NaN   \n",
       " 688         0.53                   3.3             3.4              3.1   \n",
       " 689         0.53                   3.3             3.4              3.1   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  NaN                NaN   \n",
       " 2                  NaN                NaN   \n",
       " 3                  3.8                3.3   \n",
       " 4                  3.8                3.3   \n",
       " ..                 ...                ...   \n",
       " 685                3.9                4.3   \n",
       " 686                3.2                3.8   \n",
       " 687                NaN                NaN   \n",
       " 688                2.7                3.5   \n",
       " 689                2.7                3.5   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1                                                  NaN   \n",
       " 2                                                  NaN   \n",
       " 3    ['\"A good work life balance when you compare w...   \n",
       " 4    ['\"A good work life balance when you compare w...   \n",
       " ..                                                 ...   \n",
       " 685  ['\"The work life balance is good at times but ...   \n",
       " 686  ['\"The salary is good (for a consulting firm)\"...   \n",
       " 687                                                NaN   \n",
       " 688  ['\"Remote work Open source work Great people\" ...   \n",
       " 689  ['\"Remote work Open source work Great people\" ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1                                                  NaN              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3    ['\"No work/life balance and not easy to grow i...              4.2   \n",
       " 4    ['\"No work/life balance and not easy to grow i...              4.2   \n",
       " ..                                                 ...              ...   \n",
       " 685  ['\"No work life balance (not at all)\" (in 2418...              4.3   \n",
       " 686  ['\"nice enviroment, can do attidute, high sala...              3.0   \n",
       " 687                                                NaN              NaN   \n",
       " 688  ['\"men surrounding CEO and CTO\" (in 41 reviews...              3.3   \n",
       " 689  ['\"men surrounding CEO and CTO\" (in 41 reviews...              3.3   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Health Insurance (765 comments)\\n\"A lot of o...  \n",
       " 4    ['Health Insurance (765 comments)\\n\"A lot of o...  \n",
       " ..                                                 ...  \n",
       " 685  ['Health Insurance (802 comments)\\n\"Covers man...  \n",
       " 686                                                NaN  \n",
       " 687                                                NaN  \n",
       " 688  ['Work From Home (10 comments)\\n\"Fully remote ...  \n",
       " 689  ['Work From Home (10 comments)\\n\"Fully remote ...  \n",
       " \n",
       " [690 rows x 25 columns],\n",
       " 'Hong_Kong':                        Company_name  Rating         Location  \\\n",
       " 0         Motiva Consulting Limited     NaN        Hong Kong   \n",
       " 1                   MTR Corporation     3.7        Hong Kong   \n",
       " 2                    Neuron Digital     NaN  New Territories   \n",
       " 3                Hoplite Technology     NaN        Hong Kong   \n",
       " 4                           Siemens     4.1        Hong Kong   \n",
       " ..                              ...     ...              ...   \n",
       " 865                           ASMPT     3.3        Hong Kong   \n",
       " 866  Y.H.T Professional Co. Limited     NaN        Hong Kong   \n",
       " 867                    IT-RecruitHK     NaN        Hong Kong   \n",
       " 868                           ASMPT     3.3        Hong Kong   \n",
       " 869            iN and iN Management     3.0        Hong Kong   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                       AI/ML Engineer   \n",
       " 1                                        Data Engineer   \n",
       " 2    Data Scientist / Big Data Engineer (Fresh Grad...   \n",
       " 3                              Data Engineer (BigData)   \n",
       " 4               Graduate Trainee Engineer Program 2023   \n",
       " ..                                                 ...   \n",
       " 865  (Senior) CAE Engineer (Fresh Graduate / IANG H...   \n",
       " 866                          Machine Learning Engineer   \n",
       " 867  Engineer or Technician (Double pay/Medical/Ban...   \n",
       " 868  Process Engineer (Fresh Graduate / IANG Holder...   \n",
       " 869                  Data Centre Engineer - Swift Duty   \n",
       " \n",
       "                                            Description    Job_age  Easy_apply  \\\n",
       " 0    Responsibilities:\\r\\nDesign and develop AI/ML ...        24h        True   \n",
       " 1    Data Engineer (Ref: 220000VH)\\r\\n\\r\\nResponsib...  1 day ago       False   \n",
       " 2    Job Highlight\\r\\nCreate, develop and maintain ...         8d        True   \n",
       " 3    We are constantly looking for talents @ Hoplit...        10d       False   \n",
       " 4    Graduate Trainee Engineer Program 2023\\r\\n\\r\\n...       30d+       False   \n",
       " ..                                                 ...        ...         ...   \n",
       " 865  Perform complex design analysis by using Compu...       30d+        True   \n",
       " 866  Job Requirements :\\r\\nStrong knowledge of mach...       30d+        True   \n",
       " 867  Job description:\\r\\nResponsibilities\\r\\nMonito...       30d+       False   \n",
       " 868  Define process flow, requirements and specific...       30d+        True   \n",
       " 869  Education requirements:\\r\\nBachelor of science...        15d       False   \n",
       " \n",
       "                               Salary Employees  Type_of_ownership  ...  \\\n",
       " 0    HK$25K - HK$40K (Employer est.)   1 to 50   Company - Public  ...   \n",
       " 1                                NaN    10000+   Company - Public  ...   \n",
       " 2    HK$20K - HK$24K (Employer est.)       NaN                NaN  ...   \n",
       " 3                                NaN   1 to 50  Company - Private  ...   \n",
       " 4                                NaN    10000+   Company - Public  ...   \n",
       " ..                               ...       ...                ...  ...   \n",
       " 865                              NaN    10000+   Company - Public  ...   \n",
       " 866  HK$18K - HK$22K (Employer est.)       NaN                NaN  ...   \n",
       " 867  HK$25K - HK$30K (Employer est.)   1 to 50  Company - Private  ...   \n",
       " 868                              NaN    10000+   Company - Public  ...   \n",
       " 869                              NaN   1 to 50   Company - Public  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1           0.64                   3.2             3.6              3.1   \n",
       " 2            NaN                   NaN             NaN              NaN   \n",
       " 3            NaN                   NaN             NaN              NaN   \n",
       " 4           0.90                   3.8             3.7              4.1   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 865         0.84                   3.1             3.0              3.1   \n",
       " 866          NaN                   NaN             NaN              NaN   \n",
       " 867          NaN                   NaN             NaN              NaN   \n",
       " 868         0.84                   3.1             3.0              3.1   \n",
       " 869          NaN                   NaN             NaN              NaN   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  2.9                3.3   \n",
       " 2                  NaN                NaN   \n",
       " 3                  NaN                NaN   \n",
       " 4                  3.7                4.1   \n",
       " ..                 ...                ...   \n",
       " 865                2.8                3.1   \n",
       " 866                NaN                NaN   \n",
       " 867                NaN                NaN   \n",
       " 868                2.8                3.1   \n",
       " 869                NaN                NaN   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1    ['\"good salary and benefit for staff\" (in 26 r...   \n",
       " 2                                                  NaN   \n",
       " 3                                                  NaN   \n",
       " 4    ['\"Good work life balance and\" (in 686 reviews...   \n",
       " ..                                                 ...   \n",
       " 865  ['\"1. Mediocrity but okay salary compares to t...   \n",
       " 866                                                NaN   \n",
       " 867                                                NaN   \n",
       " 868  ['\"1. Mediocrity but okay salary compares to t...   \n",
       " 869                                                NaN   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['\"salary not aligned with gov\" (in 26 reviews...              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"No work life balance for contract employee\"...              3.9   \n",
       " ..                                                 ...              ...   \n",
       " 865  ['\"Low salary and lack of organization\" (in 70...              2.5   \n",
       " 866                                                NaN              NaN   \n",
       " 867                                                NaN              NaN   \n",
       " 868  ['\"Low salary and lack of organization\" (in 70...              2.5   \n",
       " 869                                                NaN              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4    ['Health Insurance (190 comments)\\n\"does not c...  \n",
       " ..                                                 ...  \n",
       " 865                                                NaN  \n",
       " 866                                                NaN  \n",
       " 867                                                NaN  \n",
       " 868                                                NaN  \n",
       " 869                                                NaN  \n",
       " \n",
       " [870 rows x 25 columns],\n",
       " 'Hungary':                  Company_name  Rating  Location  \\\n",
       " 0    Infineon Technologies AG     4.2    Cegléd   \n",
       " 1    Infineon Technologies AG     4.2    Cegléd   \n",
       " 2    Infineon Technologies AG     4.2  Budapest   \n",
       " 3      Stafast Products, Inc.     4.1  Budapest   \n",
       " 4    Infineon Technologies AG     4.2  Budapest   \n",
       " ..                        ...     ...       ...   \n",
       " 685               Bosch Group     4.2  Budapest   \n",
       " 686               Bosch Group     4.2  Budapest   \n",
       " 687                    Sanofi     4.0  Budapest   \n",
       " 688             Innoview Kft.     3.1  Budapest   \n",
       " 689             Innoview Kft.     3.1  Budapest   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                      Soldering Technology (f/m/div)*   \n",
       " 1    Factory Integration Engineer - Equipment Autom...   \n",
       " 2    Electric Development Engineer (L&M ED) (f/m/div)*   \n",
       " 3    Outside Sales Representative, Focus on Eastern...   \n",
       " 4    Electrical Engineer / Physicist Electrical Dev...   \n",
       " ..                                                 ...   \n",
       " 685                            Data Engineer gyakornok   \n",
       " 686  Data Engineer for Data Pipeline Development in...   \n",
       " 687                                      Data Engineer   \n",
       " 688                               QA Engineer (DWH/BI)   \n",
       " 689                                  Big Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Part of your life. Part of tomorrow.\\r\\n\\r\\nIn...    30d+       False   \n",
       " 1    Part of your life. Part of tomorrow.\\r\\n\\r\\nIn...     20d       False   \n",
       " 2    Part of your life. Part of tomorrow.\\r\\n\\r\\nIn...    30d+       False   \n",
       " 3    Job Description: Outside Sales Representative,...      7d        True   \n",
       " 4    Part of your life. Part of tomorrow.\\r\\n\\r\\nWe...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 685  A cég leírása\\r\\n\\r\\nSzeretnél ötleteiddel has...     15d        True   \n",
       " 686  Company Description\\r\\n\\r\\nDo you want benefic...     19d        True   \n",
       " 687  Job Title: Data Engineer\\r\\nLocation: Budapest...    30d+       False   \n",
       " 688  Budai székhelyű, szoftverfejlesztő partnerünk ...    30d+       False   \n",
       " 689  Feladataid lesznek:\\r\\nMeghatározó szerep betö...    30d+       False   \n",
       " \n",
       "                                 Salary  Employees  Type_of_ownership  ...  \\\n",
       " 0                                  NaN     10000+   Company - Public  ...   \n",
       " 1                                  NaN     10000+   Company - Public  ...   \n",
       " 2                                  NaN     10000+   Company - Public  ...   \n",
       " 3    HUF 22M - HUF 27M (Employer est.)  51 to 200  Company - Private  ...   \n",
       " 4                                  NaN     10000+   Company - Public  ...   \n",
       " ..                                 ...        ...                ...  ...   \n",
       " 685                                NaN     10000+  Company - Private  ...   \n",
       " 686                                NaN     10000+  Company - Private  ...   \n",
       " 687                                NaN     10000+   Company - Public  ...   \n",
       " 688                                NaN    1 to 50  Company - Private  ...   \n",
       " 689                                NaN    1 to 50  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.95                   3.9             3.8              4.1   \n",
       " 1           0.95                   3.9             3.8              4.1   \n",
       " 2           0.95                   3.9             3.8              4.1   \n",
       " 3           1.00                   3.8             4.3              4.4   \n",
       " 4           0.95                   3.9             3.8              4.1   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 685         0.85                   3.9             3.8              4.2   \n",
       " 686         0.85                   3.9             3.8              4.2   \n",
       " 687         0.86                   3.6             3.9              3.8   \n",
       " 688          NaN                   3.1             2.6              3.1   \n",
       " 689          NaN                   3.1             2.6              3.1   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.7                4.1   \n",
       " 1                  3.7                4.1   \n",
       " 2                  3.7                4.1   \n",
       " 3                  4.1                4.3   \n",
       " 4                  3.7                4.1   \n",
       " ..                 ...                ...   \n",
       " 685                3.7                4.1   \n",
       " 686                3.7                4.1   \n",
       " 687                3.4                3.7   \n",
       " 688                2.8                2.9   \n",
       " 689                2.8                2.9   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Work life balance is good and manageable\" (...   \n",
       " 1    ['\"Work life balance is good and manageable\" (...   \n",
       " 2    ['\"Work life balance is good and manageable\" (...   \n",
       " 3    ['\"Great People at the company\" (in 2 reviews)...   \n",
       " 4    ['\"Work life balance is good and manageable\" (...   \n",
       " ..                                                 ...   \n",
       " 685  ['\"Fair and good salary.\" (in 268 reviews)', '...   \n",
       " 686  ['\"Fair and good salary.\" (in 268 reviews)', '...   \n",
       " 687  ['\"Good benefits and time off\" (in 491 reviews...   \n",
       " 688                                                NaN   \n",
       " 689                                                NaN   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Descent pay, work life balance\" (in 396 rev...              3.9   \n",
       " 1    ['\"Descent pay, work life balance\" (in 396 rev...              3.9   \n",
       " 2    ['\"Descent pay, work life balance\" (in 396 rev...              3.9   \n",
       " 3    ['No Cons have been reported by the Glassdoor ...              4.0   \n",
       " 4    ['\"Descent pay, work life balance\" (in 396 rev...              3.9   \n",
       " ..                                                 ...              ...   \n",
       " 685  ['\"Many tasks &amp; workload with low salary\" ...              4.0   \n",
       " 686  ['\"Many tasks &amp; workload with low salary\" ...              4.0   \n",
       " 687  ['\"Not always the best compensation and benefi...              4.3   \n",
       " 688                                                NaN              NaN   \n",
       " 689                                                NaN              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 1    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 2    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " 3                                                  NaN  \n",
       " 4    ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       " ..                                                 ...  \n",
       " 685  ['Health Insurance (33 comments)\\n\"It doesn\\'t...  \n",
       " 686  ['Health Insurance (33 comments)\\n\"It doesn\\'t...  \n",
       " 687  ['Health Insurance (179 comments)\\n\"Very good ...  \n",
       " 688                                                NaN  \n",
       " 689                                                NaN  \n",
       " \n",
       " [690 rows x 25 columns],\n",
       " 'Ireland':                                          Company_name  Rating  \\\n",
       " 0                               ficonTEC Service GmbH     NaN   \n",
       " 1                                             CSL Ltd     NaN   \n",
       " 2    GerTEK Project Management and Technical Services     NaN   \n",
       " 3                                             Ex Ordo     5.0   \n",
       " 4                         Dillon Engineering Services     5.0   \n",
       " ..                                                ...     ...   \n",
       " 895                                 Colgate-Palmolive     4.3   \n",
       " 896                       Dillon Engineering Services     5.0   \n",
       " 897                                         PE Global     4.4   \n",
       " 898                                 Jaguar Land Rover     3.9   \n",
       " 899                              Soltec (Ireland) Ltd     NaN   \n",
       " \n",
       "                Location                                          Job_title  \\\n",
       " 0                  Cork  Process and applications engineer (f/m/d)-Auto...   \n",
       " 1                Carlow                                 Tendering Engineer   \n",
       " 2                 Sligo                                Validation Engineer   \n",
       " 3                Remote           Senior Infrastructure Engineer (Ireland)   \n",
       " 4                Remote                         Senior Automation Engineer   \n",
       " ..                  ...                                                ...   \n",
       " 895              Dublin  Analytics Engineer, Digital Applications & Ana...   \n",
       " 896              Dublin                            Mechanical BIM Engineer   \n",
       " 897              Carlow                                Validation Engineer   \n",
       " 898             Shannon                                Engineering Manager   \n",
       " 899  An Muileann gCearr                                Production Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    About us\\r\\nficonTEC Ireland is located inside...    30d+        True   \n",
       " 1    Due to continued expansion, CSL is currently r...    30d+        True   \n",
       " 2    GerTEK is an engineering consultancy company s...      7d        True   \n",
       " 3    About the Role\\r\\nWe are seeking an experience...    30d+        True   \n",
       " 4    The job is for an Automation Engineer in the T...     20d        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  No Relocation Assistance Offered\\r\\n# 152384 -...      7d       False   \n",
       " 896  Dillon Engineering Services has partnered with...      5d        True   \n",
       " 897  The role:\\r\\nPE Global is currently recruiting...     28d        True   \n",
       " 898  Job Description\\r\\n\\r\\n\\r\\nREQ ID: 105041\\r\\nJ...    30d+       False   \n",
       " 899  We are looking for a Production Engineer to jo...     13d        True   \n",
       " \n",
       "                                       Salary Employees  Type_of_ownership  \\\n",
       " 0                                        NaN       NaN                NaN   \n",
       " 1                €40K - €60K (Employer est.)       NaN                NaN   \n",
       " 2               €47K - €71K (Glassdoor est.)       NaN   Company - Public   \n",
       " 3                €72K - €82K (Employer est.)   1 to 50  Company - Private   \n",
       " 4    €55.00 - €70.00 Per Hour(Employer est.)   1 to 50  Company - Private   \n",
       " ..                                       ...       ...                ...   \n",
       " 895             €39K - €43K (Glassdoor est.)    10000+   Company - Public   \n",
       " 896              €35K - €50K (Employer est.)   1 to 50  Company - Private   \n",
       " 897             €38K - €60K (Glassdoor est.)   1 to 50  Company - Private   \n",
       " 898             €37K - €59K (Glassdoor est.)    10000+  Company - Private   \n",
       " 899              €40K - €50K (Employer est.)       NaN                NaN   \n",
       " \n",
       "      ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0    ...          NaN                   NaN             NaN              NaN   \n",
       " 1    ...          NaN                   NaN             NaN              NaN   \n",
       " 2    ...          NaN                   NaN             NaN              NaN   \n",
       " 3    ...          NaN                   4.5             3.8              4.7   \n",
       " 4    ...         1.00                   5.0             5.0              5.0   \n",
       " ..   ...          ...                   ...             ...              ...   \n",
       " 895  ...         0.91                   3.7             3.9              4.2   \n",
       " 896  ...         1.00                   5.0             5.0              5.0   \n",
       " 897  ...          NaN                   4.4             4.0              4.3   \n",
       " 898  ...         0.85                   3.6             3.7              3.6   \n",
       " 899  ...          NaN                   NaN             NaN              NaN   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  NaN                NaN   \n",
       " 2                  NaN                NaN   \n",
       " 3                  4.8                4.5   \n",
       " 4                  5.0                5.0   \n",
       " ..                 ...                ...   \n",
       " 895                3.8                3.9   \n",
       " 896                5.0                5.0   \n",
       " 897                4.3                4.2   \n",
       " 898                3.3                3.8   \n",
       " 899                NaN                NaN   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1                                                  NaN   \n",
       " 2                                                  NaN   \n",
       " 3    ['\"Responsive/Honest management\" (in 2 reviews...   \n",
       " 4    ['\"Friendly and efficient service\" (in 1 revie...   \n",
       " ..                                                 ...   \n",
       " 895  ['\"Good work life balance\" (in 18 reviews)', '...   \n",
       " 896  ['\"Friendly and efficient service\" (in 1 revie...   \n",
       " 897  ['\"Good Salary\" (in 3 reviews)', '\"Good mentor...   \n",
       " 898  ['\"team outings and good salary\" (in 132 revie...   \n",
       " 899                                                NaN   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1                                                  NaN              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3    ['\"As a small but growing company, the overall...              NaN   \n",
       " 4    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Too much work Life Balance\" (in 18 reviews)...              4.2   \n",
       " 896  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 897  ['\"horrible coworkers horrible stressful work ...              NaN   \n",
       " 898  ['\"2. Average salary is low\" (in 132 reviews)'...              3.7   \n",
       " 899                                                NaN              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895  ['Vacation & Paid Time Off (27 comments)\\n\"rea...  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898  ['Health Insurance (3 comments)\\n\"Company prov...  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Israel':         Company_name  Rating       Location  \\\n",
       " 0           InspHire     5.0  Tel Aviv-Yafo   \n",
       " 1              DBArt     5.0   Hod HaSharon   \n",
       " 2              Neura     4.8       Herzliya   \n",
       " 3             SQlink     3.7         Israel   \n",
       " 4    Bright Machines     3.6         Israel   \n",
       " ..               ...     ...            ...   \n",
       " 895        VAST Data     4.2  Tel Aviv-Yafo   \n",
       " 896       Gotfriends     4.7  Tel Aviv-Yafo   \n",
       " 897           SQlink     3.7         Israel   \n",
       " 898           SQlink     3.7  Tel Aviv-Yafo   \n",
       " 899    Tailor Brands     4.4  Tel Aviv-Yafo   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                        Data Engineer   \n",
       " 1                               Data Engineer(Hybrid   \n",
       " 2                                        Data Engineer   \n",
       " 3                                 Junior Data Engineer   \n",
       " 4                                  Cloud Data Engineer   \n",
       " ..                                                 ...   \n",
       " 895               Senior Software Engineer - Data Path   \n",
       " 896  Computer Vision Engineer לחברת סטארטאפ טכנולוג...   \n",
       " 897             סטארט אפ מבטיח בדרום מגייס QA Engineer   \n",
       " 898  לסטארט אפ מצליח הממוקם בתל אביב דרוש/ה Python ...   \n",
       " 899                                      Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    A company that was founded in early 2018 with ...    30d+        True   \n",
       " 1    ✅Responsibilities:\\r\\nBe part of the group tha...    30d+       False   \n",
       " 2    Develop an effective, coherent, reliable and p...    30d+       False   \n",
       " 3    תיאור המשרה:\\r\\nחברה פיננסית מובילה מחפשת Juni...     13d       False   \n",
       " 4    RETHINK MANUFACTURING\\r\\n\\r\\nThe only way to i...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  VAST Data is looking for a Senior Software Eng...    30d+        True   \n",
       " 896  מיקום: ת\"א והמרכז\\r\\nתיאור המשרה:\\r\\nהיי לכולם...    30d+       False   \n",
       " 897  תיאור המשרה:\\r\\nדרוש/ה QA Engineer לסטארט אפ ה...     13d       False   \n",
       " 898  תיאור המשרה:\\r\\nדרוש/ה Python Data Engineer לח...    30d+       False   \n",
       " 899  About us\\r\\nWith over 30 Million users on our ...    30d+        True   \n",
       " \n",
       "      Salary     Employees  Type_of_ownership  ... CEO_approval  \\\n",
       " 0       NaN       1 to 50  Company - Private  ...          NaN   \n",
       " 1       NaN           NaN  Company - Private  ...          NaN   \n",
       " 2       NaN       1 to 50  Company - Private  ...         1.00   \n",
       " 3       NaN  1001 to 5000  Company - Private  ...          NaN   \n",
       " 4       NaN    201 to 500  Company - Private  ...         0.69   \n",
       " ..      ...           ...                ...  ...          ...   \n",
       " 895     NaN     51 to 200  Company - Private  ...         0.87   \n",
       " 896     NaN     51 to 200                NaN  ...          NaN   \n",
       " 897     NaN  1001 to 5000  Company - Private  ...          NaN   \n",
       " 898     NaN  1001 to 5000  Company - Private  ...          NaN   \n",
       " 899     NaN       1 to 50  Company - Private  ...         0.88   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     NaN             NaN              NaN                NaN   \n",
       " 1                     NaN             NaN              NaN                NaN   \n",
       " 2                     5.0             4.4              4.8                4.8   \n",
       " 3                     3.7             3.2              3.5                3.0   \n",
       " 4                     3.5             4.2              3.5                3.1   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 895                   4.0             4.1              3.9                4.2   \n",
       " 896                   3.8             4.5              3.9                4.4   \n",
       " 897                   3.7             3.2              3.5                3.0   \n",
       " 898                   3.7             3.2              3.5                3.0   \n",
       " 899                   4.3             4.4              4.3                3.9   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  NaN                                                NaN   \n",
       " 1                  NaN  ['\"Amazing collegaues, nice office, flexible.\"...   \n",
       " 2                  4.9  ['No Pros have been reported by the Glassdoor ...   \n",
       " 3                  3.6  ['\"Amazing people, salary and Benefits\" (in 10...   \n",
       " 4                  3.5  ['\"Great benefits\" (in 9 reviews)', '\"Talented...   \n",
       " ..                 ...                                                ...   \n",
       " 895                3.7  ['\"Very fun and friendly community\" (in 3 revi...   \n",
       " 896                3.3  ['\"Good people and good managers.\" (in 3 revie...   \n",
       " 897                3.6  ['\"Amazing people, salary and Benefits\" (in 10...   \n",
       " 898                3.6  ['\"Amazing people, salary and Benefits\" (in 10...   \n",
       " 899                4.1  ['\"Good people\" (in 6 reviews)', '\"Work life b...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 2    ['\"None, highly recommended company with great...              NaN   \n",
       " 3    ['\"Low salaries and not growing\" (in 10 review...              NaN   \n",
       " 4    ['\"Many layoffs in a single year.\" (in 5 revie...              3.7   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['No Cons have been reported by the Glassdoor ...              3.5   \n",
       " 896  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 897  ['\"Low salaries and not growing\" (in 10 review...              NaN   \n",
       " 898  ['\"Low salaries and not growing\" (in 10 review...              NaN   \n",
       " 899  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 895               NaN  \n",
       " 896               NaN  \n",
       " 897               NaN  \n",
       " 898               NaN  \n",
       " 899               NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Italy':                        Company_name  Rating              Location  \\\n",
       " 0                           Shimano     3.8                  Pero   \n",
       " 1                         3Brain AG     NaN                 Genoa   \n",
       " 2                STMicroelectronics     3.9               Catania   \n",
       " 3                     Valore Solare     NaN   Torri di Quartesolo   \n",
       " 4                     Area IT Group     NaN                 Italy   \n",
       " ..                              ...     ...                   ...   \n",
       " 895                     Altevia srl     NaN                   NaN   \n",
       " 896                    Alti Profili     3.7                 Italy   \n",
       " 897  Exalto Energy & Innovation Srl     NaN                  Roma   \n",
       " 898                         HRM srl     3.8                Verona   \n",
       " 899                    SADAS S.r.l.     4.6  Casalnuovo di Napoli   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                     IT Data Engineer   \n",
       " 1                         Digital ASIC Design Engineer   \n",
       " 2                      Senior Application Engineer M/F   \n",
       " 3                                Progettista elettrico   \n",
       " 4    Network Engineer - Data Center Network Specialist   \n",
       " ..                                                 ...   \n",
       " 895                                 Software Developer   \n",
       " 896                                   DataOps Engineer   \n",
       " 897         Ingegnere/Architetto - energie rinnovabili   \n",
       " 898                            Backend Developer | JEE   \n",
       " 899                          Software Developer Junior   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Introduction\\r\\nAre you a Data Engineer? Do yo...    30d+        True   \n",
       " 1    The Company\\r\\n3Brain AG is a Swiss deep tech ...      5d        True   \n",
       " 2    Position description\\r\\nPosting title\\r\\nSenio...    30d+       False   \n",
       " 3    Valore Solare, importante società nel settore ...    30d+        True   \n",
       " 4    Ricerchiamo Data Center Network Specialist e N...     23d        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  C#/DOT NET DEVELOPER\\r\\nLa risorsa verrà inser...      5d        True   \n",
       " 896  Per nostra azienda cliente, consorzio interuni...    30d+       False   \n",
       " 897  Cerchiamo ingegneri o architetti con 2 anni di...    30d+        True   \n",
       " 898  HRM oggi è un ecosistema innovativo, in cui St...      5d        True   \n",
       " 899  SADAS s.r.l., azienda di rilievo nel panorama ...    30d+        True   \n",
       " \n",
       "                           Salary  Employees  Type_of_ownership  ...  \\\n",
       " 0                            NaN  51 to 200  Company - Private  ...   \n",
       " 1                            NaN    1 to 50  Company - Private  ...   \n",
       " 2                            NaN     10000+   Company - Public  ...   \n",
       " 3                            NaN        NaN                NaN  ...   \n",
       " 4    €25K - €45K (Employer est.)        NaN                NaN  ...   \n",
       " ..                           ...        ...                ...  ...   \n",
       " 895    €1K - €2K (Employer est.)        NaN                NaN  ...   \n",
       " 896                          NaN     10000+  Company - Private  ...   \n",
       " 897    €1K - €1K (Employer est.)        NaN                NaN  ...   \n",
       " 898                          NaN  51 to 200  Company - Private  ...   \n",
       " 899                          NaN    1 to 50  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.88                   3.4             3.5              3.5   \n",
       " 1            NaN                   NaN             NaN              NaN   \n",
       " 2           0.92                   3.4             3.4              3.7   \n",
       " 3            NaN                   NaN             NaN              NaN   \n",
       " 4            NaN                   NaN             NaN              NaN   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895          NaN                   NaN             NaN              NaN   \n",
       " 896          NaN                   3.3             2.9              3.5   \n",
       " 897          NaN                   NaN             NaN              NaN   \n",
       " 898          NaN                   3.4             3.7              3.4   \n",
       " 899         1.00                   4.9             4.7              4.9   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.2                3.4   \n",
       " 1                  NaN                NaN   \n",
       " 2                  3.3                3.8   \n",
       " 3                  NaN                NaN   \n",
       " 4                  NaN                NaN   \n",
       " ..                 ...                ...   \n",
       " 895                NaN                NaN   \n",
       " 896                3.2                2.8   \n",
       " 897                NaN                NaN   \n",
       " 898                3.4                4.4   \n",
       " 899                4.9                5.0   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Average Pay with Good Bonus\" (in 18 reviews...   \n",
       " 1                                                  NaN   \n",
       " 2    ['\"Work life balance is good\" (in 114 reviews)...   \n",
       " 3                                                  NaN   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 895                                                NaN   \n",
       " 896  ['\"Regular pay fixed contracts paid benefits\" ...   \n",
       " 897                                                NaN   \n",
       " 898  ['\"Good benefits.\" (in 1 reviews)', '\"In gener...   \n",
       " 899  ['\"Good Pay and place to work\" (in 2 reviews)'...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"2)Low Salary &amp; reasonable bonus.\" (in 1...              3.9   \n",
       " 1                                                  NaN              NaN   \n",
       " 2    ['\"Work can get tidius and work life balance m...              4.0   \n",
       " 3                                                  NaN              NaN   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895                                                NaN              NaN   \n",
       " 896  ['\"Flessibility, hourly daily and salary\" (in ...              NaN   \n",
       " 897                                                NaN              NaN   \n",
       " 898  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 899  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Employee Discount (4 comments)\\n\"applies to ...  \n",
       " 1                                                  NaN  \n",
       " 2    ['Pension Plan (7 comments)\\n\"Pension plan acc...  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Japan':         Company_name  Rating  Location  \\\n",
       " 0    rinna Co., Ltd.     NaN       NaN   \n",
       " 1         EAGLYS株式会社     NaN       NaN   \n",
       " 2          オルグロー株式会社     NaN       NaN   \n",
       " 3           Fortinet     NaN       NaN   \n",
       " 4      Pepperl+Fuchs     NaN       NaN   \n",
       " ..               ...     ...       ...   \n",
       " 415             Okta     NaN       NaN   \n",
       " 416   N I DRIVE K.K.     NaN       NaN   \n",
       " 417     ブリッジグループ株式会社     NaN       NaN   \n",
       " 418  Capgemini Japan     NaN       NaN   \n",
       " 419   SOMPOシステムズ株式会社     NaN       NaN   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0    【rinna Summer Internship】技術職・Technical Role (S...   \n",
       " 1                       Internship (Frontend Engineer)   \n",
       " 2                              Data Engineer/Scientist   \n",
       " 3                 Technical Support Engineer - Fukuoka   \n",
       " 4                                 Application Engineer   \n",
       " ..                                                 ...   \n",
       " 415          Senior Technical Support Engineer - Japan   \n",
       " 416                      Servicenowエンジニア N I DRIVE株式会社   \n",
       " 417  エンジニア・アーキテクト::アプリケーションエンジニア / Application Engi...   \n",
       " 418                          Windchill Project Manager   \n",
       " 419                 DBエンジニア(DXデータ分析基盤構築/SOMPOホールディングス)   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    ジョブID\\r\\n5033\\r\\n勤務地\\r\\n渋谷区渋谷2-24-12渋谷スクランブルスク...     30d       False   \n",
       " 1    【通期インターン】新卒 秘密計算を利用したWebサービスのフロントエンドを手掛けるチャンス！...    30d+       False   \n",
       " 2    給与\\r\\n経験、スキルによって協議のうえ決定（月給制）\\r\\n待遇\\r\\n通勤費全額支給、...    30d+       False   \n",
       " 3    Role Overview\\r\\nProvide direct technical web ...    30d+       False   \n",
       " 4    You want to make your ideas reality …\\r\\n… and...     24h       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 415  Get to know Okta\\r\\n\\r\\nOkta is The World's Id...    30d+       False   \n",
       " 416  ジョブID\\r\\n4664\\r\\n勤務地\\r\\n神奈川県川崎市川崎区宮本町7-1 6F\\r\\...    30d+       False   \n",
       " 417  エンジニア・アーキテクト::アプリケーションエンジニア / Application Engi...    30d+       False   \n",
       " 418  Duties and Responsibilities:\\r\\nWe are looking...    30d+       False   \n",
       " 419  【年収】:\\r\\n474〜820万円\\r\\n\\r\\n【仕事内容】:\\r\\n・損保ジャパン、S...     13d       False   \n",
       " \n",
       "      Salary  Employees  Type_of_ownership  ...  CEO_approval  \\\n",
       " 0       NaN        NaN                NaN  ...           NaN   \n",
       " 1       NaN        NaN                NaN  ...           NaN   \n",
       " 2       NaN        NaN                NaN  ...           NaN   \n",
       " 3       NaN        NaN                NaN  ...           NaN   \n",
       " 4       NaN        NaN                NaN  ...           NaN   \n",
       " ..      ...        ...                ...  ...           ...   \n",
       " 415     NaN        NaN                NaN  ...           NaN   \n",
       " 416     NaN        NaN                NaN  ...           NaN   \n",
       " 417     NaN        NaN                NaN  ...           NaN   \n",
       " 418     NaN        NaN                NaN  ...           NaN   \n",
       " 419     NaN        NaN                NaN  ...           NaN   \n",
       " \n",
       "      Career_opportunities  Comp_&_benefits  Culture_&_values  \\\n",
       " 0                     NaN              NaN               NaN   \n",
       " 1                     NaN              NaN               NaN   \n",
       " 2                     NaN              NaN               NaN   \n",
       " 3                     NaN              NaN               NaN   \n",
       " 4                     NaN              NaN               NaN   \n",
       " ..                    ...              ...               ...   \n",
       " 415                   NaN              NaN               NaN   \n",
       " 416                   NaN              NaN               NaN   \n",
       " 417                   NaN              NaN               NaN   \n",
       " 418                   NaN              NaN               NaN   \n",
       " 419                   NaN              NaN               NaN   \n",
       " \n",
       "      Senior_management  Work/Life_balance  Pros  Cons  Benefits_rating  \\\n",
       " 0                  NaN                NaN   NaN   NaN              NaN   \n",
       " 1                  NaN                NaN   NaN   NaN              NaN   \n",
       " 2                  NaN                NaN   NaN   NaN              NaN   \n",
       " 3                  NaN                NaN   NaN   NaN              NaN   \n",
       " 4                  NaN                NaN   NaN   NaN              NaN   \n",
       " ..                 ...                ...   ...   ...              ...   \n",
       " 415                NaN                NaN   NaN   NaN              NaN   \n",
       " 416                NaN                NaN   NaN   NaN              NaN   \n",
       " 417                NaN                NaN   NaN   NaN              NaN   \n",
       " 418                NaN                NaN   NaN   NaN              NaN   \n",
       " 419                NaN                NaN   NaN   NaN              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 415               NaN  \n",
       " 416               NaN  \n",
       " 417               NaN  \n",
       " 418               NaN  \n",
       " 419               NaN  \n",
       " \n",
       " [420 rows x 25 columns],\n",
       " 'Luxembourg':                                  Company_name  Rating          Location  \\\n",
       " 0                             ING Netherlands     4.1       Télétravail   \n",
       " 1                             ING Netherlands     4.1       Télétravail   \n",
       " 2                             ING Netherlands     4.1       Télétravail   \n",
       " 3                             ING Netherlands     4.1       Télétravail   \n",
       " 4                           EyeTech Solutions     4.1        Luxembourg   \n",
       " ..                                        ...     ...               ...   \n",
       " 385                        Webasto Luxembourg     3.6      Grevenmacher   \n",
       " 386  Banque Internationale à Luxembourg (BIL)     3.7        Luxembourg   \n",
       " 387                  Université du Luxembourg     4.0  Esch-sur-Alzette   \n",
       " 388                            L.E.A.SE. S.A.     NaN             Wiltz   \n",
       " 389                                Dennemeyer     3.2        Luxembourg   \n",
       " \n",
       "                                       Job_title  \\\n",
       " 0          Senior DevOps Engineer Elastic Stack   \n",
       " 1                            Architect Security   \n",
       " 2                          Full Stack Developer   \n",
       " 3                       IT Class September 2023   \n",
       " 4                                Data Scientist   \n",
       " ..                                          ...   \n",
       " 385           Soldering Engineer/Expert (f/m/d)   \n",
       " 386  Financial Information System Manager (M/F)   \n",
       " 387                   Technician iPSC NIM (F/M)   \n",
       " 388                Test Analyst / Test Engineer   \n",
       " 389                Senior Data Engineer (f/m/d)   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Kubernetes, OpenShift & Elastic Stack - you lo...     19d       False   \n",
       " 1    As an experienced Security Architect you'll be...     19d       False   \n",
       " 2    ING Private Cloud Full Stack Developer\\r\\n\\r\\n...    30d+       False   \n",
       " 3    Are you completing your Masters' or PhD degree...    30d+       False   \n",
       " 4    EyeTech Solutions est un cabinet de recrutemen...      6d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 385  Soldering Engineer/Expert (f/m/d)\\r\\n\\r\\nLocat...    30d+       False   \n",
       " 386  Description\\r\\nFounded in 1856, Banque Interna...    30d+       False   \n",
       " 387  Technician iPSC NIM (F/M)\\r\\n(Valid from 28/03...     14d       False   \n",
       " 388  This position will cover functional and techni...    30d+       False   \n",
       " 389  Senior Data Engineer (f/m/d)\\r\\n\\r\\nTeam spiri...    30d+       False   \n",
       " \n",
       "     Salary    Employees     Type_of_ownership  ... CEO_approval  \\\n",
       " 0      NaN       10000+      Company - Public  ...         0.91   \n",
       " 1      NaN       10000+      Company - Public  ...         0.91   \n",
       " 2      NaN       10000+      Company - Public  ...         0.91   \n",
       " 3      NaN       10000+      Company - Public  ...         0.91   \n",
       " 4      NaN      1 to 50     Company - Private  ...          NaN   \n",
       " ..     ...          ...                   ...  ...          ...   \n",
       " 385    NaN       10000+     Company - Private  ...         0.82   \n",
       " 386    NaN      1 to 50     Company - Private  ...         0.99   \n",
       " 387    NaN          NaN  College / University  ...         0.90   \n",
       " 388    NaN          NaN     Company - Private  ...          NaN   \n",
       " 389    NaN  501 to 1000     Company - Private  ...         0.84   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     3.8             3.9              4.1                3.6   \n",
       " 1                     3.8             3.9              4.1                3.6   \n",
       " 2                     3.8             3.9              4.1                3.6   \n",
       " 3                     3.8             3.9              4.1                3.6   \n",
       " 4                     4.6             4.6              4.3                4.2   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 385                   3.3             3.3              3.3                3.0   \n",
       " 386                   3.0             3.4              3.3                3.0   \n",
       " 387                   3.5             4.0              3.9                3.7   \n",
       " 388                   NaN             NaN              NaN                NaN   \n",
       " 389                   3.0             3.1              3.3                2.8   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  4.1  ['\"salary is good for newbie\" (in 450 reviews)...   \n",
       " 1                  4.1  ['\"salary is good for newbie\" (in 450 reviews)...   \n",
       " 2                  4.1  ['\"salary is good for newbie\" (in 450 reviews)...   \n",
       " 3                  4.1  ['\"salary is good for newbie\" (in 450 reviews)...   \n",
       " 4                  3.1  ['\"great trainings and development in company\"...   \n",
       " ..                 ...                                                ...   \n",
       " 385                3.4  ['\"Good pay for such an easy job\" (in 39 revie...   \n",
       " 386                3.9  ['\"Salary is being paid ontime\" (in 8 reviews)...   \n",
       " 387                4.1  ['\"Salary is good.\" (in 52 reviews)', '\"Great ...   \n",
       " 388                NaN                                                NaN   \n",
       " 389                3.6  ['\"good work life balance, good colleagues, in...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"salaries are low in terms of the current co...              4.0   \n",
       " 1    ['\"salaries are low in terms of the current co...              4.0   \n",
       " 2    ['\"salaries are low in terms of the current co...              4.0   \n",
       " 3    ['\"salaries are low in terms of the current co...              4.0   \n",
       " 4    ['\"L\\'inconvénient est le micro management\" (i...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 385  ['\"Pay is average\" (in 39 reviews)', '\"Bad Man...              3.6   \n",
       " 386  ['\"Salary low for new joiners\" (in 8 reviews)'...              NaN   \n",
       " 387  ['\"Fixed salary, not possible to advance\" (in ...              NaN   \n",
       " 388                                                NaN              NaN   \n",
       " 389  ['\"Disorganized with bad management.\" (in 7 re...              5.0   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Vacation & Paid Time Off (15 comments)\\n\"Off...  \n",
       " 1    ['Vacation & Paid Time Off (15 comments)\\n\"Off...  \n",
       " 2    ['Vacation & Paid Time Off (15 comments)\\n\"Off...  \n",
       " 3    ['Vacation & Paid Time Off (15 comments)\\n\"Off...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 385  ['Health Insurance (3 comments)\\n\"Not bad. Not...  \n",
       " 386                                                NaN  \n",
       " 387                                                NaN  \n",
       " 388                                                NaN  \n",
       " 389                                                NaN  \n",
       " \n",
       " [390 rows x 25 columns],\n",
       " 'Netherlands':                                Company_name  Rating                Location  \\\n",
       " 0                                      ARAG     4.6                 Leusden   \n",
       " 1                               Acknowledge     4.5               Eindhoven   \n",
       " 2    De Rooy Slijpcentrum BV | Precision BV     NaN                  Nuenen   \n",
       " 3                                    Axians     3.6  Capelle aan den IJssel   \n",
       " 4    De Rooy Slijpcentrum BV | Precision BV     NaN                  Nuenen   \n",
       " ..                                      ...     ...                     ...   \n",
       " 895                                    ASML     4.1               Veldhoven   \n",
       " 896                                 Experis     3.6                   Gouda   \n",
       " 897           Noordzee Helikopter Nederland     NaN              Den Helder   \n",
       " 898                Randstad Groep Nederland     3.9                  Diemen   \n",
       " 899                                NN Group     3.9               The Hague   \n",
       " \n",
       "                           Job_title  \\\n",
       " 0                Lead Data Engineer   \n",
       " 1                     Data Engineer   \n",
       " 2             Supply Chain Engineer   \n",
       " 3      Services Engineer Networking   \n",
       " 4                      QHSE Manager   \n",
       " ..                              ...   \n",
       " 895  Software Applications Engineer   \n",
       " 896                   Data Engineer   \n",
       " 897      Helicopter Engineer AW-139   \n",
       " 898                  Data modelleur   \n",
       " 899                   Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Op zoek naar een organisatie met ambitieuze do...    30d+        True   \n",
       " 1    Locatie: Eindhoven (Waalre), Nederland\\r\\nUren...    30d+        True   \n",
       " 2    Jouw functie\\r\\nDe Rooy groeit en bereidt zich...    30d+        True   \n",
       " 3    Hoe ziet jouw uitdaging eruit?\\r\\nJe werkt voo...    30d+       False   \n",
       " 4    Jouw functie\\r\\nDe Rooy groeit en bereidt zich...    30d+        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Introduction to the job\\r\\nThis vacancy is a p...     12d       False   \n",
       " 896  Are you passionate about Data? Do you have kno...    30d+       False   \n",
       " 897  JOB SUMMARY\\r\\nThe helicopter B1.3 Engineer is...     12d        True   \n",
       " 898  As a data modeler you are responsible for deve...    30d+       False   \n",
       " 899  We are looking for a Data Engineer to grow our...    30d+       False   \n",
       " \n",
       "                           Salary      Employees  \\\n",
       " 0                            NaN   1001 to 5000   \n",
       " 1      €4K - €5K (Employer est.)     201 to 500   \n",
       " 2      €3K - €5K (Employer est.)            NaN   \n",
       " 3                            NaN         10000+   \n",
       " 4      €4K - €6K (Employer est.)            NaN   \n",
       " ..                           ...            ...   \n",
       " 895                          NaN         10000+   \n",
       " 896    €4K - €5K (Employer est.)  5001 to 10000   \n",
       " 897    €4K - €5K (Employer est.)            NaN   \n",
       " 898  €75K - €95K (Employer est.)    501 to 1000   \n",
       " 899    €4K - €6K (Employer est.)         10000+   \n",
       " \n",
       "                   Type_of_ownership  ... CEO_approval  Career_opportunities  \\\n",
       " 0                  Company - Public  ...         0.87                   3.3   \n",
       " 1                 Company - Private  ...          NaN                   4.5   \n",
       " 2                 Company - Private  ...          NaN                   NaN   \n",
       " 3    Subsidiary or Business Segment  ...         0.79                   3.3   \n",
       " 4                 Company - Private  ...          NaN                   NaN   \n",
       " ..                              ...  ...          ...                   ...   \n",
       " 895                Company - Public  ...         0.95                   4.0   \n",
       " 896  Subsidiary or Business Segment  ...         0.74                   3.4   \n",
       " 897                             NaN  ...          NaN                   NaN   \n",
       " 898                Company - Public  ...         0.82                   3.7   \n",
       " 899                Company - Public  ...         0.82                   3.5   \n",
       " \n",
       "     Comp_&_benefits Culture_&_values  Senior_management  Work/Life_balance  \\\n",
       " 0               3.6              3.9                3.4                3.8   \n",
       " 1               3.8              5.0                3.8                4.7   \n",
       " 2               NaN              NaN                NaN                NaN   \n",
       " 3               3.4              3.7                3.2                3.6   \n",
       " 4               NaN              NaN                NaN                NaN   \n",
       " ..              ...              ...                ...                ...   \n",
       " 895             3.9              4.1                3.6                3.9   \n",
       " 896             3.2              3.5                3.4                3.6   \n",
       " 897             NaN              NaN                NaN                NaN   \n",
       " 898             3.4              3.9                3.7                3.8   \n",
       " 899             3.5              3.8                3.5                4.0   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Great Benefits, New Office, Friendly People...   \n",
       " 1                                                  NaN   \n",
       " 2                                                  NaN   \n",
       " 3    ['\"Good salary\" (in 31 reviews)', '\"People are...   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 895  ['\"a company that thinks about the employers 2...   \n",
       " 896  ['\"Decent pay and if you get a short enough co...   \n",
       " 897                                                NaN   \n",
       " 898  ['\"Staring salary was good and co\" (in 790 rev...   \n",
       " 899  ['\"Good environment, good management, good sal...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Can be quite cliquey but people are nice if...              4.6   \n",
       " 1                                                  NaN              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3    ['\"Low salaries for the area\" (in 31 reviews)'...              NaN   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Low salary in the market\" (in 163 reviews)'...              4.1   \n",
       " 896  ['\"Low pay, most of the workers were at minimu...              3.0   \n",
       " 897                                                NaN              NaN   \n",
       " 898  ['\"Salary is low and Office Transportation is ...              3.0   \n",
       " 899  ['\"Czech buracretic culture exists in NN IT Hu...              4.0   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Maternity & Paternity Leave (4 comments)\\n\"A...  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895  ['Vacation & Paid Time Off (47 comments)\\n\"Pre...  \n",
       " 896  ['Health Insurance (67 comments)\\n\"It is good ...  \n",
       " 897                                                NaN  \n",
       " 898  ['Health Insurance (50 comments)\\n\"Excelente s...  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'New_Zealand':                Company_name  Rating     Location  \\\n",
       " 0                  ICE BASE     NaN     Auckland   \n",
       " 1            Precision Data     NaN   Wellington   \n",
       " 2                        EY     3.9     Auckland   \n",
       " 3    New Zealand Government     4.1   Wellington   \n",
       " 4                  Valocity     5.0     Auckland   \n",
       " ..                      ...     ...          ...   \n",
       " 385    JOBsmith Recruitment     NaN   Wellington   \n",
       " 386           RecruitNet NZ     NaN  East Tamaki   \n",
       " 387          H2R Consulting     3.7   Wellington   \n",
       " 388                    ZURU     4.3     Auckland   \n",
       " 389          Rocket Lab USA     3.2     Auckland   \n",
       " \n",
       "                                   Job_title  \\\n",
       " 0                             Data Engineer   \n",
       " 1                      Senior Data Engineer   \n",
       " 2                             Data Engineer   \n",
       " 3                             Data Engineer   \n",
       " 4    Data Engineer (Auckland or Wellington)   \n",
       " ..                                      ...   \n",
       " 385     Projects and Maintenance Technician   \n",
       " 386                        Design Architect   \n",
       " 387                    Enterprise Architect   \n",
       " 388                Digital Executive - Toys   \n",
       " 389                C++ Software Engineer II   \n",
       " \n",
       "                                            Description    Job_age  Easy_apply  \\\n",
       " 0    Phoenix Generation is a career pathway-focused...       30d+        True   \n",
       " 1    Why we need you & what you will be doing\\r\\nWe...        10d        True   \n",
       " 2    At EY, you’ll have the chance to build a caree...         3d       False   \n",
       " 3    Be a Data Warehouse expert while aligning data...       30d+       False   \n",
       " 4    About Valocity:\\r\\n\\r\\nOur award-winning team ...       30d+       False   \n",
       " ..                                                 ...        ...         ...   \n",
       " 385  Projects and Maintenance Technician\\r\\n\\r\\nVeo...       30d+       False   \n",
       " 386  Leading a team of 5 - this role has an attract...  1 day ago        True   \n",
       " 387  Location:\\r\\nWellington\\r\\nType:\\r\\nPermanent ...        23d       False   \n",
       " 388  Experience in paid digital media advertising, ...  1 day ago        True   \n",
       " 389  Rocket Lab\\r\\nRocket Lab is a vertically integ...       30d+        True   \n",
       " \n",
       "                                  Salary    Employees  Type_of_ownership  ...  \\\n",
       " 0                NZ$80K (Employer est.)          NaN   Company - Public  ...   \n",
       " 1     NZ$110K - NZ$145K (Employer est.)          NaN                NaN  ...   \n",
       " 2    NZ$114K - NZ$140K (Glassdoor est.)          NaN                NaN  ...   \n",
       " 3     NZ$84K - NZ$131K (Glassdoor est.)       10000+         Government  ...   \n",
       " 4     NZ$90K - NZ$127K (Glassdoor est.)      1 to 50  Company - Private  ...   \n",
       " ..                                  ...          ...                ...  ...   \n",
       " 385   NZ$62K - NZ$102K (Glassdoor est.)      1 to 50  Company - Private  ...   \n",
       " 386   NZ$60K - NZ$110K (Glassdoor est.)          NaN   Company - Public  ...   \n",
       " 387  NZ$139K - NZ$161K (Glassdoor est.)      1 to 50  Company - Private  ...   \n",
       " 388    NZ$53K - NZ$75K (Glassdoor est.)  501 to 1000                NaN  ...   \n",
       " 389   NZ$90K - NZ$130K (Glassdoor est.)  501 to 1000  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1            NaN                   NaN             NaN              NaN   \n",
       " 2            NaN                   NaN             NaN              NaN   \n",
       " 3           1.00                   3.9             3.2              3.8   \n",
       " 4            NaN                   4.0             2.0              5.0   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 385          NaN                   NaN             NaN              NaN   \n",
       " 386          NaN                   NaN             NaN              NaN   \n",
       " 387          NaN                   3.0             2.0              4.0   \n",
       " 388          NaN                   4.2             3.9              4.2   \n",
       " 389         0.88                   3.4             2.9              3.0   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  NaN                NaN   \n",
       " 2                  NaN                NaN   \n",
       " 3                  3.3                3.5   \n",
       " 4                  5.0                5.0   \n",
       " ..                 ...                ...   \n",
       " 385                NaN                NaN   \n",
       " 386                NaN                NaN   \n",
       " 387                NaN                NaN   \n",
       " 388                4.0                3.8   \n",
       " 389                2.8                2.4   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1                                                  NaN   \n",
       " 2                                                  NaN   \n",
       " 3    ['\"Job security, Work life balance ,Training, ...   \n",
       " 4                   ['\"Great culture\" (in 1 reviews)']   \n",
       " ..                                                 ...   \n",
       " 385                                                NaN   \n",
       " 386                                                NaN   \n",
       " 387           ['\"great communication\" (in 2 reviews)']   \n",
       " 388  ['\"Ambitious people driven to succeed.\" (in 5 ...   \n",
       " 389  ['\"Can not stress this enough, the people are ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1                                                  NaN              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 4    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 385                                                NaN              NaN   \n",
       " 386                                                NaN              NaN   \n",
       " 387          ['\"Parking is expensive\" (in 1 reviews)']              NaN   \n",
       " 388  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 389  ['\"Now more than ever people need employers wh...              3.8   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 385                                                NaN  \n",
       " 386                                                NaN  \n",
       " 387                                                NaN  \n",
       " 388                                                NaN  \n",
       " 389  ['Health Insurance (3 comments)\\n\"I use my par...  \n",
       " \n",
       " [390 rows x 25 columns],\n",
       " 'Norway':            Company_name  Rating Location  \\\n",
       " 0              EUMETSAT     3.4   Norway   \n",
       " 1              EUMETSAT     3.4   Norway   \n",
       " 2    Riverty Group GmbH     4.0     Oslo   \n",
       " 3                 Cambi     NaN   Remote   \n",
       " 4                  Bekk     4.8   Norway   \n",
       " ..                  ...     ...      ...   \n",
       " 355          Mastercard     4.3     Oslo   \n",
       " 356               TIDAL     4.2     Oslo   \n",
       " 357                CBRE     4.0     Oslo   \n",
       " 358     Askeladden & Co     4.7     Oslo   \n",
       " 359         ONNEC Group     3.2     Oslo   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0           Cloud Services Engineer (Based in Germany)   \n",
       " 1                 Contracts Officer (Based in Germany)   \n",
       " 2      Data Governance Engineer (m/f/d) 80% Homeoffice   \n",
       " 3                                        Data Engineer   \n",
       " 4    Sommerjobb for data scientists / data engineer...   \n",
       " ..                                                 ...   \n",
       " 355                    Infrastructure Engineer | SWIFT   \n",
       " 356                               Data Engineer (Java)   \n",
       " 357                      ICT Engineer / ICT Technician   \n",
       " 358                                   Data/ML Engineer   \n",
       " 359           Cable Installation Engineer (freelancer)   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Cloud Services Engineer\\r\\nEUMETSAT is Europe’...     20d       False   \n",
       " 1    Contracts Officer\\r\\nEUMETSAT is Europe’s mete...     15d       False   \n",
       " 2    Everything we do, starts with you.\\r\\nTogether...    30d+       False   \n",
       " 3    Cambi has developed our own tool to help our c...     21d       False   \n",
       " 4    En relevant sommerjobb er gull verdt for deg s...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 355  Our Purpose\\r\\nWe work to connect and power an...    30d+       False   \n",
       " 356  Company Description\\r\\n\\r\\nTIDAL is a global m...      8d        True   \n",
       " 357  Posted\\r\\n20-Mar-2023\\r\\nService line\\r\\nGWS S...     22d       False   \n",
       " 358  Data/ML Engineer - 2023\\r\\nAskeladden & Co har...    30d+       False   \n",
       " 359  ONNEC Group are a leading independent technolo...     29d        True   \n",
       " \n",
       "                      Salary      Employees  Type_of_ownership  ...  \\\n",
       " 0    NOK 8K (Employer est.)     201 to 500      Self-employed  ...   \n",
       " 1    NOK 8K (Employer est.)     201 to 500      Self-employed  ...   \n",
       " 2                       NaN  5001 to 10000  Company - Private  ...   \n",
       " 3                       NaN      51 to 200  Company - Private  ...   \n",
       " 4                       NaN     201 to 500  Company - Private  ...   \n",
       " ..                      ...            ...                ...  ...   \n",
       " 355                     NaN         10000+   Company - Public  ...   \n",
       " 356                     NaN  5001 to 10000   Company - Public  ...   \n",
       " 357                     NaN         10000+   Company - Public  ...   \n",
       " 358                     NaN            NaN  Company - Private  ...   \n",
       " 359                     NaN    501 to 1000  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.49                   2.4             3.7              2.9   \n",
       " 1           0.49                   2.4             3.7              2.9   \n",
       " 2           1.00                   3.7             3.3              4.0   \n",
       " 3            NaN                   NaN             NaN              NaN   \n",
       " 4            NaN                   4.1             3.8              4.9   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 355         0.95                   4.1             4.1              4.3   \n",
       " 356         0.89                   3.9             4.0              4.3   \n",
       " 357         0.90                   3.8             3.6              3.8   \n",
       " 358         1.00                   4.3             2.6              5.0   \n",
       " 359          NaN                   2.4             3.2              2.8   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  2.6                3.1   \n",
       " 1                  2.6                3.1   \n",
       " 2                  3.6                4.1   \n",
       " 3                  NaN                NaN   \n",
       " 4                  4.4                4.8   \n",
       " ..                 ...                ...   \n",
       " 355                3.9                4.2   \n",
       " 356                3.8                4.1   \n",
       " 357                3.5                3.6   \n",
       " 358                5.0                4.6   \n",
       " 359                3.3                3.3   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"this company can offer good salary, on the ...   \n",
       " 1    ['\"this company can offer good salary, on the ...   \n",
       " 2    ['\"Good company, good salary, time\" (in 20 rev...   \n",
       " 3                                                  NaN   \n",
       " 4    ['\"Smart colleagues\" (in 3 reviews)', '\"Good s...   \n",
       " ..                                                 ...   \n",
       " 355  ['\"Good work life balance with projects around...   \n",
       " 356  ['\"Great benefits and a lot of trust\" (in 143 ...   \n",
       " 357  ['\"Good benefits and overtime there\" (in 670 r...   \n",
       " 358  ['\"Great people\" (in 1 reviews)', '\"Amazing cu...   \n",
       " 359  ['\"Some of the staff were nice that were NOT o...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Some heads are creating a negative culture ...              NaN   \n",
       " 1    ['\"Some heads are creating a negative culture ...              NaN   \n",
       " 2    ['\"Poor salaries\" (in 20 reviews)', '\"Pathetic...              4.0   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"career path is not fully clear\" (in 1 revie...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 355  ['\"There is no work life balance\" (in 668 revi...              4.4   \n",
       " 356  ['\"No higher learning benefits are a downer\" (...              4.6   \n",
       " 357  ['\"the benefits could be better\" (in 670 revie...              3.6   \n",
       " 358  ['\"Not that good pay, regarding other companie...              NaN   \n",
       " 359  ['\"managers no care for staff, staff not happy...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 355  ['Vacation & Paid Time Off (125 comments)\\n\"45...  \n",
       " 356  ['Maternity & Paternity Leave (46 comments)\\n\"...  \n",
       " 357  ['Health Insurance (203 comments)\\n\"Pretty bas...  \n",
       " 358                                                NaN  \n",
       " 359                                                NaN  \n",
       " \n",
       " [360 rows x 25 columns],\n",
       " 'Poland':                          Company_name  Rating Location  \\\n",
       " 0                             Dropbox     4.6   Poland   \n",
       " 1                             Dropbox     4.6   Poland   \n",
       " 2                             Dropbox     4.6   Poland   \n",
       " 3                             Dropbox     4.6   Poland   \n",
       " 4                             Dropbox     4.6   Poland   \n",
       " ..                                ...     ...      ...   \n",
       " 895                            Onwelo     3.9   Gdańsk   \n",
       " 896                       ClearCourse     4.0   Kraków   \n",
       " 897  BAE Systems Digital Intelligence     3.7   Poznań   \n",
       " 898                            Onwelo     3.9   Poland   \n",
       " 899                          Comscore     3.4   Remote   \n",
       " \n",
       "                                     Job_title  \\\n",
       " 0           Backend Product Software Engineer   \n",
       " 1         Front End Product Software Engineer   \n",
       " 2            Infrastructure Software Engineer   \n",
       " 3    Senior Backend Product Software Engineer   \n",
       " 4     Senior Infrastructure Software Engineer   \n",
       " ..                                        ...   \n",
       " 895                 Data Engineer (Microsoft)   \n",
       " 896                        Manual QA Engineer   \n",
       " 897                             Data Engineer   \n",
       " 898                         SQL Data Engineer   \n",
       " 899                             Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Role Description\\r\\nOur strategy starts with o...    30d+       False   \n",
       " 1    Role Description\\r\\nOur strategy starts with o...    30d+       False   \n",
       " 2    Role Description\\r\\nOur Dropbox Infrastructure...    30d+       False   \n",
       " 3    Role Description\\r\\nOur strategy starts with o...    30d+       False   \n",
       " 4    Role Description\\r\\nOur Dropbox Infrastructure...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Poznaj Onwelo:\\r\\nOnwelo to nowoczesna polska ...     20d       False   \n",
       " 896  Career Level: 08 Career\\r\\nPosting Date: 02-Ap...    30d+       False   \n",
       " 897  Job Details\\r\\nLocation: Poland, Poznań\\r\\n\\r\\...    30d+       False   \n",
       " 898  Poznaj Onwelo:\\r\\nOnwelo to nowoczesna polska ...    30d+       False   \n",
       " 899  Hi there!\\r\\nWe're ComScore, a product company...      5d        True   \n",
       " \n",
       "                                 Salary     Employees  \\\n",
       " 0                                  NaN  1001 to 5000   \n",
       " 1                                  NaN  1001 to 5000   \n",
       " 2                                  NaN  1001 to 5000   \n",
       " 3                                  NaN  1001 to 5000   \n",
       " 4                                  NaN  1001 to 5000   \n",
       " ..                                 ...           ...   \n",
       " 895                                NaN    201 to 500   \n",
       " 896                                NaN   501 to 1000   \n",
       " 897                                NaN  1001 to 5000   \n",
       " 898                                NaN    201 to 500   \n",
       " 899  PLN 20K - PLN 30K (Employer est.)  1001 to 5000   \n",
       " \n",
       "                   Type_of_ownership  ... CEO_approval  Career_opportunities  \\\n",
       " 0                 Company - Private  ...         0.95                   4.3   \n",
       " 1                 Company - Private  ...         0.95                   4.3   \n",
       " 2                 Company - Private  ...         0.95                   4.3   \n",
       " 3                 Company - Private  ...         0.95                   4.3   \n",
       " 4                 Company - Private  ...         0.95                   4.3   \n",
       " ..                              ...  ...          ...                   ...   \n",
       " 895               Company - Private  ...          NaN                   4.6   \n",
       " 896               Company - Private  ...         1.00                   3.9   \n",
       " 897  Subsidiary or Business Segment  ...          NaN                   4.0   \n",
       " 898               Company - Private  ...          NaN                   4.6   \n",
       " 899                Company - Public  ...         0.67                   3.0   \n",
       " \n",
       "     Comp_&_benefits Culture_&_values  Senior_management  Work/Life_balance  \\\n",
       " 0               4.7              4.7                4.2                4.7   \n",
       " 1               4.7              4.7                4.2                4.7   \n",
       " 2               4.7              4.7                4.2                4.7   \n",
       " 3               4.7              4.7                4.2                4.7   \n",
       " 4               4.7              4.7                4.2                4.7   \n",
       " ..              ...              ...                ...                ...   \n",
       " 895             4.0              3.9                4.0                4.2   \n",
       " 896             3.8              3.9                3.9                4.0   \n",
       " 897             3.7              4.0                3.9                3.7   \n",
       " 898             4.0              3.9                4.0                4.2   \n",
       " 899             3.2              3.4                3.0                4.1   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Some teams have really good culture.\" (in 7...   \n",
       " 1    ['\"Some teams have really good culture.\" (in 7...   \n",
       " 2    ['\"Some teams have really good culture.\" (in 7...   \n",
       " 3    ['\"Some teams have really good culture.\" (in 7...   \n",
       " 4    ['\"Some teams have really good culture.\" (in 7...   \n",
       " ..                                                 ...   \n",
       " 895  ['\"good salary, B2B opportunities, nice people...   \n",
       " 896  ['\"Always ready to spend of learning and devel...   \n",
       " 897  ['\"Friendly people\" (in 33 reviews)', '\"Suppor...   \n",
       " 898  ['\"good salary, B2B opportunities, nice people...   \n",
       " 899  ['\"The main pro is that there are some really ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Eng culture very conservative, career growt...              4.8   \n",
       " 1    ['\"Eng culture very conservative, career growt...              4.8   \n",
       " 2    ['\"Eng culture very conservative, career growt...              4.8   \n",
       " 3    ['\"Eng culture very conservative, career growt...              4.8   \n",
       " 4    ['\"Eng culture very conservative, career growt...              4.8   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"management, not cooperative, no additional ...              NaN   \n",
       " 896  ['\"Career development.\" (in 4 reviews)', '\"Poo...              NaN   \n",
       " 897  ['\"Management are borderline incompetent at ti...              3.8   \n",
       " 898  ['\"management, not cooperative, no additional ...              NaN   \n",
       " 899  ['\"&gt; Not a competitive pay.\" (in 6 reviews)...              3.7   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Maternity & Paternity Leave (50 comments)\\n\"...  \n",
       " 1    ['Maternity & Paternity Leave (50 comments)\\n\"...  \n",
       " 2    ['Maternity & Paternity Leave (50 comments)\\n\"...  \n",
       " 3    ['Maternity & Paternity Leave (50 comments)\\n\"...  \n",
       " 4    ['Maternity & Paternity Leave (50 comments)\\n\"...  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899  ['Health Insurance (28 comments)\\n\"Average hea...  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Portugal':            Company_name  Rating             Location  \\\n",
       " 0           Fogbyte Lda     NaN             Portugal   \n",
       " 1              Arkadium     4.1             Portugal   \n",
       " 2              Leafwell     4.0          Home office   \n",
       " 3    Hala Systems, Inc.     NaN             Portugal   \n",
       " 4                Essity     4.0             Portugal   \n",
       " ..                  ...     ...                  ...   \n",
       " 895  AdNovum Informatik     4.4             Portugal   \n",
       " 896             Watt-IS     5.0            Carnaxide   \n",
       " 897          Spin.Works     3.2  São João da Madeira   \n",
       " 898             Flix SE     3.8             Portugal   \n",
       " 899             Xpandit     3.9             Portugal   \n",
       " \n",
       "                                       Job_title  \\\n",
       " 0                    Senior AWS DevOps Engineer   \n",
       " 1                          Full Stack Developer   \n",
       " 2                                 Data Engineer   \n",
       " 3          Software Engineer, Data Architecture   \n",
       " 4        Ingestion Senior Data Engineer (f/m/d)   \n",
       " ..                                          ...   \n",
       " 895                    Senior Software Engineer   \n",
       " 896                        Electronics Engineer   \n",
       " 897        Senior DSP/Image Processing Designer   \n",
       " 898  Team Lead Analytics Marketing Intelligence   \n",
       " 899                           Platform Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    As a modern IT service provider,\\r\\nwe impleme...    30d+        True   \n",
       " 1    Trying to be part of the next great online gam...    30d+        True   \n",
       " 2    Please visit our career page to apply: https:/...    30d+        True   \n",
       " 3    How to apply:\\r\\nPlease submit your resume and...    30d+        True   \n",
       " 4    Ingestion Senior Data Engineer (f/m/d)\\r\\nAre ...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  What you're going to do\\r\\nOur development cen...    30d+       False   \n",
       " 896  Work with us\\r\\nEmpowering clients through ene...    30d+       False   \n",
       " 897  Job Description\\r\\nAnalyse, design, develop an...    30d+       False   \n",
       " 898  We are looking for a Team Lead Marketing Intel...     14d       False   \n",
       " 899  Problemas complexos exigem a experiência certa...    30d+       False   \n",
       " \n",
       "                           Salary     Employees  Type_of_ownership  ...  \\\n",
       " 0                            NaN           NaN                NaN  ...   \n",
       " 1                            NaN     51 to 200  Company - Private  ...   \n",
       " 2    €71K - €72K (Employer est.)     51 to 200   Company - Public  ...   \n",
       " 3                            NaN           NaN                NaN  ...   \n",
       " 4                            NaN        10000+   Company - Public  ...   \n",
       " ..                           ...           ...                ...  ...   \n",
       " 895                          NaN   501 to 1000  Company - Private  ...   \n",
       " 896                          NaN           NaN   Company - Public  ...   \n",
       " 897                          NaN       1 to 50  Company - Private  ...   \n",
       " 898                          NaN  1001 to 5000  Company - Private  ...   \n",
       " 899                          NaN    201 to 500  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1           0.85                   3.8             3.9              4.2   \n",
       " 2            NaN                   3.7             3.9              4.2   \n",
       " 3            NaN                   NaN             NaN              NaN   \n",
       " 4           0.86                   3.4             3.7              4.0   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895         0.98                   4.1             3.8              4.6   \n",
       " 896          NaN                   3.0             2.0              2.0   \n",
       " 897          NaN                   2.2             1.9              2.6   \n",
       " 898         0.90                   3.3             2.9              4.0   \n",
       " 899         0.85                   4.0             3.3              4.2   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  3.9                4.2   \n",
       " 2                  4.0                4.2   \n",
       " 3                  NaN                NaN   \n",
       " 4                  3.4                3.7   \n",
       " ..                 ...                ...   \n",
       " 895                4.3                4.5   \n",
       " 896                2.0                5.0   \n",
       " 897                2.4                3.3   \n",
       " 898                3.4                4.1   \n",
       " 899                3.5                4.2   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1    ['\"Easy job And a big salary\" (in 4 reviews)',...   \n",
       " 2    ['\"Great coworkers, quick communication, enjoy...   \n",
       " 3                                                  NaN   \n",
       " 4    ['\"Good work life balance\" (in 51 reviews)', '...   \n",
       " ..                                                 ...   \n",
       " 895  ['\"People can be friendly.\" (in 17 reviews)', ...   \n",
       " 896                                                NaN   \n",
       " 897  ['\"Friendly management, who make an honest eff...   \n",
       " 898  ['\"term contract, good salary\" (in 55 reviews)...   \n",
       " 899  ['\"Remote above average salary Great place to ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['\"Salary comes intermittently строгий работод...              3.7   \n",
       " 2    ['\"Bad pay\" (in 2 reviews)', '\"Poor leadership...              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"Can do with better work life balance\" (in 5...              3.5   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Not the highest salaries on the market\" (in...              NaN   \n",
       " 896                                                NaN              NaN   \n",
       " 897  ['\"Nonexistent management, too far from that.\"...              NaN   \n",
       " 898  ['\"company has grown too fast; salaries are lo...              5.0   \n",
       " 899  ['\"Low salary compared to the market\" (in 29 r...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4    ['Health Insurance (5 comments)\\n\"There are qu...  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Romania':                   Company_name  Rating   Location  \\\n",
       " 0                 Mconnect Ltd     NaN     Băbeni   \n",
       " 1    Da Vinci Engineering GmbH     4.0  Timişoara   \n",
       " 2                     EUMETSAT     3.4    Romania   \n",
       " 3                  CrowdStrike     4.3  Bucharest   \n",
       " 4                  CrowdStrike     4.3    Romania   \n",
       " ..                         ...     ...        ...   \n",
       " 655              Connect Group     3.4    Romania   \n",
       " 656        Segula Technologies     3.2  Bucharest   \n",
       " 657                 Vertiv Co.     3.3    Romania   \n",
       " 658              Connect Group     3.4    Romania   \n",
       " 659                 GE Digital     3.8  Bucharest   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                        Data Engineer   \n",
       " 1                   Functional Safety Engineer (m/f/d)   \n",
       " 2           Cloud Services Engineer (Based in Germany)   \n",
       " 3    Sr. Backend Software Engineer - Python, Go, Op...   \n",
       " 4    Backend Software Engineer (Go, AWS, Cassandra)...   \n",
       " ..                                                 ...   \n",
       " 655                                      Test Engineer   \n",
       " 656               PLM Release Support Engineer (seats)   \n",
       " 657                          Sales Support with German   \n",
       " 658                        Quality Test Engineer (CWS)   \n",
       " 659                  Sr Staff Build & Release Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    About us\\r\\nWe are professional, agile, and ou...     27d        True   \n",
       " 1    We are searching for intelligent and innovativ...    30d+       False   \n",
       " 2    Cloud Services Engineer\\r\\nEUMETSAT is Europe’...     21d       False   \n",
       " 3    #WeAreCrowdStrike and our mission is to stop b...    30d+       False   \n",
       " 4    #WeAreCrowdStrike and our mission is to stop b...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 655  Roemenië\\r\\nFull time\\r\\nElectrical Engineerin...    30d+       False   \n",
       " 656  Descrierea locului de muncă\\r\\nPLM Release wor...     21d        True   \n",
       " 657  Who are we?\\r\\nAt Vertiv, we build products th...    30d+       False   \n",
       " 658  Roemenië\\r\\nFull time\\r\\nElectrical Engineerin...    30d+       False   \n",
       " 659  Job Description Summary\\r\\nJoin us at GE Digit...     22d       False   \n",
       " \n",
       "                       Salary     Employees  Type_of_ownership  ...  \\\n",
       " 0    RON 20K (Employer est.)           NaN                NaN  ...   \n",
       " 1                        NaN   501 to 1000  Company - Private  ...   \n",
       " 2     RON 8K (Employer est.)    201 to 500      Self-employed  ...   \n",
       " 3                        NaN  1001 to 5000   Company - Public  ...   \n",
       " 4                        NaN  1001 to 5000   Company - Public  ...   \n",
       " ..                       ...           ...                ...  ...   \n",
       " 655                      NaN     51 to 200  Company - Private  ...   \n",
       " 656                      NaN        10000+  Company - Private  ...   \n",
       " 657                      NaN        10000+   Company - Public  ...   \n",
       " 658                      NaN     51 to 200  Company - Private  ...   \n",
       " 659                      NaN  1001 to 5000   Company - Public  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0            NaN                   NaN             NaN              NaN   \n",
       " 1           0.92                   3.6             3.5              4.2   \n",
       " 2           0.49                   2.4             3.7              2.9   \n",
       " 3           0.88                   4.1             4.2              4.1   \n",
       " 4           0.88                   4.1             4.2              4.1   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 655         0.68                   3.4             3.3              3.9   \n",
       " 656         0.60                   2.9             2.5              2.9   \n",
       " 657         0.60                   3.2             3.1              3.1   \n",
       " 658         0.68                   3.4             3.3              3.9   \n",
       " 659         0.88                   3.3             3.5              3.8   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  NaN                NaN   \n",
       " 1                  3.8                3.9   \n",
       " 2                  2.6                3.1   \n",
       " 3                  3.8                4.1   \n",
       " 4                  3.8                4.1   \n",
       " ..                 ...                ...   \n",
       " 655                3.1                3.8   \n",
       " 656                2.7                3.4   \n",
       " 657                2.9                3.2   \n",
       " 658                3.1                3.8   \n",
       " 659                3.3                3.9   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0                                                  NaN   \n",
       " 1    ['\"Flexible employer, great colleagues, optima...   \n",
       " 2    ['\"this company can offer good salary, on the ...   \n",
       " 3    ['\"Culture is great\" (in 113 reviews)', '\"Grea...   \n",
       " 4    ['\"Culture is great\" (in 113 reviews)', '\"Grea...   \n",
       " ..                                                 ...   \n",
       " 655  ['\"Good people to work with\" (in 3 reviews)', ...   \n",
       " 656  ['\"Remote working, good salary, you manage you...   \n",
       " 657  ['\"13th salary\" (in 126 reviews)', '\"The peopl...   \n",
       " 658  ['\"Good people to work with\" (in 3 reviews)', ...   \n",
       " 659  ['\"Work life balance Is good\" (in 103 reviews)...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0                                                  NaN              NaN   \n",
       " 1    ['\"Le management de certains responsable\" (in ...              NaN   \n",
       " 2    ['\"Some heads are creating a negative culture ...              NaN   \n",
       " 3    ['\"Remote first culture, but will see\" (in 113...              4.3   \n",
       " 4    ['\"Remote first culture, but will see\" (in 113...              4.3   \n",
       " ..                                                 ...              ...   \n",
       " 655       ['\"Too many hours, low pay\" (in 2 reviews)']              NaN   \n",
       " 656  ['\"Low salary compared to the market\" (in 44 r...              3.5   \n",
       " 657  ['\"Low salary at least for the electronics dep...              2.8   \n",
       " 658       ['\"Too many hours, low pay\" (in 2 reviews)']              NaN   \n",
       " 659  ['\"Not a good place for seniors who has person...              3.4   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Health Insurance (9 comments)\\n\"Excellent op...  \n",
       " 4    ['Health Insurance (9 comments)\\n\"Excellent op...  \n",
       " ..                                                 ...  \n",
       " 655                                                NaN  \n",
       " 656                                                NaN  \n",
       " 657  ['Health Insurance (25 comments)\\n\"it is a reg...  \n",
       " 658                                                NaN  \n",
       " 659  ['Health Insurance (32 comments)\\n\"With this e...  \n",
       " \n",
       " [660 rows x 25 columns],\n",
       " 'Singapore':                   Company_name  Rating   Location  \\\n",
       " 0                        Klook     3.9  Singapore   \n",
       " 1                           EY     3.9  Singapore   \n",
       " 2            Titansoft Pte Ltd     4.4  Singapore   \n",
       " 3                          PwC     3.9  Singapore   \n",
       " 4                ITCAN Pte Ltd     4.0  Singapore   \n",
       " ..                         ...     ...        ...   \n",
       " 895      D L RESOURCES PTE LTD     3.7  Singapore   \n",
       " 896  JPMorgan Chase Bank, N.A.     4.0  Singapore   \n",
       " 897                  OCBC Bank     3.5  Singapore   \n",
       " 898       TECHNOPALS PTE. LTD.     4.3  Singapore   \n",
       " 899                  OCBC Bank     3.5  Singapore   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                              Data Engineer, ETL (SG)   \n",
       " 1    SAS Data Engineer, Data & Analytics, Technolog...   \n",
       " 2                                        Data Engineer   \n",
       " 3                                 Junior Data Engineer   \n",
       " 4    Associate Data Centre Engineer (fresh graduate...   \n",
       " ..                                                 ...   \n",
       " 895          Data Engineer - ETL Informatica Developer   \n",
       " 896                      Data Centre Engineer, Analyst   \n",
       " 897  Data Engineer (EDW), Group Operations & Techno...   \n",
       " 898                   Data Management Analyst/Engineer   \n",
       " 899            End User and VIP Support Engineer, GO&T   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    About Klook\\r\\n\\r\\nKlook is the go-to travel a...      7d       False   \n",
       " 1    At EY, you’ll have the chance to build a caree...      4d       False   \n",
       " 2    About us\\r\\n\\r\\nIf you believe data makes the ...    30d+       False   \n",
       " 3    Line of Service\\r\\nInternal Firm Services\\r\\nI...      9d       False   \n",
       " 4    System & alert monitoring and escalation\\r\\nBa...     14d        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Job Objectives\\r\\nThe main objective is to ana...     21d       False   \n",
       " 896  As a member of our Technology Operations team,...      6d       False   \n",
       " 897  Data Engineer (EDW), Group Operations & Techno...    30d+       False   \n",
       " 898  Roles & Responsibilties:-\\r\\n• Experience in d...      4d       False   \n",
       " 899  End User and VIP Support Engineer, GO&T - (230...      3d       False   \n",
       " \n",
       "                                   Salary     Employees  Type_of_ownership  \\\n",
       " 0                                    NaN  1001 to 5000  Company - Private   \n",
       " 1    SGD 72K - SGD 108K (Glassdoor est.)           NaN                NaN   \n",
       " 2                                    NaN    201 to 500  Company - Private   \n",
       " 3                                    NaN        10000+  Company - Private   \n",
       " 4                                    NaN  1001 to 5000  Company - Private   \n",
       " ..                                   ...           ...                ...   \n",
       " 895     SGD 5K - SGD 10K (Employer est.)     51 to 200  Company - Private   \n",
       " 896                                  NaN        10000+   Company - Public   \n",
       " 897                                  NaN        10000+   Company - Public   \n",
       " 898     SGD 6K - SGD 12K (Employer est.)    201 to 500  Company - Private   \n",
       " 899                                  NaN        10000+   Company - Public   \n",
       " \n",
       "      ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0    ...         0.91                   3.7             3.6              4.0   \n",
       " 1    ...          NaN                   NaN             NaN              NaN   \n",
       " 2    ...          NaN                   3.9             4.1              4.3   \n",
       " 3    ...          NaN                   4.1             3.5              3.9   \n",
       " 4    ...         0.80                   3.9             3.4              3.6   \n",
       " ..   ...          ...                   ...             ...              ...   \n",
       " 895  ...          NaN                   3.1             2.2              3.0   \n",
       " 896  ...         0.88                   4.0             3.8              4.0   \n",
       " 897  ...         0.88                   3.4             3.5              3.3   \n",
       " 898  ...         1.00                   4.2             3.9              3.8   \n",
       " 899  ...         0.88                   3.4             3.5              3.3   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.6                3.9   \n",
       " 1                  NaN                NaN   \n",
       " 2                  4.0                3.7   \n",
       " 3                  3.7                3.0   \n",
       " 4                  3.6                3.6   \n",
       " ..                 ...                ...   \n",
       " 895                3.1                3.2   \n",
       " 896                3.6                3.6   \n",
       " 897                3.1                3.1   \n",
       " 898                3.9                3.9   \n",
       " 899                3.1                3.1   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Team is nice, less overtime, comfy office, ...   \n",
       " 1                                                  NaN   \n",
       " 2    ['\"Good learning opportunity\" (in 5 reviews)',...   \n",
       " 3    ['\"Good work / life balance\" (in 143 reviews)'...   \n",
       " 4    ['\"Welfare benefit are good.\" (in 6 reviews)',...   \n",
       " ..                                                 ...   \n",
       " 895  ['\"Salary is good for starters\" (in 3 reviews)...   \n",
       " 896  ['\"Work Life Balance is good\" (in 361 reviews)...   \n",
       " 897  ['\"Employer friendly, Work Life balance\" (in 1...   \n",
       " 898  ['\"Good Salary Yearly / Completion Bonus\" (in ...   \n",
       " 899  ['\"Comms and benefits were decent\" (in 9 revie...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0        ['\"Salary is below average.\" (in 2 reviews)']              4.3   \n",
       " 1                                                  NaN              NaN   \n",
       " 2    ['\"Pay is very average.\" (in 3 reviews)', '\"Ev...              NaN   \n",
       " 3    ['\"no work life balance &gt;&gt;&gt; do you ne...              4.1   \n",
       " 4       ['\"Benefits are kot so great\" (in 6 reviews)']              5.0   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Salary is lower than average\" (in 3 reviews...              NaN   \n",
       " 896  ['\"Hectic work No work life balance Late Night...              4.1   \n",
       " 897  ['\"No work life balance as well\" (in 11 review...              5.0   \n",
       " 898  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 899  ['\"Culture is toxic, people with old mindset a...              5.0   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Maternity & Paternity Leave (389 comments)\\n...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896  ['Health Insurance (981 comments)\\n\"great bene...  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'South_Korea':                             Company_name  Rating  Location  \\\n",
       " 0          Amzn Corporate Services Korea     NaN       NaN   \n",
       " 1                                 데브시스터즈     NaN       NaN   \n",
       " 2                                 티맵모빌리티     NaN       NaN   \n",
       " 3                  LINE Plus corporation     NaN       NaN   \n",
       " 4                                    플리토     NaN       NaN   \n",
       " ..                                   ...     ...       ...   \n",
       " 565  Siemens Digital Industries Software     NaN       NaN   \n",
       " 566                                유엘코리아     NaN       NaN   \n",
       " 567                               데브시스터즈     NaN       NaN   \n",
       " 568                               Google     NaN       NaN   \n",
       " 569                                  메가존     NaN       NaN   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0    APAC Global Expansion Engineer, Data Center De...   \n",
       " 1                [데브시스터즈] Data Engineer - Analytics/BI   \n",
       " 2                                        Data Engineer   \n",
       " 3                               Data Platform Engineer   \n",
       " 4                  Data/Machine Learning Engineer (신입)   \n",
       " ..                                                 ...   \n",
       " 565                     S/W Development Engineer - R&D   \n",
       " 566          Project Engineer - EV Battery Testing Lab   \n",
       " 567            [데브시스터즈] Machine Learning Engineer (경력)   \n",
       " 568        Customer Engineer, Networking, Google Cloud   \n",
       " 569                    [메가존클라우드] Hybrid Cloud Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Engineering degree in Electrical, Mechanical o...     24h       False   \n",
       " 1    div { position: relative; } a { height:3.5%; w...    30d+       False   \n",
       " 2    Data Engineer\\r\\n인터넷·IT·통신·모바일·게임>데이터베이스·DBA>데...    30d+       False   \n",
       " 3    데이터에 대한 가치와 중요도가 커짐에 따라 LINE도 체계적인 데이터 관리와 효율적...    30d+       False   \n",
       " 4    Data/Machine Learning Engineer 채용 (신입)\" 강한 책임감...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 565  At Siemens, we are always challenging ourselve...      2d       False   \n",
       " 566  개요Thousands of us around the world wake up eve...     24h       False   \n",
       " 567  div { position: relative; } a { height:3.5%; w...    30d+       False   \n",
       " 568  Google welcomes people with disabilities.\\r\\nM...      3d       False   \n",
       " 569  [Hybrid Cloud Architect]팀을 소개합니다!\\r\\nHybrid Cl...      8d       False   \n",
       " \n",
       "      Salary  Employees  Type_of_ownership  ...  CEO_approval  \\\n",
       " 0       NaN        NaN                NaN  ...           NaN   \n",
       " 1       NaN        NaN                NaN  ...           NaN   \n",
       " 2       NaN        NaN                NaN  ...           NaN   \n",
       " 3       NaN        NaN                NaN  ...           NaN   \n",
       " 4       NaN        NaN                NaN  ...           NaN   \n",
       " ..      ...        ...                ...  ...           ...   \n",
       " 565     NaN        NaN                NaN  ...           NaN   \n",
       " 566     NaN        NaN                NaN  ...           NaN   \n",
       " 567     NaN        NaN                NaN  ...           NaN   \n",
       " 568     NaN        NaN                NaN  ...           NaN   \n",
       " 569     NaN        NaN                NaN  ...           NaN   \n",
       " \n",
       "      Career_opportunities  Comp_&_benefits  Culture_&_values  \\\n",
       " 0                     NaN              NaN               NaN   \n",
       " 1                     NaN              NaN               NaN   \n",
       " 2                     NaN              NaN               NaN   \n",
       " 3                     NaN              NaN               NaN   \n",
       " 4                     NaN              NaN               NaN   \n",
       " ..                    ...              ...               ...   \n",
       " 565                   NaN              NaN               NaN   \n",
       " 566                   NaN              NaN               NaN   \n",
       " 567                   NaN              NaN               NaN   \n",
       " 568                   NaN              NaN               NaN   \n",
       " 569                   NaN              NaN               NaN   \n",
       " \n",
       "      Senior_management  Work/Life_balance  Pros  Cons  Benefits_rating  \\\n",
       " 0                  NaN                NaN   NaN   NaN              NaN   \n",
       " 1                  NaN                NaN   NaN   NaN              NaN   \n",
       " 2                  NaN                NaN   NaN   NaN              NaN   \n",
       " 3                  NaN                NaN   NaN   NaN              NaN   \n",
       " 4                  NaN                NaN   NaN   NaN              NaN   \n",
       " ..                 ...                ...   ...   ...              ...   \n",
       " 565                NaN                NaN   NaN   NaN              NaN   \n",
       " 566                NaN                NaN   NaN   NaN              NaN   \n",
       " 567                NaN                NaN   NaN   NaN              NaN   \n",
       " 568                NaN                NaN   NaN   NaN              NaN   \n",
       " 569                NaN                NaN   NaN   NaN              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 565               NaN  \n",
       " 566               NaN  \n",
       " 567               NaN  \n",
       " 568               NaN  \n",
       " 569               NaN  \n",
       " \n",
       " [570 rows x 25 columns],\n",
       " 'Spain':                 Company_name  Rating           Location  \\\n",
       " 0    Boston Consulting Group     4.4             Madrid   \n",
       " 1     Fichtner GmbH & Co. KG     3.8             Madrid   \n",
       " 2                  DerbySoft     4.0          Barcelona   \n",
       " 3    Boston Consulting Group     4.4             Madrid   \n",
       " 4                   HENSOLDT     4.0             Madrid   \n",
       " ..                       ...     ...                ...   \n",
       " 895                 Deloitte     4.0              Spain   \n",
       " 896       Adroit People Ltd.     4.5  Trabajo en remoto   \n",
       " 897                Accenture     4.1              Spain   \n",
       " 898               Clevertask     3.3          Barcelona   \n",
       " 899               KPMG Spain     4.0           Zaragoza   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                                        Data Engineer   \n",
       " 1    Project Manager / Engineer Smart Cities & Smar...   \n",
       " 2                           Technical Service Engineer   \n",
       " 3                                  Senior IT Architect   \n",
       " 4    Systems Engineer Future Combat Air System Relo...   \n",
       " ..                                                 ...   \n",
       " 895                                      Data Engineer   \n",
       " 896                                      Data Engineer   \n",
       " 897     Data Engineer _ Analytics and Modeling analyst   \n",
       " 898               Data Analyst/ Engineer Junior (Beca)   \n",
       " 899                               Junior Data Engineer   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    WHAT YOU'LL DO\\r\\n\\r\\nAs a part of BCG’s GAMMA...    30d+       False   \n",
       " 1    About Us\\r\\nWith over 2.000 employees, Fichtne...     29d       False   \n",
       " 2    Job Title: Technical Service Engineer\\r\\nLocat...    30d+        True   \n",
       " 3    Who We Are\\r\\n\\r\\nBoston Consulting Group part...    30d+       False   \n",
       " 4    For the area \"Solution and Cross Functional En...     11d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  ¿Quieres desarrollar tu carrera profesional al...    30d+       False   \n",
       " 896  Technical Skills:\\r\\n8+ years of software engi...    30d+        True   \n",
       " 897  There will never be a typical day at Accenture...     29d       False   \n",
       " 898  Descripción:\\r\\n¿Eres un apasionado de la vent...     13d        True   \n",
       " 899  Estamos ampliando el equipo de FS Consulting e...     14d       False   \n",
       " \n",
       "                              Salary      Employees  Type_of_ownership  ...  \\\n",
       " 0                               NaN         10000+  Company - Private  ...   \n",
       " 1                               NaN   1001 to 5000  Company - Private  ...   \n",
       " 2                               NaN     201 to 500  Company - Private  ...   \n",
       " 3                               NaN         10000+  Company - Private  ...   \n",
       " 4                               NaN  5001 to 10000  Company - Private  ...   \n",
       " ..                              ...            ...                ...  ...   \n",
       " 895                             NaN         10000+  Company - Private  ...   \n",
       " 896  €37.50 Per Hour(Employer est.)        1 to 50  Company - Private  ...   \n",
       " 897                             NaN         10000+   Company - Public  ...   \n",
       " 898                             NaN            NaN  Company - Private  ...   \n",
       " 899                             NaN         10000+  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.95                   4.5             4.5              4.3   \n",
       " 1           0.99                   3.4             3.7              3.5   \n",
       " 2            NaN                   3.8             3.9              3.8   \n",
       " 3           0.95                   4.5             4.5              4.3   \n",
       " 4            NaN                   3.4             3.9              3.6   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895         0.94                   4.3             3.8              4.0   \n",
       " 896         1.00                   4.5             3.9              4.4   \n",
       " 897         0.88                   4.0             3.9              4.0   \n",
       " 898         1.00                   2.8             2.8              2.8   \n",
       " 899         0.87                   4.2             3.7              4.1   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  4.1                3.2   \n",
       " 1                  3.5                3.6   \n",
       " 2                  3.8                4.3   \n",
       " 3                  4.1                3.2   \n",
       " 4                  3.3                3.9   \n",
       " ..                 ...                ...   \n",
       " 895                3.8                3.3   \n",
       " 896                4.5                4.3   \n",
       " 897                3.7                3.8   \n",
       " 898                2.7                2.6   \n",
       " 899                3.8                3.3   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Work life balance, latest Technology, free ...   \n",
       " 1    ['\"busy environment to work in\" (in 4 reviews)...   \n",
       " 2    ['\"PTO, Good work life balance,\" (in 6 reviews...   \n",
       " 3    ['\"The people at BCG were great and were alway...   \n",
       " 4            ['\"people are friendly.\" (in 1 reviews)']   \n",
       " ..                                                 ...   \n",
       " 895  ['\"Great environment and networking oppurtunit...   \n",
       " 896  ['\"Good Management, Good Colleagues, Better un...   \n",
       " 897  ['\"It has been 7 months working in Accenture.....   \n",
       " 898  ['\"The atmosphere that the company creates in ...   \n",
       " 899  ['\"Work life balance is good to have\" (in 439 ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"bad work life balance though\" (in 21 review...              4.8   \n",
       " 1    ['\"here people do work very well.\" (in 4 revie...              NaN   \n",
       " 2    ['\"Work life balance is the least priority\" (i...              3.5   \n",
       " 3    ['\"No work life balance at all\" (in 12 reviews...              4.8   \n",
       " 4    ['\"boring management, low salaries, undefined ...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"Para ascender a puestos como Manager se deb...              4.2   \n",
       " 896  ['\"Has absorbed smaller companies, so experien...              NaN   \n",
       " 897  ['\"For few proj work is to much where we need ...              4.2   \n",
       " 898  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 899  ['\"No work life balance at the company due to ...              4.3   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Health Insurance (130 comments)\\n\"Fully cove...  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Health Insurance (130 comments)\\n\"Fully cove...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895  ['Health Insurance (765 comments)\\n\"A lot of o...  \n",
       " 896                                                NaN  \n",
       " 897  ['401K Plan (573 comments)\\n\"6% Match best in ...  \n",
       " 898                                                NaN  \n",
       " 899  ['Health Insurance (32 comments)\\n\"best is tha...  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Sweden':            Company_name  Rating    Location  \\\n",
       " 0        Siemens Energy     4.1    Finspång   \n",
       " 1    Riverty Group GmbH     4.0   Stockholm   \n",
       " 2        Siemens Energy     4.1    Finspång   \n",
       " 3                 Needo     NaN   Stockholm   \n",
       " 4        Siemens Energy     4.1    Finspång   \n",
       " ..                  ...     ...         ...   \n",
       " 895       Academic Work     3.7     Jädraås   \n",
       " 896     Zenseact Sweden     4.3  Gothenburg   \n",
       " 897     Mpya Sci & Tech     NaN  Gothenburg   \n",
       " 898              Wasder     NaN   Stockholm   \n",
       " 899               STACC     4.0   Stockholm   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0     Kreativ BI-utvecklare med passion för dataanalys   \n",
       " 1      Data Governance Engineer (m/f/d) 80% Homeoffice   \n",
       " 2    Examensarbete - Inverkan av materialstruktur p...   \n",
       " 3                           DevOps Engineer to Zebware   \n",
       " 4    Manager Parts Business - STG Steam Turbines, F...   \n",
       " ..                                                 ...   \n",
       " 895                        Swescan söker GIS-ingenjör!   \n",
       " 896  Data Engineer for Deep Learning within Autonom...   \n",
       " 897                   Data engineer to Mpya Sci & Tech   \n",
       " 898                                      Data Engineer   \n",
       " 899                         Data Engineer to Vetfamily   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Mid-level Professional\\r\\nKreativ BI-utvecklar...      6d       False   \n",
       " 1    Everything we do, starts with you.\\r\\nTogether...    30d+       False   \n",
       " 2    Student (Not Yet Graduated)\\r\\nExamensarbete: ...      7d       False   \n",
       " 3    Zebware are looking for a talented and passion...     23d       False   \n",
       " 4    Experienced Professional\\r\\nManager Parts Busi...     10d       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Har du akademisk GIS-utbildning och vill arbet...     16d       False   \n",
       " 896  Insights from the Team\\r\\nZenseact is flooded ...    30d+       False   \n",
       " 897  Mpya Sci & Tech är skapat av och för människor...     12d       False   \n",
       " 898  About\\r\\n\\r\\nWe are looking for a highly skill...     20d       False   \n",
       " 899  Do you have a passion for well-structured data...    30d+       False   \n",
       " \n",
       "     Salary      Employees  Type_of_ownership  ... CEO_approval  \\\n",
       " 0      NaN         10000+   Company - Public  ...         0.89   \n",
       " 1      NaN  5001 to 10000  Company - Private  ...         1.00   \n",
       " 2      NaN         10000+   Company - Public  ...         0.89   \n",
       " 3      NaN            NaN  Company - Private  ...          NaN   \n",
       " 4      NaN         10000+   Company - Public  ...         0.89   \n",
       " ..     ...            ...                ...  ...          ...   \n",
       " 895    NaN   1001 to 5000  Company - Private  ...         0.95   \n",
       " 896    NaN    501 to 1000  Company - Private  ...         1.00   \n",
       " 897    NaN      51 to 200  Company - Private  ...          NaN   \n",
       " 898    NaN            NaN                NaN  ...          NaN   \n",
       " 899    NaN            NaN   Company - Public  ...          NaN   \n",
       " \n",
       "      Career_opportunities Comp_&_benefits Culture_&_values  Senior_management  \\\n",
       " 0                     3.6             3.7              4.0                3.5   \n",
       " 1                     3.7             3.3              4.0                3.6   \n",
       " 2                     3.6             3.7              4.0                3.5   \n",
       " 3                     NaN             NaN              NaN                NaN   \n",
       " 4                     3.6             3.7              4.0                3.5   \n",
       " ..                    ...             ...              ...                ...   \n",
       " 895                   3.5             2.8              3.7                3.4   \n",
       " 896                   3.8             4.1              4.3                3.7   \n",
       " 897                   NaN             NaN              NaN                NaN   \n",
       " 898                   NaN             NaN              NaN                NaN   \n",
       " 899                   NaN             NaN              NaN                NaN   \n",
       " \n",
       "      Work/Life_balance                                               Pros  \\\n",
       " 0                  3.9  ['\"Good Work life balance No Micromanagement\" ...   \n",
       " 1                  4.1  ['\"Easy life, No pressure, Flexible times, Lov...   \n",
       " 2                  3.9  ['\"Good Work life balance No Micromanagement\" ...   \n",
       " 3                  NaN                                                NaN   \n",
       " 4                  3.9  ['\"Great work life balance Decent salary\" (in ...   \n",
       " ..                 ...                                                ...   \n",
       " 895                3.7  ['\"Good salary\" (in 44 reviews)', '\"Great cult...   \n",
       " 896                4.5  ['\"young, creative, leading, amazing colleague...   \n",
       " 897                NaN                                                NaN   \n",
       " 898                NaN                                                NaN   \n",
       " 899                NaN  ['\"Friendly people and many events and food\" (...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"No work life balance.\" (in 204 reviews)', '...              4.2   \n",
       " 1    ['\"too much bureaucracy, lazy people, too much...              4.0   \n",
       " 2    ['\"No work life balance.\" (in 204 reviews)', '...              4.2   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"Low salaries.\" (in 22 reviews)', '\"Lack of ...              4.2   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"low salary to live with due to expensive\" (...              NaN   \n",
       " 896  ['\"Too much freedom\" (in 5 reviews)', '\"Limite...              NaN   \n",
       " 897                                                NaN              NaN   \n",
       " 898                                                NaN              NaN   \n",
       " 899  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Vacation & Paid Time Off (12 comments)\\n\"Goo...  \n",
       " 1                                                  NaN  \n",
       " 2    ['Vacation & Paid Time Off (12 comments)\\n\"Goo...  \n",
       " 3                                                  NaN  \n",
       " 4    ['Vacation & Paid Time Off (12 comments)\\n\"Goo...  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Switzerland':                            Company_name  Rating     Location  \\\n",
       " 0                               Baloise     3.9        Basel   \n",
       " 1                  Pilatus Aircraft Ltd     3.1        Stans   \n",
       " 2                            AMAG Group     3.8         Cham   \n",
       " 3                         Universal-Job     5.0       Sarnen   \n",
       " 4                      Joe Security LLC     NaN      Reinach   \n",
       " ..                                  ...     ...          ...   \n",
       " 895                            Lionstep     4.7      Herisau   \n",
       " 896                          BakerHicks     4.5        Basel   \n",
       " 897  Bitplane AG / Andor Technology Inc     NaN    Schlieren   \n",
       " 898             Die Schweizerische Post     3.7  Switzerland   \n",
       " 899                        Michael Page     4.4         Biel   \n",
       " \n",
       "                                     Job_title  \\\n",
       " 0         Data Engineer (w/m) | 80% oder mehr   \n",
       " 1    Leiter Data Analytics & Data Science (a)   \n",
       " 2        Big Data & Platform Engineer 80-100%   \n",
       " 3           Data Engineer Messtechnik (m/w/d)   \n",
       " 4                   Data Scientist (60%-100%)   \n",
       " ..                                        ...   \n",
       " 895                           System Engineer   \n",
       " 896            Data Management Engineer (m/f)   \n",
       " 897              Software Engineer Microscopy   \n",
       " 898          Mobile Software Engineer Flutter   \n",
       " 899   Manufacturing Engineer - Medical Device   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    * Werde Teil unseres Investment Controlling Te...    30d+       False   \n",
       " 1    Fliegen Sie mit uns in die Zukunft und werden ...     23d       False   \n",
       " 2    Big Data & Platform Engineer 80-100%\\r\\nAMAG G...    30d+       False   \n",
       " 3    Beschreibung\\r\\nBei der digitalen Transformati...     19d       False   \n",
       " 4    Join Joe Security to Deeply Analyze Cyber Thre...      8d        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  On behalf of GEOINFO IT AG, we are searching f...    30d+       False   \n",
       " 896  As a subsidiary of the Morgan Sindall Group, B...     14d       False   \n",
       " 897  Bitplane/Andor produces a world leading softwa...    30d+        True   \n",
       " 898  Do you want to make a difference? SwissSign is...     14d       False   \n",
       " 899  In this position, you assist in the implementa...     24h       False   \n",
       " \n",
       "                                  Salary      Employees  Type_of_ownership  \\\n",
       " 0                                   NaN  5001 to 10000   Company - Public   \n",
       " 1                                   NaN   1001 to 5000   Company - Public   \n",
       " 2                                   NaN  5001 to 10000  Company - Private   \n",
       " 3                                   NaN      51 to 200   Company - Public   \n",
       " 4                                   NaN            NaN                NaN   \n",
       " ..                                  ...            ...                ...   \n",
       " 895                                 NaN     201 to 500   Company - Public   \n",
       " 896                                 NaN    501 to 1000   Company - Public   \n",
       " 897  CHF 70K - CHF 110K (Employer est.)        1 to 50  Company - Private   \n",
       " 898                                 NaN         10000+   Company - Public   \n",
       " 899                                 NaN     201 to 500  Company - Private   \n",
       " \n",
       "      ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0    ...         1.00                   3.2             3.4              4.1   \n",
       " 1    ...         0.58                   2.6             2.9              3.1   \n",
       " 2    ...         0.78                   3.1             3.5              3.6   \n",
       " 3    ...          NaN                   4.0             5.0              5.0   \n",
       " 4    ...          NaN                   NaN             NaN              NaN   \n",
       " ..   ...          ...                   ...             ...              ...   \n",
       " 895  ...          NaN                   4.6             4.5              4.6   \n",
       " 896  ...          NaN                   4.0             4.1              4.6   \n",
       " 897  ...          NaN                   NaN             NaN              NaN   \n",
       " 898  ...          NaN                   3.3             3.6              3.5   \n",
       " 899  ...         0.93                   3.7             3.7              3.5   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.4                4.2   \n",
       " 1                  2.6                3.0   \n",
       " 2                  3.0                3.3   \n",
       " 3                  5.0                4.0   \n",
       " 4                  NaN                NaN   \n",
       " ..                 ...                ...   \n",
       " 895                4.3                4.1   \n",
       " 896                3.9                4.5   \n",
       " 897                NaN                NaN   \n",
       " 898                3.2                3.5   \n",
       " 899                3.3                3.4   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Friendly people and work atmosphere\" (in 4 ...   \n",
       " 1    ['\"High EBIT bonus.\" (in 1 reviews)', '\"Challe...   \n",
       " 2    ['\"Tolle Benefits\" (in 2 reviews)', '\"Cadre fa...   \n",
       " 3                                                  NaN   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 895  ['\"great team\" (in 6 reviews)', '\"I got the ch...   \n",
       " 896  ['\"Nice and helpful colleagues and management\"...   \n",
       " 897                                                NaN   \n",
       " 898  ['\"Feels like retirement, the manager are not ...   \n",
       " 899  ['\"Colleagues were friendly and good bonus\" (i...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Low salary, no technical training provided ...              NaN   \n",
       " 1               ['\"Too much workload\" (in 1 reviews)']              4.2   \n",
       " 2    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895  ['\"But it\\'s definitely challenging also in a ...              NaN   \n",
       " 896  ['\"Deadline pressures and management of timesh...              NaN   \n",
       " 897                                                NaN              NaN   \n",
       " 898  ['\"The head of „x“ are usually low profile pro...              1.0   \n",
       " 899  ['\"Bonuses aren\\'t as competitive, some politi...              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 895               NaN  \n",
       " 896               NaN  \n",
       " 897               NaN  \n",
       " 898               NaN  \n",
       " 899               NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'Taiwan':                    Company_name  Rating  Location  \\\n",
       " 0                        Google     NaN       NaN   \n",
       " 1                      台灣積體電路製造     NaN       NaN   \n",
       " 2                      台灣積體電路製造     NaN       NaN   \n",
       " 3     Tookitaki Holding PTE LTD     NaN       NaN   \n",
       " 4                      台灣積體電路製造     NaN       NaN   \n",
       " ..                          ...     ...       ...   \n",
       " 685   Tookitaki Holding PTE LTD     NaN       NaN   \n",
       " 686                       Dcard     NaN       NaN   \n",
       " 687           Dell Technologies     NaN       NaN   \n",
       " 688  Vishay Intertechnology Inc     NaN       NaN   \n",
       " 689          萬達寵物WonderPet-寵物公園     NaN       NaN   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0    Data Center Facilities Mechanical Engineer (En...   \n",
       " 1                                        Data Engineer   \n",
       " 2    Data Analytics and Supply Chain Management Eng...   \n",
       " 3                           Big Data Engineer : Taiwan   \n",
       " 4                              Data Scientist Engineer   \n",
       " ..                                                 ...   \n",
       " 685                         Big Data Engineer : Taiwan   \n",
       " 686                       Backend Engineer (Python) TW   \n",
       " 687               2023 Hardware Engineer Summer Intern   \n",
       " 688                                        RD Engineer   \n",
       " 689                        Data Engineer 數據工程師 / BI工程師   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Google welcomes people with disabilities.\\r\\nM...     21d       False   \n",
       " 1    Role\\r\\nAre you a creative IT professional wit...    30d+       False   \n",
       " 2    TSMC為維護身心障礙者就業權益，特別規劃聘僱身障人才專區，歡迎領有身障手冊者加入TSMC的...    30d+       False   \n",
       " 3    Reporting to: Technical Lead\\r\\nResponsibiliti...     10d        True   \n",
       " 4    Job Responsibility:\\r\\nIdeal candidates would ...    30d+       False   \n",
       " ..                                                 ...     ...         ...   \n",
       " 685  Reporting to: Technical Lead\\r\\nResponsibiliti...     10d        True   \n",
       " 686  工作內容\\r\\nDcard 是在年輕族群有極高滲透率與影響力的社群平台。我們致力於打造一個讓...     15d       False   \n",
       " 687  2023 Hardware Engineer Summer Intern\\r\\nThe re...    30d+       False   \n",
       " 688  Help us build the DNA of tech.\\r\\n\\r\\nVishay C...      9d       False   \n",
       " 689  工作內容\\r\\n【團隊目標】\\r\\n我們正在打造寵物產業裡少數高度數據驅動、數位化的新零售通...    30d+       False   \n",
       " \n",
       "      Salary  Employees  Type_of_ownership  ...  CEO_approval  \\\n",
       " 0       NaN        NaN                NaN  ...           NaN   \n",
       " 1       NaN        NaN                NaN  ...           NaN   \n",
       " 2       NaN        NaN                NaN  ...           NaN   \n",
       " 3       NaN        NaN                NaN  ...           NaN   \n",
       " 4       NaN        NaN                NaN  ...           NaN   \n",
       " ..      ...        ...                ...  ...           ...   \n",
       " 685     NaN        NaN                NaN  ...           NaN   \n",
       " 686     NaN        NaN                NaN  ...           NaN   \n",
       " 687     NaN        NaN                NaN  ...           NaN   \n",
       " 688     NaN        NaN                NaN  ...           NaN   \n",
       " 689     NaN        NaN                NaN  ...           NaN   \n",
       " \n",
       "      Career_opportunities  Comp_&_benefits  Culture_&_values  \\\n",
       " 0                     NaN              NaN               NaN   \n",
       " 1                     NaN              NaN               NaN   \n",
       " 2                     NaN              NaN               NaN   \n",
       " 3                     NaN              NaN               NaN   \n",
       " 4                     NaN              NaN               NaN   \n",
       " ..                    ...              ...               ...   \n",
       " 685                   NaN              NaN               NaN   \n",
       " 686                   NaN              NaN               NaN   \n",
       " 687                   NaN              NaN               NaN   \n",
       " 688                   NaN              NaN               NaN   \n",
       " 689                   NaN              NaN               NaN   \n",
       " \n",
       "      Senior_management  Work/Life_balance  Pros  Cons  Benefits_rating  \\\n",
       " 0                  NaN                NaN   NaN   NaN              NaN   \n",
       " 1                  NaN                NaN   NaN   NaN              NaN   \n",
       " 2                  NaN                NaN   NaN   NaN              NaN   \n",
       " 3                  NaN                NaN   NaN   NaN              NaN   \n",
       " 4                  NaN                NaN   NaN   NaN              NaN   \n",
       " ..                 ...                ...   ...   ...              ...   \n",
       " 685                NaN                NaN   NaN   NaN              NaN   \n",
       " 686                NaN                NaN   NaN   NaN              NaN   \n",
       " 687                NaN                NaN   NaN   NaN              NaN   \n",
       " 688                NaN                NaN   NaN   NaN              NaN   \n",
       " 689                NaN                NaN   NaN   NaN              NaN   \n",
       " \n",
       "      Benefits_reviews  \n",
       " 0                 NaN  \n",
       " 1                 NaN  \n",
       " 2                 NaN  \n",
       " 3                 NaN  \n",
       " 4                 NaN  \n",
       " ..                ...  \n",
       " 685               NaN  \n",
       " 686               NaN  \n",
       " 687               NaN  \n",
       " 688               NaN  \n",
       " 689               NaN  \n",
       " \n",
       " [690 rows x 25 columns],\n",
       " 'Turkey':                                 Company_name  Rating  Location  \\\n",
       " 0                                    PLANSEE     3.5  Istanbul   \n",
       " 1    MEDCAPTAIN MEDICAL TECHNOLOGY CO., LTD.     NaN  Istanbul   \n",
       " 2                               Inpsyde GmbH     4.8    Turkey   \n",
       " 3                                 Accelleron     4.0  Istanbul   \n",
       " 4            Galaksiya Bilişim Teknolojileri     NaN     İzmir   \n",
       " ..                                       ...     ...       ...   \n",
       " 865                             REEF Parking     2.7  Istanbul   \n",
       " 866                                     DFDS     3.8  Istanbul   \n",
       " 867                                   Doktar     NaN  Istanbul   \n",
       " 868                           P.I. Works Inc     3.8  Istanbul   \n",
       " 869                                  Airties     3.8  Istanbul   \n",
       " \n",
       "                                              Job_title  \\\n",
       " 0                         Key Account Engineer (w/m/x)   \n",
       " 1                 Technology Service Engineer (Turkey)   \n",
       " 2    DevOps Engineer / IT Support (m/w/d) - Remote/...   \n",
       " 3    Service Engineer - Turbocharging / Servis Mühe...   \n",
       " 4                             DevOps Engineer - Remote   \n",
       " ..                                                 ...   \n",
       " 865                         IT Infrastructure Engineer   \n",
       " 866                                      Data Engineer   \n",
       " 867                                      Data Engineer   \n",
       " 868                       Software & Big Data Engineer   \n",
       " 869                             Software Engineer - IT   \n",
       " \n",
       "                                            Description    Job_age  Easy_apply  \\\n",
       " 0    CERATIZIT is a high-tech engineering group spe...       30d+       False   \n",
       " 1    Responsibilities：\\r\\nA. Support distributors o...       30d+        True   \n",
       " 2    We are Inpsyde ‒ The biggest WordPress agency ...       30d+       False   \n",
       " 3    As a Service Engineer, you will execute servic...       30d+       False   \n",
       " 4    DevOps Engineer\\r\\nObjectives of this Role:\\r\\...         3d        True   \n",
       " ..                                                 ...        ...         ...   \n",
       " 865  REEF transforms urban spaces into community hu...  1 day ago        True   \n",
       " 866  Data Engineer\\r\\nDo you want to be a valued pa...       30d+       False   \n",
       " 867  Job Description\\r\\nDoktar is an Agritech compa...       30d+       False   \n",
       " 868  P.I. Works is the first company in the world t...         5d        True   \n",
       " 869  At Airties we are on a mission to empower broa...        14d       False   \n",
       " \n",
       "                       Salary      Employees  Type_of_ownership  ...  \\\n",
       " 0                        NaN  5001 to 10000   Company - Public  ...   \n",
       " 1    TRY 16K (Employer est.)            NaN                NaN  ...   \n",
       " 2                        NaN      51 to 200  Company - Private  ...   \n",
       " 3                        NaN         10000+   Company - Public  ...   \n",
       " 4                        NaN            NaN                NaN  ...   \n",
       " ..                       ...            ...                ...  ...   \n",
       " 865                      NaN         10000+  Company - Private  ...   \n",
       " 866                      NaN         10000+   Company - Public  ...   \n",
       " 867                      NaN            NaN   Company - Public  ...   \n",
       " 868                      NaN     201 to 500  Company - Private  ...   \n",
       " 869                      NaN     201 to 500  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.76                   3.2             3.1              3.3   \n",
       " 1            NaN                   NaN             NaN              NaN   \n",
       " 2           1.00                   4.5             4.3              4.8   \n",
       " 3           0.89                   3.7             3.6              4.0   \n",
       " 4            NaN                   NaN             NaN              NaN   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 865         0.26                   2.6             2.7              2.4   \n",
       " 866         0.86                   3.5             3.5              3.7   \n",
       " 867          NaN                   NaN             NaN              NaN   \n",
       " 868         0.82                   3.5             3.8              4.1   \n",
       " 869         0.85                   3.5             3.4              3.5   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  2.8                3.3   \n",
       " 1                  NaN                NaN   \n",
       " 2                  4.7                5.0   \n",
       " 3                  3.5                3.8   \n",
       " 4                  NaN                NaN   \n",
       " ..                 ...                ...   \n",
       " 865                2.2                2.4   \n",
       " 866                3.2                3.9   \n",
       " 867                NaN                NaN   \n",
       " 868                3.5                3.7   \n",
       " 869                3.4                3.1   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Good work life balance and Family culture.\"...   \n",
       " 1                                                  NaN   \n",
       " 2    ['\"flexible working hours\" (in 5 reviews)', '\"...   \n",
       " 3    ['\"processes are well defined and work life ba...   \n",
       " 4                                                  NaN   \n",
       " ..                                                 ...   \n",
       " 865  ['\"Pay is good\" (in 126 reviews)', '\"Amazing p...   \n",
       " 866  ['\"DFDS has many great people who like to enjo...   \n",
       " 867                                                NaN   \n",
       " 868  ['\"Salary is competitive compared to market\" (...   \n",
       " 869  ['\"challenging good salary respect to employee...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Low salary\" (in 3 reviews)', '\"Management i...              4.3   \n",
       " 1                                                  NaN              NaN   \n",
       " 2    ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 3    ['\"No work life balance and no decent hike\" (i...              3.7   \n",
       " 4                                                  NaN              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 865  ['\"year career to come work for Reef Technolog...              3.5   \n",
       " 866  ['\"None i loved it great people\" (in 32 review...              2.5   \n",
       " 867                                                NaN              NaN   \n",
       " 868                   ['\"Low salary\" (in 13 reviews)']              4.0   \n",
       " 869  ['\"Average salary\" (in 13 reviews)', '\"Toxic p...              NaN   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3    ['Health Insurance (87 comments)\\n\"Lack of cov...  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 865  ['Health Insurance (11 comments)\\n\"No STD - th...  \n",
       " 866                                                NaN  \n",
       " 867                                                NaN  \n",
       " 868                                                NaN  \n",
       " 869                                                NaN  \n",
       " \n",
       " [870 rows x 25 columns],\n",
       " 'United_Kingdom':                         Company_name  Rating         Location  \\\n",
       " 0                  Places for People     4.0              NaN   \n",
       " 1                        Rolls-Royce     4.0   Derby, England   \n",
       " 2                     Appriss Retail     3.7           Remote   \n",
       " 3                   Mercury Holidays     3.3           Remote   \n",
       " 4                 eSynergy Solutions     4.4           Remote   \n",
       " ..                               ...     ...              ...   \n",
       " 895                          Travtus     NaN  London, England   \n",
       " 896                     Sharp Brains     3.7  London, England   \n",
       " 897            Next Ventures Limited     3.8           Remote   \n",
       " 898  Competition & Markets Authority     4.3  London, England   \n",
       " 899       Opus Recruitment Solutions     3.6  London, England   \n",
       " \n",
       "                                   Job_title  \\\n",
       " 0                             Data Engineer   \n",
       " 1                Data Engineer - Submarines   \n",
       " 2                             Data Engineer   \n",
       " 3                             Data Engineer   \n",
       " 4                             Data Engineer   \n",
       " ..                                      ...   \n",
       " 895                  Data Engineer (Junior)   \n",
       " 896                       SQL Data Engineer   \n",
       " 897                     Azure Data Engineer   \n",
       " 898                   CMA1828 Data Engineer   \n",
       " 899  Azure Data Engineer - 6 month contract   \n",
       " \n",
       "                                            Description    Job_age  Easy_apply  \\\n",
       " 0    We are Places for People Group, we're a social...        14d       False   \n",
       " 1    Job Description\\r\\nData Engineer - Submarines\\...  1 day ago       False   \n",
       " 2    About Appriss Retail:\\r\\nWe deliver software-a...         3d       False   \n",
       " 3    Mercury Holidays is a travel company based in ...        14d        True   \n",
       " 4    Are you a thought leader who has worked in the...         8d       False   \n",
       " ..                                                 ...        ...         ...   \n",
       " 895  Data Engineer (Junior)\\r\\nOur technology\\r\\nAd...        13d        True   \n",
       " 896  Are you a capable SQL data engineer who is pas...       30d+        True   \n",
       " 897  Type Contract\\r\\nAZURE DATA ENGINEER – 12 MONT...        28d       False   \n",
       " 898  Details\\r\\nReference number\\r\\n281947\\r\\nSalar...        24h       False   \n",
       " 899  I am currently on the lookout for an experienc...        17d       False   \n",
       " \n",
       "                            Salary    Employees  Type_of_ownership  ...  \\\n",
       " 0            £47K (Employer est.)       10000+  Company - Private  ...   \n",
       " 1    £45K - £59K (Glassdoor est.)       10000+   Company - Public  ...   \n",
       " 2                             NaN    51 to 200  Company - Private  ...   \n",
       " 3            £30K (Employer est.)    51 to 200  Company - Private  ...   \n",
       " 4                             NaN    51 to 200  Company - Private  ...   \n",
       " ..                            ...          ...                ...  ...   \n",
       " 895          £45K (Employer est.)          NaN                NaN  ...   \n",
       " 896   £50K - £55K (Employer est.)      1 to 50  Company - Private  ...   \n",
       " 897                           NaN    51 to 200  Company - Private  ...   \n",
       " 898   £55K - £62K (Employer est.)  501 to 1000         Government  ...   \n",
       " 899  £35K - £57K (Glassdoor est.)          NaN  Company - Private  ...   \n",
       " \n",
       "     CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0           0.96                   3.9             3.8              4.0   \n",
       " 1           0.89                   3.7             3.7              3.8   \n",
       " 2           1.00                   3.6             3.8              3.4   \n",
       " 3           1.00                   2.6             2.4              1.9   \n",
       " 4           1.00                   4.2             4.4              4.4   \n",
       " ..           ...                   ...             ...              ...   \n",
       " 895          NaN                   NaN             NaN              NaN   \n",
       " 896          NaN                   3.9             3.3              3.7   \n",
       " 897         0.69                   3.7             4.0              3.3   \n",
       " 898         1.00                   3.8             2.4              4.2   \n",
       " 899         0.72                   3.8             3.5              3.8   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.8                3.9   \n",
       " 1                  3.3                4.0   \n",
       " 2                  3.6                4.0   \n",
       " 3                  2.2                2.3   \n",
       " 4                  4.3                4.2   \n",
       " ..                 ...                ...   \n",
       " 895                NaN                NaN   \n",
       " 896                3.6                3.5   \n",
       " 897                3.5                3.1   \n",
       " 898                4.0                3.9   \n",
       " 899                3.7                2.9   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Good manager.\" (in 3 reviews)', '\"Innovativ...   \n",
       " 1    ['\"Good work life balance with flex time\" (in ...   \n",
       " 2    ['No Pros have been reported by the Glassdoor ...   \n",
       " 3    ['\"Friendly place to work and every day was di...   \n",
       " 4    ['\"Culture is brilliant, the people are lovely...   \n",
       " ..                                                 ...   \n",
       " 895                                                NaN   \n",
       " 896  ['\"The company is great for starting your care...   \n",
       " 897  ['\"Team reliable team, payment usually on time...   \n",
       " 898  ['\"good career progression\" (in 1 reviews)', '...   \n",
       " 899  ['No Pros have been reported by the Glassdoor ...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['\"Some managers are only out for themselves.\"...              5.0   \n",
       " 1    ['\"poor work life balance, forever in arears, ...              3.8   \n",
       " 2    ['\"Management do not share much when it comes ...              NaN   \n",
       " 3    ['\"Lack of communication from mangers\" (in 1 r...              NaN   \n",
       " 4    ['\"Long hours but normal in recruitment\" (in 4...              NaN   \n",
       " ..                                                 ...              ...   \n",
       " 895                                                NaN              NaN   \n",
       " 896  ['\"Low salary + no medical insurance/internet ...              NaN   \n",
       " 897  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 898  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 899  ['\"Good communication can be hard\" (in 1 revie...              4.0   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0                                                  NaN  \n",
       " 1    ['Health Insurance (18 comments)\\n\"Good benefi...  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns],\n",
       " 'United_States':                           Company_name  Rating        Location  \\\n",
       " 0    Lakeshore Learning Materials, LLC     4.1  Long Beach, CA   \n",
       " 1                        Oxfaa Pvt.Ltd     NaN     Trenton, NJ   \n",
       " 2                         Wevision LLC     NaN      Irvine, CA   \n",
       " 3                       zettalogix.Inc     NaN          Remote   \n",
       " 4                          Gridiron IT     4.5  Washington, DC   \n",
       " ..                                 ...     ...             ...   \n",
       " 895               Findability Sciences     NaN     Houston, TX   \n",
       " 896                     Smallboard.com     5.0    Pasadena, CA   \n",
       " 897                                ASA     3.6          Remote   \n",
       " 898                             Spruce     3.3      Austin, TX   \n",
       " 899                    Bloom Insurance     3.5          Remote   \n",
       " \n",
       "                             Job_title  \\\n",
       " 0       Junior Data Engineer (Remote)   \n",
       " 1                   AWS Data Engineer   \n",
       " 2                       Data Engineer   \n",
       " 3                 Azure Data Engineer   \n",
       " 4                       Data Engineer   \n",
       " ..                                ...   \n",
       " 895  Snowflake Data Engineer pipeline   \n",
       " 896                     Data Engineer   \n",
       " 897           Snowflake Data Engineer   \n",
       " 898                   Data Engineer I   \n",
       " 899                   Data Engineer I   \n",
       " \n",
       "                                            Description Job_age  Easy_apply  \\\n",
       " 0    Company Description\\r\\n\\r\\nAt Lakeshore, we cr...     24h        True   \n",
       " 1    Required Skill:\\r\\n· Prior experience with wri...     24h        True   \n",
       " 2    Job description\\r\\nWe build services, data pla...     13d        True   \n",
       " 3    Azure Data Engineer\\r\\nJackson, MS (100% REMOT...      2d        True   \n",
       " 4    Seeking a Data Engineer local to Washington, D...     14d        True   \n",
       " ..                                                 ...     ...         ...   \n",
       " 895  Snowflake data engineers will be responsible f...     17d        True   \n",
       " 896  Role: Database/Data Engineer Location: Hybrid ...     24h        True   \n",
       " 897  At least 5 years working with t-SQL and enterp...    30d+        True   \n",
       " 898  Who We Are\\r\\nAt Spruce, our mission is to cha...    30d+        True   \n",
       " 899  Data Engineer I\\r\\nTo support internal and ext...    30d+        True   \n",
       " \n",
       "                                       Salary     Employees  Type_of_ownership  \\\n",
       " 0    $27.00 - $31.00 Per Hour(Employer est.)  1001 to 5000  Company - Private   \n",
       " 1    $75.00 - $80.00 Per Hour(Employer est.)           NaN   Company - Public   \n",
       " 2    $55.00 - $85.00 Per Hour(Employer est.)       1 to 50  Company - Private   \n",
       " 3    $50.00 - $55.00 Per Hour(Employer est.)       1 to 50  Company - Private   \n",
       " 4    $65.00 - $75.00 Per Hour(Employer est.)     51 to 200  Company - Private   \n",
       " ..                                       ...           ...                ...   \n",
       " 895  $42.14 - $70.29 Per Hour(Employer est.)           NaN                NaN   \n",
       " 896           $103K - $160K (Glassdoor est.)           NaN  Company - Private   \n",
       " 897  $77.00 - $78.00 Per Hour(Employer est.)     51 to 200  Company - Private   \n",
       " 898            $72K - $103K (Glassdoor est.)     51 to 200  Company - Private   \n",
       " 899                                      NaN    201 to 500  Company - Private   \n",
       " \n",
       "      ... CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       " 0    ...         0.98                   3.8             3.7              4.1   \n",
       " 1    ...          NaN                   NaN             NaN              NaN   \n",
       " 2    ...          NaN                   NaN             NaN              NaN   \n",
       " 3    ...          NaN                   NaN             NaN              NaN   \n",
       " 4    ...         0.86                   4.5             4.3              4.4   \n",
       " ..   ...          ...                   ...             ...              ...   \n",
       " 895  ...          NaN                   NaN             NaN              NaN   \n",
       " 896  ...          NaN                   NaN             NaN              NaN   \n",
       " 897  ...         0.86                   3.6             3.4              3.5   \n",
       " 898  ...         0.61                   3.2             3.3              3.4   \n",
       " 899  ...         0.75                   3.5             3.3              3.5   \n",
       " \n",
       "      Senior_management  Work/Life_balance  \\\n",
       " 0                  3.8                4.1   \n",
       " 1                  NaN                NaN   \n",
       " 2                  NaN                NaN   \n",
       " 3                  NaN                NaN   \n",
       " 4                  4.3                4.7   \n",
       " ..                 ...                ...   \n",
       " 895                NaN                NaN   \n",
       " 896                NaN                NaN   \n",
       " 897                3.6                3.5   \n",
       " 898                3.3                3.5   \n",
       " 899                3.1                3.5   \n",
       " \n",
       "                                                   Pros  \\\n",
       " 0    ['\"Good work environment growth and work life ...   \n",
       " 1                                                  NaN   \n",
       " 2                                                  NaN   \n",
       " 3                                                  NaN   \n",
       " 4    ['\"Their recruiters are very professional and ...   \n",
       " ..                                                 ...   \n",
       " 895                                                NaN   \n",
       " 896  ['\"Great place to work; flexible hours\" (in 1 ...   \n",
       " 897  ['\"Salary is on time.\" (in 5 reviews)', '\"Very...   \n",
       " 898  ['No Pros have been reported by the Glassdoor ...   \n",
       " 899  ['\"Life Balance, Salary is average, Good learn...   \n",
       " \n",
       "                                                   Cons  Benefits_rating  \\\n",
       " 0    ['No Cons have been reported by the Glassdoor ...              4.0   \n",
       " 1                                                  NaN              NaN   \n",
       " 2                                                  NaN              NaN   \n",
       " 3                                                  NaN              NaN   \n",
       " 4    ['\"no PTO but understandable because its a con...              3.7   \n",
       " ..                                                 ...              ...   \n",
       " 895                                                NaN              NaN   \n",
       " 896  ['No Cons have been reported by the Glassdoor ...              NaN   \n",
       " 897  ['\"slow HR work and low salary\" (in 5 reviews)...              NaN   \n",
       " 898    ['\"Pay and benefits are less.\" (in 1 reviews)']              3.7   \n",
       " 899  ['\"Salary is Average\" (in 5 reviews)', '\"Very ...              2.6   \n",
       " \n",
       "                                       Benefits_reviews  \n",
       " 0    ['Employee Discount (14 comments)\\n\"This compa...  \n",
       " 1                                                  NaN  \n",
       " 2                                                  NaN  \n",
       " 3                                                  NaN  \n",
       " 4                                                  NaN  \n",
       " ..                                                 ...  \n",
       " 895                                                NaN  \n",
       " 896                                                NaN  \n",
       " 897                                                NaN  \n",
       " 898                                                NaN  \n",
       " 899                                                NaN  \n",
       " \n",
       " [900 rows x 25 columns]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSVs_folder = \"data/RAW/Data Engineer\"\n",
    "dfs = get_dfs_from_CSVs_in_folder(CSVs_folder)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name             object\n",
       "Rating                  float64\n",
       "Location                 object\n",
       "Job_title                object\n",
       "Description              object\n",
       "Job_age                  object\n",
       "Easy_apply                 bool\n",
       "Salary                   object\n",
       "Employees                object\n",
       "Type_of_ownership        object\n",
       "Sector                   object\n",
       "Founded                 float64\n",
       "Industry                 object\n",
       "Revenue_USD              object\n",
       "Friend_recommend        float64\n",
       "CEO_approval            float64\n",
       "Career_opportunities    float64\n",
       "Comp_&_benefits         float64\n",
       "Culture_&_values        float64\n",
       "Senior_management       float64\n",
       "Work/Life_balance       float64\n",
       "Pros                     object\n",
       "Cons                     object\n",
       "Benefits_rating         float64\n",
       "Benefits_reviews         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Location</th>\n",
       "      <th>Job_title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Type_of_ownership</th>\n",
       "      <th>...</th>\n",
       "      <th>CEO_approval</th>\n",
       "      <th>Career_opportunities</th>\n",
       "      <th>Comp_&amp;_benefits</th>\n",
       "      <th>Culture_&amp;_values</th>\n",
       "      <th>Senior_management</th>\n",
       "      <th>Work/Life_balance</th>\n",
       "      <th>Pros</th>\n",
       "      <th>Cons</th>\n",
       "      <th>Benefits_rating</th>\n",
       "      <th>Benefits_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infineon Technologies AG</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Villach</td>\n",
       "      <td>Component Verification and Product Characteriz...</td>\n",
       "      <td>You are looking for a new challenge to bring i...</td>\n",
       "      <td>30d+</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>['\"Good work life balance and learning\" (in 15...</td>\n",
       "      <td>['\"Descent pay, work life balance\" (in 153 rev...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['Health Insurance (22 comments)\\n\"basic medli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-612 UK Ltd.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vienna</td>\n",
       "      <td>Freelance Hardware/ Data Centre Field Engineer</td>\n",
       "      <td>REQUIREMENTS\\r\\nWe will consider self-employed...</td>\n",
       "      <td>17d</td>\n",
       "      <td>True</td>\n",
       "      <td>€41.65 - €50.90 Per Hour(Employer est.)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Infineon Technologies AG</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Villach</td>\n",
       "      <td>Senior Staff Engineer Digital Verification (f/...</td>\n",
       "      <td>You are looking for a new challenge to bring i...</td>\n",
       "      <td>30d+</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>['\"Good work life balance and learning\" (in 15...</td>\n",
       "      <td>['\"Descent pay, work life balance\" (in 153 rev...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['Health Insurance (22 comments)\\n\"basic medli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infineon Technologies AG</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Villach</td>\n",
       "      <td>Senior Staff Engineer Product Development for ...</td>\n",
       "      <td>Do you want to get to know the development of ...</td>\n",
       "      <td>27d</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>['\"Good work life balance and learning\" (in 15...</td>\n",
       "      <td>['\"Descent pay, work life balance\" (in 153 rev...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['Health Insurance (22 comments)\\n\"basic medli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Infineon Technologies AG</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Villach</td>\n",
       "      <td>Product Application Engineer (f/m/div)*</td>\n",
       "      <td>You enjoy working in an international team, in...</td>\n",
       "      <td>30d+</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000+</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>['\"Good work life balance and learning\" (in 15...</td>\n",
       "      <td>['\"Descent pay, work life balance\" (in 153 rev...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['Health Insurance (22 comments)\\n\"basic medli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Company_name  Rating Location  \\\n",
       "0  Infineon Technologies AG     4.2  Villach   \n",
       "1             B-612 UK Ltd.     NaN   Vienna   \n",
       "2  Infineon Technologies AG     4.2  Villach   \n",
       "3  Infineon Technologies AG     4.2  Villach   \n",
       "4  Infineon Technologies AG     4.2  Villach   \n",
       "\n",
       "                                           Job_title  \\\n",
       "0  Component Verification and Product Characteriz...   \n",
       "1     Freelance Hardware/ Data Centre Field Engineer   \n",
       "2  Senior Staff Engineer Digital Verification (f/...   \n",
       "3  Senior Staff Engineer Product Development for ...   \n",
       "4            Product Application Engineer (f/m/div)*   \n",
       "\n",
       "                                         Description Job_age  Easy_apply  \\\n",
       "0  You are looking for a new challenge to bring i...    30d+       False   \n",
       "1  REQUIREMENTS\\r\\nWe will consider self-employed...     17d        True   \n",
       "2  You are looking for a new challenge to bring i...    30d+       False   \n",
       "3  Do you want to get to know the development of ...     27d       False   \n",
       "4  You enjoy working in an international team, in...    30d+       False   \n",
       "\n",
       "                                    Salary Employees Type_of_ownership  ...  \\\n",
       "0                                      NaN    10000+  Company - Public  ...   \n",
       "1  €41.65 - €50.90 Per Hour(Employer est.)       NaN               NaN  ...   \n",
       "2                                      NaN    10000+  Company - Public  ...   \n",
       "3                                      NaN    10000+  Company - Public  ...   \n",
       "4                                      NaN    10000+  Company - Public  ...   \n",
       "\n",
       "  CEO_approval  Career_opportunities Comp_&_benefits Culture_&_values  \\\n",
       "0         0.95                   3.9             3.8              4.1   \n",
       "1          NaN                   NaN             NaN              NaN   \n",
       "2         0.95                   3.9             3.8              4.1   \n",
       "3         0.95                   3.9             3.8              4.1   \n",
       "4         0.95                   3.9             3.8              4.1   \n",
       "\n",
       "   Senior_management  Work/Life_balance  \\\n",
       "0                3.7                4.1   \n",
       "1                NaN                NaN   \n",
       "2                3.7                4.1   \n",
       "3                3.7                4.1   \n",
       "4                3.7                4.1   \n",
       "\n",
       "                                                Pros  \\\n",
       "0  ['\"Good work life balance and learning\" (in 15...   \n",
       "1                                                NaN   \n",
       "2  ['\"Good work life balance and learning\" (in 15...   \n",
       "3  ['\"Good work life balance and learning\" (in 15...   \n",
       "4  ['\"Good work life balance and learning\" (in 15...   \n",
       "\n",
       "                                                Cons  Benefits_rating  \\\n",
       "0  ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       "1                                                NaN              NaN   \n",
       "2  ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       "3  ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       "4  ['\"Descent pay, work life balance\" (in 153 rev...              3.9   \n",
       "\n",
       "                                    Benefits_reviews  \n",
       "0  ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       "1                                                NaN  \n",
       "2  ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       "3  ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       "4  ['Health Insurance (22 comments)\\n\"basic medli...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del CSVs_folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove rows only with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_dfs(dfs: dict[str, pd.DataFrame], method: Callable):\n",
    "    differences = {}\n",
    "\n",
    "    for country_name, country_df in dfs.items():\n",
    "\n",
    "        country_df_before_len = country_df.shape[0]\n",
    "        dfs[country_name]: pd.DataFrame = method(country_df)\n",
    "        country_df_after_len = dfs[country_name].shape[0]\n",
    "\n",
    "        is_difference = country_df_before_len != country_df_after_len\n",
    "\n",
    "        if is_difference:\n",
    "            differences[country_name] = (\n",
    "                country_df_before_len, \n",
    "                country_df_after_len\n",
    "                )\n",
    "\n",
    "    print(differences)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "dfs = process_dfs(dfs=dfs, method = lambda x: x.dropna(how='all'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': (744, 111), 'Austria': (840, 652), 'Belgium': (900, 250), 'Czech_Republic': (900, 202), 'Denmark': (570, 317), 'Finland': (300, 192), 'France': (900, 324), 'Germany': (900, 291), 'Greece': (690, 172), 'Hong_Kong': (870, 564), 'Hungary': (690, 293), 'Ireland': (900, 493), 'Israel': (900, 368), 'Italy': (900, 326), 'Japan': (420, 308), 'Luxembourg': (390, 192), 'Netherlands': (900, 150), 'New_Zealand': (390, 332), 'Norway': (360, 122), 'Poland': (900, 189), 'Portugal': (900, 685), 'Romania': (660, 482), 'Singapore': (900, 184), 'South_Korea': (570, 401), 'Spain': (900, 183), 'Sweden': (900, 257), 'Switzerland': (900, 151), 'Taiwan': (690, 129), 'Turkey': (870, 120), 'United_Kingdom': (900, 120), 'United_States': (900, 244)}\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(dfs: dict[str, pd.DataFrame]):\n",
    "    differences = {}\n",
    "\n",
    "    for country_name, country_df in dfs.items():\n",
    "\n",
    "        country_df_before_len = country_df.shape[0]\n",
    "        dfs[country_name]: pd.DataFrame = country_df.drop_duplicates(subset=country_df.columns.difference(['Job_age']))\n",
    "        country_df_after_len = dfs[country_name].shape[0]\n",
    "\n",
    "        is_difference = country_df_before_len != country_df_after_len\n",
    "\n",
    "        if is_difference:\n",
    "            differences[country_name] = (\n",
    "                country_df_before_len, \n",
    "                country_df_after_len\n",
    "                )\n",
    "\n",
    "\n",
    "    print(differences)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "dfs = remove_duplicates(dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is huge amount of duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del remove_duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clean jobs to only relevant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_languages = {\n",
    "     'Austria': [\n",
    "          \"German\"\n",
    "     ],\n",
    "     'Belgium': [\n",
    "          \"French\", \"Dutch\", \"German\"\n",
    "     ],\n",
    "     'Canada': [\n",
    "          \"French\"\n",
    "     ],\n",
    "     # By Glassdoor is also Slovakia\n",
    "     'Czech_Republic': [\n",
    "          \"Czech\", \"Slovakian\", \"Hungarian\"\n",
    "     ],\n",
    "     'Denmark': [\n",
    "          \"Danish\"\n",
    "     ],\n",
    "     'Finland': [\n",
    "          \"Finnish\", \"Swedish\"\n",
    "     ],\n",
    "     # Occitan and Catalan are pretty similar\n",
    "     'France': [\n",
    "          \"French\", \"Catalan\", \"Italian\", \"Basque\"\n",
    "     ],\n",
    "     'Germany': [\n",
    "          \"German\"\n",
    "     ],\n",
    "     'Greece': [\n",
    "          \"Greek\"\n",
    "     ],\n",
    "     'Hungary': [\n",
    "          \"Hungarian\", \"Romanian\"\n",
    "     ],\n",
    "     # Irish is almost not spoken\n",
    "     'Ireland': [],\n",
    "     'Israel': [\n",
    "          \"Hebrew\", \"Arabic\"\n",
    "     ],\n",
    "     'Italy': [\n",
    "          \"Italian\",\n",
    "          \"German\",\n",
    "          \"French\",\n",
    "          \"Catalan\",\n",
    "          \"Greek\",\n",
    "          \"Slovenian\"\n",
    "     ],\n",
    "     'Luxembourg': [\n",
    "          \"German\", \"French\"\n",
    "     ],\n",
    "     'Netherlands': [\n",
    "          \"Dutch\", \"Frisian\"\n",
    "     ],\n",
    "     'Norway': [\n",
    "          \"Norwegian\"\n",
    "     ],\n",
    "     'Poland': [\n",
    "          \"Polish\"\n",
    "     ],\n",
    "     'Portugal': [\n",
    "          \"Portuguese\"\n",
    "     ],\n",
    "     'Romania': [\n",
    "          \"Romanian\"\n",
    "     ],\n",
    "     'Spain': [\n",
    "          \"Spanish\", \"Basque\", \"Catalan\", \"Galician\"\n",
    "     ],\n",
    "     'Sweden': [\n",
    "          \"Swedish\", \"Finnish\"\n",
    "     ],\n",
    "     'Switzerland': [\n",
    "          \"German\", \"French\", \"Italian\"\n",
    "     ],\n",
    "     'Turkey': [\n",
    "          \"Turkish\", \"Kurdish\"\n",
    "     ],\n",
    "     'United_States': [\n",
    "          \"Spanish\"\n",
    "     ],\n",
    "     # We can leave Scottish and Gaelic\n",
    "     'United_Kingdom': [],\n",
    "     # Yeah there are Ryukyuan languages, but because of the size population and \n",
    "     # implementation problems we can skip it, because the gain would not be that much.\n",
    "     'Japan': [\n",
    "          \"Japanese\"\n",
    "    ],\n",
    "     'South_Korea': [\n",
    "          \"Korean\"\n",
    "    ],\n",
    "     'Taiwan': [\n",
    "          \"Chinese_TR\"\n",
    "    ],\n",
    "     'Singapore': [\n",
    "          \"Chinese_SP\"\n",
    "    ],\n",
    "     \"New_Zealand\": [],\n",
    "     \"Australia\": [],\n",
    "     \"Hong_Kong\": [\n",
    "          \"Chinese_TR\"\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_non_eng = {\n",
    "    'Arabic': [\"مهندس الجوالات\", \"مطور الجوالات\", \"مطور أندرويد\", \"مهندس أندرويد\", \"مهندس طبي\", \"جيولوجي هندسي\", \"مهندس جيوتقني\", \"مهندس كهربائي\", \"مدير مشروع\", \"مهندس جودة\", \"مهندس ميكانيكي\", \"مهندس تصميم ميكانيكي\", \"مطور تطبيقات جوال\"],\n",
    "\t'Basque': [\"Mugikor Ingeniaria\", \"Mugikor Garatzailea\", \"Android Garatzailea\", \"Android Ingeniaria\", \"Biomedikuntza Ingeniaria\", \"Geologi Ingeniaria\", \"Geotekniko Ingeniaria\", \"Elektrizitate Ingeniaria\", \"Proiektu Kudeatzailea\", \"Kalitate Ingeniaria\", \"Mekanika Ingeniaria\", \"Mekanika Diseinu Ingeniaria\", \"Mugikor Aplikazio Garatzailea\"],\n",
    "\t'Catalan': [\"Enginyer Mòbil\", \"Desenvolupador Mòbil\", \"Desenvolupador Android\", \"Enginyer Android\", \"Enginyer Biomèdic\", \"Enginyer Geològic\", \"Enginyer Geotècnic\", \"Enginyer Elèctric\", \"Cap de Projecte\", \"Enginyer de Qualitat\", \"Enginyer Mecànic\", \"Enginyer de Disseny Mecànic\", \"Desenvolupador d'Apps Mòbils\"],\n",
    "\t'Czech': [\"Mobilní inženýr\", \"Mobilní vývojář\", \"Vývojář Androidu\", \"Android inženýr\", \"Biomedicínský inženýr\", \"Inženýrský geolog\", \"Geotechnický inženýr\", \"Elektroinženýr\", \"Manažer projektu\", \"Kvalitní inženýr\", \"Mechanický inženýr\", \"Inženýr návrhu strojů\", \"Vývojář mobilních aplikací\"],\n",
    "\t'German': [\"Mobile Ingenieur\", \"Mobile Entwickler\", \"Android Entwickler\", \"Android Ingenieur\", \"Biomedizinischer Ingenieur\", \"Ingenieurgeologe\", \"Geotechnischer Ingenieur\", \"Elektroingenieur\", \"Projektmanager\", \"Qualitätsingenieur\", \"Maschinenbauingenieur\", \"Maschinenbau-Konstrukteur\", \"Mobile App Entwickler\"],\n",
    "\t'Danish': [\"Mobil ingeniør\", \"Mobiludvikler\", \"Android-udvikler\", \"Android-ingeniør\", \"Biomedicinsk ingeniør\", \"Ingeniørgeolog\", \"Geoteknisk ingeniør\", \"Elektrisk ingeniør\", \"Projektleder\", \"Kvalitetsingeniør\", \"Mekanisk ingeniør\", \"Mekanisk-design ingeniør\", \"Mobil app-udvikler\"],\n",
    "\t'Spanish': [\"Ingeniero móvil\", \"Desarrollador móvil\", \"Desarrollador de Android\", \"Ingeniero de Android\", \"Ingeniero biomédico\", \"Geólogo de ingeniería\", \"Ingeniero geotécnico\", \"Ingeniero eléctrico\", \"Gerente de proyectos\", \"Ingeniero de calidad\", \"Ingeniero mecánico\", \"Ingeniero de diseño mecánico\", \"Desarrollador de aplicaciones móviles\"],\n",
    "\t'Finnish': [\"Mobiili-insinööri\", \"Mobiilikehittäjä\", \"Android-kehittäjä\", \"Android-insinööri\", \"Biomediainsinööri\", \"Geologian insinööri\", \"Geotekninen insinööri\", \"Sähköinsinööri\", \"Projektipäällikkö\", \"Laatuinsinööri\", \"Mekaaninen insinööri\", \"Mekaanisen suunnittelun insinööri\", \"Mobiilisovelluskehittäjä\"],\n",
    "\t'French': [\"Ingénieur mobile\", \"Développeur mobile\", \"Développeur Android\", \"Ingénieur Android\", \"Ingénieur biomédical\", \"Géologue d'ingénierie\", \"Ingénieur géotechnique\", \"Ingénieur électrique\", \"Chef de projet\", \"Ingénieur qualité\", \"Ingénieur mécanique\", \"Ingénieur en conception mécanique\", \"Développeur d'applications mobiles\"],\n",
    "\t'Frisian': [\"Mobile yngenieur\", \"Mobile ûntwikkele\", \"Android-ûntwikkele\", \"Android yngenieur\", \"Biomedysk yngenieur\", \"Engineering geolooch\", \"Geotechnysk yngenieur\", \"Elektaryske yngenieur\", \"Projektmanager\", \"Kwaliteit yngenieur\", \"Mekanysk yngenieur\", \"Mekanyske-ûntwerp yngenieur\", \"Mobile app-ûntwikkele\"],\n",
    "\t'Galician': [\"Enxeñeiro móbil\", \"Desenvolvedor móbil\", \"Desenvolvedor de Android\", \"Enxeñeiro de Android\", \"Enxeñeiro biomédico\", \"Xeólogo de enxeñería\", \"Enxeñeiro xeotécnico\", \"Enxeñeiro eléctrico\", \"Xestor de proxectos\", \"Enxeñeiro de calidade\", \"Enxeñeiro mecánico\", \"Enxeñeiro de deseño mecánico\", \"Desenvolvedor de aplicacións móbeis\"],\n",
    "\t'Greek': [\"Μηχανικός κινητής τηλεφωνίας\", \"Προγραμματιστής κινητής τηλεφωνίας\", \"Προγραμματιστής Android\", \"Μηχανικός Βιοϊατρικής Τεχνολογίας\", \"Γεωλόγος Μηχανικός\", \"Μηχανικός Γεωτεχνικών\", \"Ηλεκτρολόγος Μηχανικός\", \"Διευθυντής έργου\", \"Μηχανικός Ποιότητας\", \"Μηχανικός Μηχανολογίας\", \"Μηχανικός Μηχανολογίας-Σχεδιασμού\", \"Προγραμματιστής κινητών εφαρμογών\"],\n",
    "\t'Hebrew': [\"מהנדס מובייל\", \"מפתח יישומים מוביילים\", \"מפתח אנדרואיד\", \"מהנדס ביו-רפואי\", \"גיאולוג מהנדס\", \"מהנדס גיאו-טכני\", \"מהנדס חשמל\", \"מנהל פרויקט\", \"מהנדס איכות\", \"מהנדס מכונות\", \"מהנדס מכונות-תכנון\", \"מפתח יישומי ניידים\"],\n",
    "\t'Hungarian': [\"Mobil mérnök\", \"Mobil fejlesztő\", \"Android fejlesztő\", \"Android mérnök\", \"Biomedikus mérnök\", \"Mérnöki geológus\", \"Geotechnikai mérnök\", \"Elektromos mérnök\", \"Projektmenedzser\", \"Minőségi mérnök\", \"Gépészmérnök\", \"Gépészmérnök-tervező\", \"Mobil alkalmazás fejlesztő\"],\n",
    "\t'Italian': [\"Ingegnere mobile\", \"Sviluppatore mobile\", \"Sviluppatore Android\", \"Ingegnere Android\", \"Ingegnere biomedico\", \"Geologo ingegnere\", \"Ingegnere geotecnico\", \"Ingegnere elettrico\", \"Project Manager\", \"Ingegnere della qualità\", \"Ingegnere meccanico\", \"Ingegnere meccanico-design\", \"Sviluppatore di app mobili\"],\n",
    "\t'Kurdish': [\"Mühendis-ı mobîl\", \"Pêşkêşvan-ı mobîl\", \"Pêşkêşvan-ı Android\", \"Mühendis-ı Android\", \"Mühendis-ı bîomedîkal\", \"Cîhêk-î mühendîsî\", \"Mühendîs-î geoteknik\", \"Mühendîs-î elektrîkî\", \"Pêwendîdar-î projeyan\", \"Mühendîs-î quality\", \"Mühendîs-î mekanîkî\", \"Mühendîs-î mekanîkî-têkildarî dizaynê\", \"Pêşkêşvan-î app-ê mobîl\"],\n",
    "\t'Dutch': [\"Mobiele ingenieur\", \"Mobiele ontwikkelaar\", \"Android ontwikkelaar\", \"Android ingenieur\", \"Biomedisch ingenieur\", \"Ingenieur geologie\", \"Geotechnisch ingenieur\", \"Elektrotechnisch ingenieur\", \"Projectmanager\", \"Kwaliteitsingenieur\", \"Werktuigbouwkundig ingenieur\", \"Werktuigbouwkundig-ontwerp ingenieur\", \"Mobiele app-ontwikkelaar\"],\n",
    "\t'Norwegian': [\"Mobil ingeniør\", \"Mobilutvikler\", \"Android-utvikler\", \"Android-ingeniør\", \"Biomedisinsk ingeniør\", \"Geologiingeniør\", \"Geoteknisk ingeniør\", \"Elektroingeniør\", \"Prosjektleder\", \"Kvalitetsingeniør\", \"Maskiningeniør\", \"Mekanisk designingeniør\", \"Utvikler av mobilapper\"],\n",
    "\t'Polish': [\"Inżynier mobilny\", \"Deweloper mobilny\", \"Deweloper Androida\", \"Inżynier Androida\", \"Inżynier biomedyczny\", \"Inżynier geologii\", \"Inżynier geotechniki\", \"Inżynier elektryk\", \"Kierownik projektu\", \"Inżynier jakości\", \"Inżynier mechanik\", \"Inżynier mechaniki i projektowania\", \"Twórca aplikacji mobilnych\"],\n",
    "\t'Portuguese': [\"Engenheiro Móvel\", \"Desenvolvedor Móvel\", \"Desenvolvedor Android\", \"Engenheiro Android\", \"Engenheiro Biomédico\", \"Geólogo de Engenharia\", \"Engenheiro Geotécnico\", \"Engenheiro Elétrico\", \"Gerente de Projeto\", \"Engenheiro de Qualidade\", \"Engenheiro Mecânico\", \"Engenheiro de Design Mecânico\", \"Desenvolvedor de Aplicativos Móveis\"],\n",
    "\t'Romanian': [\"Inginer Mobil\", \"Dezvoltator mobil\", \"Dezvoltator Android\", \"Inginer Android\", \"Inginer Biomedical\", \"Geolog Inginer\", \"Inginer Geotehnic\", \"Inginer Electric\", \"Manager de proiect\", \"Inginer de calitate\", \"Inginer mecanic\", \"Inginer de design mecanic\", \"Dezvoltator de aplicații mobile\"],\n",
    "\t'Slovakian': [\"Mobilný inžinier\", \"Mobilný vývojár\", \"Vývojár Androidu\", \"Android inžinier\", \"Biomedicínsky inžinier\", \"Inžinier geológie\", \"Geotechnický inžinier\", \"Elektrický inžinier\", \"Manažér projektu\", \"Kvalitný inžinier\", \"Mechanický inžinier\", \"Inžinier návrhu mechaniky\", \"Vývojár mobilných aplikácií\"],\n",
    "\t'Slovenian': [\"Mobilni inženir\", \"Mobilni razvijalec\", \"Razvijalec Androida\", \"Android inženir\", \"Biomedicinski inženir\", \"Inženir geologije\", \"Geotehnični inženir\", \"Elektroinženir\", \"Vodja projekta\", \"Inženir kakovosti\", \"Mehanski inženir\", \"Inženir oblikovanja mehanike\", \"Razvijalec mobilnih aplikacij\"],\n",
    "\t'Swedish': [\"Mobilingenjör\", \"Mobilutvecklare\", \"Androidutvecklare\", \"Androidingenjör\", \"Biomedicinsk ingenjör\", \"Ingenjör i geologi\", \"Geoteknisk ingenjör\", \"Elektroingenjör\", \"Projektledare\", \"Kvalitetsingenjör\", \"Mekanisk ingenjör\", \"Ingenjör för mekanisk design\", \"Mobilapputvecklare\"],\n",
    "\t'Turkish': [\"Mobil mühendisi\", \"Mobil Geliştirici\", \"Android Geliştirici\", \"Android Mühendisi\", \"Biyomedikal Mühendisi\", \"Jeoloji Mühendisi\", \"Zemin Mekaniği Mühendisi\", \"Elektrik Mühendisi\", \"Proje Yöneticisi\", \"Kalite Mühendisi\", \"Mekanik Mühendisi\", \"Mekanik-Tasarım Mühendisi\", \"Mobil Uygulama Geliştiricisi\"],\n",
    "    'Japanese': [\"モバイルエンジニア\", \"モバイル開発者\", \"Android開発者\", \"Androidエンジニア\", \"バイオメディカルエンジニア\", \"エンジニアリングジオロジスト\", \"地質工学技術者\", \"電気技師\", \"プロジェクトマネージャー\", \"品質エンジニア\", \"機械エンジニア\", \"機械設計エンジニア\", \"モバイルアプリ開発者\"],\n",
    "\t'Korean': [\"모바일 엔지니어\", \"모바일 개발자\", \"안드로이드 개발자\", \"안드로이드 엔지니어\", \"바이오의공학 엔지니어\", \"공학 지질학자\", \"지질기술 엔지니어\", \"전기 기술자\", \"프로젝트 매니저\", \"품질 엔지니어\", \"기계 엔지니어\", \"기계설계 엔지니어\", \"모바일 앱 개발자\"],\n",
    "\t'Chinese_TR': [\"移动工程师\", \"移动开发人员\", \"安卓开发人员\", \"安卓工程师\", \"生物医学工程师\", \"工程地质学家\", \"岩土工程师\", \"电气工程师\", \"项目经理\", \"质量工程师\", \"机械工程师\", \"机械设计工程师\", \"移动应用程序开发人员\"],\n",
    "\t'Chinese_SP': [\"移动工程师\", \"移动开发人员\", \"安卓开发人员\", \"安卓工程师\", \"生物医学工程师\", \"工程地质学家\", \"岩土工程师\", \"电气工程师\", \"项目经理\", \"质量工程师\", \"机械工程师\", \"机械设计工程师\", \"移动应用程序开发人员\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "specializations_non_english = {\n",
    "\t'Arabic': [\"مهندس\", \"هندسة\", \"مستشار\", \"معماري\", \"متخصص\", \"مدير\", \"مطور\", \"عمارة\", \"مسؤول\", \"رئيس\", \"قائد\", \"مشرف\", \"منسق\", \"تنفيذي\"],\n",
    "\t'Basque': [\"Ingeniaria\", \"Ingeniaritza\", \"Konsultore\", \"arkitektoa\", \"ESPECIALISTA\", \"Kudeatzailea\", \"Garatzailea\", \"Arkitektura\", \"Administratzailea\", \"Koordinatzailea\", \"Koordinatzaile\", \"Gobernuko\"],\n",
    "\t'Catalan': [\"Enginyer\", \"Enginyeria\", \"Consultor\", \"arquitecte\", \"ESPECIALISTA\", \"Gerent\", \"Desenvolupador\", \"Arquitectura\", \"Administrador\", \"Executiu\"],\n",
    "\t'Czech': [\"Inženýr\", \"Konstruktér\", \"Architekt\", \"SPECIALISTA\", \"Manažer\", \"Vývojář\", \"Architektura\", \"Správce\",\"Dozorce\", \"Koordinátor\", \"Výkonný\"],\n",
    "\t'German': [\"Ingenieur\", \"Berater\", \"Architekt\", \"SPEZIALIST\", \"Manager\", \"Entwickler\", \"Architektur\", \"Architecture\", \"Administrator\", \"Vorgesetzter\", \"Koordinator\", \"Führungskraft\"],\n",
    "\t'Danish': [\"Ingeniør\", \"Konsulent\", \"Arkitekt\", \"SPECIALIST\", \"Manager\", \"Udvikler\", \"Arkitektur\", \"Administrator\", \"Koordinator\"],\n",
    "\t'Dutch': [\"SPECIALIST\", \"Engineering\", \"Manager\", \"architect\", \"Beheerder\", \"Ingenieur\", \"Adviseur\", \"Ontwikkelaar\", \"Architect\", \"Techniek\", \"Architectuur\", \"Toezichthouder\", \"Coördinator\", \"Uitvoerend\"],\n",
    "\t'Spanish': [\"Ingeniero\", \"Consultor\", \"Arquitecto\", \"Especialista\", \"Gerente\", \"Desarrollador\", \"Arquitectura\", \"Administrador\", \"Ejecutivo\"],\n",
    "\t'Finnish': [\"Insinööri\", \"Konsultti\", \"Arkkitehti\", \"ERITYISOSAAMINEN\", \"Johtaja\", \"Kehittäjä\", \"Arkkitehtuuri\", \"Ylläpitäjä\", \"Esimies\", \"Koordinaattori\", \"Johtaja\"],\n",
    "\t'French': [\"Architecte\", \"Développeur\", \"architecte\", \"SPECIALISTE\", \"SPÉCIALISTE\", \"Consultant\", \"Conseiller\", \"Ingénieur\", \"Administrateur\", \"Architecture\", \"Manager\", \"Ingénierie\", \"Superviseur\", \"Coordinateur\", \"Cadre\"],\n",
    "\t'Frisian': [\"Ynżenier\", \"Ynženiering\", \"Konsultant\", \"arkitekt\", \"SPESJALIST\", \"Manager\", \"Ûntwikkelers\", \"Arktitektuer\", \"Administrator\", \"Koördinator\", \"Uitvoerend\"],\n",
    "\t'Galician': [\"Inxeniero\", \"Enxeñería\", \"Consultor\", \"arquitecto\", \"ESPECIALISTA\", \"Xestor\", \"Desenvolvedor\", \"Arquitectura\", \"Administrador\", \"Coordinador\", \"Executivo\"],\n",
    "\t'Greek': [\"Μηχανικός\", \"Σύμβουλος\", \"Αρχιτέκτονας\", \"ΕΙΔΙΚΟΣ\", \"Διευθυντής\", \"Προγραμματιστής\", \"Αρχιτεκτονική\", \"Διαχειριστής\", \"Επιβλέπων\", \"Συντονιστής\", \"Διευθυντής\"],\n",
    "\t'Hebrew': [\"מהנדס\", \"הנדסה\", \"יועץ\", \"אדריכל\", \"מומחה\", \"מנהל\", \"מפתח\", \"ארכיטקטורה\", \"מנהל מערכות\", \"מנהל\", \"רכז\", \"מבצע\"],\n",
    "\t'Hungarian': [\"Mérnök\", \"Mérnöki\", \"Tanácsadó\", \"építész\", \"SZAKÉRTŐ\", \"Menedzser\", \"Fejlesztő\", \"Architektúra\", \"Rendszergazda\", \"Felügyelő\", \"Koordinátor\", \"Vezető\"],\n",
    "\t'Italian': [\"Ingegnere\", \"Consulente\", \"architetto\", \"SPECIALISTA\", \"Manager\", \"Sviluppatore\", \"Architettura\", \"Amministratore\", \"Supervisore\", \"Coordinatore\", \"Esecutivo\"],\n",
    "\t'Kurdish': [\"Mûhandis\", \"Mûhendisî\", \"Pêşkêşker\", \"pargîdaniyar\", \"XWESER\", \"Manajer\", \"Pêşgir\", \"Arkîtektur\", \"Peywendkar\", \"Koordinator\", \"Xwedî\"],\n",
    "\t'Norwegian': [\"Ingeniør\", \"Konsulent\", \"arkitekt\", \"SPECIALIST\", \"Manager\", \"Utvikler\", \"Arkitektur\", \"Administrator\", \"Veileder\", \"Koordinator\", \"Leder\"],\n",
    "\t'Polish': [\"Inżynier\", \"Konsultant\", \"Architekt\", \"Specjalista\", \"Manager\", \"Programista\", \"Administrator\", \"Przełożony\", \"Koordynator\", \"Wykonawczy\"],\n",
    "\t'Portuguese': [\"Engenheiro\", \"Consultor\", \"Arquiteto\", \"Especialista\", \"Gerente\", \"Desenvolvedor\", \"Arquitetura\", \"Administrador\", \"Coordenador\", \"Executivo\"],\n",
    "\t'Romanian': [\"Inginer\", \"Consultant\", \"Arhitect\", \"Specialist\", \"Manager\", \"Dezvoltator\", \"Arhitectura\", \"Administrator\", \"Supraveghetor\", \"Coordonator\", \"Executiv\"],\n",
    "\t'Slovakian': [\"Inžinier\", \"Konzultant\", \"architekt\", \"SPECIALISTA\", \"Manažér\", \"Vývojár\", \"Architektúra\", \"Správca\", \"Dozorca\", \"Koordinátor\", \"Výkonný\"],\n",
    "\t'Slovenian': [\"Inženir\", \"Inženiring\", \"Svetovalec\", \"arhitekt\", \"SPECIALIST\", \"Vodja\", \"Razvijalec\", \"Arhitektura\", \"Administrator\", \"Nadzornik\", \"Koordinator\", \"Izvršni\"],\n",
    "\t'Swedish': [\"Ingenjör\", \"Konsult\", \"Arkitekt\", \"Specialist\", \"Chef\", \"Utvecklare\", \"Arkitektur\", \"Administratör\", \"Handledare\", \"Koordinator\", \"Verkställande\"],\n",
    "\t'Turkish': [\"Mühendis\", \"Danışman\", \"Mimar\", \"Uzman\", \"Yönetici\", \"Geliştirici\", \"Mimarlık\", \"Yönetici\", \"Koordinatör\", \"Yönetici\"],\n",
    "\t'Japanese': [\"エンジニア\", \"エンジニアリング\", \"コンサルタント\", \"アーキテクト\", \"スペシャリスト\", \"マネージャー\", \"開発者\", \"アーキテクチャー\", \"管理者\", \"責任者\", \"リーダー\"],\n",
    "    'Korean': [\"엔지니어\", \"엔지니어링\", \"컨설턴트\", \"건축가\", \"전문가\", \"매니저\", \"개발자\", \"아키텍처\", \"관리자\", \"책임자\", \"리더\", \"감독자\", \"코디네이터\", \"임원\"],\n",
    "    'Chinese_TR': [\"工程師\", \"工程\", \"顧問\", \"建築師\", \"專家\", \"經理\", \"開發者\", \"架構\", \"管理員\", \"負責人\", \"領導\", \"監督者\", \"協調員\", \"行政人員\"],\n",
    "    'Chinese_SP': [\"工程师\", \"工程\", \"顾问\", \"建筑师\", \"专家\", \"经理\", \"开发者\", \"架构\", \"管理员\", \"负责人\", \"领导\", \"监督者\", \"协调员\", \"行政人员\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_terms_non_english = {\n",
    "\t'Arabic': [\"بيانات\", \"ETL\", \"سحابة\", \"تحليلي\", \"تحليلات\", \"ذكاء الأعمال\", \"تحليلات الأعمال\", \"قاعدة بيانات\", \"خط أنابيب\", \"بيانات وصفية\", \"رصد\", \"مركز بيانات\"],\n",
    "\t'Basque': [\"Datuak\", \"ETL\", \"Cloud\", \"Analitikoa\", \"Analitika\", \"BI\", \"Negozioaren Inteligentzia\", \"Negozioaren Analitika\", \"Datubasea\", \"Pipeline-a\", \"Metadatuak\", \"Monitoreo\", \"Datuen zentroa\"],\n",
    "\t'Catalan': [\"Dades\", \"ETL\", \"Núvol\", \"Analític\", \"Anàlisi de dades\", \"BI\", \"Intel·ligència de negocis\", \"Anàlisi de negocis\", \"Base de dades\", \"Pipeline\", \"Metadades\", \"Monitorització\", \"Centre de dades\"],\n",
    "\t'Czech': [\"Data\", \"ETL\", \"Cloud\", \"Analytický\", \"Analytika\", \"BI\", \"Business Intelligence\", \"Business Analytics\", \"Databáze\", \"Pipeline\", \"Metadata\", \"Monitorování\", \"Datacentrum\"],\n",
    "\t'German': [\"Daten\", \"ETL\", \"Cloud\", \"Analytisch\", \"Analytics\", \"BI\", \"Business Intelligence\", \"Business Analytics\", \"Datenbank\", \"Pipeline\", \"Metadaten\", \"Überwachung\", \"Rechenzentrum\", \"Datenzentrum\"],\n",
    "\t'Danish': [\"Data\", \"ETL\", \"Cloud\", \"Analytisk\", \"Analyse\", \"BI\", \"Forretningsanalyse\", \"Database\", \"Pipeline\", \"Metadata\", \"Overvågning\", \"Datacenter\"],\n",
    "\t'Dutch': [\"Business Analytics\", \"Monitoring\", \"Leiding\", \"Metadata\", \"Cloud\", \"Business Intelligence\", \"Data\", \"Analytisch\", \"BI\", \"Database\", \"Datacenter\", \"Pipeline\", \"Analytics\", \"Bedrijfsanalyse\", \"Bedrijfsinformatie\", \"ETL\"],\n",
    "\t'Spanish': [\"Datos\", \"ETL\", \"Nube\", \"Analítico\", \"Análisis\", \"BI\", \"Inteligencia de Negocios\", \"Análisis de Negocios\", \"Base de datos\", \"Pipeline\", \"Metadatos\", \"Monitoreo\", \"Centro de datos\"],\n",
    "\t'Finnish': [\"Data\", \"ETL\", \"Pilvi\", \"Analytiikka\", \"BI\", \"Liiketoiminta-analytiikka\", \"Tietokanta\", \"Putkisto\", \"Metatiedot\", \"Seuranta\", \"Tietokeskus\"],\n",
    "\t'French': [\"Architecte\", \"Développeur\", \"architecte\", \"SPECIALISTE\", \"SPÉCIALISTE\", \"Consultant\", \"Conseiller\", \"Ingénieur\", \"Administrateur\", \"Architecture\", \"Manager\", \"Ingénierie\"],\n",
    "\t'Frisian': [\"Data\", \"ETL\", \"Cloud\", \"Analytysk\", \"Analitika\", \"BI\", \"Bisykens Intelligence\", \"Bisykens Analytics\", \"Database\", \"Pipeline\", \"Metadata\", \"Monitoring\", \"Datacenter\"],\n",
    "\t'Galician':  [\"Datos\", \"ETL\", \"Nube\", \"Analítica\", \"Analítica de datos\", \"BI\", \"Intelixencia de negocios\", \"Analítica de negocios\", \"Base de datos\", \"Pipeline\", \"Metadatos\", \"Monitorización\", \"Centro de datos\"],\n",
    "\t'Greek': [\"Δεδομένα\", \"ETL\", \"Νέφος\", \"Αναλυτική\", \"Ανάλυση\", \"BI\", \"Επιχειρηματική Νοημοσύνη\", \"Επιχειρηματική Αναλυτική\", \"Βάση δεδομένων\", \"Αγωγός\", \"Μεταδεδομένα\", \"Παρακολούθηση\", \"Κέντρο δεδομένων\"],\n",
    "\t'Hebrew': [\"נתונים\", \"ETL\", \"ענן\", \"ניתוח\", \"ניתוח נתונים\", \"BI\", \"בינה מעסיקתית\", \"אנליטיקה עסקית\", \"מסד נתונים\", \"צינורות נתונים\", \"מטא נתונים\", \"מעקב\", \"מרכז נתונים\"],\n",
    "\t'Hungarian': [\"Mérnök\", \"Mérnöki\", \"Tanácsadó\", \"építész\", \"SZAKÉRTŐ\", \"Menedzser\", \"Fejlesztő\", \"Architektúra\", \"Rendszergazda\"],\n",
    "\t'Italian': [\"Dati\", \"ETL\", \"Cloud\", \"Analitico\", \"Analytics\", \"BI\", \"Business Intelligence\", \"Business Analytics\", \"Database\", \"Pipeline\", \"Metadati\", \"Monitoraggio\", \"Centro dati\"],\n",
    "\t'Kurdish': [\"Zanist\", \"ETL\", \"Pirsgirêk\", \"Analytîk\", \"Analîz\", \"BI\", \"Zanistên Kar\", \"Analîzên Kar\", \"Bingehbazî\", \"Pîpelya\", \"Meta-Data\", \"Pêşwazî\", \"Navenda Zanistê\"],\n",
    "\t'Norwegian': [\"Data\", \"ETL\", \"Sky\", \"Analytisk\", \"Analytics\", \"BI\", \"Forretningsinnsikt\", \"Forretningsanalyse\", \"Database\", \"Pipeline\", \"Metadata\", \"Overvåking\", \"Databehandlingssenter\"],\n",
    "\t'Polish': [\"Dane\", \"ETL\", \"Chmura\", \"Analityczny\", \"Analityka\", \"BI\", \"Business Intelligence\", \"Analityka Biznesowa\", \"Bazy Danych\", \"Pipeline\", \"Metadane\", \"Monitorowanie\", \"Centrum Danych\"],\n",
    "\t'Portuguese': [\"Dados\", \"ETL\", \"Nuvem\", \"Analítico\", \"Análise\", \"BI\", \"Inteligência de Negócios\", \"Análise de Negócios\", \"Banco de Dados\", \"Pipeline\", \"Metadados\", \"Monitoramento\", \"Centro de Dados\"],\n",
    "\t'Romanian': [\"Date\", \"ETL\", \"Noroi\", \"Analitic\", \"Analize\", \"BI\", \"Business Intelligence\", \"Analiză de afaceri\", \"Bază de date\", \"Conductă\", \"Metadate\", \"Monitorizare\", \"Centru de date\"],\n",
    "\t'Slovakian': [\"Data\", \"ETL\", \"Cloud\", \"Analytický\", \"Analytika\", \"BI\", \"Business Intelligence\", \"Business Analytics\", \"Databáza\", \"Pipeline\", \"Metadata\", \"Monitorovanie\", \"Datacentrum\"],\n",
    "\t'Slovenian': [\"Podatki\", \"ETL\", \"Oblak\", \"Analitični\", \"Analitika\", \"BI\", \"Poslovna Inteligenca\", \"Poslovna Analitika\", \"Podatkovna Baza\", \"Cevovod\", \"Metapodatki\", \"Spremljanje\", \"Podatkovni Center\"],\n",
    "\t'Swedish': [\"Data\", \"ETL\", \"Moln\", \"Analys\", \"Analytik\", \"BI\", \"Affärsinriktad Analys\", \"Business Intelligence\", \"Databas\", \"Pipeline\", \"Metadata\", \"Övervakning\", \"Datacenter\"],\n",
    "\t'Turkish': [\"Veri\", \"ETL\", \"Bulut\", \"Analitik\", \"Analiz\", \"BI\", \"İş Zekası\", \"İş Analizi\", \"Veritabanı\", \"Boru Hattı\", \"Meta Veri\", \"İzleme\", \"Veri Merkezi\"],\n",
    "\t'Japanese': [\"データ\", \"ETL\", \"クラウド\", \"分析的な\", \"アナリティクス\", \"BI\", \"ビジネスインテリジェンス\", \"ビジネスアナリティクス\", \"データベース\", \"パイプライン\", \"メタデータ\", \"モニタリング\", \"データセンター\"],\n",
    "    'Korean': [\"데이터\", \"ETL\", \"클라우드\", \"분석적인\", \"애널리틱스\", \"BI\", \"비즈니스 인텔리전스\", \"비즈니스 애널리틱스\", \"데이터베이스\", \"파이프라인\", \"메타데이터\", \"모니터링\", \"데이터 센터\"],\n",
    "    'Chinese_TR': [\"數據\", \"ETL\", \"雲端\", \"分析\", \"分析學\", \"商業智慧\", \"商業分析\", \"數據庫\", \"管道\", \"元數據\", \"監控\", \"數據中心\"],\n",
    "    'Chinese_SP': [\"数据\", \"ETL\", \"云端\", \"分析\", \"分析学\", \"商业智慧\", \"商业分析\", \"数据库\", \"管道\", \"元数据\", \"监控\", \"数据中心\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_engineering_job(job_title: str, country = None):\n",
    "\n",
    "    specializations = [\"Engineer\", \"Engineering\", \"Consultant\", \"Architect\", \"Specialist\", \"Manager\", \"Developer\", \"Architecture\", \"Administrator\", \"Head of\", \"Lead\", \"Director\", \"Supervisor\", \"Coordinator\", \"Executive\"]\n",
    "\n",
    "    # Back-end, fullstack, data scientists are somehow different domains, but sometimes in some companies they are just the same roles as data engineers\n",
    "    data_terms = [\"Data\", \"ETL\", \"Cloud\", \"Analytical\", \"Analytics\", \"BI\", \"Buisness Intelligence\", \"Buisness Analytics\", \"Database\", \"Pipeline\", \"Metadata\", \"Monitoring\", \"Datacenter\"]\n",
    "\n",
    "    any_in_specs = isinstance(job_title, str) and any(isinstance(spec, str) and spec.lower() in job_title.lower() for spec in specializations)\n",
    "\n",
    "    any_in_terms = isinstance(job_title, str) and any(isinstance(term, str) and term.lower() in job_title.lower() for term in data_terms)\n",
    "\n",
    "    invalid = [\n",
    "        \"Mobile engineer\", \"Mobile Developer\", \"Android Developer\", \n",
    "        \"Android Engineer\", \"Biomedical Engineer\", \"Engineering Geologist\", \n",
    "        \"Geotechnical Engineer\", \"Electrical Engineer\", \"Project Manager\", \n",
    "        \"Quality Engineer\", \"QA Engineer\", \"Mechanical Engineer\", \n",
    "        \"Mechanical-Design Engineer\", \"Mobile App Developer\", \"Full-Stack\", \n",
    "        \"Fullstack\", \"Full Stack\", \"Machine Learning Engineer\", \"Front\", \"FrontEnd\",\n",
    "        \"Client-Side\", \"Support Engineer\", \"Data Scientist\", \"Computer Vision Engineer\",\n",
    "        \"C# Software Engineer\", \"Verification engineer\", \"Networking Software Engineer\",\n",
    "        \"Machine Learning Engineer\", \"Manual\", \"Deep Learning Engineer\", \"Reliability Engineer\",\n",
    "        \"Field\", \"Account Manager\", \"Solutions Engineer\"\n",
    "               ]\n",
    "    \n",
    "    if country:\n",
    "\n",
    "        country_languages = countries_languages[country]\n",
    "\n",
    "        for language in country_languages:\n",
    "\n",
    "            _invalid_not_eng = invalid_non_eng[language]\n",
    "            _specializations_not_eng = specializations_non_english[language]\n",
    "            _data_terms_not_eng = data_terms_non_english[language]\n",
    "\n",
    "            invalid.extend(_invalid_not_eng)\n",
    "            specializations.extend(_specializations_not_eng)\n",
    "            data_terms.extend(_data_terms_not_eng)\n",
    "        \n",
    "    is_valid = isinstance(job_title, str) and not any(isinstance(term, str) and term.lower() in job_title.lower() for term in invalid)\n",
    "\n",
    "    return any_in_specs and any_in_terms and is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_unique_and_its_len(df: pd.Series):\n",
    "    print(f\"{len(df.unique())} :\\n{df.unique()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Austria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Austria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622 :\n",
      "['Component Verification and Product Characterization Engineer (f/m/div)*'\n",
      " 'Freelance Hardware/ Data Centre Field Engineer'\n",
      " 'Senior Staff Engineer Digital Verification (f/m/div)*'\n",
      " 'Senior Staff Engineer Product Development for GaN-based ICs (f/m/div)*'\n",
      " 'Product Application Engineer (f/m/div)*'\n",
      " 'Senior Firewall Engineer (all genders)'\n",
      " 'Security Engineer (all genders)'\n",
      " 'Sr. Software Development Engineer – C++' 'Android Mobile Developer'\n",
      " 'Junior Data Warehouse und BI Engineer'\n",
      " 'Oracle APEX Entwickler (m/w/d) - PL/SQL Data Base Engineer'\n",
      " 'Data Engineer, CRM (m/f/x)'\n",
      " 'Solution Manager (all genders) #databricks #cloud #python'\n",
      " 'Senior Java Software Engineer for George (all genders)'\n",
      " 'Process Manager (m/f/d) 80% Homeoffice'\n",
      " 'Internship - Data Science or Software Engineering'\n",
      " 'Junior PHP and Machine Learning Engineer'\n",
      " 'Data Engineer im Bereich Data Integration (m/w/d)'\n",
      " 'Data Engineer (m/w/d)' 'Data Engineer (f/m/d)'\n",
      " 'CBS Data Engineer (f/m/x)' 'Data Engineer' 'System Engineer (m/w/d)'\n",
      " 'Senior Data Engineer | Europe | Remote' 'Data Engineer*' 'Data engineer'\n",
      " 'Drei TV Tester - 20h (f/m/d)' 'Data Engineer temp. 24 months (w/m/d)'\n",
      " 'Senior Data Engineer (m/f/x)' '.NET Web Developer (m/w/d) I 90% Remote'\n",
      " 'Systemadministrator Backup (m/w/d) Junior - Senior System Engineer'\n",
      " '.NET/React Developer (m/w/d)' '.NET Software Engineer – Vienna, Austria'\n",
      " 'Data Analyst & -Engineer (m/w/d)'\n",
      " 'Data Warehouse DevOps Engineer (f/m/x)'\n",
      " 'Data Engineer (m/f/d) - maternity cover 1yr.' 'Backend Engineer (m/f/x)'\n",
      " 'Data Virtualization (Denodo) Engineer (f/m/x)' 'IT Support Engineer'\n",
      " 'Performance Test Engineer' 'Junior - Data Engineer (m/f/x)'\n",
      " '(Senior) Data Engineer (m/w/d) (Remote innerhalb von...'\n",
      " 'Full Stack Engineer (m/f/x)' 'Data Engineer (f/m/x)'\n",
      " 'IT Security Engineer (m/w/d) - Cyber Security Agent' 'Project Engineer'\n",
      " 'Administrator' 'Data Scientist / Machine Learning Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/div.)'\n",
      " 'Software Tester / Verification and Validation (V&V) Engineer'\n",
      " 'CAMO Engineer Avionics (m/f/d)'\n",
      " 'Group Product Owner Financial Crime Prevention Data Platform (f/m/x)'\n",
      " 'Software-Engineer für Data Analytics und Chatbot Design (all genders)'\n",
      " 'Data Engineer (m/f/d)' 'Internship - Data Science' 'DevOps Engineer'\n",
      " 'Data Center Engineer - Austria - Vienna - Part time'\n",
      " 'Data Scientist (f/m/x)' 'Junior Database Engineer (m/f/d)'\n",
      " 'Development Engineer and Data Analyst (w/m/d)'\n",
      " 'IT Support Engineers für die Bereiche Azure Data Analytics, eCommerce oder Business Applications (m/w/d)'\n",
      " 'Full Stack Software Engineer – Focus on Big Data (m/w/d)'\n",
      " 'Data Engineer / Data Analyst (m/w) (m/w/d)'\n",
      " 'Data Warehouse & BI Engineer (m/w/d)'\n",
      " 'JUNIOR MACHINE LEARNING ENGINEER (M/W/D)' 'Junior/Senior Data Engineer'\n",
      " '(Senior) Data Science Engineer - Digitale Services (m/w/x)'\n",
      " 'Kunst-Auktions-Software Data Analyst oder Engineer (Student / Absolvent)'\n",
      " 'C++ Entwickler (m/w/d) mit Schwerpunkt Machine Learning'\n",
      " 'Advanced Engineering eDrive Control Systems and Software Engineer (m/f/d)'\n",
      " 'Data Engineer / Software Engineer (w/m/d)'\n",
      " 'Data Engineer bzw. Data Engineerin' 'Automotive Product Owner (m/f/d)'\n",
      " 'Senior Data Engineer (f/m/div)*'\n",
      " 'International Graduate Program: Senior Engineer Technology Development (f/m/div)*'\n",
      " 'Optical Engineer - Technology Development'\n",
      " 'Senior Software Engineer for Data Products & Services (f/m/d)'\n",
      " 'JUNIOR - DATA ENGINEER' 'Data Analyst - Business Intelligence (m/w/d)'\n",
      " 'Data Scientist/Engineer (f/m/d)'\n",
      " 'Data Engineer – Python and SQL Specialist (m/f/d)'\n",
      " 'Data Engineer für Business Intelligence und Analytics (m/w/d)'\n",
      " 'IT System Administrator'\n",
      " 'Corporate IT - Senior Data Center Engineer (m/w)'\n",
      " 'Senior Mobile App Developer (f/m/x)'\n",
      " 'Logistic Support Analysis (LSA) Engineer (m/f/d) 1'\n",
      " 'Software Engineer for George (all genders)'\n",
      " 'Data Engineer für Wind- und Solarkraftwerke im In- und Ausland (w/m/d)'\n",
      " 'Junior System Engineer (Ultra-Wide-Band) (m/f/d)'\n",
      " 'Internship, Material Work Planner'\n",
      " 'Hardware Development Engineer (f/m/d)'\n",
      " 'Junior Data Engineer (Teilzeit oder Vollzeit)' 'Data Analyst (w/d/m)'\n",
      " 'Software Engineer (All genders)'\n",
      " 'Senior Data Engineer für Vertriebssysteme (m/w/x)'\n",
      " 'Development Engineer in the field of powertrain system development (f/m/d)'\n",
      " 'DATA CENTER ENGINEER Business Operations (m/w/d)'\n",
      " 'Senior Data Engineer (m/w/d)' 'Senior Big Data Engineer (w/m/x)'\n",
      " 'Jr. System Administrator (m/f/d)'\n",
      " 'Solution Engineer für Data Infrastructure- und Cloud-Lösungen (m/w/d)'\n",
      " 'Lean Ops Engineer B2C (all genders) - Summer Internship'\n",
      " 'Software Engineer (m/w/d) - Fokus Data Science & Artificial Intelligence'\n",
      " 'Business Intelligence Planning Specialist (f/m/d)'\n",
      " 'data- & software engineer - digitalisierung (w/m/d)'\n",
      " 'Praktikum/Bachelor- oder Masterarbeit IIOT-Data Engineering für Industrie 4.0 (m/w/d)'\n",
      " 'internship: Improvement Engineer (f/m/d)' 'Data Analyst (m/w/d)'\n",
      " 'Data Engineer - Maschinendaten & IoT (m/w/d)'\n",
      " 'Big Data Engineer - Cloud Services (m/w/d)'\n",
      " 'Software Engineer (f/m/d): Big Life Science Data'\n",
      " 'Trainee Software Engineering (f/m/d)'\n",
      " 'DevOps for Data Products & Services (f/m/d)'\n",
      " 'System Application Engineer (f/m/div)*'\n",
      " 'IT Support Engineer (m/f/x) for internal Employee Support'\n",
      " 'Data Engineer – Fokus Event Streaming (m/w/x)'\n",
      " 'Junior Software Quality Engineer (m/w/d)'\n",
      " 'Engineering Change Management Engineer (m/w/d)'\n",
      " 'Software Development Engineer (f/m/d)'\n",
      " 'Junior Data Warehouse & BI Engineer'\n",
      " 'Internship: Lab Verification Engineer in the AMS Lab of the Development Center Villach (f/m/div)*'\n",
      " 'Junior Research Engineer (m/w/d) im Bereich Datenanalysen und Verkehrssicherheitsforschung'\n",
      " 'Data Scientist/Engineer (w/m/d)' 'IT System Engineer Security (w/m/d)'\n",
      " 'Data Engineer in F&E-Oberflächentechnik (m/w/d)'\n",
      " 'Software Developer C#/dotnet (m/w/d)' 'Expert Device Technology (m/w/d)'\n",
      " 'Senior Staff Product Definition Engineer (f/m/div)*'\n",
      " 'IT Support und Process Engineer (m/w/d) – Vollzeit in Wien'\n",
      " 'Machine Learning Engineer (m|f|d)' 'Data Engineer Wasserkraft (w/m/d)'\n",
      " 'Material Work Planner' 'Requirements Engineer Data Warehouse (d/m/w)'\n",
      " 'Software Development Lead for Data Engineering (f/m/x)'\n",
      " 'Data Warehouse & BI Engineer (m/w/x)' 'Cloud Solution Engineer (f/m/x)'\n",
      " 'Trainee Cloud Engineer' 'MACHINE LEARNING ENGINEER (M/W/D)'\n",
      " 'Geoinformation System Engineer (f/m/d)'\n",
      " 'Senior Full-Stack Engineer - TypeScript (m/f/x)'\n",
      " 'Senior Java Software Developer with focus on Big Data and High Performance Computing (m/f/x)'\n",
      " 'DATA STRATEGY CONSULTANT (M/F/D)'\n",
      " 'Research Engineer (m/w/d) – Backend Developer:in for Data-Centric Services Klimawandelfolgen und Gefahrenschutz'\n",
      " 'IT Service/Product Owner (Sales Domain) (m/f/d)'\n",
      " 'IT Operations Engineer (f/m/d)' 'Vibration Measurement Engineer (m/f/d)'\n",
      " 'Senior Mechanical Engineer (m/w/d)' 'IT Network Engineer'\n",
      " 'Software Developer for Data Flow Solutions f/m/d'\n",
      " 'Cloud Engineer (m/w/x)' 'SRE/Cloud Engineer (f/m/x)'\n",
      " 'Data Engineer Realtime Viewership Prediction'\n",
      " 'Product Development Engineer for GaN-based Multichip Products (f/m/div)*'\n",
      " 'Software Quality Engineer for Solar Energy (m/f/x)'\n",
      " 'Lead Product Engineer (m/f/x): Technical Domain Leadership'\n",
      " 'Data Science Consultant (m/w/d)' 'Data Visualization Consultant (w/m/x)'\n",
      " '(Pre-)Sales Consultant für Data Infrastructure- und Cloud-Lösungen (m/w/d)'\n",
      " 'Analytics Solution Architect (w/m/x)'\n",
      " 'System Engineer – Monitoring & Analytics (m/f/d)'\n",
      " 'System Engineer (w/m/d) VMware SDDC' 'Frontend Engineering Intern'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Transition Engineer Transition and Launch (m/f/d)'\n",
      " 'Facility Shift Engineer (f/m/d)' 'Software Developer in Vienna (m/f/d)'\n",
      " 'Data Engineer (m/w/x)' 'CAM Engineer (f/m/d)'\n",
      " 'Data & Software Engineer (m/w/d)'\n",
      " 'Data Engineer – Maschinendaten & IoT (w/m/x)'\n",
      " 'Software Developer Backend - B2B & Wholesale IT Products (f/m/d)'\n",
      " 'Software-Engineer Data Analytics und Chatbot Design (w/m/d)'\n",
      " 'Data Warehouse Engineer m/w/d' 'Cloud Engineer für Salesforce (m/w/d)'\n",
      " 'Development Engineer Vehicle Application*'\n",
      " 'Flight Operations Engineer & EFB Administrator (m/f/d)'\n",
      " 'Product Manager / IT Project Lead'\n",
      " 'Senior Data Engineer (m/w/d) - Innsbruck'\n",
      " 'Calibration Engineer for Transmission, Hybrid, BEV & E-Axle (f/m/d)'\n",
      " 'Sr. Engineer Chassis Systems (m/f/d)'\n",
      " 'Digital Business Analyst:in for myUNIQA' 'Systems Engineering Intern'\n",
      " 'Staff Engineer Wafer Technology & Device Development (f/m/div)*'\n",
      " 'Graduate Secondary Technical College (HTL) Development Engineer Commercial Vehicles f/m/d'\n",
      " 'Trainee Cloud Engineer *' 'Mechanical Engineer (F/M/D)'\n",
      " 'Sales Engineer DACH (Technical Pre-Sales Specialist for Data Protection Solutions)'\n",
      " 'BI Engineerin bzw. BI Engineer'\n",
      " 'Lean Ops Engineer (all genders) - Summer Internship'\n",
      " 'Reliability Engineer (m/f/d)'\n",
      " 'Internship: High Speed and RF LAB Validation Engineer in Development Center Villach (f/m/div)*'\n",
      " 'Service Engineer f/m/d' 'Application Engineer (Junior)'\n",
      " 'Full Stack Engineer - Maritime' 'Data Engineer (w/m/x)'\n",
      " 'Senior C++/Node.js Software Engineer (m/f/x) for our Agent team'\n",
      " 'Senior Cloud Engineer (m/f/x) Open Source - remote'\n",
      " 'DevOps Engineer (m/w/d)' 'Software Quality Engineer (m/w/d)'\n",
      " 'Software Engineer - Launchpad' 'Clean Room Engineer (m/f/d)'\n",
      " 'HTL Development Engineer for Software and Functions (f/m/d)'\n",
      " 'Senior Network Engineer (m/w/d)'\n",
      " 'Internship: Full-Stack Software Developer' 'DWH Data Engineer (m/w/d)'\n",
      " 'Business Intelligence Specialist (w/m/d)'\n",
      " 'Database Reliability Engineer (f/m/x)' 'Data Engineer (m/f/x)'\n",
      " 'IT Operation Engineer – Big Data (m/w)'\n",
      " 'Embedded Software Engineer (m/f/d)' 'Advanced Quality Engineer (f/m/d)'\n",
      " 'Web Portal Fullstack Developer - B2B IT Products (f/m/d)'\n",
      " 'IT Applications specialist' '(Senior) Data Engineer (w/m/d)'\n",
      " 'Software Engineer – Input Data Analysis'\n",
      " 'Senior System Engineer Private Cloud (m/f/d) (Remote within Germany or Austria)'\n",
      " 'Senior Product Security Engineer (m/f/x)'\n",
      " 'Backend Software Engineer (m/f/d)' 'Software Engineer (m/w/div.)'\n",
      " 'Technical Consultant – Knowledge Graph Technologies (m/f/d)- Vienna, Austria'\n",
      " 'Data Warehouse Engineer (w/m/x)' 'Service Engineer w/m/d'\n",
      " 'Senior Software Engineer (PHP) – Remote or Vienna'\n",
      " 'System Engineer (f/m/x)' 'Lead Partner Technology Strategist'\n",
      " 'Associate Solutions Engineer - Bachelor/Master (Graduate) - Austria'\n",
      " 'Frontend developer in Portugal' 'Electrical Systems Support Engineer'\n",
      " 'Expert Packaging Engineer (m/w/d)'\n",
      " 'Architect Tractor and Mobile Working Machines f/m/d'\n",
      " 'Technical Service Manager (m/w/d)'\n",
      " 'Powertrain eDrive Specialist (m/w/d)'\n",
      " 'Scientific programmer (f/m/d, part-time 20h/week)'\n",
      " 'Expert Technical Development (m/w/d)'\n",
      " 'Staff / Senior Frontend Engineer - Platform'\n",
      " 'Research Engineer (m/f/d) for Applied AI'\n",
      " 'Talents of tomorrow - Technical graduate Process Engineering - Technical Development (m/f/d)'\n",
      " 'Full Stack Engineer (All genders)'\n",
      " 'Senior Sales Engineer - Security - Opportunity for Working Remotely , Vienna'\n",
      " 'Go Software Engineer, Commercial Systems'\n",
      " 'Senior Software Engineer PHP – Voice Services (m/w/d)'\n",
      " 'Senior Web Application Security Engineer (w/m/d)'\n",
      " 'Lead Engineer Systems Engineering w/m/d' 'Senior Engineer (w/m/d)'\n",
      " 'Fullstack Developer' 'Associate Software Engineer (all genders)'\n",
      " 'Process Quality Engineer for FE/ME operations (m/f/d)'\n",
      " 'Senior Python Developer – Remote or Vienna'\n",
      " 'AWS DEV OPS Engineer (m/f/x)'\n",
      " 'Solution Architect Data Vault 2.0 (w/m/x)' 'CLOUD ENGINEER (M/W/D)'\n",
      " 'Electrical & Network Engineer (f/m/d)'\n",
      " 'Azure Dataplatform Engineer (f/m/d) A1 Group, Vienna Austria | Group Business Transformation & Execution'\n",
      " 'IT Infrastructure Engineer(P3)' 'CAMO Engineer (Avionics) (m/f/d)'\n",
      " 'Regulatory Affairs Associate (m/w/d)'\n",
      " 'ADAS/AD Systems Engineering w/m/d' 'ADAS/AD Systems Engineering f/m/d'\n",
      " 'Model based Calibration and Simulation Engineer f/m/d'\n",
      " 'Network & Security Specialist Schwerpunkt Firewall & Security'\n",
      " 'Student Trainee in the field of vehicle energy management (f/m/d)'\n",
      " 'IT System Administrator CAx/PDM/PLM f/m/d'\n",
      " 'Developer Data Warehouse (d/m/w)'\n",
      " 'Software Development Engineer in Test (SDET)'\n",
      " 'Team Lead Data Center (m/w/d)' 'QA Engineer - Full Stack'\n",
      " 'Research Engineer Artificial Intelligence & Machine Learning (m/w/d)'\n",
      " 'Junior NMS Engineer (f/m/*)' 'Junior Webtracking Engineer'\n",
      " 'Quality Engineer (m/w/d) – Voll-/Teilzeit in Wien'\n",
      " 'Staff Frontend Engineer (Design System) (f/m/d)' 'RAM Engineer (f/m/d)'\n",
      " 'Senior Application Engineer (f/m/d)' 'AWS Cloud Engineer'\n",
      " 'Senior Software Engineer (f/m/x)' 'Human Factors Engineer'\n",
      " 'Senior Backend Engineer (f/m/d) - Remote'\n",
      " 'Ubuntu Quality Engineering Manager'\n",
      " 'Senior Research Engineer (m/f/d) for Integrated Mobility Planning'\n",
      " 'Information Security Engineer (m/f/d)'\n",
      " 'Senior Fullstack Engineer (Frontend focus) (m/f/d)'\n",
      " 'Software Engineering Squad Leader - Container/Virtualisation - LXD'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Team Lead DevSecOps (m/f/d) - Home office possible'\n",
      " 'Technical Project Leader (f/m/d)' 'Growth Software Engineer (Python)'\n",
      " 'Presales Engineer Europe and Africa – Mobile Telecom Solutions Austria'\n",
      " 'Data Engineer | 30h (m/w/d)' 'Senior Plant Quality Engineer'\n",
      " 'Software Engineer m/f/d'\n",
      " 'Frontend (vue.js) Engineer - Bitpanda Pro (Funding)'\n",
      " 'Lead Engineer Cyber Security Engineering f/m/d'\n",
      " 'Scrum Master Automotive Software (m/f/d)'\n",
      " 'Azure DEV OPS Engineer (m/f/x)'\n",
      " 'Infrastructure Operations Engineer (f/m/d)'\n",
      " 'Global Product Manager E-Motor Test Systems & Drives f/m/d'\n",
      " 'Cleanroom Process Engineer - Wet Processing (f/m/d)'\n",
      " 'Lead Software Engineer for NeuroTech'\n",
      " 'Team Lead (m/f/x) Web Development'\n",
      " 'Presales Engineer Europe and Africa – Mobile Telecom Solutions Vienna'\n",
      " 'Senior Observability Platform Developer - Python/Go'\n",
      " 'Process Design Engineer (m/f/d)' 'Summertemp (m/f/d)'\n",
      " 'Software Engineer (Backend) (m/f/d)'\n",
      " 'Lead Engineer Power Electronic Development f/m/d'\n",
      " 'C++ System Architect (m/f/d)' 'Lead Engineer Systems Engineering f/m/d'\n",
      " 'Kalibrationsingenieur:in für Getriebe, Hybrid, Batterie-Elektrofahrzeuge & E-Achse'\n",
      " 'Cloud Architect (m/f/x)'\n",
      " 'Senior Software Engineer Quality Assurance (f/m/d) - Salzburg/Graz/Wien (AUT), Dresden (DE) / Home Office or Teleworking'\n",
      " 'Development Engineer Product Testing - Junior f/m/d' 'Platform Engineer'\n",
      " 'Sr. Research Engineer (m/f/d) Vehicle Architecture' 'APPRENTICE'\n",
      " 'AI Security Engineer' 'Manufacturing Engineer (m/f/d)'\n",
      " 'Network Security Architect & Engineer (m/w/d)'\n",
      " 'George DevOps Engineer (all genders)'\n",
      " 'Dual Studies in Computer Science (Bachelor) @ cargo-partner (m/w)'\n",
      " 'Junior Integration Engineer'\n",
      " 'Senior Product Experience Designer (m/f/x) - Application Security'\n",
      " 'Softwareentwickler:in für Data Flow Solutions'\n",
      " 'Senior Java Software Engineer with SQL (m/f/x)' 'IoT Project Engineers'\n",
      " 'Junior Scientist (m/f/d) Biotechnology/Biochemistry'\n",
      " 'Development Engineer Software - MFLOPS Project - PhD Fellowship f/m/d'\n",
      " 'Chassis & Powertrain Engineer (m/f/d)'\n",
      " 'R&D Product Engineer (all genders)'\n",
      " 'Electrical Engineer for Automation and Industrial Solutions f/m/d'\n",
      " 'IT-Infrastructure Engineer (m/f/d)'\n",
      " 'Expert Packaging Secondary Engineer (m/w/d)'\n",
      " 'Forward-deployed Data Engineer' 'Manufacturing Engineer f/m/d'\n",
      " 'Yield Enhancement Engineer (m/f/d)'\n",
      " 'Development Engineer object-oriented Software Configuration f/m/d'\n",
      " 'Application Engineer Battery Test Systems f/m/d'\n",
      " 'Lead Engineer Testing Projects w/m/d'\n",
      " 'Project Administration (m/f/d) from 30h/week'\n",
      " 'MLOps Data Engineer w/m/d'\n",
      " 'E-mobility Advanced Electric Engineer (m/f/d)'\n",
      " 'Sr. Research Engineer (m/f/d) System Architecture'\n",
      " 'Cleanroom Process Engineer - Plasma Etch (f/m/d)'\n",
      " 'IT Architect/Technical Product Owner (f/m/*)'\n",
      " 'Automotive Software System Architect (m/w/d)' 'DevOps Engineer (m/f/d)'\n",
      " 'Automotive Software Architect (m/f/d)'\n",
      " 'Technical Team Leader - IC Substrate Manufacturing (m/f/d)'\n",
      " 'Engineering Manager - DevOps'\n",
      " 'Senior Application & DevOps Engineer (w/m/d)'\n",
      " 'Network System Engineer (w/m/x)' 'DevOps Engineer (m/w/div.)'\n",
      " 'DevOps Engineering Manager' 'Senior DevOps Engineer - Backend (m/f/x)'\n",
      " 'Backend Developer (m/w/d)'\n",
      " 'Engineering Manager- Ceph & Distributed Storage'\n",
      " 'Business Analyst (m/f/x)' 'Network Security Engineer (w/m/x)'\n",
      " 'Junior DevOPS/Database Engineer' 'Senior Backend Developer (m/f/d)'\n",
      " 'Web Developer (m/w/d)'\n",
      " 'Internship: Test & Measurement Innovation (m/f/d)'\n",
      " 'FPGA Developer (m/f/d)' 'Agile Program Manager (m/f/x)'\n",
      " 'Senior Automation Engineer f/m/d'\n",
      " 'Cloud Plattform / DevOps Engineer (m/w/d)' 'DevOps Engineer (m/w/x)'\n",
      " 'Junior Project Engineer Cyber Security (m/w/d)'\n",
      " 'Production Planning & Materials Management (m/f/d)'\n",
      " 'Java Software Engineer (w/m/d)'\n",
      " 'Technical Writer w/m/d (Karenzvertretung für 28 Std./Woche)'\n",
      " 'DevOps Engineer (w/m/x)' 'Process Engineer' 'Signal Processing Engineer'\n",
      " '(Junior) Software Engineer (w/m/x) in der objektorientierten Programmierung'\n",
      " 'Site Reliability Engineer (m/f/x) Digital Marketing'\n",
      " 'Junior DevOps Engineer' 'Datenbankentwickler für DWH (w/m/d)'\n",
      " 'Scrum Master/Agile Coach (f/m/*)' 'Firmware Manufacturing Engineer'\n",
      " 'Team Lead Automotive Software (m/f/d)'\n",
      " 'Security Engineer Automotive f/m/d'\n",
      " 'Trainee Automatisierungstechnik (m/w/d) - Software'\n",
      " 'Reinigungskraft Teilzeit (m/w/d)' 'Security Engineer Automotive w/m/d'\n",
      " 'Manufacturing Engineer w/m/d'\n",
      " 'Manufacturing Engineering Body in White & Paint'\n",
      " 'Model Based Systems Engineer*' 'IT Infrastructure Lead Engineer (w/m/x)'\n",
      " 'Software Engineer (w/m/d)' 'Full-stack Software Developer (m/w/x)'\n",
      " 'Advanced System Engineer im Bereich Technologieentwicklung (m/w/d)'\n",
      " 'Director of DTC E-Commerce'\n",
      " 'Global CRM Solution Architect and Operations (m/f/d)'\n",
      " 'Anti-Cheat Software Engineer C/C++'\n",
      " 'Assistant in Automotive Engineering f/m/d'\n",
      " 'Senior Infrastructure & Security Engineer'\n",
      " 'Solution Design - Requirements Engineer (f/m/d)'\n",
      " 'Senior Software Engineer in Forschung und Entwicklung Software Analyse (w/m/d) Vollzeit / Teilzeit'\n",
      " 'Lead Engineer Software & Controls w/m/d'\n",
      " 'Senior Java/Angular Full Stack Software Engineer (f/m/x)'\n",
      " 'Thesis - Development of sensors & test systems for fuel cells of electric vehicles'\n",
      " 'Cleanroom Process Engineer - E-beam Photolithography (f/m/d)'\n",
      " 'Frontend Developer (w/m/x)'\n",
      " '(Junior/Senior) IoT Engineer (w/m/x) mit Coding-Anteil'\n",
      " 'PAYROLL ANALYST' 'System Integration Engineer Linux'\n",
      " 'Software Engineer C++/C#'\n",
      " 'Continuous Improvement Manager PMCC Division (m/f/d)'\n",
      " 'Cloud Software Developer' 'Expert Engineering - Human Factors (m/w/d)'\n",
      " 'Calibrator for Diagnostic Coordination f/m/d'\n",
      " 'Lead Engineer Power Electronic Development w/m/d'\n",
      " 'Advanced Robotics Application Engineer'\n",
      " 'Embedded Systems Expert (m/f/d)'\n",
      " 'Software Engineering Manager - Container and Virtualisation Infrastructure'\n",
      " 'Linux Server Software Engineering Manager'\n",
      " 'Tech Lead & Development Manager (m/f/d)' 'Product Data Manager (m/w/d)'\n",
      " 'Senior Product Manager'\n",
      " 'Principal Software Engineer - Java | Europe | Remote'\n",
      " 'Senior Data Engineer' 'Chapter Lead - ad:s (m/f/d)'\n",
      " 'System Designer Software / Automation Engineer w/m/d 1'\n",
      " 'Senior Java Software Developer (m/f/x)'\n",
      " 'Cloud Security Architect (m/f/d)' 'Testautomation Engineer (m/f/d)'\n",
      " 'Green Hydrogen Process Engineer (m/f/d)'\n",
      " 'Softwareentwickler (m/w/d) Mobile Applikationen'\n",
      " 'Senior Software Engineer (iOS) (m/f/d)'\n",
      " 'Senior Engineer Core Platform Support' 'Projektmanager (m/w/d) Service'\n",
      " 'Senior Quality Engineer' 'Advanced Robotics Software Engineer (m/f/d)'\n",
      " 'IT System Engineer (m/w/d)' 'PE Engineer (f/m/d)'\n",
      " 'Senior Engineer (m/f/x)' 'Senior Automation Test Engineer'\n",
      " 'Senior QA Automation Engineer (f/m/x)'\n",
      " 'Calibration Engineer (W/M) - Wels (AT)' 'Senior NMS Engineer (f/m/*)'\n",
      " 'Sr. Research Engineer(m/f/d) Energy Management/Performance Simulation'\n",
      " 'ELECTRONIC ENGINEER (m/f/d)'\n",
      " 'Test Engineer for Acceptance Testing (f/m/d)'\n",
      " 'Marketing Engineer (f/m/d)'\n",
      " 'Lead Engineer Cyber Security Engineering w/m/d'\n",
      " 'Commissioning Engineer f/m/d' 'EMEA ESS – Head of Customer PMO'\n",
      " '(Senior) Java Engineer für Test Automation Tool (w/m/x)'\n",
      " 'Research Engineer, Hearing Implant Fitting (m/f/d)'\n",
      " 'Full Stack Quality Engineer'\n",
      " 'Senior Expert Compensation & Benefits (m/f/d)'\n",
      " 'Engineering Manager - Digital Workplace' 'Senior Systems Engineer'\n",
      " 'Lead Engineer Battery Design Module & Thermal w/m/d'\n",
      " 'Technical Project Lead for ADAS/AD, Connectivity and ADAS/AD EE integration f/m/d'\n",
      " 'Senior DevOps Cloud Engineer' 'Java Software Developer (m/f/x)'\n",
      " 'Senior Technical Consultant – Knowledge Graph / Wissensgraph Technologien (m/w/d)- Ulm, Deutschland'\n",
      " 'Principal Software Architect' 'Application Engineer (w/m/x)'\n",
      " 'Senior Analysis Engineer - NVH Simulation f/m/d' 'E-Commerce Manager'\n",
      " 'Development Environment Engineer (f/m/d)'\n",
      " 'Sr. Research Engineer - Dynamic Motion Control (m/f/d)'\n",
      " 'QA Automation Tester (m/f/d)' 'QA Engineer - MNA' 'Senior QA Engineer'\n",
      " 'Senior Automation QA Engineer (C#)'\n",
      " 'Senior Test Automation Engineer (m/w/d)' 'QA Engineer'\n",
      " 'Thesis - Additive Manufacturing of instrumentation & test systems for fuel cell vehicles'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " 'Senior Global Automation Engineer (all genders)'\n",
      " 'Lead Proposal Manager (m/f/d) Management Consulting'\n",
      " 'Software Test Engineer' 'Lead Electrical Setroute Administrator'\n",
      " 'Senior Engineer Automation & Digitalization (m/f/d)' 'Product Manager'\n",
      " 'Lead Engineer Battery Design Module & Thermal f/m/d'\n",
      " 'Lead Engineer Software & Controls f/m/d' 'Sales Analytics Manager'\n",
      " 'Engineering Manager, Commercial Systems'\n",
      " 'Senior Engineer - Functional Safety (m/f)' 'Software Engineer JAVA'\n",
      " 'Senior C#/.NET Developer' 'Senior Software Engineer - Telco'\n",
      " 'Application & Support Engineer f/m/d' 'Project Manager NVH f/m/d'\n",
      " 'Senior Software Engineer - Digital Workplace'\n",
      " 'Quality Assurance Engineer (f/m/x)'\n",
      " 'Lead Principal Application Engineer (f/m/div)*'\n",
      " 'Software Engineer, Commercial Systems' 'Test Engineer w/m/d'\n",
      " 'Senior Cloud Engineer (m/f/x)'\n",
      " 'Senior Engineer Fab Simulation (w/m/div)*' 'Application Engineer f/m/d'\n",
      " 'Automation Hardware Engineer (m/w/d)'\n",
      " 'Calibration Engineer Operating Strategy f/m/d'\n",
      " 'Experienced Embedded C Software Developer (m/f/d)'\n",
      " 'Lead Engineer im Bereich H2 Produktentwicklung (m/w/d)'\n",
      " 'Project Manager Verification & Validation f/m/d'\n",
      " 'Senior Java Developer | Europe | Remote'\n",
      " 'Full Stack Developer - Java & Angular (f/m/d)'\n",
      " 'Development Engineer Product Testing - Experienced f/m/d'\n",
      " 'Senior DataOps Engineer'\n",
      " 'Embedded & Desktop Linux Software Engineer - Optimisation'\n",
      " 'Application Engineer w/m/d' 'Linux System Engineer'\n",
      " 'Web Developer (w/m/x)' 'Senior React Native Engineer'\n",
      " 'Lead Engineer TGA & Systemintegration f/m/d'\n",
      " 'Lead Engineer Vehicle Dynamics f/m/d' 'DevOps Engineer f/m/d'\n",
      " 'Specialist Testing Operation (via Personaldienstleister) w/m/d'\n",
      " 'Head of Research & Innovation f/m/d'\n",
      " 'Project Manager Chassis Function Development f/m/d'\n",
      " 'Cloud DevOps Engineer (m/w/d)'\n",
      " 'Program Manager Vehicle Integration f/m/d (Location Steyr)'\n",
      " 'Lead Engineer TGA & Systemintegration w/m/d'\n",
      " 'Advanced Robotics Process Engineer (m/f/d)' 'Software Engineer'\n",
      " 'Team Lead Datacenter (m/w/d)' 'Mobile Developer (m/w/d)'\n",
      " 'Application & Support Engineer w/m/d'\n",
      " 'Engineer Electric Motor Design (m/w/d)'\n",
      " 'Senior Full Stack Engineer - Onshore'\n",
      " 'Silicon Alliances Business Development Lead'\n",
      " 'Senior Software Engineer - Embedded & Desktop Linux Optimisation'\n",
      " 'Office Manager (m/w/d)' 'Embedded Systems Engineer - Robotics'\n",
      " 'Simulation Engineer - Funktionale Entwicklung & ADAS (m/w/d)'\n",
      " 'Projektmanager Automotive (m/w/d)'\n",
      " 'Quality Engineer Change & Concern Management f/m/d'\n",
      " 'Application Debugging Engineer'\n",
      " 'Senior Backend Developer / Software Architect - Java (f/m/*)'\n",
      " 'Senior Web Developer - Workplace Engineering'\n",
      " 'System Designer Software / Automation Engineer (w/m/d) 1'\n",
      " 'Senior DevOps Engineer' 'Head of Research & Innovation w/m/d'\n",
      " 'Equipment Performance Monitoring Engineer(P3)'\n",
      " 'Experienced Product/System Designer Electrical (Battery Test & Emulation Products) f/m/d'\n",
      " 'Buyer Service Purchasing f/m/d' 'DevOps Engineer w/m/d'\n",
      " 'Lead Engineer Vehicle Dynamics w/m/d'\n",
      " 'Senior Software Development Engineer - FinTech (m/w/d)'\n",
      " 'Group Expert Civil & Steel Structures (m/f/d)'\n",
      " 'PLQ-Techniker Fahrwerk (m/w/d)' 'Product Engineer (m/w/d)'\n",
      " 'Senior Backend Engineer' 'Senior Database Developer'\n",
      " 'Python Engineer (Blockchain Integration)' 'Backend Engineer (w/m/x)'\n",
      " 'Application Engineer (full-time, f/m/d)' 'Sustainable Design Engineer'\n",
      " 'Senior DevOps Engineer (m/w/d) - 90% Home-Office - JOB'\n",
      " 'Tool Validation Engineer' 'Enterprise Architect/in'\n",
      " 'FMEA Moderator f/m/d' 'Software Engineer C++ (f/m/d)'\n",
      " 'System Simulation Engineer *' 'Solution Design Engineer (f/m/d)'\n",
      " 'Assistent:in im Bereich Automotive Engineering'\n",
      " 'Software Engineer PHP - Cloud Management Platform (m/w/d)'\n",
      " 'SENIOR ELEKTRONIKINGENIEUR (m/f/d)' '(Senior) DevOps Consultant'\n",
      " 'Entwicklungsingenieur:in Systementwicklung Antriebsstrang'\n",
      " 'Entwicklungsingenieur Telematik/Connected Services (m/w/d)'\n",
      " 'Senior System Engineer (f/m/d)' 'Measurement Technician f/m/d'\n",
      " 'Proposal Engineer Offer Team Engineered Pumps (m/f/d)'\n",
      " 'Clojure / Java Software Engineer – Backend Focus (f/m/x)'\n",
      " 'Senior Salesforce Engineer, Commercial Systems' 'PHP Engineer (f/m/x)'\n",
      " 'Projektleiter Testing (m/w/d)' 'Senior Projektcontroller (m/w/d)'\n",
      " 'Teamleader E/E Architecture and Software (m/w/d)'\n",
      " 'Maintenance Engineer (m/f/d) - Electricity & Instrumentation'\n",
      " 'Technischer Projektleiter für ADAS/AD, Konnektivität und ADAS/AD EE Integration w/m/d'\n",
      " 'Quality Assurance Engineer (m/f/d)'\n",
      " 'Project Engineer Zonenmanagement Fahrzeugarchitektur (m/w/d)'\n",
      " 'Senior Konstrukteur (m/w/d)'\n",
      " 'Simulation Engineer für Funktionale Entwicklung & ADAS (m/w/d)'\n",
      " 'Sales Director - Cloud Sales Partner' 'Integration Engineer (EDI/EAI)'\n",
      " 'E-Mobility Advanced System Engineer (m/w/d)'\n",
      " 'CeMM International PhD Program in Molecular Technologies and Systems Medicine'\n",
      " 'Senior Project Engineer Zonenmanagement Fahrzeugarchitektur (m/w/d)'\n",
      " 'Senior Expert Solution Architect (f/m/d)'\n",
      " 'IT - Business Analyst and IT - Process Design Consultant (m/f/d)'\n",
      " 'Software Engineer - 7551' 'E/E Architect (m/w/d)'\n",
      " 'Key Account Manager – Data Center'\n",
      " 'Entwicklungsingenieur Fahrwerk (m/w/d)'\n",
      " 'Senior IT - Business Analyst and IT - Process Design Consultant (m/f/d)'\n",
      " 'Entwicklungsingenieur/Gesamtfahrzeug (m/w/d)'\n",
      " 'Project Manager Transmission & E-Axle Design f/m/d'\n",
      " 'Stellvertretender Teamleiter Testing (m/w/d) Fahrzeugsicherheit'\n",
      " 'Senior Entwicklungsingenieur Sondermaschinenbau (m/w/d)'\n",
      " 'PhD Thesis “Enhanced flexibility and robustness of autonomous systems through human - machine interaction”'\n",
      " 'Thesis - HV System Integration, aspects for the (geometrical) vehicle integration'\n",
      " 'E/E System Architect LV/HV (m/w/d)' 'Automic Workflow Engineer (m/f/d)'\n",
      " 'Thesis - Wheel Corner Integration, aspects for the (geometrical) vehicle integration'\n",
      " 'Solution Experience Lead - Principal Product Manager (m/f/x)'\n",
      " 'Senior Project Engineer Zonenmanagement Fahrzeugarchitektur (m/w)'\n",
      " 'Technical Consultant – Knowledge Graph Technologies (m/f/d)- Ulm, Germany'\n",
      " 'Manufacturing E/E Integration Engineer'\n",
      " 'Projektleiter Sondermaschinenbau (m/w/d)'\n",
      " 'Project Manager - Supply Chain Expansion'\n",
      " 'Entwicklungsingenieur EE Hardware Bauteile (m/w/d)'\n",
      " 'Cone-Beam Computed Tomography Enhancement Researcher (PhD student position)'\n",
      " 'OEM Partner Sales Director'\n",
      " 'Produktmanager:in E-Motor Testsysteme & Antriebe'\n",
      " 'IoT Sales Director - Linux' 'Test Engineer (w/m/d)'\n",
      " 'Senior Technical Consultant – Knowledge Graph Technologies (m/f/d)- Ulm, Germany'\n",
      " 'Thesis - Analysis of predictive fast charging algorithms to improve battery lifetime'\n",
      " 'DevOps/Site Reliability Engineer' 'Senior Automation QA Engineer - REX'\n",
      " 'Service Standard Pumps (m/w/d)'\n",
      " 'Pflichtpraktikant:in FH im Bereich Cost Engineering & Supply Chain Management'\n",
      " 'Senior .NET Software Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "        lambda row: is_data_engineering_job(row['Job_title'], \n",
    "        salary_type\n",
    "    ), \n",
    "    axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 :\n",
      "['Junior Data Warehouse und BI Engineer'\n",
      " 'Oracle APEX Entwickler (m/w/d) - PL/SQL Data Base Engineer'\n",
      " 'Data Engineer, CRM (m/f/x)'\n",
      " 'Solution Manager (all genders) #databricks #cloud #python'\n",
      " 'Internship - Data Science or Software Engineering'\n",
      " 'Data Engineer im Bereich Data Integration (m/w/d)'\n",
      " 'Data Engineer (m/w/d)' 'Data Engineer (f/m/d)'\n",
      " 'CBS Data Engineer (f/m/x)' 'Data Engineer'\n",
      " 'Senior Data Engineer | Europe | Remote' 'Data Engineer*' 'Data engineer'\n",
      " 'Data Engineer temp. 24 months (w/m/d)' 'Senior Data Engineer (m/f/x)'\n",
      " 'Data Analyst & -Engineer (m/w/d)'\n",
      " 'Data Warehouse DevOps Engineer (f/m/x)'\n",
      " 'Data Engineer (m/f/d) - maternity cover 1yr.'\n",
      " 'Data Virtualization (Denodo) Engineer (f/m/x)'\n",
      " 'Junior - Data Engineer (m/f/x)'\n",
      " '(Senior) Data Engineer (m/w/d) (Remote innerhalb von...'\n",
      " 'Data Engineer (f/m/x)' 'Data Engineer (m/w/div.)'\n",
      " 'Software-Engineer für Data Analytics und Chatbot Design (all genders)'\n",
      " 'Data Engineer (m/f/d)'\n",
      " 'Data Center Engineer - Austria - Vienna - Part time'\n",
      " 'Junior Database Engineer (m/f/d)'\n",
      " 'Development Engineer and Data Analyst (w/m/d)'\n",
      " 'Data Engineer / Data Analyst (m/w) (m/w/d)'\n",
      " 'Data Warehouse & BI Engineer (m/w/d)' 'Junior/Senior Data Engineer'\n",
      " '(Senior) Data Science Engineer - Digitale Services (m/w/x)'\n",
      " 'Kunst-Auktions-Software Data Analyst oder Engineer (Student / Absolvent)'\n",
      " 'Data Engineer / Software Engineer (w/m/d)'\n",
      " 'Data Engineer bzw. Data Engineerin' 'Senior Data Engineer (f/m/div)*'\n",
      " 'Senior Software Engineer for Data Products & Services (f/m/d)'\n",
      " 'JUNIOR - DATA ENGINEER'\n",
      " 'Data Engineer – Python and SQL Specialist (m/f/d)'\n",
      " 'Data Engineer für Business Intelligence und Analytics (m/w/d)'\n",
      " 'Corporate IT - Senior Data Center Engineer (m/w)'\n",
      " 'Data Engineer für Wind- und Solarkraftwerke im In- und Ausland (w/m/d)'\n",
      " 'Junior Data Engineer (Teilzeit oder Vollzeit)'\n",
      " 'Senior Data Engineer für Vertriebssysteme (m/w/x)'\n",
      " 'DATA CENTER ENGINEER Business Operations (m/w/d)'\n",
      " 'Senior Data Engineer (m/w/d)' 'Senior Big Data Engineer (w/m/x)'\n",
      " 'Solution Engineer für Data Infrastructure- und Cloud-Lösungen (m/w/d)'\n",
      " 'Software Engineer (m/w/d) - Fokus Data Science & Artificial Intelligence'\n",
      " 'data- & software engineer - digitalisierung (w/m/d)'\n",
      " 'Praktikum/Bachelor- oder Masterarbeit IIOT-Data Engineering für Industrie 4.0 (m/w/d)'\n",
      " 'Data Engineer - Maschinendaten & IoT (m/w/d)'\n",
      " 'Big Data Engineer - Cloud Services (m/w/d)'\n",
      " 'Software Engineer (f/m/d): Big Life Science Data'\n",
      " 'Data Engineer – Fokus Event Streaming (m/w/x)'\n",
      " 'Junior Data Warehouse & BI Engineer'\n",
      " 'Data Engineer in F&E-Oberflächentechnik (m/w/d)'\n",
      " 'Data Engineer Wasserkraft (w/m/d)'\n",
      " 'Requirements Engineer Data Warehouse (d/m/w)'\n",
      " 'Software Development Lead for Data Engineering (f/m/x)'\n",
      " 'Data Warehouse & BI Engineer (m/w/x)' 'Cloud Solution Engineer (f/m/x)'\n",
      " 'Trainee Cloud Engineer'\n",
      " 'Senior Java Software Developer with focus on Big Data and High Performance Computing (m/f/x)'\n",
      " 'DATA STRATEGY CONSULTANT (M/F/D)'\n",
      " 'Research Engineer (m/w/d) – Backend Developer:in for Data-Centric Services Klimawandelfolgen und Gefahrenschutz'\n",
      " 'Software Developer for Data Flow Solutions f/m/d'\n",
      " 'Cloud Engineer (m/w/x)' 'SRE/Cloud Engineer (f/m/x)'\n",
      " 'Data Engineer Realtime Viewership Prediction'\n",
      " 'Data Science Consultant (m/w/d)' 'Data Visualization Consultant (w/m/x)'\n",
      " '(Pre-)Sales Consultant für Data Infrastructure- und Cloud-Lösungen (m/w/d)'\n",
      " 'Analytics Solution Architect (w/m/x)'\n",
      " 'System Engineer – Monitoring & Analytics (m/f/d)'\n",
      " 'Software Engineer - Data Platform' 'Data Engineer (m/w/x)'\n",
      " 'Data & Software Engineer (m/w/d)'\n",
      " 'Data Engineer – Maschinendaten & IoT (w/m/x)'\n",
      " 'Software-Engineer Data Analytics und Chatbot Design (w/m/d)'\n",
      " 'Data Warehouse Engineer m/w/d' 'Cloud Engineer für Salesforce (m/w/d)'\n",
      " 'Senior Data Engineer (m/w/d) - Innsbruck' 'Trainee Cloud Engineer *'\n",
      " 'Sales Engineer DACH (Technical Pre-Sales Specialist for Data Protection Solutions)'\n",
      " 'BI Engineerin bzw. BI Engineer' 'Data Engineer (w/m/x)'\n",
      " 'Senior Cloud Engineer (m/f/x) Open Source - remote'\n",
      " 'DWH Data Engineer (m/w/d)' 'Data Engineer (m/f/x)'\n",
      " 'IT Operation Engineer – Big Data (m/w)' '(Senior) Data Engineer (w/m/d)'\n",
      " 'Software Engineer – Input Data Analysis'\n",
      " 'Senior System Engineer Private Cloud (m/f/d) (Remote within Germany or Austria)'\n",
      " 'Data Warehouse Engineer (w/m/x)'\n",
      " 'Architect Tractor and Mobile Working Machines f/m/d'\n",
      " 'Solution Architect Data Vault 2.0 (w/m/x)' 'CLOUD ENGINEER (M/W/D)'\n",
      " 'Azure Dataplatform Engineer (f/m/d) A1 Group, Vienna Austria | Group Business Transformation & Execution'\n",
      " 'Developer Data Warehouse (d/m/w)' 'Team Lead Data Center (m/w/d)'\n",
      " 'AWS Cloud Engineer'\n",
      " 'Senior Research Engineer (m/f/d) for Integrated Mobility Planning'\n",
      " 'Presales Engineer Europe and Africa – Mobile Telecom Solutions Austria'\n",
      " 'Data Engineer | 30h (m/w/d)'\n",
      " 'Presales Engineer Europe and Africa – Mobile Telecom Solutions Vienna'\n",
      " 'Senior Observability Platform Developer - Python/Go'\n",
      " 'Cloud Architect (m/f/x)' 'Forward-deployed Data Engineer'\n",
      " 'MLOps Data Engineer w/m/d'\n",
      " 'E-mobility Advanced Electric Engineer (m/f/d)'\n",
      " 'Junior DevOPS/Database Engineer'\n",
      " 'Cloud Plattform / DevOps Engineer (m/w/d)' 'Cloud Software Developer'\n",
      " 'Product Data Manager (m/w/d)' 'Senior Data Engineer'\n",
      " 'Cloud Security Architect (m/f/d)' 'Senior DevOps Cloud Engineer'\n",
      " 'Sales Analytics Manager' 'Senior Cloud Engineer (m/f/x)'\n",
      " 'Senior DataOps Engineer' 'Cloud DevOps Engineer (m/w/d)'\n",
      " 'Team Lead Datacenter (m/w/d)'\n",
      " 'Equipment Performance Monitoring Engineer(P3)'\n",
      " 'Senior Database Developer'\n",
      " 'Software Engineer PHP - Cloud Management Platform (m/w/d)'\n",
      " 'Sales Director - Cloud Sales Partner'\n",
      " 'E-Mobility Advanced System Engineer (m/w/d)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 Belgium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Belgium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 :\n",
      "['Data Engineer (met affiniteit voor Data Science)' 'data manager'\n",
      " 'Python Engineer on FAIR Data' 'Electromagnetic Simulations Engineer'\n",
      " 'Senior Big Data Engineer' 'System Engineer'\n",
      " 'Mechanical Design Engineer (Hudson Sharp)'\n",
      " 'ICT Traineeship - Data & Data Analytics' 'Field Engineer'\n",
      " 'Senior R&D Test Engineer (Fuel cells)' 'Azure Data Platform Engineer'\n",
      " 'Full-stack Developer | Digital Manufacturing Platform' 'DATA ENGINEER'\n",
      " 'IT System Engineer' 'Process Engineer Automotive Industry'\n",
      " 'Photonics Design Engineer' 'Medior Data Protection Engineer'\n",
      " 'DATA ENGINEER & MODELER' 'Data Engineer Traineeship'\n",
      " 'Junior Integration Engineer' 'Field and solutions engineer IOT'\n",
      " 'Supply Chain Engineer' 'R&D Electronics Engineer'\n",
      " 'ICT Traineeship - Development & Packages' 'Validation Engineer - CSV'\n",
      " 'R&D Professional – Power Systems / Data Engineer'\n",
      " 'Manufacturing Test Engineer' 'Junior Data Warehouse Engineer'\n",
      " 'ICT Traineeship - Networks, Telecom & Cybersecurity' 'Developer'\n",
      " 'Junior System Engineer' 'ICT Traineeship - Infrastructure & Operations'\n",
      " 'Azure Data Engineer' 'AWS Data Engineer'\n",
      " 'Service technieker regio Waregem/Kortrijk' 'Data Engineer'\n",
      " 'DevOps Engineer (Data)' 'Process Engineer - Lithography'\n",
      " 'Onderzoeker Innovatie Renovatie Brussel' 'Automation Engineer'\n",
      " 'Service technieker regio Knokke/Maldegem' 'Software Engineer - Kotlin'\n",
      " 'Data Engineer / Architect'\n",
      " '5G Core Network Operations and Automation Engineer'\n",
      " 'Project Engineer Sustainability' 'Data engineer / ETL consultant'\n",
      " 'Cloud Data Engineer' 'Datacenter Engineer'\n",
      " 'Enterprise Data Information Architect'\n",
      " 'Computer vision and image recognition data scientist' 'Project Engineer'\n",
      " 'Onderzoeker Smart Buildings Brussel' 'production supervisor'\n",
      " 'System Administrator' 'Proces Ingenieur | Groeitraject'\n",
      " 'Business Process Analyst' 'QA Complaint specialist'\n",
      " 'PRODUCTION DATA ENGINEER' 'R&D Data Steward' 'Data Migration Consultant'\n",
      " 'Electrical Engineer - Power Systems Transmission & Distribution'\n",
      " 'Cloud Engineer / Cloud Architect' 'Site Test Engineer'\n",
      " 'Production Engineer' 'quality system engineer'\n",
      " 'Continous improvement Engineer' 'R&D Engineer Remote Sensing'\n",
      " 'DevOps engineer' 'Software engineer Supply Chain'\n",
      " 'Industrial Data Engineer' 'Signalling Design Engineer'\n",
      " 'Engineer Elektriciteit' 'Reliability Engineer'\n",
      " 'Product Manager Entertainment - Data Collaborations'\n",
      " 'Senior DevOps Developer' 'Audit and Compliance Specialist'\n",
      " 'Computer System Validation Engineer' 'Catenary Engineer'\n",
      " 'Network & security engineer' 'IT Infrastructure Engineer'\n",
      " 'GMP Support Engineer' 'ICT Service Engineer'\n",
      " 'Maintenance & Reliability Engineer' 'Security Engineer'\n",
      " 'Linux system engineer' 'Cloud system engineer'\n",
      " 'Quality & Compliance Engineer' 'Robotics Automation Consultant'\n",
      " 'PRICING ANALYST' 'Process Engineer | Doorgroeimogelijkheden'\n",
      " 'Technical Analyst Financial Crime'\n",
      " 'Connectivity Support Engineer 2nd Line' 'QA Support Engineer'\n",
      " 'Cloud solution architect' 'DATA & APPLICATION SPECIALIST'\n",
      " 'Manager - IoT' 'Process Engineer' 'Reporting Systems Analyst'\n",
      " 'Plant Engineer automotive industrie' 'Ingénieur DWH'\n",
      " 'Data Engineer - First IT' 'Software Engineer'\n",
      " 'Master Data Process Engineer' 'Data analyst'\n",
      " 'Information Security Engineer' 'IT DATA ENGINEER'\n",
      " 'Data engineer | rechterhand IT Manager'\n",
      " 'Senior Logistics Automation Engineer' 'Medical Advisor'\n",
      " 'IT Allround - support engineer' 'Service Engineer'\n",
      " 'Technical Support Engineer' 'Chief Data Architect'\n",
      " 'Traineeship Quality & Compliance' 'SENIOR SYSTEM ENGINEER'\n",
      " 'Allround IT Support Engineer' 'Maintenance Engineer'\n",
      " 'Network Support Engineer' 'System Engineer Azure' 'Process engineer'\n",
      " 'MSAT Process Engineer' 'Service Planning Coordinator'\n",
      " 'Technisch bediende metaal'\n",
      " 'Technical Support Engineer Infection Control' 'Ingenieur schoolverlater'\n",
      " 'Bouwkundig ingenieur studiedienst' 'Field technician Turbines'\n",
      " 'Data Analyst | Diminuer ton empreinte carbone'\n",
      " 'IT Systeem- en Netwerkbeheerder' 'IT Project Manager'\n",
      " 'Senior Data Engineer' 'Elektrisch Ingenieur' 'Solution Architect'\n",
      " 'MAINTENANCE ENGINEER' 'Exellys - Data Engineer'\n",
      " 'Ervaren Software Engineer' 'Facility Manager' 'Content writer'\n",
      " 'AUTOMATION SYSTEM & DATA ENGINEER' 'Junior R&D Engineer'\n",
      " 'Mechanical engineer - Arendonk' 'Proces ingenieur'\n",
      " 'NTT Data - Automation Engineer - Pharmaceutical sector'\n",
      " 'Elektrisch Ingenieur Waterzuivering' 'DevOps Engineer*'\n",
      " 'Business Analyst' 'SERVICES PROJECT MANAGER FR/EN' 'BI Azure Analyst'\n",
      " 'Supply Chain Controller'\n",
      " 'DevOps Engineer | CloudFirst Technology | Brussels'\n",
      " 'Budget Controller - Bruxelles' 'Field technieker turbines'\n",
      " 'Open-source Database Reliability Engineer'\n",
      " 'JUNIOR CYBER SECURITY ENGINEER - FINTECH SAAS - CDI'\n",
      " 'Compliance Engineer' 'IT SYSTEM ENGINEER - HYBRID - GREEN ENERGY'\n",
      " 'Kalytech Consulting - Technical Support Engineer (2nd/3rd level)'\n",
      " 'NETWORK & SECURITY ENGINEER - Cisco / Palo Alto / WIFI & NAC'\n",
      " '(IT) Data Engineer' 'R&D Energie in Gebouwen'\n",
      " 'JUNIOR SYSTEM ENGINEER | Milieu'\n",
      " 'Development Engineer - Data & Reporting'\n",
      " 'Cybersecurity Engineer | FinTech IT infrastructure'\n",
      " 'Energy & Facility Engineer | Borgloon'\n",
      " 'Process Manager Internal Logistics'\n",
      " 'Ingenieur - Business Developer | Sector : Energie en Verlichting B2B | Kruisem'\n",
      " 'Bioinformatics Data Engineer' 'Ploegbaas elektriciteit'\n",
      " 'Exellys - Test (Automation) Engineer' 'SYSTEM ENGINEER | Familiebedrijf'\n",
      " 'Paradigm - Data Engineer'\n",
      " 'CYBER SECURITY ENGINEER - SAAS - 4 Days Homeworking'\n",
      " 'WINDOWS SYSTEM ENGINEER - Green Energy - brussels' 'data engineer'\n",
      " 'Database Devops Engineer' 'production data engineer'\n",
      " 'Productie Ingenieur' 'Productie ingenieur Nachtploeg | Dilsen Stokkem'\n",
      " 'SYSTEM ADMINISTRATOR / Hybride met bedrijfswagen'\n",
      " 'LINUX SYSTEM ADMINISTRATOR / Hybride met bedrijfswagen'\n",
      " 'Cybersecurity Engineer | Digital Finance'\n",
      " '3D Engineer (M/V) - Design & Engineer' 'System Engineer (Azure)'\n",
      " 'Cybersecurity Engineer |IT Solutions Finance sector' 'SECURITY ENGINEER'\n",
      " 'R&D Automation Engineer' 'Product Manager (SaaS)'\n",
      " 'R&D-ingenieur Automation' 'junior system engineer'\n",
      " 'Supply chain- en productiecoördinator'\n",
      " 'Senior Cybersecurity Engineer | Fintech Software Solutions'\n",
      " 'PRODUCTIE INGENIEUR' 'CYBER SECURTY - SAAS IN FINTECH - HOMEWORKING OP'\n",
      " 'Production Data Engineer' 'IT System Engineer | Energy Engineering'\n",
      " 'data analist' 'IT System Engineer - Finance Industry'\n",
      " 'SOFTWARE DEVELOPER' 'System Engineer (Azure omgeving)'\n",
      " '1st LINE SUPPORT ENGINEER / IT Consultancy' 'Lab Equipment Engineer'\n",
      " 'Cloud Engineer' 'Senior Cybersecurity Engineer | Finance'\n",
      " 'R&D Ingenieur Modellering'\n",
      " 'IT System Engineer | Leader in the Marketing & CRM Solutions'\n",
      " 'supply chain & productie data controller'\n",
      " 'Cyber Security Engineer - leader in Fintech Software Solutions'\n",
      " 'SAS Data Engineer' 'Automation engineer | Brussels | Packaging industry'\n",
      " 'Business Applications Developer'\n",
      " 'Nieuwe VacatureJava Developer | Internationaal| Spring boot,'\n",
      " 'Senior Service Engineer' 'Ploegbaas mechanica en koeling'\n",
      " 'Medior Data Engineer' 'Nieuwe vacature: Java Developer'\n",
      " 'DATA ENGINEER | IT sector'\n",
      " 'Elektrisch Ingenieur voor duurzame infrastructuurprojecten in de publieke sector.'\n",
      " 'Vivid Resourcing - Software Engineer / Computer Vision Engineer'\n",
      " 'PROCESS ENGINEER MANAGER | Transportation Company, Brussels'\n",
      " 'DWH Engineer' 'Cyber Security Engineer| IT Solutions - Law'\n",
      " 'DATA ENGINEER | Azure' 'Analytical Maintenance Engineer'\n",
      " 'Entry Level CRA'\n",
      " 'Kwantitatief analist(e) interne modellen – banken (V/M/X) - Master'\n",
      " 'Continuous Improvement Engineer' 'Maintenance Reliability Engineer'\n",
      " 'Data Governance Manager']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 :\n",
      "['Data Engineer (met affiniteit voor Data Science)' 'data manager'\n",
      " 'Python Engineer on FAIR Data' 'Senior Big Data Engineer'\n",
      " 'Azure Data Platform Engineer' 'DATA ENGINEER'\n",
      " 'Medior Data Protection Engineer' 'DATA ENGINEER & MODELER'\n",
      " 'Data Engineer Traineeship'\n",
      " 'R&D Professional – Power Systems / Data Engineer'\n",
      " 'Junior Data Warehouse Engineer' 'Azure Data Engineer'\n",
      " 'AWS Data Engineer' 'Data Engineer' 'DevOps Engineer (Data)'\n",
      " 'Data Engineer / Architect' 'Project Engineer Sustainability'\n",
      " 'Data engineer / ETL consultant' 'Cloud Data Engineer'\n",
      " 'Datacenter Engineer' 'Enterprise Data Information Architect'\n",
      " 'PRODUCTION DATA ENGINEER' 'Data Migration Consultant'\n",
      " 'Cloud Engineer / Cloud Architect' 'Industrial Data Engineer'\n",
      " 'Product Manager Entertainment - Data Collaborations'\n",
      " 'Cloud system engineer' 'Cloud solution architect'\n",
      " 'DATA & APPLICATION SPECIALIST' 'Data Engineer - First IT'\n",
      " 'Master Data Process Engineer' 'IT DATA ENGINEER'\n",
      " 'Data engineer | rechterhand IT Manager' 'Chief Data Architect'\n",
      " 'Senior Data Engineer' 'Exellys - Data Engineer'\n",
      " 'AUTOMATION SYSTEM & DATA ENGINEER'\n",
      " 'NTT Data - Automation Engineer - Pharmaceutical sector'\n",
      " 'DevOps Engineer | CloudFirst Technology | Brussels' '(IT) Data Engineer'\n",
      " 'Development Engineer - Data & Reporting' 'Bioinformatics Data Engineer'\n",
      " 'Paradigm - Data Engineer' 'data engineer' 'Database Devops Engineer'\n",
      " 'production data engineer' 'Production Data Engineer' 'Cloud Engineer'\n",
      " 'SAS Data Engineer' 'Medior Data Engineer' 'DATA ENGINEER | IT sector'\n",
      " 'DATA ENGINEER | Azure' 'Analytical Maintenance Engineer'\n",
      " 'Data Governance Manager']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'], \n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3 Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 :\n",
      "['Analytics Engineer' 'BI Engineer' 'Analytics Implementation Engineer'\n",
      " 'Process Analyst Engineer - Steel Plant'\n",
      " 'Financial Engineer/ACM Business Analyst'\n",
      " 'Lead Analytics Solutions Engineer'\n",
      " 'Analyst/Senior Associate, AML - Financial Engineering & Modeling (FEM)'\n",
      " 'Business Analyst & Process Engineer'\n",
      " 'Business Analyst Engineering Budget Planning'\n",
      " 'Senior Manager, Project Delivery Business Analyst, Retail & Small Business (Contract)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 :\n",
      "['Analytics Engineer' 'BI Engineer' 'Analytics Implementation Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4 Czech_Republic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also Slovak Republic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Czech_Republic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 :\n",
      "['Cloud Services Engineer (Based in Germany)' 'Technical Systems Manager'\n",
      " 'PLC Engineer ( F/M/X )' 'Application Engineer - suitable for graduates'\n",
      " 'Data Engineer' 'Data Engineer - Part time for a student'\n",
      " 'Junior ESG Data Analyst / Tester'\n",
      " 'Global Support Engineer – Mobile Data Network Czechia'\n",
      " 'Data Processing and Automation-Development Engineer (f/m/d)'\n",
      " 'HUS Data Engineer' 'Data QA Engineer' 'Presales Engineer EMEA'\n",
      " 'Cloud Engineer (f/m/d)' 'DATA ENGINEER (BIOTECH)' 'Data Engineer (AI)'\n",
      " 'Part-time - Junior Data Analytics Engineer' 'Industrial Engineer'\n",
      " 'Quality Engineer' 'Erlang Engineer (m/f)' 'Lead Data Engineer'\n",
      " 'Collaboration Solution Engineer - Part time for a student'\n",
      " 'Manufacturing Engineer, Mikulov' 'DATA ENGINEER' 'Sr Data Engineer'\n",
      " 'Data Analytics Engineer'\n",
      " 'Middle/Senior Python developer for the American company'\n",
      " 'Catia V5 Design Engineer (m/ž) - projekt. řízení automotive'\n",
      " 'Strojní Engineer (procesy a technologie)' 'Senior Quality Engineer'\n",
      " 'Technolog/Industrial Engineer' 'IT Trainee & Development Program (CZ)'\n",
      " 'Administrator' 'Technical Support Engineer' 'Intern Front-End Engineer'\n",
      " 'IT Trainee Program - Junior Cloud Engineer' 'Data Engineer - ML Ops'\n",
      " 'Risk Platform - Site Reliability Engineer'\n",
      " 'Fiber Optic Network Engineer' 'Staff Data Engineer'\n",
      " 'HV battery Development Engineer (f/m/d)'\n",
      " 'Risk Platform - Support Engineer'\n",
      " 'Digital Prototype Development Engineer (f/m/d)'\n",
      " 'Junior Software Engineer' 'Wireless Engineer - EMEA'\n",
      " 'Mastercard Graduate Launch Program 2023 - Data Engineer - Prague, Czech Republic'\n",
      " 'Test and application engineer'\n",
      " 'Junior Machine Learning Engineer - Part time for a student'\n",
      " 'Support Specialist / Engineer - Imaging Systems' 'Cloud Engineer - AWS'\n",
      " 'Data Engineer/Scientist' 'IT Trainee & Development Program'\n",
      " 'Development Engineer in e-Drive – Test Automation (f/m/d)'\n",
      " 'Mechanical Engineer' 'Test Engineer' 'IT-supportingeniør'\n",
      " 'Fullstack Engineer, Artificial Intelligence' 'Search Platform Engineer'\n",
      " 'Cloud Engineer' 'Datový analytik/Data Engineer' 'Senior Data Engineer'\n",
      " 'Data Engineer II' 'Global Azure Data Architect (Engineer)'\n",
      " 'Cloud Data Engineer - AI & Data Team' 'Design Engineer'\n",
      " 'Software Engineer - Secure Network Analytics & XDR'\n",
      " 'IT Trainee Program - Junior Oracle Applications Engineer'\n",
      " 'Front End Lead Engineer'\n",
      " 'Data Engineer / Analyst Senior - migrace do cloudu'\n",
      " 'MacOS Detection Engineer' 'Cloud DevOps Engineer'\n",
      " 'Junior Python Developer - Datové integrace'\n",
      " 'Internship/SW Developer - part time' 'Senior Machine Learning Engineer'\n",
      " 'Junior QA Engineer (Part time possible)'\n",
      " 'Lead Machine Learning Engineer'\n",
      " 'Data Analyst/ Machine Learning Engineer'\n",
      " 'evergreen Public 360° Senior Cloud Data & AI Engineer'\n",
      " 'Cyber Lab Engineer - Junior' 'Junior Manual Test Engineer'\n",
      " 'Senior Threat Intelligence Researcher Espionage' 'QA Engineer'\n",
      " 'Automotive Application Support Engineer - Imaging EMEA M/F'\n",
      " 'Industrial Engineer - JUNIOR (m/ž)'\n",
      " 'L1 Support Engineer with English 24x7' 'Junior DevOps Engineer'\n",
      " 'Data Engineer / IT konzultant' 'Senior Python Developer'\n",
      " 'Senior QA Automation Engineer (Python)'\n",
      " 'Senior Engineer for Manufacturing Execution Systems (MES)'\n",
      " 'Software Engineer _ multiple roles'\n",
      " 'Technical Sales Support Engineer in Electrical Engineering'\n",
      " 'QA Automation Engineer (Python, JavaScript, Python)'\n",
      " 'Data Engineer for Procurement' 'Senior Test Engineer'\n",
      " 'Data engineer - Technology consulting' 'Junior SAS Engineer'\n",
      " 'Windows Engineer' 'Testing Engineer – Zkušební technik - junior'\n",
      " 'Test Technician' 'Tester'\n",
      " 'Technical Account Manager (TAM) Cloud/Networking/K8s - Opportunity for Working Remotely Prague,'\n",
      " 'Controls System Engineer' 'BIG DATA ENGINEER'\n",
      " 'Senior Software Engineer - Multiple Roles' 'Machine Learning Engineer'\n",
      " 'Application SW Engineer' 'ENGINEER, SOFTWARE QA'\n",
      " 'Software Engineer - Data Platform' 'QA Automation Engineer'\n",
      " 'Data engineer for streaming web and mobile application'\n",
      " 'QA Engineer / Tester' 'Data Engineer ✔' 'Data Platform Engineer'\n",
      " 'Big Data Engineer' 'Data Analytics and PC&IS Dry Laundry GTO'\n",
      " 'System Test Engineer'\n",
      " 'Lead Software Engineer, Technology - Europe Remote'\n",
      " 'Automation Test Analyst' 'Python Engineer'\n",
      " 'Senior Open-Source Engineer 80-100%'\n",
      " 'QA Automation Engineer (Mid/Senior)'\n",
      " 'Auto/Manual QA Engineer (Internal Web Services)'\n",
      " 'Senior Software Engineer - Data Cloud' 'Senior QA Automation Engineer'\n",
      " 'System Test Engineer - DRS' 'Supplier Quality Engineer'\n",
      " 'Application Manager'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud'\n",
      " 'SAP Labs iXp Intern - Software Engineer (DPC, part-time)'\n",
      " 'Software Data Platform Engineer'\n",
      " 'Internship - Mechanical Design Engineer (Warehouse Automation - Brno)'\n",
      " 'Core Software Engineer' 'QA Engineer for a cybersecurity product'\n",
      " 'Test Automation Engineer' 'Automation Engineer for Marine & Ports'\n",
      " 'Automation Project Engineer - long-term internship/part-time'\n",
      " 'Data Engineer - Engineering & Manufacturing'\n",
      " 'Senior Test Automation Engineer'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Senior Software Engineer - Big Data'\n",
      " 'Process Engineer / Continuous Improvement Manager'\n",
      " 'Quality Assurance Engineer - Rally Software' 'Software Engineer'\n",
      " 'Associate Software Engineer' 'Software Engineer C# (WPF)'\n",
      " 'DPČ-Associate Steel Structure Engineer'\n",
      " 'Data Engineer - ETL Developer (SSIS)' 'Staff Software Engineer'\n",
      " 'Software Engineer - Launchpad'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " 'Remote Senior Software Engineer (Java)'\n",
      " 'Test Analyst & Automation Engineer' 'Junior Steel Structure Engineer'\n",
      " 'Experienced backend developer for the Data Store platform'\n",
      " 'Technical Project Manager for Software Work Package Manager role'\n",
      " 'ENGINEER, SOFTWARE' 'Senior Data Engineer / ETL Developer (SSIS)'\n",
      " 'Software Engineer II - Python' 'DPČ - Associate Offer Engineer'\n",
      " 'QA Automation Engineer (Python)'\n",
      " 'Staff Software Engineer (Python) - 22333' 'Software Engineer II - AI/ML'\n",
      " 'Software Data platform Engineer'\n",
      " 'Go Software Engineer, Commercial Systems'\n",
      " 'Intern - Junior Software Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 :\n",
      "['Cloud Services Engineer (Based in Germany)' 'Data Engineer'\n",
      " 'Data Engineer - Part time for a student'\n",
      " 'Data Processing and Automation-Development Engineer (f/m/d)'\n",
      " 'HUS Data Engineer' 'Cloud Engineer (f/m/d)' 'DATA ENGINEER (BIOTECH)'\n",
      " 'Data Engineer (AI)' 'Part-time - Junior Data Analytics Engineer'\n",
      " 'Lead Data Engineer' 'DATA ENGINEER' 'Sr Data Engineer'\n",
      " 'Data Analytics Engineer' 'IT Trainee Program - Junior Cloud Engineer'\n",
      " 'Data Engineer - ML Ops' 'Staff Data Engineer'\n",
      " 'Mastercard Graduate Launch Program 2023 - Data Engineer - Prague, Czech Republic'\n",
      " 'Cloud Engineer - AWS' 'Data Engineer/Scientist' 'Cloud Engineer'\n",
      " 'Datový analytik/Data Engineer' 'Senior Data Engineer' 'Data Engineer II'\n",
      " 'Global Azure Data Architect (Engineer)'\n",
      " 'Cloud Data Engineer - AI & Data Team'\n",
      " 'Software Engineer - Secure Network Analytics & XDR'\n",
      " 'Data Engineer / Analyst Senior - migrace do cloudu'\n",
      " 'Cloud DevOps Engineer'\n",
      " 'evergreen Public 360° Senior Cloud Data & AI Engineer'\n",
      " 'Data Engineer / IT konzultant' 'Data Engineer for Procurement'\n",
      " 'Data engineer - Technology consulting' 'BIG DATA ENGINEER'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Data engineer for streaming web and mobile application'\n",
      " 'Data Engineer ✔' 'Data Platform Engineer' 'Big Data Engineer'\n",
      " 'Senior Software Engineer - Data Cloud'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud'\n",
      " 'Software Data Platform Engineer'\n",
      " 'Data Engineer - Engineering & Manufacturing'\n",
      " 'Senior Software Engineer - Big Data'\n",
      " 'Data Engineer - ETL Developer (SSIS)'\n",
      " 'Experienced backend developer for the Data Store platform'\n",
      " 'Senior Data Engineer / ETL Developer (SSIS)'\n",
      " 'Software Data platform Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.5 Denmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Denmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 :\n",
      "['Data Engineer' 'Lead Instrument Engineer'\n",
      " 'Data Scientist / Machine Learning Engineer, FinTech Remote'\n",
      " 'Data engineer' 'Field Engineer - NextGen Graduate Program'\n",
      " 'Lead Structural Engineer' 'Lead Electrical Engineer'\n",
      " 'Security Engineer II - Product Security'\n",
      " 'Data Engineer for Research Data Management' 'Analytics Engineer'\n",
      " 'Student Intern - Road Engineer for COWI in Odense, Denmark'\n",
      " 'C++ Software Engineer' 'Machine Learning Engineer' 'Systems Engineer'\n",
      " 'Data Manager' 'Data Engineers and Data Analysts for Copenhagen'\n",
      " 'Data Engineer for Telia'\n",
      " 'R&D Engineer Intern – for fast growing MedTech Start-Up' 'Engineer'\n",
      " 'Forward-deployed Data Engineer' 'Operations Engineer til Big Data'\n",
      " 'Data Platform Engineer' 'Experienced Data Engineer'\n",
      " 'Energy analyst with a flair for data' 'Senior Data Engineer - Viby'\n",
      " 'Machine Learning Engineer Intern – for fast growing MedTech Start-Up'\n",
      " 'Senior Engineer' 'Fullstack Software Engineer'\n",
      " 'Staff Software Engineer, Eats | Aarhus, Denmark'\n",
      " 'Software Engineer II - Vulnerability Platform'\n",
      " 'Lead Engineer - Drive Fintech Innovation'\n",
      " 'Staff Software Engineer, Stateful Fleet Management'\n",
      " 'Sr Software Engineer, Stateful Container Management'\n",
      " 'Quality Assurance Engineer' 'Deep Learning Engineer'\n",
      " 'Data Center Engineer'\n",
      " 'Staff Software Engineer, Fleet Reliability and Performance'\n",
      " 'Data Engineer Consultant' 'Data Engineer til ny dataplatform'\n",
      " 'Sr Machine Learning Research Engineer @ Corti'\n",
      " 'Data Engineer til bekæmpelse af skatteunddragelse'\n",
      " 'Graduate Software Engineer' 'Well Performance Engineer'\n",
      " 'Associate Data Engineer' 'QCQA Denmark Data Centre'\n",
      " 'Data Engineer - QuantumBlack' 'VIE Program_DATA SCIENTIST (M/F)_DENMARK'\n",
      " 'QA Engineer' 'Vi søger en Data Engineer som vil være med'\n",
      " 'Data Engineer Director' 'Production Engineer - Quality (PDF)'\n",
      " 'Lead Data Engineer' 'Data Engineer - Consulting' 'Lead Engineer'\n",
      " 'Staff Software Engineer - Platform'\n",
      " 'Sr Software Engineer, Fleet Reliability and Performance'\n",
      " 'Embedded Software Engineer - IoT Solutions - Denmark'\n",
      " 'B1.3 Licensed Engineer - Denmark'\n",
      " 'Head of Data & Insights and RTE (m/f/d)'\n",
      " 'Early Career Program - Field Engineering 2023 (Denmark)'\n",
      " 'Data Engineer (Nordic based)' 'Azure Data Engineer'\n",
      " 'Data Engineer – help develop our new data platform'\n",
      " 'Senior Software Engineer, Energy Transition Platform, Hybrid 2+ days on average in our fantastic office! (Full remote available in some cases)'\n",
      " 'Python developer' 'Student Assistant Software Engineer'\n",
      " 'Software Engineer who likes Cloud Infrastructure and Data Engineering - Aarhus'\n",
      " 'Electronics Engineer with PCBA test and verification...'\n",
      " 'Software Engineer for LEGO® House'\n",
      " 'Interns for Digital Solutions - Aarhus'\n",
      " 'Data Engineer/Project Lead - Copenhagen' 'Frontend Engineer'\n",
      " 'Backend Developer. Anywhere.'\n",
      " 'Cloud Operations Engineer in FOSS Software Services'\n",
      " 'Vivino Frontend Interns - Web Team' 'Software Test Engineer'\n",
      " 'LEAD Engineer Process Design'\n",
      " 'Senior Data & Automation Engineer HR Technology' 'Research Engineer'\n",
      " 'Data engineer med øje for den tværgående arkitektur'\n",
      " 'Senior Data Engineer in SuperAI' 'Business Data Engineer'\n",
      " 'Senior Digitalization Engineer' 'Data Engineer (PI/PAT) Just graduated'\n",
      " 'Innovation Scientist / Engineer'\n",
      " 'Data Engineers til bekæmpelse af international skatteunddragelse'\n",
      " 'Senior Engineer (Hire to Retire)'\n",
      " 'Design Engineer - Traffic Management System'\n",
      " 'Senior Engineer - Shape the Future of Fintech' 'Cloud Data Engineer'\n",
      " 'Cloud Engineer for our Computer Vision and MLOps area'\n",
      " 'Computer Vision Engineer' 'MES Engineer / MES Technical Consultant'\n",
      " 'Data Solutions Engineer'\n",
      " 'Koordinator til fremtidens BI Analytics platform i Forsvaret'\n",
      " 'Software Engineer' 'Cloud Engineer' 'Platform Engineer'\n",
      " 'Lead Software Engineer for LEGO® House' 'Cloud Operations Engineer'\n",
      " 'Associate Cloud Engineer' 'SAP Data Lifecycle Engineer'\n",
      " 'System Engineer' 'Senior Software Engineer (PHP) - Google Sheets'\n",
      " 'IoT Platform Software Engineer' 'udviklere' 'Experienced Cloud Engineer'\n",
      " 'Senior C# Software Engineer for Data Warehouse Product Area'\n",
      " 'DCS Automation Engineers for Expansion Projects'\n",
      " 'VP Engineering (Network Design)'\n",
      " 'Senior Software Engineer for cutting-edge robotics company ?'\n",
      " 'Energikonsulent' 'Technical Product Owner'\n",
      " 'Lead Engineer, Developer Experience'\n",
      " 'ZBC Data i Vordingborg søger snarest muligt en kollega'\n",
      " 'Student Front End Engineer' 'Software Engineer, Developer Experience'\n",
      " 'Software Engineer, Energy Transition Platform, Hybrid 2+ days on average in our fantastic office! (Full remote available in some cases)'\n",
      " 'Senior Digital Hardware Engineer – Leading subsea technology'\n",
      " 'AWS Analytics Platform Engineer' 'Application Engineer'\n",
      " 'Senior Quantum Software Engineer' 'System Operations Engineer'\n",
      " 'Service Engineer (m/w/d)'\n",
      " 'Cloud udvikler (studiejob) - Cutting Edge(React, Javascript, Python, AWS, Linux etc. )'\n",
      " 'System Engineer (Windows)'\n",
      " 'Tech Leads til at sætte retning for Data Warehouse og Business Intelligence-området i Skatteforvaltningen'\n",
      " 'Quantum Processor integration lead for semiconducting platform'\n",
      " 'Koordinator - Total Instrument Management'\n",
      " 'FERMENTATION OPTIMIZATION SCIENTIST, NOVOZYMES, KALUNDBORG'\n",
      " 'Engineering change management technical PM'\n",
      " 'Entry Data Platform Engineer - Aarhus or Gdansk' 'Senior Data Engineer'\n",
      " 'Maintenance & Engineering Sustainability Specialist - Branderup / Rødkærsbro'\n",
      " 'Join the Graduate Programme in PwC’s Risk Assurance team'\n",
      " 'Data Specialist, Snowflake Engineer - Copenhagen:'\n",
      " 'MES Validation Engineer' 'Senior DevOps Engineer'\n",
      " 'PRAKTIKANT - COMPLIANCE' 'Experienced ML Engineer to SuperAI'\n",
      " 'Senior Data Analyst' 'Principal Software Engineer'\n",
      " 'Backend Software Engineer'\n",
      " 'Medarbejder til risikostyring og risikoanalyser'\n",
      " 'Senior Software Engineer'\n",
      " 'Customer Success Manager - Enterprise Data Solutions'\n",
      " 'Senior Software Developer with passion for Cloud and Machine Learning'\n",
      " 'FLUID SYSTEM ENGINEER TO HARDI INTERNATIONAL A/S'\n",
      " 'BIM Mechanical Design Engineer - Student'\n",
      " 'Cybersecurity Leader, Ringsted' 'Software Engineer- Backend'\n",
      " 'In Country Engineer Nordics'\n",
      " 'PRODUCTION ENGINEER – leading subsea technology'\n",
      " 'Data Engineer with an interest in creating value from data'\n",
      " 'Senior Metocean Engineer and Project Manager'\n",
      " 'Student Assistant- Digital Services for COWI in Lyngby, Denmark'\n",
      " 'Data Protection Specialist' 'Frontend Developer'\n",
      " 'Development Engineer, Student' 'Fermentation scientist'\n",
      " 'System Administrator'\n",
      " 'Vedligeholdelsesspecialist til vores Power-to-X pilot-produktion og laboratorium'\n",
      " 'Software Engineer with Java' 'Data Warehouse Engineer'\n",
      " 'Fermentation engineer / technician'\n",
      " 'System Engineer – automatisering og modernisering af operationel teknologi'\n",
      " 'Ingeniørpraktik efteråret 2023 - Business Development Engineer'\n",
      " 'Test Engineer' 'Laboratory Technician, Protein Engineering'\n",
      " 'Electrical Project Engineer' 'Clinical Evidence Lead (PDF)'\n",
      " 'Test Automation Engineer - Applications / Global world-leading audio and video technology group'\n",
      " 'Expert Scientific Data Architect'\n",
      " 'Systemingeniør – bliv ekspert på LabView og TestStand'\n",
      " 'Embedded Software / Firmware Development Engineer' 'Solution Architect'\n",
      " 'Byplanlægger til Hvidovre Kommune'\n",
      " 'Akademiker med ansvar for Environmental Monitoring (EM)'\n",
      " 'Product Specialist in Water Metering'\n",
      " 'Senior C++ Engineer – LEGO Digital Designer' 'Field Test Engineer'\n",
      " 'Commercial Analyst' 'ENGAGERET PROJEKTMEDARBEJDER SØGES TIL BYSTRATEGI'\n",
      " 'Field Service Engineer Nordic' 'Database & BI Specialist'\n",
      " 'DevOps Engineer with a passion for Linux' 'Junior HPC Cloud Engineer'\n",
      " 'Senior HPC Cloud Engineer'\n",
      " 'Scientist/Associate Scientist, High-Throughput Screening'\n",
      " 'Kommunikativ og struktureret Ingeniør/maskinmester'\n",
      " 'Asset Managers til Banedanmarks Sikringsanlæg'\n",
      " 'Robotics & Software Engineer'\n",
      " 'Senior Cloud Data Engineer for FLSmidth – Copenhagen'\n",
      " 'System Architects' 'Praktikant til Digital Engineering hos Atkins'\n",
      " 'Certification Engineer' 'DevOps Engineer (with database interest)'\n",
      " 'SENIOR MECHANICAL ENGINEER, 3D printing' 'Software engineer'\n",
      " 'Junior Consultant - Emendo Improvement'\n",
      " 'UI software developer – leading subsea technology'\n",
      " 'Senior Embedded Software Developer – leading subsea technology'\n",
      " 'ABB Sattline/800xA automation engineer - AFRY in Copenhagen'\n",
      " 'Test Automation Engineer'\n",
      " '(Senior) Robotics Field Service Engineer, Denmark'\n",
      " 'Research Engineer, Test & Validation'\n",
      " 'UI desktop w/cloud, C++ / C# Software Developer' 'Automation Engineer'\n",
      " 'PI Historian Lead Automation Engineer'\n",
      " 'Lead APP udvikler til Android – gør en forskel for Politiets arbejde i felten'\n",
      " 'Erfarne ServiceNow konsulenter til Devoteam Technology Consulting'\n",
      " 'Teknisk Tester' 'MAC Engineer (Freelancer)'\n",
      " 'Network Engineer for Operation Technology'\n",
      " 'Infrastructure Automation Developer' 'Data Engineer til Verdo'\n",
      " 'Compliance Engineer' 'Vil du arbejde med miljø og virksomheder?'\n",
      " 'Anlægsingeniør til bygværker og vejafvanding'\n",
      " 'Administrativ assistent til HSE-området i NNE'\n",
      " 'Medical Device Development - Advanced R&D Test Engineer'\n",
      " 'IT Design Engineer - Traffic Management System' 'Engineering Director'\n",
      " 'Cable Installation Engineer (freelancer)' 'Facility Controls Engineer'\n",
      " 'Product Configurator'\n",
      " 'Senior embedded software developer – leading subsea technology'\n",
      " '.Net Developer' 'Python Developer'\n",
      " 'Production engineer til NCC Industry'\n",
      " 'UI/UX Intern at Valtech Aarhus - Fall 2023'\n",
      " 'Data Center Production Operations Engineer'\n",
      " 'Specialist, Production Excellence'\n",
      " 'Associate Software Engineer - Microsoft Dynamics 365'\n",
      " 'Embedded Engineer | Copenhagen | 6 Months + extension'\n",
      " 'Senior Frontend Software Engineer' 'Mechanical Engineer'\n",
      " 'Software Asset Manager'\n",
      " 'Senior Frontend Software Engineer – Decarbonization'\n",
      " 'Løsningsarkitekt til samfundskritiske og komplekse systemområder indenfor staten'\n",
      " 'Machine Software Developer Engineer I' 'Java Developer'\n",
      " 'SAP Basis Automation Engineer'\n",
      " 'Test Engineer Expert for non-functional tests - onsite'\n",
      " 'PLC Engineer/Supporter' 'VIE Program_FULL STACK DEVELOPER(M/F)_DENMARK'\n",
      " 'Analytiker til strategisk planlægning af reinvesteringer i den danske jernbane'\n",
      " 'MEP Project Engineer/Lead - Mechanical'\n",
      " 'Systemingeniør til LEOPARD 2, hærens tungeste og mest komplekse platform - Her går barnedrømmen i opfyldelse og bliver til voksenliv med mening'\n",
      " 'Presales System Engineer' 'Network Engineer'\n",
      " 'Teamleder til ny analysefunktion i Region H'\n",
      " 'Software Engineer (a.k.a. Java Developer)'\n",
      " 'Udvikler til Industri Automation'\n",
      " 'Projektmedarbejder til klimatilpasning' 'Sommerassistance medhjælper'\n",
      " '5X Configuration & Systems Engineering(5X C&SE)'\n",
      " 'Reliability Engineer til Topsøe'\n",
      " 'Praktik inden for Kvalitet og Bæredygtighed'\n",
      " 'MEP Project Engineer/Lead – Commissioning'\n",
      " 'Stamdatamedarbejder til Stam- og geodatasektionen i Forsvarsministeriets Ejendomsstyrelse'\n",
      " 'MEP Project Engineer/Lead - Electrical'\n",
      " 'Embedded Software Engineer - Aircraft satcom systems for safety and entertainment'\n",
      " 'DevOps Engineer'\n",
      " 'Handlekraftig ingeniør med godt overblik og solidt GMP fundament'\n",
      " 'Software Quality Control agent'\n",
      " 'Technical sales engineer, Søborg, Denmark'\n",
      " 'QA Automation Engineer - Málaga, Spain' 'AI Security Engineer'\n",
      " 'Senior Engineer, Customer & Channel Management'\n",
      " 'SENIOR ENGINEER – CLOUD APPLICATION SECURITY'\n",
      " 'Udvikler til Industri-Automation' 'Release Train Engineer'\n",
      " 'Software Developer Graduate – Junior - Senior'\n",
      " 'MEDARBEJDER TIL GEOSCIENCE'\n",
      " 'Key Account for cloud connected stationary battery solutions'\n",
      " 'Softwareudvikler til elektronikvirksomhed'\n",
      " 'Technical Supporter/Teknisk Support' 'Projektleder'\n",
      " 'Entry Software Engineer - Aarhus or Gdansk']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 :\n",
      "['Data Engineer' 'Data engineer'\n",
      " 'Data Engineer for Research Data Management' 'Analytics Engineer'\n",
      " 'Data Manager' 'Data Engineers and Data Analysts for Copenhagen'\n",
      " 'Data Engineer for Telia' 'Forward-deployed Data Engineer'\n",
      " 'Operations Engineer til Big Data' 'Data Platform Engineer'\n",
      " 'Experienced Data Engineer' 'Senior Data Engineer - Viby'\n",
      " 'Software Engineer II - Vulnerability Platform' 'Data Center Engineer'\n",
      " 'Staff Software Engineer, Fleet Reliability and Performance'\n",
      " 'Data Engineer Consultant' 'Data Engineer til ny dataplatform'\n",
      " 'Data Engineer til bekæmpelse af skatteunddragelse'\n",
      " 'Associate Data Engineer' 'Data Engineer - QuantumBlack'\n",
      " 'Vi søger en Data Engineer som vil være med' 'Data Engineer Director'\n",
      " 'Lead Data Engineer' 'Data Engineer - Consulting'\n",
      " 'Sr Software Engineer, Fleet Reliability and Performance'\n",
      " 'Head of Data & Insights and RTE (m/f/d)' 'Data Engineer (Nordic based)'\n",
      " 'Azure Data Engineer'\n",
      " 'Data Engineer – help develop our new data platform'\n",
      " 'Software Engineer who likes Cloud Infrastructure and Data Engineering - Aarhus'\n",
      " 'Data Engineer/Project Lead - Copenhagen'\n",
      " 'Cloud Operations Engineer in FOSS Software Services'\n",
      " 'Senior Data & Automation Engineer HR Technology'\n",
      " 'Data engineer med øje for den tværgående arkitektur'\n",
      " 'Senior Data Engineer in SuperAI' 'Business Data Engineer'\n",
      " 'Data Engineer (PI/PAT) Just graduated'\n",
      " 'Data Engineers til bekæmpelse af international skatteunddragelse'\n",
      " 'Cloud Data Engineer'\n",
      " 'Cloud Engineer for our Computer Vision and MLOps area' 'Cloud Engineer'\n",
      " 'Cloud Operations Engineer' 'Associate Cloud Engineer'\n",
      " 'SAP Data Lifecycle Engineer' 'Experienced Cloud Engineer'\n",
      " 'AWS Analytics Platform Engineer'\n",
      " 'Tech Leads til at sætte retning for Data Warehouse og Business Intelligence-området i Skatteforvaltningen'\n",
      " 'Entry Data Platform Engineer - Aarhus or Gdansk' 'Senior Data Engineer'\n",
      " 'Maintenance & Engineering Sustainability Specialist - Branderup / Rødkærsbro'\n",
      " 'Data Specialist, Snowflake Engineer - Copenhagen:'\n",
      " 'Customer Success Manager - Enterprise Data Solutions'\n",
      " 'Senior Software Developer with passion for Cloud and Machine Learning'\n",
      " 'BIM Mechanical Design Engineer - Student'\n",
      " 'Data Engineer with an interest in creating value from data'\n",
      " 'Data Protection Specialist' 'Data Warehouse Engineer'\n",
      " 'Expert Scientific Data Architect' 'Database & BI Specialist'\n",
      " 'Junior HPC Cloud Engineer' 'Senior HPC Cloud Engineer'\n",
      " 'Senior Cloud Data Engineer for FLSmidth – Copenhagen'\n",
      " 'DevOps Engineer (with database interest)'\n",
      " 'UI desktop w/cloud, C++ / C# Software Developer'\n",
      " 'Data Engineer til Verdo' 'Data Center Production Operations Engineer'\n",
      " 'SENIOR ENGINEER – CLOUD APPLICATION SECURITY']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.6 Finland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Finland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 :\n",
      "['SOFTWARE ENGINEER' 'Research Engineer/Scientist'\n",
      " 'Backend engineer (Data science interest)' 'Data Engineer'\n",
      " 'Data Engineer (Azure)' 'Data Engineer, Data Architect'\n",
      " 'Data & Cloud Engineer, Betolar Oyj' 'Data Integration Engineer'\n",
      " 'Grow as Data Engineer with us - Solita´s personalised onboarding program'\n",
      " 'Sales Engineer' 'Data and Analytics Developer'\n",
      " 'Data Scientist, Data Engineer, Data Architect' 'Desktop Engineer'\n",
      " 'Data Engineers' 'IT Support' 'Consumer Data Specialist'\n",
      " 'Embedded Software Engineer – location free (in Finland)'\n",
      " 'Data Engineer, Tietohallinto, Helsinki'\n",
      " 'Data engineer, Data architect, BI Consultant, Data specialist'\n",
      " 'Data Engineer (AWS' 'Forward-deployed Data Engineer'\n",
      " 'Optical System Engineer' 'DATA ENGINEER' 'Sr Data Engineer'\n",
      " 'Junior Software Engineer' 'AI Engineer, Large Language Models'\n",
      " 'Cloud Data Engineer' 'Lead Data Scientist/Engineer'\n",
      " 'EUC L1 Support Engineer' 'Service Support Engineer'\n",
      " 'Diffractive Optics Designer' 'Software Test Engineer'\n",
      " 'Azure Data Engineer' 'Group Component & Design Engineer'\n",
      " 'Research Engineer (m/f/d) for Polyolefin Process Technology Development'\n",
      " 'Data Engineer Specialising in Python and AzureDatabricks' 'RAN Engineer'\n",
      " 'Senior Data Engineer' 'Product Engineer, Services'\n",
      " 'Software Engineer - App Stores Backend (Remote)' 'Core Network Engineer'\n",
      " 'Cloud Data Platform Engineer' 'Software Engineer'\n",
      " 'Azure Developer | Azure Architect| Azure Data Engineer'\n",
      " 'Data Architect (Azure)'\n",
      " 'Data Engineer (Mid/Senior experience) - Fluent Finnish required'\n",
      " 'Azure Data Platform Specialist / Azure Data Engineer' 'Data Architect'\n",
      " 'Senior Azure Data Engineer' 'R&D Engineer'\n",
      " 'Lead Data Engineer, Helsinki,Finland'\n",
      " 'Senior Data Engineer (Stock Options Included!)'\n",
      " 'Internal Senior Data Engineer / Architect'\n",
      " 'Senior Azure Data Engineer / Architect'\n",
      " 'Senior Backend Engineer - Python'\n",
      " \"Azure Data Engineer's & Azure Data Architect's\"\n",
      " 'Senior MES Engineer - Quality Data Management'\n",
      " '(Senior) Mechanical Design Engineer' 'Senior ML Engineer'\n",
      " 'Machine Learning Engineer'\n",
      " '(Senior) Robotics Field Service Engineer, Finland'\n",
      " 'MAC Engineer (Freelancer)' 'Senior Cybersecurity Engineer'\n",
      " 'Cable Installation Engineer (freelancer)'\n",
      " 'Senior Backend Engineer (Campaign Overview and Reporting)'\n",
      " 'Senior Search Engineer (Solr Platform)' 'Software Project Engineer'\n",
      " 'Senior Test Automation Engineer' 'Senior Machine Learning Engineer'\n",
      " 'Lead Engineer'\n",
      " 'Metallurgist / Mineral Processing Sales Engineer - Mining flotation plants'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Backend Engineer (Ad-lib Integrations)'\n",
      " 'Senior Machine Learning Engineer (Barcodes) - Remote, Europe'\n",
      " 'Research Engineer' 'AI Security Engineer' 'AI Engineer'\n",
      " 'Solution Test Engineer' 'Design Engineer, Mechanical'\n",
      " 'Machine Learning Architect' 'Site Reliability Engineer'\n",
      " 'Azure Data Engineer (End Customer / Internal Role)'\n",
      " 'MACHINE LEARNING LOVER' 'Lead Partner Technology Strategist - Finland'\n",
      " 'RF Field Applications Engineer - Finland' 'Field Applications Engineer'\n",
      " 'Senior Software Engineer (PHP) - DWH Team'\n",
      " 'Embedded Software Test Engineer - Finland' 'Tietovarastoarkkitehti'\n",
      " 'Senior Software Engineer (PHP) - Connectors Engineering'\n",
      " 'Chief Development Engineer, Inverter Development'\n",
      " 'Data Engineer / Python, DataBricks and realtime streaming'\n",
      " 'AI Scientist - Computer Vision' 'Mechanical Engineer'\n",
      " 'Test Engineer, Banking sector'\n",
      " 'FAST Rotation Program-Field Applications Engineer Track - FInland'\n",
      " 'Senior Software Engineer, Services Team'\n",
      " 'Structural Analyst Engineer (FEA) - Finland or Spain'\n",
      " 'Lead Piping Engineer'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Senior Process Engineer' 'Design Engineer'\n",
      " 'Software Project Engineer for Industrial Digitalization Projects'\n",
      " 'Software Engineer (experience in computer security) Abu Dhabi'\n",
      " 'Software Engineer - Launchpad' 'DevOps Engineer'\n",
      " 'Radio Access Network Engineer' 'Sr. System Test Engineer'\n",
      " 'Applications Engineering Intern' 'APU VPS Engineer'\n",
      " 'Field Quality Engineer EMEA/AMER' 'Process Engineer'\n",
      " 'Senior Backend Engineer (Ad-lib Integrations)' 'Frontend Developer'\n",
      " 'Quality Engineer, Operational Quality'\n",
      " 'Senior Embedded Software Engineer - Espoo'\n",
      " 'Senior Backend Engineer (Go)' 'Senior Field Application Engineer'\n",
      " 'DevOps Engineer (Finland based)' 'Senior Software Engineer, React'\n",
      " 'FPGA Verification Engineer' 'Senior Python Backend Developer'\n",
      " 'Electric Engineer' 'IT Systems & Network Administrator'\n",
      " 'Control Platform New Product Introduction (NPI) Engineer - Electronics'\n",
      " 'Senior Sales Engineer - Finland' 'Supplier Quality Engineer'\n",
      " 'Technical Architect BI-Cloud' 'AVIONICS ENGINEER'\n",
      " 'Mine Planning Technician'\n",
      " 'Thermal System Engineer, Spacecraft - Finland' 'SEHQ Engineer'\n",
      " 'Lead Quality Software Engineer - Finland' 'Technical Specialist, SoC SW'\n",
      " 'Cloud Platform Engineer'\n",
      " 'Antenna Network Engineer (Satellite Communications Network)'\n",
      " 'Senior Application Engineer (Platform)'\n",
      " 'IoT Senior Engineer / Embedded Systems'\n",
      " 'Senior Systems Integration Engineer'\n",
      " 'Full-stack Engineer - Usage Tracking (Python)'\n",
      " 'Senior Full Stack Engineer (Go & React/ Typescript)'\n",
      " 'Embedded Linux Software Engineer'\n",
      " 'Automotive Embedded Software Engineer'\n",
      " 'Go Software Engineer, Commercial Systems'\n",
      " 'Systems Engineer - SASE Specialist'\n",
      " 'Software Engineer - Ubuntu Build Infrastructure'\n",
      " 'Senior C++ Mobile Software Engineer' 'Senior Software Service Engineers'\n",
      " 'GSI Partner Sales Director' 'After Sales Engineer'\n",
      " 'Metallurgist / Mineral Processing Sales Engineer - Mining flotation plants H/F Finlande'\n",
      " 'Senior Software Engineer - Digital Workplace'\n",
      " 'Senior Software Engineer (Feed Management)' 'Senior DevOps Engineer'\n",
      " 'Staff Geospatial Software Engineer'\n",
      " 'Embedded Software Engineer - Finland'\n",
      " 'Senior Full-stack Engineer - Usage Tracking (Rails / Python)'\n",
      " 'Senior Software Developer (TypeScript, node.js, react)'\n",
      " 'Senior RF Engineer - Finland' 'Low-level Android Engineer'\n",
      " 'Senior Software Engineer - Telco'\n",
      " 'Senior Antenna Network Engineer (Satellite Communications Network)'\n",
      " 'Senior Software Architect, Commercial Systems']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 :\n",
      "['Backend engineer (Data science interest)' 'Data Engineer'\n",
      " 'Data Engineer (Azure)' 'Data Engineer, Data Architect'\n",
      " 'Data & Cloud Engineer, Betolar Oyj' 'Data Integration Engineer'\n",
      " 'Grow as Data Engineer with us - Solita´s personalised onboarding program'\n",
      " 'Data and Analytics Developer' 'Data Engineers'\n",
      " 'Consumer Data Specialist' 'Data Engineer, Tietohallinto, Helsinki'\n",
      " 'Data engineer, Data architect, BI Consultant, Data specialist'\n",
      " 'Data Engineer (AWS' 'Forward-deployed Data Engineer' 'DATA ENGINEER'\n",
      " 'Sr Data Engineer' 'Cloud Data Engineer' 'Azure Data Engineer'\n",
      " 'Data Engineer Specialising in Python and AzureDatabricks'\n",
      " 'Senior Data Engineer' 'Cloud Data Platform Engineer'\n",
      " 'Azure Developer | Azure Architect| Azure Data Engineer'\n",
      " 'Data Architect (Azure)'\n",
      " 'Data Engineer (Mid/Senior experience) - Fluent Finnish required'\n",
      " 'Azure Data Platform Specialist / Azure Data Engineer' 'Data Architect'\n",
      " 'Senior Azure Data Engineer' 'Lead Data Engineer, Helsinki,Finland'\n",
      " 'Senior Data Engineer (Stock Options Included!)'\n",
      " 'Internal Senior Data Engineer / Architect'\n",
      " 'Senior Azure Data Engineer / Architect'\n",
      " \"Azure Data Engineer's & Azure Data Architect's\"\n",
      " 'Senior MES Engineer - Quality Data Management'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Azure Data Engineer (End Customer / Internal Role)'\n",
      " 'Data Engineer / Python, DataBricks and realtime streaming'\n",
      " 'Software Engineer (experience in computer security) Abu Dhabi'\n",
      " 'Technical Architect BI-Cloud' 'Cloud Platform Engineer'\n",
      " 'Senior C++ Mobile Software Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.7 France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'France'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 :\n",
      "['Data Engineer h/f - Paris'\n",
      " 'Software Engineer F/H (Dev.se Full-stack, DevOps, SRE) - 63'\n",
      " 'Manager QA Testing Automation H/F'\n",
      " 'Senior Data Engineer (online marketplace development)'\n",
      " 'Alternance Software Engineer C/C++ Data structure - Lyon (F/H)'\n",
      " 'Software engineer fullstack - Lancement nouveau produit H/F'\n",
      " 'Data Analytics Leader' 'CDD - DATA ENGINEER F/H'\n",
      " 'Python Engineer on FAIR Data (relocation to Belgium)'\n",
      " 'Data Engineer H/F - Remote & Hybride' 'Senior Cybersecurity Engineer'\n",
      " 'Alternance - Data engineering F/H' 'Application Engineer'\n",
      " 'Chef de projet Digital dans contexte Data & Aerospace F/H'\n",
      " 'Cybersecurity Auditor' 'DATA ENGINEER (H/F)'\n",
      " 'Principal Engineer .Net | Archi Microservice & Azure | Grand Groupe - 75/100K - H/F'\n",
      " 'Data Engineer - Big Data - Nantes - F/H'\n",
      " 'Développeur / Software Engineer F/H'\n",
      " 'System Engineer for advanced projects'\n",
      " 'CDI - Développeur PYTHON / Data engineer (Media) H/F'\n",
      " 'Machine Learning Engineer en Alternance' 'Lead tech Data engineer'\n",
      " 'Data Engineer (H/F)' 'Data Engineer - Expert en modélisation H/F'\n",
      " 'Senior C++ Software Developer - Network Diagram'\n",
      " 'Global Software Department Manager- Above RTE'\n",
      " 'CDI - Data Engineer (Média) (F/H)'\n",
      " 'Data engineer - analyste développeur (H/F)' 'Data Manager H/F'\n",
      " \"Project QA/QC Engineer / Ingénieur(e) qualité d'études et travaux\"\n",
      " 'Data engineer - data factory (H/F)' 'Lead Data Analyst (H / F)'\n",
      " 'IT ENGINEER SHINE VISION H/F'\n",
      " 'Junior Process Engineer in Semiconductor Industry - Grenoble (F) / Dresden (D) / Austin (USA)'\n",
      " 'Développeur Big Data (F/H)' 'CDI - Data Engineer F/H'\n",
      " 'Software Engineer-(H/F)' 'SOFTWARE ENGINEER BIG DATA-(H/F)'\n",
      " 'CDI - Cloud Data Engineer (Média) F/H' 'Data engineer - H/F'\n",
      " 'Packaging Implementation Project Leader (H/F)'\n",
      " 'Data Engineer avec une coloration devops - H/F' 'Data Engineer -(H/F)'\n",
      " 'Avionic System & Software Cost engineer (f/m)' 'Data engineer H/F'\n",
      " 'Data architect - data factory (H/F)'\n",
      " 'DATA ENGINEER (H/F/X) - INTERMÉDIAIRE'\n",
      " 'CDI - Data Engineer (Industry) (F/H)'\n",
      " 'Chef de projet Data - data factory h/f (H/F)'\n",
      " 'Data engineer manufacturing (H/F)' 'Tech Lead Data engineer H/F'\n",
      " 'Ingénieur·e de Recherche Data-Science' 'Data engineer confirmé H/F'\n",
      " 'Alternance - 1 à 2 ans - Data Engineer Trade Finance & BI F/H'\n",
      " 'Senior Data Engineer - H/F'\n",
      " 'Alternant - 1 an - Developer Data Engineer - Sensitivities F/H'\n",
      " 'Senior Java Software Engineer / Devops-(H/F)' 'Data Engineer (F/H)'\n",
      " \"Ingénieur QA Tests Automatisés d'API - Editeur Data & IA - H/F\"\n",
      " 'data engineer (remote) H/F (IT) / Freelance'\n",
      " 'Product Owner Data & Analytics H/F'\n",
      " 'Data engineer Azure Junior- Confirmé 1 - 5 ans (IT) / Freelance'\n",
      " 'Data Engineer SAP BW/HANA et Snowflake'\n",
      " 'INFOTEL - Data Engineer (H/F) - Niort (IT) / Freelance'\n",
      " 'Chef de Projet Data / Data Steward - Lyon'\n",
      " 'Digital Data Engineer - Niort - H/F (IT) / Freelance'\n",
      " 'Data Engineer Confirmé (H/F) (IT) / Freelance'\n",
      " 'Data Scientist / Engineer (H/F) (IT) / Freelance' 'Data Engineer H/F'\n",
      " 'Data Engineer ODI Confirmé H/F (IT) / Freelance'\n",
      " 'Data Engineer H/F (IT) / Freelance' 'Data engineer h/f (IT) / Freelance'\n",
      " 'DATA Engineer (IT) / Freelance' 'Data Engineer (H/F) (IT) / Freelance'\n",
      " 'Data Engineer Python (IT) / Freelance'\n",
      " 'Data Engineer (H/F) - 33 (IT) / Freelance'\n",
      " 'Data engineer (IT) / Freelance'\n",
      " 'Data Engineer Python/airflow (IT) / Freelance'\n",
      " 'Data engineer Scala confirmé : Scala/Spark et SQL (IT) / Freelance'\n",
      " 'Data engineer confirmé : Python, PySpark, Spark (IT) / Freelance'\n",
      " 'Data Engineer Python Pyspark (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP (IT) / Freelance'\n",
      " 'Data Engineer - Python H/F (IT) / Freelance'\n",
      " 'Alternance - 1 an - Data Engineer - BigData CIO CIB F/H'\n",
      " 'Data Engineer - Paris - Contrat de 6 mois renouvelable (IT) / Freelance'\n",
      " 'Data Engineer (IT) / Freelance' 'Digital Data Engineer (IT) / Freelance'\n",
      " 'Data engineer expérimenté BI-BigData - FULL REMOTE (IT) / Freelance'\n",
      " 'Data Engineer Expert (IT) / Freelance'\n",
      " 'Data Engineer - Outils de pilotages & reporting (IT) / Freelance'\n",
      " 'Data Engineer ODI / Cognos (IT) / Freelance'\n",
      " 'Data Engineer BI H/F (IT) / Freelance'\n",
      " 'Data Engineer Python / Spark / AWS (IT) / Freelance'\n",
      " 'DATA_ENGINEER_PYTHON_SPARK (IT) / Freelance'\n",
      " 'Data engineer: GCP pySpark 5 ans d’expériences, (pur data engineer) (IT) / Freelance'\n",
      " 'Développeur Python / Django / Data engineer (IT) / Freelance'\n",
      " 'Data Engineer Azure (H/F) (IT) / Freelance'\n",
      " 'Data engineer expérimenté BI-BigData Télétravail (IT) / Freelance'\n",
      " 'DATA ENGINEER Gcp (IT) / Freelance'\n",
      " 'Data Engineer - Support N2 (IT) / Freelance'\n",
      " 'Data Engineer Confirmé H/F (IT) / Freelance'\n",
      " 'FREELANCE ASAP / Data Engineer (IT) / Freelance'\n",
      " 'Data Engineer GCP (IT) / Freelance'\n",
      " 'Data Engineer AWS (ETL Matillion) (IT) / Freelance'\n",
      " 'Développeur Data / Data Engineer (IT) / Freelance'\n",
      " 'YG - Data Engineer / Développeur back python / javascript (IT) / Freelance'\n",
      " 'Data Engineer (Logstash / ELK) (IT) / Freelance'\n",
      " 'Data Engineer - ETL Talend, environnement Cloud (IT) / Freelance'\n",
      " 'Data Engineer Confirmé - F/H (IT) / Freelance'\n",
      " 'Data Engineer Python, Flask, Kafka, Streamlit (IT) / Freelance'\n",
      " 'Data Scientist / Data Engineer confirmé - F/H'\n",
      " 'Data engineer Azure / Python (h/f) (IT) / Freelance'\n",
      " 'Data Engineer confirmé GCP (IT) / Freelance'\n",
      " 'Data engineer GCP Kafka-Scala (IT) / Freelance'\n",
      " 'Data Engineer - Python (IT) / Freelance'\n",
      " 'Data engineer AZURE - Durable Functions (IT) / Freelance'\n",
      " 'Data Engineer - JAVA/Python (IT) / Freelance'\n",
      " 'Data Engineer /OPS/ Run (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP SPARK SCALA (IT) / Freelance'\n",
      " 'Data Engineer F/H (IT) / Freelance'\n",
      " 'Data Engineer - Secteur Nantes - H/F (IT) / Freelance'\n",
      " 'Data Engineer Azure (IT) / Freelance'\n",
      " 'Data Engineer - Talend for Big Data H/F (IT) / Freelance'\n",
      " 'Digital Data Engineer (H/F) (IT) / Freelance'\n",
      " 'DATA ENGINEER SPARK / SCALA (IT) / Freelance'\n",
      " 'Data Engineer / Durable Functions Azure Functions (Senior) / Client Grand Compte Énergie (IT) / Freelance'\n",
      " 'Data Engineer Reglementaire (IT) / Freelance'\n",
      " 'Data Engineer Microsoft SSIS (H/F) - Secteur Santé - Paris (IT) / Freelance'\n",
      " 'Data Engineer /Ingestion de données (IT) / Freelance'\n",
      " 'Un Data Engineer Azure Data Factory sur Paris (IT) / Freelance'\n",
      " 'Un Data Engineer sur Grenoble (IT) / Freelance'\n",
      " 'Data engineer / développeur _île-de-France (IT) / Freelance'\n",
      " 'Data engineer Scala / Spark / Cassandra (IT) / Freelance'\n",
      " 'Data Engineer Pyspark & devops (Azure) (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP JAVA (IT) / Freelance'\n",
      " 'Data Scientist (IT) / Freelance'\n",
      " 'Technical Data Engineer Dynatrace Specialist (H/F) (IT) / Freelance'\n",
      " 'Data Engineer Azure Data Factory (IT) / Freelance'\n",
      " 'Sénior Data Engineer AZURE / Databricks Banque (IT) / Freelance'\n",
      " 'Analytics Engineer H/F (IT) / Freelance'\n",
      " 'Data Engineer - Alteryx / Tableau / VBA (IT) / Freelance'\n",
      " 'Data Engineer (ref : ZM) (IT) / Freelance'\n",
      " 'Senior Data Engineer GCP (IT) / Freelance'\n",
      " 'Data Engineer - Apache AirFlow Python (IT) / Freelance'\n",
      " 'DATA ENGINEER (IT) / Freelance'\n",
      " 'Data Engineer AWS Orchestration Airflow (IT) / Freelance'\n",
      " 'DATA ENGINEER SPARK / SCALA / CLOUD (IT) / Freelance'\n",
      " 'Data engineer expert / Tech lead (IT) / Freelance'\n",
      " 'Data Engineer Talend Anglais courant (IT) / Freelance'\n",
      " 'Research Engineer in Physics of Industrial Processes F/M'\n",
      " 'DATA ENGINEER H/F (IT) / Freelance'\n",
      " 'Data Engineer Senior (azure) h/f (IT) / Freelance'\n",
      " 'Un développeur Big Data sur Toulouse 3j/semaine Remote (IT) / Freelance'\n",
      " 'Stage Assistant Data Engineer H/F'\n",
      " 'Data Engineer - Senior (IT) / Freelance'\n",
      " 'Cloud Engineer AWS (IT) / Freelance'\n",
      " 'Data Engineer - Expertise SQL/BigQuery (IT) / Freelance'\n",
      " 'Data engineer expert spark/scala (IT) / Freelance'\n",
      " 'Lead Analytics and Data Engineer (IT) / Freelance'\n",
      " 'Data engineer GCP (IT) / Freelance'\n",
      " 'Data Reliability Engineer (IT) / Freelance'\n",
      " 'Numeric simulation engineer (IT) / Freelance'\n",
      " 'Data Engineer - ETL Stambia (IT) / Freelance'\n",
      " 'Un Data Engineer (Python Pyspark SQL - AWS ) sur IDF -92 (IT) / Freelance'\n",
      " 'Machine Learning Engineer (IT) / Freelance'\n",
      " 'Data Engineer Azure Data Factory senior Confirmé / Senior Paris 13ème (IT) / Freelance'\n",
      " 'Data Engineer Spark, Scala, Maven, Cloud AWS ou Azure (IT) / Freelance'\n",
      " 'Data Engineer Java/Big Data (IT) / Freelance'\n",
      " 'Data Engineer Java/Spark expérimenté (IT) / Freelance'\n",
      " 'Cloud Engineer Back-end Python (IT) / Freelance'\n",
      " 'Data Engineer Lead Tech / Référent technique informatique (IT) / Freelance'\n",
      " 'Référent Python / Devops / Data / AWS (IT) / Freelance'\n",
      " 'Develop Senior Data Scientist - ML Engineer (IT) / Freelance'\n",
      " 'ingenieur informatique DATA (IT) / Freelance'\n",
      " 'Lead Data Engineer (IT) / Freelance' 'DevOps Engineer - H/F'\n",
      " 'Data Engineer Senior / TechLead Java/Spark (IT) / Freelance'\n",
      " 'Data Engineer (Stambia/Talend/GCP) (IT) / Freelance'\n",
      " 'Développeur Python back-end / AWS (IT) / Freelance'\n",
      " 'Développeur Python Sénior / Data Engineering AWS (IT) / Freelance'\n",
      " 'Tech lead cloud / dev Python (IT) / Freelance'\n",
      " 'Lead Data Engineer (Bon niveau en anglais requis) (IT) / Freelance'\n",
      " 'Talend Expert (IT) / Freelance'\n",
      " 'Data Engineering manager informatique (IT) / Freelance'\n",
      " 'ingénieur informatique BI Talend (IT) / Freelance'\n",
      " 'Digital Data Engineer Train Canaux Digitaux (IT) / Freelance'\n",
      " 'Tech Lead Data Engineering & Data Science (IT) / Freelance'\n",
      " 'Data Management - Data engineer Scrum (IT) / Freelance'\n",
      " 'DataOps Engineer (H/F) - Paris (IT) / Freelance'\n",
      " 'Ingénieur.e de recherche en systèmes de mesure et objets connectés'\n",
      " 'Développer Développeur Fullstack js, Data recommendation system (IT) / Freelance'\n",
      " 'ingénieur informatique DATA MSBI H/F (IT) / Freelance'\n",
      " 'Tech lead cloud engineer GCP (H/f) (IT) / Freelance'\n",
      " 'architecte informatique Cloud Azure (IT) / Freelance'\n",
      " 'ingénieur informatique QA Data (IT) / Freelance'\n",
      " 'ingénieur informatique OPS( F/H) (IT) / Freelance'\n",
      " 'Data Architect (IT) / Freelance'\n",
      " 'ingénieur informatique cybersécurité informatique (anglais courant) H/F (IT) / Freelance'\n",
      " 'Date Engineer (F/H) - Massy ou Roubaix (IT) / Freelance'\n",
      " 'Data ingénieur informatique, Expert Spark/Python (H/F) 75 (IT) / Freelance'\n",
      " 'DevOps (IT) / Freelance' 'Performance engineer (IT) / Freelance'\n",
      " 'responsable informatique Equipe BI & Data Management (IT) / Freelance'\n",
      " 'Chef de projet informatique BI et Référent technique informatique outils de pilotages et reporting (IT) / Freelance'\n",
      " 'Ops / Site Reliability Engineer (SRE) (IT) / Freelance'\n",
      " 'Data ingénieur informatique (IT) / Freelance'\n",
      " 'Alternance - 1 an - Développeur .Net/API/SQL/DevOps CIO CIB F/H'\n",
      " 'Release Train manager informatique (IT) / Freelance'\n",
      " 'Tech Lead PYTHON (IT) / Freelance'\n",
      " 'Un ingénieur informatique BI-BigData en Full télétravail (IT) / Freelance'\n",
      " 'ingénieur informatique de production informatique / support N2 / Big Data (IT) / Freelance'\n",
      " 'Product Owner Data Usage (IT) / Freelance'\n",
      " 'Tech Lead Data (MongoDB) (IT) / Freelance'\n",
      " 'Ops / Site Reliability Engineer (IT) / Freelance'\n",
      " 'TECH LEAD DATA AWS (IT) / Freelance' 'Tech Lead (IT) / Freelance'\n",
      " '6 consultant informatique DevOps (IT) / Freelance'\n",
      " 'Mission de Chef de projet informatique Supply Chain Datahub (IT) / Freelance'\n",
      " 'Business analyste informatique Vie Règlementaires (IT) / Freelance'\n",
      " 'Product Owner Data (H/F) - LYON (IT) / Freelance'\n",
      " 'Product Owner - Big data - Suppy Chain (IT) / Freelance'\n",
      " 'Senior Business analyste informatique Retail/Supply (IT) / Freelance'\n",
      " 'SRE OU SITE RELIABILITY ENGINEER E (IT) / Freelance'\n",
      " 'Devops (IT) / Freelance'\n",
      " 'Business analyste informatique freelance (IT) / Freelance'\n",
      " 'TECH LEAD M (IT) / Freelance'\n",
      " 'Product Owner Senior (Freelance) (IT) / Freelance'\n",
      " 'Business analyste informatique / consultant informatique BI (IT) / Freelance'\n",
      " 'Tech Lead Java SpringBoot Vue.JS BDD Kafka (IT) / Freelance'\n",
      " 'consultant informatique BI (IT) / Freelance'\n",
      " 'Business analyste informatique Data (IT) / Freelance'\n",
      " 'Scrum Master (IT) / Freelance'\n",
      " 'Tech Lead / Référent Azure (h/f) (IT) / Freelance'\n",
      " 'SRE Devops python network performance management (IT) / Freelance'\n",
      " 'Business analyste informatique DATA VAULT (IT) / Freelance'\n",
      " 'Data analyste informatique Expérimenté - Nantes/Angers H/F (IT) / Freelance'\n",
      " 'Tech Lead Java JS (IT) / Freelance'\n",
      " 'Tech Lead Java/Vuejs H/F (IT) / Freelance'\n",
      " 'Senior Cloud R&D Engineer f/m' 'Développeur Logiciel Data (H/F)'\n",
      " 'Product Owner Data (F/H)' 'R&D Engineer II C++ f/m'\n",
      " 'Développeur Python / Spark (F/H)' 'Product Manager AI/ML f/m'\n",
      " 'Product Marketing Manager f/m'\n",
      " 'Expert technique informatique fullstack Java Angular DevOps (H/F) (IT) / Freelance'\n",
      " 'Architecte Google Cloud (F/H)' 'Data Scientist Engineer H/F'\n",
      " 'Data Engineer - AgriTech - Full Remote'\n",
      " 'Reforestation Data Engineer - Images aériennes [Full Remote]'\n",
      " 'Senior Data Engineer (H/F)'\n",
      " 'Tech Lead Data Engineer Azure- Issy les Moulineaux - H/F'\n",
      " 'STAGE - Ingénieur Données / INTERNSHIP - Data Engineer (H/F)'\n",
      " 'Data Engineer / Développeur BI BigData H/F' 'Data Engineer Be Mo Tech'\n",
      " 'Data Engineer cloud H/F' 'Stage Data Engineer - Projet Data-to-Lead'\n",
      " 'Data Engineer SPARK et (Python&pandas) ou SCALA'\n",
      " 'Tech Lead Data Engineer F/H' 'Data Engineer AirFlow Remote'\n",
      " 'Data Engineer, Analyste développeur décisionnel'\n",
      " 'Data Engineer Junior F/M/X' 'D&A – Lead Data Engineer'\n",
      " 'Data Engineer SQL (f/m/d)' 'Data engineer Spark / Scala'\n",
      " 'Data Engineer H/F -Ile de France ou Nantes-' 'Data Engineer'\n",
      " 'Lead Data Engineer' 'Software Engineer - Intern - (France)'\n",
      " 'Site IE Data Engineer (F/M/X)'\n",
      " 'Business Intelligence (BI) /Data Analyst (DeFi) - Digital Assets'\n",
      " 'tech lead data engineer' 'Data Engineer (M/F)' 'Stagiaire Data Engineer'\n",
      " 'SOFTWARE DATA ENGINEER (H/F)' 'Data Analytics Engineer H/F'\n",
      " 'Data Engineer - Alternance - Issy Les Moulineaux - H/F'\n",
      " 'Digital Industrial Data Model Engineer'\n",
      " 'Senior Data Engineer – Data Platform / AWS / Archi Distribuée (f/m/x)'\n",
      " 'Data Engineer confirmé (H/F)' 'Cloud Data Engineer (H/F) - POEI'\n",
      " 'Senior Data Engineer' 'Data Visualization Engineer M/F'\n",
      " 'Data Engineer|Retail – H/F Lille' 'AZURE DATA ENGINEER H/F'\n",
      " 'APPRENTI(E) DATA ENGINEER ON HELICOPTER FLIGHTS (H/F)'\n",
      " 'Consultant Junior Data Engineer - Paris - 2023']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 :\n",
      "['Data Engineer h/f - Paris'\n",
      " 'Senior Data Engineer (online marketplace development)'\n",
      " 'Alternance Software Engineer C/C++ Data structure - Lyon (F/H)'\n",
      " 'Data Analytics Leader' 'CDD - DATA ENGINEER F/H'\n",
      " 'Python Engineer on FAIR Data (relocation to Belgium)'\n",
      " 'Data Engineer H/F - Remote & Hybride'\n",
      " 'Alternance - Data engineering F/H' 'DATA ENGINEER (H/F)'\n",
      " 'Data Engineer - Big Data - Nantes - F/H'\n",
      " 'CDI - Développeur PYTHON / Data engineer (Media) H/F'\n",
      " 'Lead tech Data engineer' 'Data Engineer (H/F)'\n",
      " 'Data Engineer - Expert en modélisation H/F'\n",
      " 'CDI - Data Engineer (Média) (F/H)'\n",
      " 'Data engineer - analyste développeur (H/F)' 'Data Manager H/F'\n",
      " 'Data engineer - data factory (H/F)' 'Lead Data Analyst (H / F)'\n",
      " 'CDI - Data Engineer F/H' 'SOFTWARE ENGINEER BIG DATA-(H/F)'\n",
      " 'CDI - Cloud Data Engineer (Média) F/H' 'Data engineer - H/F'\n",
      " 'Data Engineer avec une coloration devops - H/F' 'Data Engineer -(H/F)'\n",
      " 'Data engineer H/F' 'Data architect - data factory (H/F)'\n",
      " 'DATA ENGINEER (H/F/X) - INTERMÉDIAIRE'\n",
      " 'CDI - Data Engineer (Industry) (F/H)'\n",
      " 'Data engineer manufacturing (H/F)' 'Tech Lead Data engineer H/F'\n",
      " 'Data engineer confirmé H/F'\n",
      " 'Alternance - 1 à 2 ans - Data Engineer Trade Finance & BI F/H'\n",
      " 'Senior Data Engineer - H/F'\n",
      " 'Alternant - 1 an - Developer Data Engineer - Sensitivities F/H'\n",
      " 'Data Engineer (F/H)' 'data engineer (remote) H/F (IT) / Freelance'\n",
      " 'Data engineer Azure Junior- Confirmé 1 - 5 ans (IT) / Freelance'\n",
      " 'Data Engineer SAP BW/HANA et Snowflake'\n",
      " 'INFOTEL - Data Engineer (H/F) - Niort (IT) / Freelance'\n",
      " 'Digital Data Engineer - Niort - H/F (IT) / Freelance'\n",
      " 'Data Engineer Confirmé (H/F) (IT) / Freelance' 'Data Engineer H/F'\n",
      " 'Data Engineer ODI Confirmé H/F (IT) / Freelance'\n",
      " 'Data Engineer H/F (IT) / Freelance' 'Data engineer h/f (IT) / Freelance'\n",
      " 'DATA Engineer (IT) / Freelance' 'Data Engineer (H/F) (IT) / Freelance'\n",
      " 'Data Engineer Python (IT) / Freelance'\n",
      " 'Data Engineer (H/F) - 33 (IT) / Freelance'\n",
      " 'Data engineer (IT) / Freelance'\n",
      " 'Data Engineer Python/airflow (IT) / Freelance'\n",
      " 'Data engineer Scala confirmé : Scala/Spark et SQL (IT) / Freelance'\n",
      " 'Data engineer confirmé : Python, PySpark, Spark (IT) / Freelance'\n",
      " 'Data Engineer Python Pyspark (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP (IT) / Freelance'\n",
      " 'Data Engineer - Python H/F (IT) / Freelance'\n",
      " 'Alternance - 1 an - Data Engineer - BigData CIO CIB F/H'\n",
      " 'Data Engineer - Paris - Contrat de 6 mois renouvelable (IT) / Freelance'\n",
      " 'Data Engineer (IT) / Freelance' 'Digital Data Engineer (IT) / Freelance'\n",
      " 'Data engineer expérimenté BI-BigData - FULL REMOTE (IT) / Freelance'\n",
      " 'Data Engineer Expert (IT) / Freelance'\n",
      " 'Data Engineer - Outils de pilotages & reporting (IT) / Freelance'\n",
      " 'Data Engineer ODI / Cognos (IT) / Freelance'\n",
      " 'Data Engineer BI H/F (IT) / Freelance'\n",
      " 'Data Engineer Python / Spark / AWS (IT) / Freelance'\n",
      " 'DATA_ENGINEER_PYTHON_SPARK (IT) / Freelance'\n",
      " 'Data engineer: GCP pySpark 5 ans d’expériences, (pur data engineer) (IT) / Freelance'\n",
      " 'Développeur Python / Django / Data engineer (IT) / Freelance'\n",
      " 'Data Engineer Azure (H/F) (IT) / Freelance'\n",
      " 'Data engineer expérimenté BI-BigData Télétravail (IT) / Freelance'\n",
      " 'DATA ENGINEER Gcp (IT) / Freelance'\n",
      " 'Data Engineer - Support N2 (IT) / Freelance'\n",
      " 'Data Engineer Confirmé H/F (IT) / Freelance'\n",
      " 'FREELANCE ASAP / Data Engineer (IT) / Freelance'\n",
      " 'Data Engineer GCP (IT) / Freelance'\n",
      " 'Data Engineer AWS (ETL Matillion) (IT) / Freelance'\n",
      " 'Développeur Data / Data Engineer (IT) / Freelance'\n",
      " 'YG - Data Engineer / Développeur back python / javascript (IT) / Freelance'\n",
      " 'Data Engineer (Logstash / ELK) (IT) / Freelance'\n",
      " 'Data Engineer - ETL Talend, environnement Cloud (IT) / Freelance'\n",
      " 'Data Engineer Confirmé - F/H (IT) / Freelance'\n",
      " 'Data Engineer Python, Flask, Kafka, Streamlit (IT) / Freelance'\n",
      " 'Data engineer Azure / Python (h/f) (IT) / Freelance'\n",
      " 'Data Engineer confirmé GCP (IT) / Freelance'\n",
      " 'Data engineer GCP Kafka-Scala (IT) / Freelance'\n",
      " 'Data Engineer - Python (IT) / Freelance'\n",
      " 'Data engineer AZURE - Durable Functions (IT) / Freelance'\n",
      " 'Data Engineer - JAVA/Python (IT) / Freelance'\n",
      " 'Data Engineer /OPS/ Run (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP SPARK SCALA (IT) / Freelance'\n",
      " 'Data Engineer F/H (IT) / Freelance'\n",
      " 'Data Engineer - Secteur Nantes - H/F (IT) / Freelance'\n",
      " 'Data Engineer Azure (IT) / Freelance'\n",
      " 'Data Engineer - Talend for Big Data H/F (IT) / Freelance'\n",
      " 'Digital Data Engineer (H/F) (IT) / Freelance'\n",
      " 'DATA ENGINEER SPARK / SCALA (IT) / Freelance'\n",
      " 'Data Engineer / Durable Functions Azure Functions (Senior) / Client Grand Compte Énergie (IT) / Freelance'\n",
      " 'Data Engineer Reglementaire (IT) / Freelance'\n",
      " 'Data Engineer Microsoft SSIS (H/F) - Secteur Santé - Paris (IT) / Freelance'\n",
      " 'Data Engineer /Ingestion de données (IT) / Freelance'\n",
      " 'Un Data Engineer Azure Data Factory sur Paris (IT) / Freelance'\n",
      " 'Un Data Engineer sur Grenoble (IT) / Freelance'\n",
      " 'Data engineer / développeur _île-de-France (IT) / Freelance'\n",
      " 'Data engineer Scala / Spark / Cassandra (IT) / Freelance'\n",
      " 'Data Engineer Pyspark & devops (Azure) (IT) / Freelance'\n",
      " 'DATA ENGINEER GCP JAVA (IT) / Freelance'\n",
      " 'Technical Data Engineer Dynatrace Specialist (H/F) (IT) / Freelance'\n",
      " 'Data Engineer Azure Data Factory (IT) / Freelance'\n",
      " 'Sénior Data Engineer AZURE / Databricks Banque (IT) / Freelance'\n",
      " 'Analytics Engineer H/F (IT) / Freelance'\n",
      " 'Data Engineer - Alteryx / Tableau / VBA (IT) / Freelance'\n",
      " 'Data Engineer (ref : ZM) (IT) / Freelance'\n",
      " 'Senior Data Engineer GCP (IT) / Freelance'\n",
      " 'Data Engineer - Apache AirFlow Python (IT) / Freelance'\n",
      " 'DATA ENGINEER (IT) / Freelance'\n",
      " 'Data Engineer AWS Orchestration Airflow (IT) / Freelance'\n",
      " 'DATA ENGINEER SPARK / SCALA / CLOUD (IT) / Freelance'\n",
      " 'Data engineer expert / Tech lead (IT) / Freelance'\n",
      " 'Data Engineer Talend Anglais courant (IT) / Freelance'\n",
      " 'DATA ENGINEER H/F (IT) / Freelance'\n",
      " 'Data Engineer Senior (azure) h/f (IT) / Freelance'\n",
      " 'Stage Assistant Data Engineer H/F'\n",
      " 'Data Engineer - Senior (IT) / Freelance'\n",
      " 'Cloud Engineer AWS (IT) / Freelance'\n",
      " 'Data Engineer - Expertise SQL/BigQuery (IT) / Freelance'\n",
      " 'Data engineer expert spark/scala (IT) / Freelance'\n",
      " 'Lead Analytics and Data Engineer (IT) / Freelance'\n",
      " 'Data engineer GCP (IT) / Freelance'\n",
      " 'Data Engineer - ETL Stambia (IT) / Freelance'\n",
      " 'Un Data Engineer (Python Pyspark SQL - AWS ) sur IDF -92 (IT) / Freelance'\n",
      " 'Data Engineer Azure Data Factory senior Confirmé / Senior Paris 13ème (IT) / Freelance'\n",
      " 'Data Engineer Spark, Scala, Maven, Cloud AWS ou Azure (IT) / Freelance'\n",
      " 'Data Engineer Java/Big Data (IT) / Freelance'\n",
      " 'Data Engineer Java/Spark expérimenté (IT) / Freelance'\n",
      " 'Cloud Engineer Back-end Python (IT) / Freelance'\n",
      " 'Data Engineer Lead Tech / Référent technique informatique (IT) / Freelance'\n",
      " 'Lead Data Engineer (IT) / Freelance'\n",
      " 'Data Engineer Senior / TechLead Java/Spark (IT) / Freelance'\n",
      " 'Data Engineer (Stambia/Talend/GCP) (IT) / Freelance'\n",
      " 'Développeur Python Sénior / Data Engineering AWS (IT) / Freelance'\n",
      " 'Tech lead cloud / dev Python (IT) / Freelance'\n",
      " 'Lead Data Engineer (Bon niveau en anglais requis) (IT) / Freelance'\n",
      " 'Data Engineering manager informatique (IT) / Freelance'\n",
      " 'Digital Data Engineer Train Canaux Digitaux (IT) / Freelance'\n",
      " 'Tech Lead Data Engineering & Data Science (IT) / Freelance'\n",
      " 'Data Management - Data engineer Scrum (IT) / Freelance'\n",
      " 'DataOps Engineer (H/F) - Paris (IT) / Freelance'\n",
      " 'Tech lead cloud engineer GCP (H/f) (IT) / Freelance'\n",
      " 'architecte informatique Cloud Azure (IT) / Freelance'\n",
      " 'Data Architect (IT) / Freelance'\n",
      " 'Tech Lead Data (MongoDB) (IT) / Freelance'\n",
      " 'TECH LEAD DATA AWS (IT) / Freelance'\n",
      " 'Business analyste informatique / consultant informatique BI (IT) / Freelance'\n",
      " 'consultant informatique BI (IT) / Freelance'\n",
      " 'Senior Cloud R&D Engineer f/m' 'Architecte Google Cloud (F/H)'\n",
      " 'Data Engineer - AgriTech - Full Remote'\n",
      " 'Reforestation Data Engineer - Images aériennes [Full Remote]'\n",
      " 'Senior Data Engineer (H/F)'\n",
      " 'Tech Lead Data Engineer Azure- Issy les Moulineaux - H/F'\n",
      " 'STAGE - Ingénieur Données / INTERNSHIP - Data Engineer (H/F)'\n",
      " 'Data Engineer / Développeur BI BigData H/F' 'Data Engineer Be Mo Tech'\n",
      " 'Data Engineer cloud H/F' 'Stage Data Engineer - Projet Data-to-Lead'\n",
      " 'Data Engineer SPARK et (Python&pandas) ou SCALA'\n",
      " 'Tech Lead Data Engineer F/H' 'Data Engineer AirFlow Remote'\n",
      " 'Data Engineer, Analyste développeur décisionnel'\n",
      " 'Data Engineer Junior F/M/X' 'D&A – Lead Data Engineer'\n",
      " 'Data Engineer SQL (f/m/d)' 'Data engineer Spark / Scala'\n",
      " 'Data Engineer H/F -Ile de France ou Nantes-' 'Data Engineer'\n",
      " 'Lead Data Engineer' 'Site IE Data Engineer (F/M/X)'\n",
      " 'tech lead data engineer' 'Data Engineer (M/F)' 'Stagiaire Data Engineer'\n",
      " 'SOFTWARE DATA ENGINEER (H/F)' 'Data Analytics Engineer H/F'\n",
      " 'Data Engineer - Alternance - Issy Les Moulineaux - H/F'\n",
      " 'Digital Industrial Data Model Engineer'\n",
      " 'Senior Data Engineer – Data Platform / AWS / Archi Distribuée (f/m/x)'\n",
      " 'Data Engineer confirmé (H/F)' 'Cloud Data Engineer (H/F) - POEI'\n",
      " 'Senior Data Engineer' 'Data Visualization Engineer M/F'\n",
      " 'Data Engineer|Retail – H/F Lille' 'AZURE DATA ENGINEER H/F'\n",
      " 'APPRENTI(E) DATA ENGINEER ON HELICOPTER FLIGHTS (H/F)'\n",
      " 'Consultant Junior Data Engineer - Paris - 2023']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.8 Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Germany'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 :\n",
      "['Data Engineer (w/m/d)'\n",
      " '(Senior) Data Engineer (w/m/d) – Marketing & Communications'\n",
      " 'Cloud Solution Engineer - BI/IoT on Azure (m/w/d)'\n",
      " 'SAP Data Engineer in Kraichtal-Gochsheim mit Homeoffice'\n",
      " 'Senior Data Engineer (m/f/d)'\n",
      " 'Data Warehouse Architekt / Entwickler / Data Engineer (m/w/d)'\n",
      " 'Software Engineer (w/m/d)'\n",
      " 'Lead Analytics Engineer / BI Engineer (m/f/d)'\n",
      " 'System Engineer Fahrerassistenzsysteme / Autonomes Fahren (m/w/d)'\n",
      " 'Data Engineer (w/m/d) Automotive Testing Unit'\n",
      " 'BI Engineer mit Schwerpunkt Analyse (w/m/d)'\n",
      " 'Streaming Data Engineer (w/m/d)' 'DevOps Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/d) Big Data | Python | Azure in Ulm'\n",
      " 'SENIOR DATA ENGINEER (M/W/D)' 'Consultant Data Engineer (m/w/d)'\n",
      " 'Avionics System Engineer (f/w/d)'\n",
      " '(Senior) Data Engineer Analytics (m/w/d)'\n",
      " 'Senior ML/AI Engineer (m/w/d)'\n",
      " 'Big Data Engineer - Data Warehouse / E-Commerce / Big Data Architektur (m/w/d)'\n",
      " 'RNA Data Engineer (w/m/d)' 'Manager Data Engineering'\n",
      " 'DevOps Engineer (m/w/d) Data Management Solutions'\n",
      " 'Data Platforms Engineer (m/w/x)'\n",
      " 'Senior Test Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Data Engineer (m/w/d) B2B-Versandhandel'\n",
      " 'Senior Cloud - Data-Engineer (m/w/d)'\n",
      " 'Senior Associate Data Engineering'\n",
      " 'IT System Engineer - Network Services (m/w/d) in Voll-/Teilzeit'\n",
      " 'Data Engineer (m/w/d) Big Data | Python | Azure Remote/Ulm/Stuttgart'\n",
      " 'Data Engineer (m/w/d) – Data & Analytics Plattform'\n",
      " 'Solution Architect Marketing (m/w/d)' 'Senior Software Engineer (m/w/d)'\n",
      " 'Software Entwicklerin / Engineering (m/w/d) Python | Big Data Remote/Ulm/Stuttgart'\n",
      " 'Cloud FinOps Engineer / Architekt (w/m/d)'\n",
      " 'Lead Operations Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Analytics Engineer - Data Science / Python / JavaScript / Home Office (m/w/d)'\n",
      " 'BI Engineer Cloud (m/w/d)' 'Data Engineer / BI Specialist (m/w/d)'\n",
      " 'DevOps / Cloud Engineer Platform (m/w/d)'\n",
      " 'Data Scientist / Feature Engineer (m/w/d)'\n",
      " 'SAP BW Data Engineer (m/w/d) Remote/Karlsruhe/Berlin/Ulm/Stuttgart'\n",
      " 'Senior Data Engineer *' 'IT-Consultant (m/w/d) Data Engineer'\n",
      " '(Junior) Data Engineer (all genders) Data | Azure | Python'\n",
      " 'Teamleiter (m/w/d) Data Engineering'\n",
      " 'Software Engineer (m/w/d) Systementwicklung' 'Software Engineer (m/w/d)'\n",
      " 'Cloud Architect im Big Data Umfeld (m/w/d) / Azure / Java Remote/Ulm'\n",
      " '(Senior) Data Engineer (m/w/d) BI/ Data Warehouse'\n",
      " 'Senior Data Engineer - Python (w/m/d) in Stuttgart'\n",
      " 'Data Warehouse Engineer (m/w/d)'\n",
      " 'Cloud Engineer / Cloud Solution Architect (w/m/d)'\n",
      " 'Systems Engineer IT B2B (all genders)'\n",
      " 'Cloud Data Engineer (AWS) (m/w/d)'\n",
      " 'Software Engineer / Python Software Entwickler (m/w/d)'\n",
      " 'System Engineer / Linux (m/w/d)' 'Senior Data Engineer - Python (w/m/d)'\n",
      " 'BI Data Engineer (m/w/d)'\n",
      " 'Senior Cloud Engineer for Remote Application Streaming (m/w/d)'\n",
      " 'Data Architect/Engineer (m/w/d)'\n",
      " 'Engineer Data und Analytics Ticketautomaten (w/m/d)'\n",
      " 'Fullstack Software Engineer (all genders)'\n",
      " 'Professional Software-Engineer BI & Analytics (w/m/d)'\n",
      " 'IT System Engineer - IT Infrastruktur / CISCO (m/w/d)'\n",
      " 'Analytics Engineer (m/w/x) Data'\n",
      " 'Web Application Software Engineer (all genders)'\n",
      " 'Solution Architect Successfactors & HR Systems (m/f/d)'\n",
      " '(Junior) Data Engineer m/f/t' 'Principal Software Engineer Java (m/f/d)'\n",
      " 'SAP ABAP Developer CO/PS (m/w/d) in Teilzeit/ Vollzeit'\n",
      " 'Data Engineer:in' 'Referent*in als Data Engineer'\n",
      " 'Full Stack Developer Cloud (m/w/d) Remote/Ulm/Stuttgart'\n",
      " 'Projektleiter:in Data Engineering' 'Senior Big Data Engineer (w/m/d)'\n",
      " 'Cloud Data Engineer:in'\n",
      " 'Senior Backend / Full-Stack Software Engineer (m/f)'\n",
      " '(Senior) Data Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Master Data Engineer - 3D-CAD / SAP / Technische Redaktion (m/w/d)'\n",
      " 'Edge (Cloud) Computing Engineer:in' 'Senior Data Engineer (w/m/d)'\n",
      " 'Development Engineer Electronics and Software'\n",
      " '(Senior) Data Engineer - Marketing & Communications'\n",
      " 'Backend Software Engineer (all genders)'\n",
      " 'IT Infrastructure/DevOps Engineer (w/m/d)'\n",
      " '(Senior) Solution Engineer (m/w/d) SAP Data Migration - Technology Center'\n",
      " 'SAP S/4 Hana Solution Architect (m/f/d)'\n",
      " 'Business Intelligence Engineer - SQL / SSIS / Datenmodelle / Reporting (m/w/d)'\n",
      " 'Senior Solution Engineer Cloud Data (m/w/d)'\n",
      " 'IT Test Analyst Schwerpunkt Testautomatisierung im Backend-Bereich (m/w/d)'\n",
      " 'Engineering Lead Analytics Platform (m/f/d)'\n",
      " 'Cloud System Engineer - Containerisierung / DevOps / Kubernetes / Ubuntu (m/w/d)'\n",
      " 'Software Engineer mobile app development (m/w/d)'\n",
      " 'Requirements Engineer (m/w/d)' 'Cloud Engineer (m/w/d)'\n",
      " 'DevOps Engineer:in / Entwickler:in'\n",
      " 'Cloud Security Engineer Prisma (m/f/d)' 'Data Engineer (DWH) (m/f/d)'\n",
      " 'Junior DevOps Engineer (m/w/d)'\n",
      " 'Data Governance Engineer (m/f/d) 80% Homeoffice'\n",
      " 'Senior Software Engineer - Dispatching (m/f/d)'\n",
      " 'Wirtschaftsinformatiker - Data Engineering, Big Data (m/w/d)'\n",
      " '(Senior) System Engineer (m/f/d) - Asset & Config. Mgt.'\n",
      " '(Senior-) Platform Engineer (w/m/d)' 'Google Cloud Engineer (m/w/d)'\n",
      " 'Software Engineer- Embedded Systems (f/m/d)'\n",
      " 'Solution Engineer Data Center Technologies (m/w/d) für AIRBUS'\n",
      " 'Java Backend Engineer (m/f/d)' '(Senior) IT Consultant (w/m/d)'\n",
      " 'Senior Cloud Engineer - Remote möglich (m/w/d)'\n",
      " '(Senior) Cloud Engineer (m/f/d) Security'\n",
      " 'Data Engineer Marketing Intelligence (m/w/d)'\n",
      " 'Big Data Scientist (m/w/d)'\n",
      " 'Informatiker - Data Engineering, Business Intelligence (m/w/d)'\n",
      " 'IT System Engineer (w/m/d) Storage'\n",
      " 'Software System Engineer (m/f/d) Aerospace'\n",
      " 'Data Analystics Engineer (m/w/d) Technologieumfeld'\n",
      " 'Cloud Architect / Cloud Data Engineer / DevOps Engineer / Data Scientist (m/w/d)'\n",
      " 'Software Engineer m/w/d' 'Senior Software Cloud Architect (f/m/d)'\n",
      " 'Senior DevOps Engineer:in – Infrastruktur für die DB Content Hub'\n",
      " 'Anti-Abuse Software Engineer (m/f/d)'\n",
      " 'Software Development Engineer II - C++ (m/f/d) for 3D/Computer Vision'\n",
      " 'Senior QA Automation Engineer (m/f/d)'\n",
      " 'Data Engineer (m/w/d) - Data Scientist' 'Data Engineer (f/m/d)'\n",
      " 'Software Engineer (m/f/d) Aviation'\n",
      " 'Data Engineer – Machine Learning Schwerpunkt Biometrie (w/m/d)'\n",
      " 'Data Engineer Business Intelligence (m/w/d)'\n",
      " 'Linux Softwareentwickler (m/w/d) Software Developer / Software Engineer'\n",
      " 'Data Engineer (m/w/d) Business Intelligence'\n",
      " 'Software Architect / Data Engineer Azure DevOps (w/m/d) - befristet bis 12/'\n",
      " 'Data Engineer (m/w/d) - Entwickler Datenprodukte'\n",
      " 'Principal Engineer (Python) or Senior Data Engineer (m/w/d)'\n",
      " 'Data Engineer / Data Analyst (w/m/d)'\n",
      " '(Junior) System Engineer (w/m/d) Private & Hybrid Cloud'\n",
      " 'Lead SAP Service Manager (m/f/d)'\n",
      " 'Cloud Engineer - Datenmanagement / Business Intelligence / AWS / SQL (m/w/d)'\n",
      " 'Cloud Plattform Engineer (m/w/d)'\n",
      " 'CRM Engineer DevOps (m/w/d), mit Homeoffice-Option'\n",
      " 'Cloud Engineer (gn)'\n",
      " '(Senior) IT Consultant Celonis Data Engineer (m/w/x)'\n",
      " 'Big Data Engineer (m/w/d)'\n",
      " 'Embedded Linux Entwickler mit C/ C++ (m/w/d) - Embedded Software Engineer'\n",
      " 'Senior Data Engineer (m/w/d) / Machine Learning Engineer (m/w/d)'\n",
      " 'Senior Cloud Data Engineer (all genders)'\n",
      " 'Data Engineer, Data Scientist oder Data Architect (m/w/d)'\n",
      " 'Data Engineer - Life Science (f/m/d)'\n",
      " 'Requirements Engineer Eurofighter (m/f/d) for AIRBUS'\n",
      " 'Flight Control System Engineer (m/w/d)'\n",
      " 'Junior Data Engineer Data Warehouse (m/w/d)'\n",
      " 'Software Quality Assurance Engineer (m/f/d)' 'DevOps Engineer (m/w/x)'\n",
      " 'IT Automation Engineer (m/w/d)'\n",
      " 'Systemtest Engineer Flight Test (m/f/d) for AIRBUS'\n",
      " 'Software Engineering Lead - Data (m/w/d)'\n",
      " 'Computer Scientist - Data Science Programming, SAS, R (m/f/d)'\n",
      " 'Microscopy Software Engineer' 'System Engineer (m/w/d)'\n",
      " 'Junior Machine Learning Engineer (f/m/d) am Standort Hamburg'\n",
      " 'Software Engineer'\n",
      " 'Development Engineer (f/m/d) - Component developer for PEM-electrolysis cells'\n",
      " 'Senior Software Engineer Medical Devices (m/f/x)'\n",
      " 'Data Engineer SAP BI/BW (m/w/d)'\n",
      " 'Computer Scientist, Life Scientist - Validation, QA (m/f/d)'\n",
      " 'Mircosoft BI Engineer (m/w/d)' 'Data Platform Engineer (m|f|x)'\n",
      " 'Software Engineer - Backend, Datenintegration, SQL, Oracle (m/w/d)'\n",
      " 'IT DevOps PEGA Engineer (m/f/d)'\n",
      " 'Data Engineer - Data Warehouse / Datenpflege / Home Office (m/w/d)'\n",
      " 'Freelance Hardware/ Data Centre Field Engineer'\n",
      " 'System Engineer (m/f/d) Flight Test'\n",
      " 'Big Data Developer (w/m/d) - Remote'\n",
      " 'Frontend Engineer (m/f/d) // Remote'\n",
      " 'Software Engineer Java/Projektmanagement (m/w/d)'\n",
      " 'QA Automation Engineer (f/m/d)'\n",
      " 'Computer Scientist - DevOps Engineering, Elastic Stack (m/f/d)'\n",
      " 'Data Engineer' 'Data-Warehouse-Engineer (w/m/d)'\n",
      " 'DevOps / SRE Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Data Engineer - Python / PlanerAI / Kundenservice / Home Office (m/w/d)'\n",
      " 'Data Engineer/ Data Analyst (w/m/d)'\n",
      " 'IT System Engineer Remote Operations & Support (w/m/d)'\n",
      " 'Data Engineer - Datenbanken / Machine Learning / Python / Linux / Docker (m/w/d)'\n",
      " 'Data Analyst / Engineer im Bereich automatisiertes Fahren (m/w/d)'\n",
      " 'IT Engineer Citrix/Workspace Management (m/f/d)'\n",
      " 'Junior Consultant SAP ERP / SAP S4HANA (m/w/d)'\n",
      " 'Quality Assurance Automation Engineer (f/m/d)'\n",
      " 'Head of Backend Development - E Commerce / PHP / SQL / Home Office (m/w/d)'\n",
      " 'System Engineer Mission Support Systems (m/f/d)'\n",
      " 'Junior Scala Developer (M/F/X)'\n",
      " '(Senior) Software Engineer/Data Science (m/f/d) DSL'\n",
      " 'Remote - Senior Cloud Engineer (m/w/d)'\n",
      " 'Cloud Software Engineer / Backend Engineer (m/f/d) // Remote'\n",
      " 'Systems Engineer (w/m/d) Aviation'\n",
      " 'Software Engineer in Test - Backend (m/f/d)'\n",
      " 'Remote - Machine Learning Engineer (m/w/d)'\n",
      " '(Senior) DevOps Engineer (m/f/t) - Cloud Security Operations'\n",
      " 'AWS Engineer (DevOps experience preferred) (m/f/d)'\n",
      " 'DevOps Engineer Cloud - Linux / Scrum / SQL / Container Architektur (m/w/d)'\n",
      " 'Software Engineer (m/f/d) AUTOSAR ADAS Platform at Mercedes-Benz - Exclusive Direct Placement'\n",
      " 'Full Stack Developer (w/m/d) Factory Cloud'\n",
      " 'Lead Software Engineer / Architect Cloud / Data / DevOps (w/m/d)'\n",
      " 'MICROSOFT BI ENGINEER (m/w/d)'\n",
      " 'Avionic Equipment System-Engineer (m/f/d)'\n",
      " 'Embedded Linux Engineer (m/f/d) Yocto'\n",
      " '(Senior) Software Engineer IoT (f/m/d)' 'Data Engineer (m/f/d)'\n",
      " 'Data Engineer Produktion (m/w/d)'\n",
      " 'IT Engineer Citrix / Workspace Management (m/w/d)'\n",
      " 'Automotive Software Architects & Engineers (m/f/d) - Different Direct Placements at Mercedes-Benz'\n",
      " 'DevOps Engineer (m/f/d) - at Mercedes-Benz - Exclusive Direct Placement'\n",
      " 'Computer Scientist - Frontend, Angular, UI, UX (m/f/d)'\n",
      " 'Full-Stack Software Engineer (m/f/d) // Remote' 'Data Scientist (f/m/x)'\n",
      " 'Softwareentwickler - Identity Management, Access Management (m/w/d)'\n",
      " 'Data Scientist (w/m/d)' 'IT Infrastructure System Engineer (m/w/d)'\n",
      " '(Senior) DevOps Engineer (m/w/d) - Data Analytics Platform'\n",
      " 'Flutter Developer (m/w/d)' 'Lead Mechanical Engineer - Data Center'\n",
      " 'Lead Electrical Engineer - Data Center'\n",
      " 'Senior Cloud Software Architect (m/f/d) // Remote'\n",
      " '(Senior) Software Engineer (f/m/d)'\n",
      " 'Remote - Fullstack Software Engineer (m/w/d)'\n",
      " 'Datacenter Engineer w/m/d DevOps' 'Platform Services Engineer (m/f/d)'\n",
      " 'Systems Engineer Storage/ NAS (w/m/d)'\n",
      " 'Remote - Senior Software Engineer (m/w/d)'\n",
      " 'Software Engineer mit Python (m/w/d)'\n",
      " 'IT CLOUD SECURITY ENGINEER (m/w/d)'\n",
      " 'SIEM Content Developer / Detection Engineer'\n",
      " 'Software Test Engineer / Softwaretester / Testingenieur (m/w/d)'\n",
      " 'DevOps Engineer - You build it, you run it! (m/w/d)'\n",
      " 'Full Stack Engineer' 'Data Architect/ Data Engineer (m/w/d)'\n",
      " 'SQL / BI Entwickler (m/w/d) - IT Data Engineer'\n",
      " 'Data Engineer - Python / SQL (m/w/d)' 'Systems Engineer'\n",
      " 'Developer- and Validation Engineer (m/f/d) Aeroacoustics'\n",
      " 'Dual Master Data Architect - Software Development / Python / Java / SQL (m/f/d)'\n",
      " 'Software Engineer (m/w/d) Data Analytics Solutions'\n",
      " '(Junior) Data Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/d) Business Intelligence Microsoft'\n",
      " 'Frontend-Entwickler (m/w/d)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 :\n",
      "['Data Engineer (w/m/d)'\n",
      " '(Senior) Data Engineer (w/m/d) – Marketing & Communications'\n",
      " 'Cloud Solution Engineer - BI/IoT on Azure (m/w/d)'\n",
      " 'SAP Data Engineer in Kraichtal-Gochsheim mit Homeoffice'\n",
      " 'Senior Data Engineer (m/f/d)'\n",
      " 'Data Warehouse Architekt / Entwickler / Data Engineer (m/w/d)'\n",
      " 'Lead Analytics Engineer / BI Engineer (m/f/d)'\n",
      " 'Data Engineer (w/m/d) Automotive Testing Unit'\n",
      " 'BI Engineer mit Schwerpunkt Analyse (w/m/d)'\n",
      " 'Streaming Data Engineer (w/m/d)' 'Data Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/d) Big Data | Python | Azure in Ulm'\n",
      " 'SENIOR DATA ENGINEER (M/W/D)' 'Consultant Data Engineer (m/w/d)'\n",
      " '(Senior) Data Engineer Analytics (m/w/d)'\n",
      " 'Big Data Engineer - Data Warehouse / E-Commerce / Big Data Architektur (m/w/d)'\n",
      " 'RNA Data Engineer (w/m/d)' 'Manager Data Engineering'\n",
      " 'DevOps Engineer (m/w/d) Data Management Solutions'\n",
      " 'Data Platforms Engineer (m/w/x)'\n",
      " 'Senior Test Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Data Engineer (m/w/d) B2B-Versandhandel'\n",
      " 'Senior Cloud - Data-Engineer (m/w/d)'\n",
      " 'Senior Associate Data Engineering'\n",
      " 'Data Engineer (m/w/d) Big Data | Python | Azure Remote/Ulm/Stuttgart'\n",
      " 'Data Engineer (m/w/d) – Data & Analytics Plattform'\n",
      " 'Software Entwicklerin / Engineering (m/w/d) Python | Big Data Remote/Ulm/Stuttgart'\n",
      " 'Cloud FinOps Engineer / Architekt (w/m/d)'\n",
      " 'Lead Operations Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Analytics Engineer - Data Science / Python / JavaScript / Home Office (m/w/d)'\n",
      " 'BI Engineer Cloud (m/w/d)' 'Data Engineer / BI Specialist (m/w/d)'\n",
      " 'DevOps / Cloud Engineer Platform (m/w/d)'\n",
      " 'SAP BW Data Engineer (m/w/d) Remote/Karlsruhe/Berlin/Ulm/Stuttgart'\n",
      " 'Senior Data Engineer *' 'IT-Consultant (m/w/d) Data Engineer'\n",
      " '(Junior) Data Engineer (all genders) Data | Azure | Python'\n",
      " 'Teamleiter (m/w/d) Data Engineering'\n",
      " 'Cloud Architect im Big Data Umfeld (m/w/d) / Azure / Java Remote/Ulm'\n",
      " '(Senior) Data Engineer (m/w/d) BI/ Data Warehouse'\n",
      " 'Senior Data Engineer - Python (w/m/d) in Stuttgart'\n",
      " 'Data Warehouse Engineer (m/w/d)'\n",
      " 'Cloud Engineer / Cloud Solution Architect (w/m/d)'\n",
      " 'Cloud Data Engineer (AWS) (m/w/d)'\n",
      " 'Senior Data Engineer - Python (w/m/d)' 'BI Data Engineer (m/w/d)'\n",
      " 'Senior Cloud Engineer for Remote Application Streaming (m/w/d)'\n",
      " 'Data Architect/Engineer (m/w/d)'\n",
      " 'Engineer Data und Analytics Ticketautomaten (w/m/d)'\n",
      " 'Professional Software-Engineer BI & Analytics (w/m/d)'\n",
      " 'Analytics Engineer (m/w/x) Data' '(Junior) Data Engineer m/f/t'\n",
      " 'Data Engineer:in' 'Referent*in als Data Engineer'\n",
      " 'Projektleiter:in Data Engineering' 'Senior Big Data Engineer (w/m/d)'\n",
      " 'Cloud Data Engineer:in'\n",
      " '(Senior) Data Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Master Data Engineer - 3D-CAD / SAP / Technische Redaktion (m/w/d)'\n",
      " 'Edge (Cloud) Computing Engineer:in' 'Senior Data Engineer (w/m/d)'\n",
      " '(Senior) Data Engineer - Marketing & Communications'\n",
      " '(Senior) Solution Engineer (m/w/d) SAP Data Migration - Technology Center'\n",
      " 'Senior Solution Engineer Cloud Data (m/w/d)'\n",
      " 'Engineering Lead Analytics Platform (m/f/d)'\n",
      " 'Cloud System Engineer - Containerisierung / DevOps / Kubernetes / Ubuntu (m/w/d)'\n",
      " 'Software Engineer mobile app development (m/w/d)'\n",
      " 'Cloud Engineer (m/w/d)' 'Cloud Security Engineer Prisma (m/f/d)'\n",
      " 'Data Engineer (DWH) (m/f/d)'\n",
      " 'Data Governance Engineer (m/f/d) 80% Homeoffice'\n",
      " 'Wirtschaftsinformatiker - Data Engineering, Big Data (m/w/d)'\n",
      " 'Google Cloud Engineer (m/w/d)'\n",
      " 'Solution Engineer Data Center Technologies (m/w/d) für AIRBUS'\n",
      " 'Senior Cloud Engineer - Remote möglich (m/w/d)'\n",
      " '(Senior) Cloud Engineer (m/f/d) Security'\n",
      " 'Data Engineer Marketing Intelligence (m/w/d)'\n",
      " 'Informatiker - Data Engineering, Business Intelligence (m/w/d)'\n",
      " 'Data Analystics Engineer (m/w/d) Technologieumfeld'\n",
      " 'Senior Software Cloud Architect (f/m/d)' 'Data Engineer (f/m/d)'\n",
      " 'Data Engineer – Machine Learning Schwerpunkt Biometrie (w/m/d)'\n",
      " 'Data Engineer Business Intelligence (m/w/d)'\n",
      " 'Data Engineer (m/w/d) Business Intelligence'\n",
      " 'Software Architect / Data Engineer Azure DevOps (w/m/d) - befristet bis 12/'\n",
      " 'Data Engineer (m/w/d) - Entwickler Datenprodukte'\n",
      " 'Principal Engineer (Python) or Senior Data Engineer (m/w/d)'\n",
      " 'Data Engineer / Data Analyst (w/m/d)'\n",
      " '(Junior) System Engineer (w/m/d) Private & Hybrid Cloud'\n",
      " 'Cloud Engineer - Datenmanagement / Business Intelligence / AWS / SQL (m/w/d)'\n",
      " 'Cloud Plattform Engineer (m/w/d)' 'Cloud Engineer (gn)'\n",
      " '(Senior) IT Consultant Celonis Data Engineer (m/w/x)'\n",
      " 'Big Data Engineer (m/w/d)' 'Senior Cloud Data Engineer (all genders)'\n",
      " 'Data Engineer - Life Science (f/m/d)'\n",
      " 'Junior Data Engineer Data Warehouse (m/w/d)'\n",
      " 'Software Engineering Lead - Data (m/w/d)'\n",
      " 'Data Engineer SAP BI/BW (m/w/d)' 'Mircosoft BI Engineer (m/w/d)'\n",
      " 'Data Platform Engineer (m|f|x)'\n",
      " 'Data Engineer - Data Warehouse / Datenpflege / Home Office (m/w/d)'\n",
      " 'Big Data Developer (w/m/d) - Remote' 'Data Engineer'\n",
      " 'Data-Warehouse-Engineer (w/m/d)'\n",
      " 'DevOps / SRE Engineer (m/w/d) Data Analytics Platform'\n",
      " 'Data Engineer - Python / PlanerAI / Kundenservice / Home Office (m/w/d)'\n",
      " 'Data Engineer/ Data Analyst (w/m/d)'\n",
      " 'Data Engineer - Datenbanken / Machine Learning / Python / Linux / Docker (m/w/d)'\n",
      " 'Data Analyst / Engineer im Bereich automatisiertes Fahren (m/w/d)'\n",
      " '(Senior) Software Engineer/Data Science (m/f/d) DSL'\n",
      " 'Remote - Senior Cloud Engineer (m/w/d)'\n",
      " 'Cloud Software Engineer / Backend Engineer (m/f/d) // Remote'\n",
      " '(Senior) DevOps Engineer (m/f/t) - Cloud Security Operations'\n",
      " 'DevOps Engineer Cloud - Linux / Scrum / SQL / Container Architektur (m/w/d)'\n",
      " 'Lead Software Engineer / Architect Cloud / Data / DevOps (w/m/d)'\n",
      " 'MICROSOFT BI ENGINEER (m/w/d)' 'Data Engineer (m/f/d)'\n",
      " 'Data Engineer Produktion (m/w/d)'\n",
      " '(Senior) DevOps Engineer (m/w/d) - Data Analytics Platform'\n",
      " 'Senior Cloud Software Architect (m/f/d) // Remote'\n",
      " 'Datacenter Engineer w/m/d DevOps' 'IT CLOUD SECURITY ENGINEER (m/w/d)'\n",
      " 'Data Architect/ Data Engineer (m/w/d)'\n",
      " 'SQL / BI Entwickler (m/w/d) - IT Data Engineer'\n",
      " 'Data Engineer - Python / SQL (m/w/d)'\n",
      " 'Dual Master Data Architect - Software Development / Python / Java / SQL (m/f/d)'\n",
      " 'Software Engineer (m/w/d) Data Analytics Solutions'\n",
      " '(Junior) Data Engineer (m/w/d)'\n",
      " 'Data Engineer (m/w/d) Business Intelligence Microsoft']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'], \n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.9 Greece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Greece'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 :\n",
      "['Software Engineer C#, .NET (code_MSW)'\n",
      " 'Structural Engineer - Πολιτικός Μηχανικός Δομοστατικός'\n",
      " 'Junior Production Engineer (Data Analyst)'\n",
      " 'Data Engineer / BI Engineer @Crete'\n",
      " 'Data Engineer / BI Engineer @Patras'\n",
      " 'Data Engineer / BI Engineer @ Ioannnina'\n",
      " 'Data Engineer / BI Engineer @Thessaloniki'\n",
      " 'Senior Site Reliability Engineer *Remote*' 'Azure Data Engineer'\n",
      " 'Data Engineer' 'Junior Data Εngineer'\n",
      " 'Sr. Manager, Regulatory Quality Assurance Data Solutions Engineer'\n",
      " 'Engineer - Sustainability Consultant' 'Maintenance Engineer'\n",
      " 'Software Engineer (On behalf of our Client)'\n",
      " 'Junior Data Center Engineer' 'Junior Software Development Engineer'\n",
      " 'Support Engineer'\n",
      " 'Director, Go to Market Strategy and Transformation - REMOTE or FLEX'\n",
      " 'Senior Network Engineer' 'Data Platform Engineer DWH'\n",
      " 'Back-End Software Engineer' 'Embedded Software Engineer'\n",
      " 'DevOps Engineer - Work from home' 'Web Software Engineer (Zing)'\n",
      " 'Cyber Security Engineer' 'Junior Analytics Software Engineer (remote)'\n",
      " 'Technical Data Engineer (Customer Excellence)' 'Java Software Engineer'\n",
      " 'Manager, Solutions Engineer' 'Sr. Associate, Solution Engineer'\n",
      " 'Solution Engineer, Sr. Associate'\n",
      " 'Manager, Scientific Software Engineer' 'Manager, Software Engineer'\n",
      " 'Sr Associate, Test Engineer' 'Associate, Solution Engineer'\n",
      " 'Senior Software Engineer, Front-end'\n",
      " 'Manager - Cloudflare and Security Operations Engineer'\n",
      " 'Data Engineer Greece' 'Junior Network Engineer'\n",
      " 'Working Student - Software Development' 'Data Analyst'\n",
      " 'Technology Consulting - Big Data Engineer'\n",
      " 'Junior Data Engineer / Reporting Specialist'\n",
      " 'Associate - Solution Engineer – Master Data'\n",
      " 'Software Development Engineer in Test - Work from home'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Technology Consulting Graduate Opportunities @ Patras'\n",
      " 'Sr. Big Data Engineer' 'ML Ops and Data Engineer' 'Cloud Data Engineer'\n",
      " 'Senior Data Engineer' 'Data Engineer / BI Consultant in Athens'\n",
      " 'Data Engineer / BI Consultant in Thessaloniki'\n",
      " 'Software Engineer - Patras' 'PHP Developer (Support Engineer)'\n",
      " 'Big Data Engineer' 'Sr. Associate, Software Engineer'\n",
      " 'Virtual Reality Software Engineer'\n",
      " 'Data Engineers (Technology Associate Program(TAP)) @ Thessaloniki'\n",
      " 'Associate Software Engineer' 'Data Engineer (Junior/Mid-level) – Athens'\n",
      " 'Data Engineer - Sr. Associate'\n",
      " 'Battery Electromagnetic Simulation Engineer (Fixed-Term Contract)'\n",
      " 'SOFTWARE ENGINEERS' 'Data Scientist' 'Software Engineer'\n",
      " 'Manager, Regulatory Quality Assurance Data Engineer'\n",
      " 'Associate Solutions Engineer - Bachelor/Master (Graduate) - Greece'\n",
      " 'Working Student/SW Engineer' 'Data Software Engineer'\n",
      " 'Data Engineer (Azure/Databricks)' 'Software Engineer - Launchpad'\n",
      " 'Machine Learning Engineer' 'Solutions Engineer, Manager'\n",
      " 'Electrical Engineer(m/f/d), Motor Design & System Optimization - Athens, Greece'\n",
      " 'Business Intelligence Engineer' 'Lead Data Engineer'\n",
      " 'Data Warehouse Manager / Engineer'\n",
      " 'Software Engineer / Developer in Technology Consulting - Thessaloniki'\n",
      " 'IIOT Java Software Engineer'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Software Engineer - Ubuntu Build Infrastructure'\n",
      " 'Software Engineer with ML experience' 'Azure Data Engineer in Athens'\n",
      " 'Senior Big Data Engineer' 'Software Engineer 4 - Apstra'\n",
      " 'Backend Software Engineer' 'Python Software Engineer'\n",
      " 'Identity & Access Management Engineer' 'Manager, Test Engineer'\n",
      " 'Full stack engineer' 'Data Platform Engineer - Greece'\n",
      " 'Software Development Engineer – Back End'\n",
      " 'Level 1 Remote/Local Desktop Support Engineer' 'IP Network Engineer'\n",
      " 'Data Management & Software Specialist' 'Speech Engineer'\n",
      " 'Software Production Engineer'\n",
      " 'Silicon Alliances Business Development Lead'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Lead Frontend Engineer'\n",
      " 'Senior Backup Engineer (Job based in Athens, Greece)'\n",
      " 'C# Software Engineer (with German) (m/w/d) SEG/02/23'\n",
      " 'Design Verification Engineer' 'Frontend Software Engineer'\n",
      " 'Software Engineer C#, .NET' 'Test Analyst'\n",
      " 'Front-End Software Engineer (React, TypeScript), Athens or Greece Remote'\n",
      " 'Senior Test Software Development Engineer' 'Network Engineer'\n",
      " 'Senior Data Engineer @ Thessaloniki' 'AI Design & Development Engineer'\n",
      " 'Senior Associate Software Engineering'\n",
      " 'Software Engineer - Data Services' 'Business Development Engineer'\n",
      " 'ERP Software Engineer' 'Software Development Engineer (m/f/d)'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud'\n",
      " 'Senior Software Engineer (Python)'\n",
      " 'Reporting Engineer Team Lead (Big Data)'\n",
      " 'Electronics/Electrical Engineer' 'Full Stack Software Engineer'\n",
      " 'Software Development Engineer – Web Front End'\n",
      " 'Senior Data Software Engineer (AWS/Databricks/PySpark)'\n",
      " 'Software Engineer 3' 'Senior Software .NET Engineer'\n",
      " 'Lead Back-end & DevOps Engineer (Remote EU - fluent)'\n",
      " 'Senior Frontend Software Engineer' 'Trading Platform Support Engineer'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " 'Embedded & Desktop Linux Software Engineer - Optimisation'\n",
      " 'Test Automation Engineer' 'Junior Test Software Developer'\n",
      " 'Senior Application Development Engineer'\n",
      " 'Junior & Senior Network Engineer'\n",
      " 'Back-End Software Engineer (Node.js, TypeScript), Athens or Greece Remote'\n",
      " 'Energy Engineer (design of energy software, energy engineering)'\n",
      " '.Net Software Engineer' 'Technical Office Engineer'\n",
      " 'Software Engineer (Java)' 'Offshore Electrical Engineer / technician'\n",
      " 'PLC Software Automation Engineer'\n",
      " 'Cyber Intelligence Center Senior Security Analyst (Athens, Thessaloniki, Patras)'\n",
      " 'Civil Engineer Solar PV (m/f/d)' 'Senior Mobile Data Engineer'\n",
      " 'Senior Computer Vision Engineer'\n",
      " 'Senior Lead Engineer - Java - Payments Technology - Vice President - Athens'\n",
      " 'Software Development Engineer – .NET Front End' 'Site Engineer (Power)'\n",
      " 'Accounts Receivable Accountant I *Remote*'\n",
      " 'Senior Application Test Engineer' 'Manufacturing Operator|Greece']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 :\n",
      "['Junior Production Engineer (Data Analyst)'\n",
      " 'Data Engineer / BI Engineer @Crete'\n",
      " 'Data Engineer / BI Engineer @Patras'\n",
      " 'Data Engineer / BI Engineer @ Ioannnina'\n",
      " 'Data Engineer / BI Engineer @Thessaloniki' 'Azure Data Engineer'\n",
      " 'Data Engineer' 'Engineer - Sustainability Consultant'\n",
      " 'Junior Data Center Engineer' 'Data Platform Engineer DWH'\n",
      " 'Junior Analytics Software Engineer (remote)'\n",
      " 'Technical Data Engineer (Customer Excellence)'\n",
      " 'Manager - Cloudflare and Security Operations Engineer'\n",
      " 'Data Engineer Greece' 'Technology Consulting - Big Data Engineer'\n",
      " 'Junior Data Engineer / Reporting Specialist'\n",
      " 'Associate - Solution Engineer – Master Data'\n",
      " 'Software Engineer - Data Platform' 'Sr. Big Data Engineer'\n",
      " 'ML Ops and Data Engineer' 'Cloud Data Engineer' 'Senior Data Engineer'\n",
      " 'Data Engineer / BI Consultant in Athens'\n",
      " 'Data Engineer / BI Consultant in Thessaloniki' 'Big Data Engineer'\n",
      " 'Data Engineers (Technology Associate Program(TAP)) @ Thessaloniki'\n",
      " 'Data Engineer (Junior/Mid-level) – Athens'\n",
      " 'Data Engineer - Sr. Associate'\n",
      " 'Manager, Regulatory Quality Assurance Data Engineer'\n",
      " 'Data Software Engineer' 'Data Engineer (Azure/Databricks)'\n",
      " 'Lead Data Engineer' 'Data Warehouse Manager / Engineer'\n",
      " 'Azure Data Engineer in Athens' 'Senior Big Data Engineer'\n",
      " 'Data Platform Engineer - Greece' 'Data Management & Software Specialist'\n",
      " 'Senior Data Engineer @ Thessaloniki' 'Software Engineer - Data Services'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud'\n",
      " 'Reporting Engineer Team Lead (Big Data)'\n",
      " 'Senior Data Software Engineer (AWS/Databricks/PySpark)'\n",
      " 'Senior Mobile Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.10 Hungary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = \"Hungary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247 :\n",
      "['Soldering Technology (f/m/div)*'\n",
      " 'Factory Integration Engineer - Equipment Automation (f/m/div)*'\n",
      " 'Electric Development Engineer (L&M ED) (f/m/div)*'\n",
      " 'Outside Sales Representative, Focus on Eastern Europe'\n",
      " 'Electrical Engineer / Physicist Electrical Development Power Semiconductors (f/m/div)'\n",
      " 'Site Reliability Engineer' 'Data Engineer' 'Data Analyst'\n",
      " 'Data Engineer (software/application)' 'Spark Data Engineer'\n",
      " 'Lead Back-end & DevOps Engineer (Remote EU - fluent)'\n",
      " 'AI Data Engineer Intern' 'Sr Data Engineer'\n",
      " 'Full-Stack Software Engineer' 'Data Scientist Intern - EEA'\n",
      " 'Deep Learning Engineer'\n",
      " 'Data Engineer for Data Pipeline Development in automotive standards'\n",
      " 'Software Engineer Intern - 8 months (Budapest, Hungary) - ET&I'\n",
      " 'Senior Data Engineer' 'Cloud Data Engineer' 'Data Engineer gyakornok'\n",
      " '(Remote, Hungary) Senior Data Engineer'\n",
      " 'Data Engineer for IoT (REF1449L)' 'Logistic Engineer'\n",
      " 'Pályakezdő IT Szoftverfejlesztő' 'Adaptive AUTOSAR Software Engineer'\n",
      " 'Data Analytics Engineer' 'Chemical Engineer'\n",
      " 'Machine Learning Operations Engineer'\n",
      " 'Mainframe Automation Engineer (Hybrid)'\n",
      " 'Chief Revenue Officer (Remote EU - Fluent)'\n",
      " 'Senior Engineer (Manufacturing Technology)'\n",
      " 'Business Automation - Technology Engineer'\n",
      " 'Experienced Azure Data Engineer' 'Product Engineer'\n",
      " 'Quality Assurance / Test Engineer' 'Software engineer'\n",
      " 'Software Engineer - Launchpad'\n",
      " 'Cloud Native Software Engineer with Kafka Experience - Emerging Technologies and Incubation (ET&I)'\n",
      " 'evosoft - C++ software engineer - building automation'\n",
      " 'evosoft - C++ Software Engineer - Industry Automation'\n",
      " 'Junior IT Engineer' 'Lead Python Engineer (Data Enrichment)'\n",
      " 'Software Development Engineer' 'Software Engineer'\n",
      " 'Junior Workplace IT Engineer' 'Data DevOps Engineer' 'Big Data Engineer'\n",
      " 'IT engineer' 'Software Integration Engineer (Pega)'\n",
      " 'Cisco Data 2nd line engineer' 'Associate Engineer'\n",
      " 'Lead Data Software Engineer' 'Recording Analysis Engineer'\n",
      " 'Software Test Engineer (MOM)'\n",
      " 'Staff Software Engineer – Data Enablement' 'Data Security Engineer'\n",
      " 'Data Platform Engineer' 'Medior Software Engineer in Test'\n",
      " 'Software Development Engineer in Test'\n",
      " 'Software Engineer (mid or senior level) for Data Platform'\n",
      " 'Quality Assurance Engineer' 'Data Engineer Lead'\n",
      " 'Data Integration Engineer' 'Go Software Engineer, Commercial Systems'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Test Engineer for Cloud Billing - REF989F' 'Software Quality Engineer'\n",
      " 'Software Engineer - C++ & Games experience'\n",
      " 'Test Engineer Virtual Verification' 'System Engineer - Suspension'\n",
      " 'Senior Technical Leader / Data Engineer' 'SENIOR DATA ENGINEER'\n",
      " 'Principal Engineer - ET&I'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud' 'Testing Engineer'\n",
      " 'Senior Software Test Engineer' 'DWH/BI Engineer' 'Staff Data Engineer'\n",
      " 'Data Validation Engineer' 'Software Engineer - HU'\n",
      " 'Supplier Quality Engineer – Key Expert for Digitalisation'\n",
      " 'Embedded & Desktop Linux Software Engineer - Optimisation'\n",
      " 'Application Support Engineer' 'Development Engineer Optics'\n",
      " 'Principal Software Engineer' 'QA Automation Test Engineer'\n",
      " 'SAN Storage Migration Engineer' 'Cloud Platform Engineer'\n",
      " 'Systems Senior Software Engineer' 'Junior Validation Engineer'\n",
      " 'Senior Data Service Engineer' 'MLOps Engineer (m/f/x)'\n",
      " 'Software Engineer - C++' 'System Engineer' 'endpoint system engineer'\n",
      " 'SYSTEM INTEGRATION & VALIDATION ENGINEER M/W -' 'C++ Developer'\n",
      " 'information risk officer' 'Senior Software Engineer (Data Hub)'\n",
      " 'Junior Quality Engineer' 'Senior Opensource Software Engineer'\n",
      " 'oracle system engineer'\n",
      " 'Data Engineer gyakornok - Budapest, XIII. kerület'\n",
      " 'System Quality Engineer' 'Senior Data Software Engineer'\n",
      " 'Data Engineer - OPJ állás Budapest' 'Backend Engineer'\n",
      " 'Software Test Engineer' 'Full-stack Engineer (m/f/x)'\n",
      " 'CNS ECE Solutions Technical Sales Remote Support – RF Design Engineer'\n",
      " 'QA Engineer (Manual Tester)' 'Network Security Expert (f/m/d)'\n",
      " 'Microsoft Systems Engineer'\n",
      " 'FEM Simulation Engineer (thermal) - SiC Semiconductor Modules'\n",
      " 'senior data engineer' 'Cloud Infrastructure Engineer - Climate Change'\n",
      " 'Lead Software Engineer (Revolut Business)' 'system engineer tam'\n",
      " 'System Test and Integration Engineer' 'Senior Data DevOps Engineer'\n",
      " 'Lead Data Engineer' 'Development Engineer - Fuel Cell'\n",
      " 'Data Engineer - OPJ'\n",
      " 'Senior Golang/Go Software Engineer - Emerging Technologies & Incubation'\n",
      " 'Experienced/Senior Data Engineer (REF1603Y)' 'Senior FrontEnd Engineer'\n",
      " 'Quality Engineer' 'Power Electronic System Test Developer'\n",
      " 'Senior Software Engineer, vSIM - Remote EU'\n",
      " 'Senior Supplier Quality Engineer' 'Senior Software Engineer - Telco'\n",
      " 'Senior Software Development Engineer (API)'\n",
      " '(Senior) Data Engineer - Marketing & Communications'\n",
      " 'Manufacturing Engineer' 'Senior Software Engineer'\n",
      " 'Kubernetes Engineer for 5G Cloud Native Core - Packet Core'\n",
      " 'senior network engineer' 'Simulation Engineer for dynamics field'\n",
      " 'windows system engineer' 'Software Engineer on network field (REF223C)'\n",
      " 'Smart Factory Engineer' 'Principal Software Development Engineer'\n",
      " 'Software Engineer -Backend Java' 'Software engineer - Java'\n",
      " 'Senior Software Quality Engineer' 'Senior Software Engineer - Backend'\n",
      " 'Resident Manufacturing Engineer - Mechanical/Electrical'\n",
      " 'Software Engineer, vSIM - Remote EU'\n",
      " 'Lead Manufacturing Engineer - EDM technology'\n",
      " 'Data Infrastructure Specialist Engineer (Devops)'\n",
      " 'Software Engineer (Ground Truth Framework)' 'QA Engineer (DWH/BI)'\n",
      " 'Cloud Engineer - Big Data'\n",
      " 'Senior Software Engineer - Digital Workplace' 'Senior Testing Engineer'\n",
      " 'Senior Machine Learning Engineer'\n",
      " 'Automation Engineer for Professional Services'\n",
      " 'Senior Software Engineer - Site Reliability Engineer (SRE) (Remote)'\n",
      " 'Computer Vision Software Engineer Intern'\n",
      " 'Microsoft Infrastructure Engineer'\n",
      " 'Migration Software Development Engineer in Test (SDET)'\n",
      " 'dba oracle engineer' 'Software Engineer- C++' 'Cloud Software Engineer'\n",
      " 'Senior Software Engineer (Cloud and Data focus)'\n",
      " 'Big Data Solution Engineer' 'AWS Cloud Engineer'\n",
      " 'Cloud Software Engineer állás Budapest'\n",
      " 'Software Engineer – „IDAM-EHL-SE” állás Budapest'\n",
      " 'Software Engineer - \"EHL-AW\" állás Budapest'\n",
      " 'Data Center Engineer - (Located in the area of Hungary Budapest ONLY) - Part time'\n",
      " 'DevOps Engineer with Cloud' 'Cloud DevOps Engineer (REF270Z)'\n",
      " 'Network and Firewall Engineer állás Budapest' 'System Test Engineer'\n",
      " 'Automation Engineer állás Gyöngyös'\n",
      " 'Mobile/IOS Quality Assurance Engineer - IBM Budapest Lab'\n",
      " 'Test Tool Engineer' 'Junior DeskSide Support Engineer, Analyst'\n",
      " 'informatica senior system engineer'\n",
      " 'Senior Software Engineer - C++ & Games experience'\n",
      " 'Test Automation Engineer' 'Network Automation Engineer (REF1561J)'\n",
      " 'DevOps Engineer' 'IT Engineer (német)' 'Senior Test Automation Engineer'\n",
      " 'DevOps AWS Engineer' 'DevOps Engineer Cloud of Things (REF923S)'\n",
      " 'Senior DevOps Engineer with Cloud'\n",
      " 'DevOps Engineer (VNI 1 NNA_ACC Squad) (REF220B)'\n",
      " 'DevOps Engineer (REF215M)' 'Senior DevOps Engineer'\n",
      " 'Senior AWS DevOps Engineer' 'DEVOPS ENGINEER M/W -'\n",
      " 'DevOps Engineer: DNS Expert' 'Lead DevOps Engineer'\n",
      " 'Software Developer - IBM Budapest Lab' 'Application Engineer - HU'\n",
      " 'Senior DevOps Engineer (Unreal Engine) - Metaverse'\n",
      " 'Principal Software Engineer, Blockchain'\n",
      " '(Remote, Hungary) Senior FullStack Engineer'\n",
      " 'Solution Support Engineer - SAP Concur ICS' 'Software Engineer (Java)'\n",
      " 'cloud data engineer' 'Associate Solution Support Engineer'\n",
      " 'Linux System Engineer - QA, Tooling, Automation' 'Packaging Engineer'\n",
      " 'Senior AWS Solution Engineer Budapest' 'Cloud Support Engineer'\n",
      " 'Senior DevOps Engineer - Hungary'\n",
      " 'Junior Vehicle based Validation Engineer' 'Network Operations Engineer'\n",
      " 'Design Engineer' 'Senior Devops Engineer (REF214E)'\n",
      " 'Power BI Platform Engineer' 'Kubernetes Systems Engineer, EngProd'\n",
      " 'Infrastructure Specialist Engineer' 'Infrastructure Security Engineer'\n",
      " 'Machine Learning Engineer' 'Technical Support Engineer Hungary'\n",
      " 'Maintenance Project Engineer' 'Ops/DevOps Engineer'\n",
      " 'Field Service Engineer Hungary' 'Azure DevOps Engineer'\n",
      " 'Connectivity Network Engineer' 'Functional safety quality engineer'\n",
      " 'Service Application Engineer SGT-600/700/750 (MGT)'\n",
      " 'Internship = Development FrontEndtechniques of Speech to Text integration workflows'\n",
      " 'HR Director (Remote EU - Fluent)' 'Integrations Engineer'\n",
      " 'Senior LEAN Engineer' 'Process Engineer Intern' 'Project Engineer'\n",
      " 'Network Engineer' 'Sales Engineer HU'\n",
      " 'Service Engineer - Power BI Developer' 'Product Certification Engineer'\n",
      " 'Service Assurance Engineer' 'Cloud Analyzer Specialist - SAN SAS - JS5'\n",
      " 'Quotation Engineer - French Speaker'\n",
      " 'Power Plant Operation & Maintanance Engineer'\n",
      " 'Senior Site Reliability Engineer - Tech Modernization squad'\n",
      " 'Project Manager - Project Data Structure & Processes' 'LCA Expert'\n",
      " 'DevOps Engineer – Cloud' 'Junior Software Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 :\n",
      "['Data Engineer' 'Data Engineer (software/application)'\n",
      " 'Spark Data Engineer' 'AI Data Engineer Intern' 'Sr Data Engineer'\n",
      " 'Data Engineer for Data Pipeline Development in automotive standards'\n",
      " 'Senior Data Engineer' 'Cloud Data Engineer' 'Data Engineer gyakornok'\n",
      " '(Remote, Hungary) Senior Data Engineer'\n",
      " 'Data Engineer for IoT (REF1449L)' 'Data Analytics Engineer'\n",
      " 'Experienced Azure Data Engineer'\n",
      " 'Cloud Native Software Engineer with Kafka Experience - Emerging Technologies and Incubation (ET&I)'\n",
      " 'Lead Python Engineer (Data Enrichment)' 'Data DevOps Engineer'\n",
      " 'Big Data Engineer' 'Cisco Data 2nd line engineer'\n",
      " 'Lead Data Software Engineer' 'Staff Software Engineer – Data Enablement'\n",
      " 'Data Security Engineer' 'Data Platform Engineer'\n",
      " 'Software Engineer (mid or senior level) for Data Platform'\n",
      " 'Data Engineer Lead' 'Data Integration Engineer'\n",
      " 'Test Engineer for Cloud Billing - REF989F'\n",
      " 'Senior Technical Leader / Data Engineer' 'SENIOR DATA ENGINEER'\n",
      " 'Software Engineer - Micro/Private/Bare-Metal Cloud' 'DWH/BI Engineer'\n",
      " 'Staff Data Engineer' 'Data Validation Engineer'\n",
      " 'Cloud Platform Engineer' 'Senior Data Service Engineer'\n",
      " 'Senior Software Engineer (Data Hub)'\n",
      " 'Data Engineer gyakornok - Budapest, XIII. kerület'\n",
      " 'Senior Data Software Engineer' 'Data Engineer - OPJ állás Budapest'\n",
      " 'senior data engineer' 'Cloud Infrastructure Engineer - Climate Change'\n",
      " 'Senior Data DevOps Engineer' 'Lead Data Engineer' 'Data Engineer - OPJ'\n",
      " 'Experienced/Senior Data Engineer (REF1603Y)'\n",
      " '(Senior) Data Engineer - Marketing & Communications'\n",
      " 'Kubernetes Engineer for 5G Cloud Native Core - Packet Core'\n",
      " 'Data Infrastructure Specialist Engineer (Devops)'\n",
      " 'Cloud Engineer - Big Data' 'Cloud Software Engineer'\n",
      " 'Senior Software Engineer (Cloud and Data focus)'\n",
      " 'Big Data Solution Engineer' 'AWS Cloud Engineer'\n",
      " 'Cloud Software Engineer állás Budapest'\n",
      " 'Data Center Engineer - (Located in the area of Hungary Budapest ONLY) - Part time'\n",
      " 'DevOps Engineer with Cloud' 'Cloud DevOps Engineer (REF270Z)'\n",
      " 'Mobile/IOS Quality Assurance Engineer - IBM Budapest Lab'\n",
      " 'DevOps Engineer Cloud of Things (REF923S)'\n",
      " 'Senior DevOps Engineer with Cloud' 'cloud data engineer'\n",
      " 'Power BI Platform Engineer' 'Service Engineer - Power BI Developer'\n",
      " 'Cloud Analyzer Specialist - SAN SAS - JS5' 'DevOps Engineer – Cloud']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.11 Ireland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Ireland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 :\n",
      "['Process and applications engineer (f/m/d)-Automation Background'\n",
      " 'Tendering Engineer' 'Validation Engineer'\n",
      " 'Senior Infrastructure Engineer (Ireland)' 'Senior Automation Engineer'\n",
      " 'Back End Engineer - OATS (Ireland)' 'Mechanical BIM Engineer'\n",
      " 'Senior Engineer' 'Analytics Engineer, Digital Applications & Analytics'\n",
      " 'QA Specialist' 'Senior Site Engineer - Dublin'\n",
      " 'Automation Test Engineer, eCommerce' 'Principal Devops Engineer'\n",
      " 'Splunk Engineer' 'Mechanical Engineer / CM - Leixlip'\n",
      " 'Senior Site Engineer - Limerick/Cork/Kerry' 'Quality Engineer'\n",
      " 'In-Die Engineer' 'IT Support Engineer - German Speaking'\n",
      " 'M&E Project Manager' 'Lead Electrical Engineer' 'Site Engineer'\n",
      " 'IT Support Engineer - Italian Speaking'\n",
      " 'Technical Support Engineer - Performance'\n",
      " 'Research & Development Engineer' 'Senior Software Engineer - DevSec Ops'\n",
      " 'R&D Engineer' 'Inventory Manager' 'Manufacturing Engineer'\n",
      " 'M+E Design Engineer'\n",
      " 'Project Engineer (2 year fixed term contract) - Wexford, Ireland'\n",
      " 'Engineering Manager' 'Quality Control Engineer/process engineer'\n",
      " 'Technical Support Engineer - Performance (m/f/d)'\n",
      " 'Mechanical Engineer QS' 'Process Development Engineer II'\n",
      " 'R&D Senior Engineer' 'Senior Software Engineer' 'Software Delivery Lead'\n",
      " 'Fire & Security Engineer' 'Principle R&D Engineer'\n",
      " 'Junior Project Engineer' 'Maintenance Engineer' 'Software Developer'\n",
      " 'Google Cloud Platform - GCP Technical Architect'\n",
      " 'Civil Engineer - Laois' 'Senior R&D Engineer' 'Engineer'\n",
      " 'Structural Engineer' 'Electrical Project Manager - London'\n",
      " 'Senior Civil Engineer' 'Senior AutoSAR Developer' 'Junior CSV Engineer'\n",
      " 'Technical Support Engineer - NAS' 'Civil Engineer - Cork'\n",
      " 'Civil Engineer - Meath' 'Graduate Production Engineer'\n",
      " 'BIM Designers - Multiple Opportunities' 'Manufacturing Engineer II'\n",
      " 'Civil Engineer - Wexford' 'Senior Design Quality Engineer'\n",
      " 'R&D Manager - Product Engineering / Sustaining Engineering'\n",
      " 'Intermediate/Senior Civil Engineer' 'Senior Quality Engineering Manager'\n",
      " 'Quality Engineer- Design Assurance' 'Electrical QA/QC Engineer'\n",
      " 'Senior Project Engineer - Main Contractor'\n",
      " 'Machine Vision Engineer - Contract (AM17784)'\n",
      " 'Senior Manufacturing Engineer' 'Associate Quality Engineer'\n",
      " 'Quality Engineer - Team Leader' 'Quality Engineer (NPI)'\n",
      " 'Process Engineer' 'R&D Manager' 'Cleanroom Certification Engineer'\n",
      " 'Automation Engineer - County Waterford (AM17401)'\n",
      " 'Senior Manufacturing Engineer - Freudenberg Medical'\n",
      " 'Senior Quality Engineer- Design Assurance'\n",
      " 'Senior Linguistic Tester with French'\n",
      " 'Automation/Manufacturing Engineer - County Waterford (DT17624)'\n",
      " 'Lead Project Engineer' 'Mechanical Engineer'\n",
      " 'BIM Engineer / Technician - Data Centres' 'Quality Engineer II'\n",
      " 'Director - Buildings & Structures'\n",
      " 'Mechanical Design Engineer - County Galway (AM17944)'\n",
      " 'Process Engineer - Wastewater Treatment' 'BIM Engineer'\n",
      " 'Business Applications Engineer - County Wexford (DT17924)'\n",
      " 'Site Lead Facilities'\n",
      " 'Artificial Intelligence and Machine Learning Software Engineer (DT17886)'\n",
      " 'Senior Pre Market Supplier Quality Engineer'\n",
      " 'Senior Mechanical Engineer - MEP Consultancy'\n",
      " 'API Process Engineer - County Cork (AM17682)'\n",
      " 'Site Reliability Engineer' 'Software Engineering Team Lead'\n",
      " 'Controls Engineer - South Dublin (AM17868)'\n",
      " 'Manufacturing Engineer - County Clare (AM16777)'\n",
      " 'Senior Web Developer (PHP) & Systems Integration Engineer'\n",
      " 'Principal Electrical Design Engineer - Data Centres'\n",
      " 'MEICA - Process Design Engineer - Water/Wastewater'\n",
      " 'Project Engineer - Piping' 'Field Service Technician'\n",
      " 'Production Support Engineer - North Dublin (AM17732)'\n",
      " 'R&D Lead Mechanical Design Engineer - County Galway (AM17945)'\n",
      " 'Resident Engineer - Public Park Upgrade' 'IT Compliance Specialist'\n",
      " 'Quality Engineer - Projects' 'Quality Technician'\n",
      " 'Mechanic / Automotive Diagnostics Test Technician - VCS'\n",
      " 'Quality Systems Engineer - South Dublin (DT17765)'\n",
      " 'Mechanical Project Engineer'\n",
      " 'TEST AND PROJECT ENGINEER - WEST DUBLIN (AM17520)'\n",
      " 'Validation Engineer for Manufacturing Quality - County Waterford (DT17804)'\n",
      " 'Inhouse Technical Support Engineer'\n",
      " 'Telecomm Engineer - Multi Disciplinary Consultancy'\n",
      " 'Graduate Manufacturing Engineer - County Clare (AM16777y)'\n",
      " 'Mechanical Site Engineer'\n",
      " 'Civil Design Engineer - Infrastructure Projects'\n",
      " 'Health, Safety and Compliance Officer'\n",
      " 'API Process Engineer - Co Cork (DT17682)'\n",
      " 'Commissioning Engineer (water treatment systems) (AM17477)'\n",
      " 'Mechanical Site Engineer - Pharma Piping' 'Town Planner'\n",
      " 'Quality Team Lead' 'Mechanical Design Engineer - Building Services'\n",
      " 'SENIOR QA ENGINEER (AM17370)' 'QA Engineer - M&E Contractor'\n",
      " 'Process Designer - Multi Disciplinary Consultancy'\n",
      " 'Senior BIM Technician'\n",
      " 'Automation/Manufacturing Engineer - County Waterford (AM17624)'\n",
      " 'Quality Technician - West Dublin (AM17942)' 'Building Services Engineer'\n",
      " 'Quality Engineer - Product Transfer (DT17908)'\n",
      " 'Project Engineer - Mechanical Building Services Engineer'\n",
      " 'R&D Test Development Engineer - County Galway (AM17943)'\n",
      " 'Process Lead - Multi Disciplinary Consultancy'\n",
      " 'Senior Quality Engineer - West Dublin (AM17430)'\n",
      " 'Test Software Development Engineer (DT17485)'\n",
      " 'Refrigeration Service Engineers for opportunities nationwide (AM16185)'\n",
      " 'Mechanical Services Site Supervisor'\n",
      " 'Senior Project Manager - Buildings & Infrastructure'\n",
      " 'Lead I&C Engineer - Industrial Projects' 'Technical Support Manager'\n",
      " 'Mechanical Engineer - Facilities Management'\n",
      " 'Senior Technical Support Manager-Cloud'\n",
      " 'Junior Electrical Engineer - MEP Contractor'\n",
      " 'Continuous Product Improvement (CPI) Scientist - North Wicklow (AM17931)'\n",
      " 'EHS Leader - County Louth (AM17760)'\n",
      " 'Senior Resident Engineer - Infrastrucutre Projects'\n",
      " 'Senior Civil Engineer - Industrial Projects'\n",
      " 'Building Services Engineer (QA/QC) - Main Contractor'\n",
      " 'Electrical Engineer - Water Treatment' 'Site Service Engineer'\n",
      " 'Senior Software Engineer - Database Specialist (DT17890)'\n",
      " 'Senior Quantity Surveyor'\n",
      " 'Senior Fire Engineer - Engineering Consultancy'\n",
      " 'Senior CPI Scientist - North Wicklow (AM17930)' 'Electrical Engineer'\n",
      " 'Medical Field Service Engineer' 'Electrical Engineers'\n",
      " 'R&D Senior Software Engineer - N. Dublin (DT17879)'\n",
      " 'Mechanical Commissioning Engineer'\n",
      " 'Senior Continuous Product Improvement Scientist - Wicklow (DT17830)'\n",
      " 'Senior Electrical Engineer - Consultancy' 'Electrical Design Engineer'\n",
      " 'Maintenance Planner'\n",
      " 'Electrical Technical Director - Regional Manager - Consultancy'\n",
      " 'Senior BIM Designer -Industrial Projects'\n",
      " 'BIM Lead -Industrial Projects' 'Electrical Service Engineer'\n",
      " 'Mechanical Project Manager' 'LAB IT TECHNICAL LEAD (AM17326)'\n",
      " 'Senior Project Planner' 'Senior Automation Engineer- BioPharma'\n",
      " 'Senior Electrical Service Engineer' 'QA Manager'\n",
      " 'Process Department Manager - PSSS'\n",
      " 'Senior Electrical Design Engineer - Industrial Projects'\n",
      " 'Lead Process Engineer - Pharma' 'Equipment/Maintenance Engineer - CMMS'\n",
      " 'Electrical Design Engineer (Intermediate level)' 'BIM Lead'\n",
      " 'Mechanical Site Manager' 'Electrical Supervisor'\n",
      " 'Software Engineer - Product Data Quality' 'QA/RA Project Engineer'\n",
      " 'Customer Support Engineer' 'Electrical QAQC Engineer' 'CQV Engineer'\n",
      " 'QA Validation Specialist - Biotech' 'Data Engineer - Web Scraper'\n",
      " 'Project Cost Manager - Engineering Consultancy'\n",
      " 'Validation Engineer (Hybrid)' 'Automation Project Engineer - Dublin'\n",
      " 'Data Engineer' 'Customer Support Representative (English+German)'\n",
      " 'CyberArk (PAM) Engineer' 'Electrical Design Engineer - Oil & Gas'\n",
      " 'Database Engineer' 'Electrical Project Engineer'\n",
      " 'Business Intelligence Engineer' 'Electrical Commissioning Engineer'\n",
      " 'Civil / Structural Design Engineer - OnShore Oil & Gas'\n",
      " 'Building Services Engineer (Electrical) - Dublin'\n",
      " 'Helpdesk Support Engineer' 'Junior Mechanical Engineer HVAC'\n",
      " 'Grade VII Data Engineer' 'Data Centre Network Engineer'\n",
      " 'R&D Senior Software Engineer - North Dublin (AM17879)'\n",
      " 'Software Engineer, Data Pipelines - Opportunity for Working Remotely Cork,'\n",
      " '2023 Software Development Engineer Internship'\n",
      " 'BI Developers, Data Architects, Data Engineers - Data Analytics, Digital'\n",
      " 'Data Centre Engineer' 'Cloud Engineer ( Data Analytics) Hybrid'\n",
      " 'Data Center Technician' 'Chemical Process Engineer' 'Big Data Engineer'\n",
      " 'Software Development Engineer (Data Science Team)'\n",
      " 'Quantitative Data Engineer' 'Senior Data Engineer- Hybrid'\n",
      " 'Data Engineer - QuantumBlack' 'Staff Software Engineer'\n",
      " 'Legal Data Specialist' 'Data Scientist / ML Engineer'\n",
      " 'Technical Support Engineer'\n",
      " 'Senior Software Engineer – CargoWise Customs, Ireland'\n",
      " 'Data engineer II' 'Software Engineer Intern'\n",
      " 'Intern Instrumentation Engineer 2023 Dublin'\n",
      " 'Data Core technical authority engineer' 'Data Engineer (AWS)'\n",
      " 'R&D Incentives Technical Consultant (Engineer)'\n",
      " 'SOFTWARE ENGINEER INTERN'\n",
      " 'Data Analyst - Associate Tools (Cork or Republic of Ireland remote)'\n",
      " 'Software Engineer' 'QA/Test Analyst (Manual)'\n",
      " 'Software Engineer (Node.js)' 'Customer Experience Engineer'\n",
      " 'Cloud Operations Engineer (AWS) - Ireland (Remote)'\n",
      " 'Frontend Software Engineer'\n",
      " 'NET software development engineer, Tanzu - Opportunity for Working Remotely Cork,'\n",
      " 'Senior Software Engineer, Query'\n",
      " 'Continuous Product Improvement (CPI) Scientist - N. Wicklow (DT17931)'\n",
      " 'Technical Engineer (Airbus)'\n",
      " 'Kingspan Graduate Programme - Technical and R&D'\n",
      " 'Data Cabling Engineer Data Cabling Engineer' 'DevOps Engineer (Data)'\n",
      " 'AI & ML Software Engineer' 'Software Engineer II'\n",
      " 'Platform Data Engineer' 'Product Designer, Analytics'\n",
      " 'Data Analytics Engineer' 'Data Engineer (C#, Python, Elastic)'\n",
      " 'Process Engineer - Waste Water' 'AX Support Technician'\n",
      " 'Energy Infrastructure Engineer' 'Systems/Software Engineer (Intern)'\n",
      " 'Quality Systems Engineer - South Dublin (AM17765)'\n",
      " 'Process Engineer - Pharma' 'Data Scientist'\n",
      " 'eCommerce Technician - Limerick or Dublin' 'Staff Data Scientist'\n",
      " 'Data Engineer (Azure)' 'Data Engineer II'\n",
      " 'Data Scientist (12mo Fixed Term Contract)'\n",
      " 'Graduate Automation Engineer' 'BIM/CAD Designer (Revit)' 'Drivers 2023'\n",
      " 'Java Software Engineer' 'Senior Product Designer (Ireland/UK)'\n",
      " 'Senior Data Engineer' 'Software Consultant - Technical Writer'\n",
      " 'Senior Data Scientist - QuantumBlack' 'Junior Data Engineer'\n",
      " 'NLP Data Engineer' 'Senior Data Scientist'\n",
      " 'Senior Applied Scientist - Ads (Remote)' 'Data Science Intern'\n",
      " 'Assoc. Software Engineer' 'Incident and Problem Manager'\n",
      " 'Principal data engineer' 'Front End Software Development Engineer'\n",
      " 'Software Support Engineer' 'Process & Product Development Engineer'\n",
      " 'Senior QA Engineer' 'Support Engineer / Product Specialist'\n",
      " 'Software Engineer - Big Data Devops' 'Principal Data Engineer'\n",
      " 'BIM - 3D Piping Designer' 'Analytical Scientist 2'\n",
      " 'Process Engineer - Industrial Diamond'\n",
      " 'Graduate Transportation Planner - Dublin 2022/23'\n",
      " 'Senior Software Development Engineer - Big Data Service'\n",
      " 'Data Center Engineering Operation Technician (Shift)'\n",
      " 'Digital Power Technical Engineer' 'Data Engineer/Analyst - Actuarial'\n",
      " 'Junior Software Engineer'\n",
      " 'BizOps Engineer (DevOps | Cloud Infrastructure | Site Reliability) All Experience Levels'\n",
      " 'AES Staff Engineer, Product Applications'\n",
      " 'INFRASTRUCTURE ENGINEER INTERN' 'Senior Data Scientist- Hybrid'\n",
      " 'Engineer, Process Optimisation' 'Senior Production Planner'\n",
      " 'R&D ENGINEER I' 'Digital Power Project Technical Engineer'\n",
      " 'BIM Technician/CAD Technician' 'R&D Deep Learning Engineer'\n",
      " 'Tier 3 Support Engineer' 'Infrastructure Market Data Engineer'\n",
      " 'Process Scientist - Bioprocessing' 'Test Engineer-R&D'\n",
      " 'Data Engineer, Mid to Staff' 'Senior Quality Engineer'\n",
      " 'Designer FTTH - NBI' 'VFX Systems Engineer'\n",
      " 'VFX Systems Engineer - On site'\n",
      " 'Graduate Geologist, Engineering Geologist, Geotechnical Engineer'\n",
      " 'Broadcast Engineer' 'Junior Energy Engineer'\n",
      " 'Staff Manufacturing Systems Engineer' 'HVAC Engineer'\n",
      " 'Process Engineer - Tipperary'\n",
      " 'Staff Engineer, Product Design for Additive'\n",
      " 'Staff Engineer, Manufacturing' 'Digital Solutions Engineer'\n",
      " 'Staff Engineer - Process Development, Advanced Operations'\n",
      " 'Process Engineer - Meath' 'Senior Validation Engineer'\n",
      " 'Senior Process Development Engineer' 'Solutions Engineering Manager'\n",
      " 'Senior Project Engineer (Additive)' 'Senior Engineer, Quality Assurance'\n",
      " 'Senior Engineer, Manufacturing' 'QA Validation Engineer'\n",
      " 'Service Supervisor' 'P21 Design Engineer' '3D Software Developer'\n",
      " 'Automation Engineer' 'Senior Staff Design Manufacturing Engineer'\n",
      " 'Senior Engineer - Process Development, Advanced Operations'\n",
      " 'Field Services Engineer' 'ARCGIS Specialist - Kilkenny'\n",
      " 'Electrical Engineer - Main Contractor' 'Warehouse Manager'\n",
      " 'Commercial & Planning Manager' 'Maintenance & Facilities Planner'\n",
      " 'Air Conditioning Technician' 'Mechanical Design Engineer'\n",
      " 'Staff Enterprise System Specialist' 'IT Lab Systems Engineer'\n",
      " 'Electrical Technician' 'Senior Field Service Engineer (Switchgear)'\n",
      " 'Graduate Mechanical Engineer' 'Mechanical Engineer (Inter - Senior)'\n",
      " 'Senior Site Reliability Engineer' 'BIM Coordinator'\n",
      " 'Lead Software Engineer' 'Commissioning Engineer (Solar & Wind)'\n",
      " 'Civil Project Engineer' 'Senior Backend Engineer - Java'\n",
      " 'HVAC Field Service Engineer'\n",
      " 'Electrical Project Engineer (Data Centre Projects, Europe)'\n",
      " 'Senior Fire Protection Systems Commissioning Engineer'\n",
      " 'Senior Site Engineer' 'Senior Backend Engineer- Python'\n",
      " 'Senior DevOps Engineer – Remote in Ireland' 'Business Analyst'\n",
      " 'Senior BIM Engineer' 'IT Administrator'\n",
      " 'Lead Electrical Design Engineer'\n",
      " 'Mechanical Engineer (Data Centre project, Europe)'\n",
      " 'Cloud Infrastructure Intern' 'IT Systems and Support engineer'\n",
      " 'MEP CAD Engineer' 'Field Service Engineer (Switchgear)'\n",
      " 'Data Center Site Representative'\n",
      " 'Mechanical QAQC Engineer (Data Centre in Europe)'\n",
      " 'Graduate Measured Surveyor'\n",
      " 'GCP Cloud DevOps Engineer I Shared Technology' 'Senior Process Engineer'\n",
      " 'Mechanical Engineer (Projects in Europe)' 'DevOps Engineer'\n",
      " '2K Games Dublin - Fast Track Engineering Graduate Programme'\n",
      " 'Internship Engineer Account Manager IRELAND based in Paris|DASSAULT SYSTEMS'\n",
      " 'VIE - Junior Developer H/F' 'Senior .NET Software Engineer- Hybrid'\n",
      " 'BMS Engineer'\n",
      " 'Senior Software Development Engineer, VMware Cloud Services - Opportunity for Working Remotely Cork,'\n",
      " 'System Development Engineer, AWS DNS' 'QC Analyst - In Process'\n",
      " 'Test Development Engineer (Biosensors)' 'Service Desk Analyst'\n",
      " 'Environmental, Health & Safety Officer'\n",
      " 'System Design Verification Engineer'\n",
      " 'Software Development Engineer I - Customer Success Engineering'\n",
      " 'Software Development Engineer in Test'\n",
      " 'Linux Server Support Engineer/Technician' 'Cloud Data Engineer'\n",
      " 'Backend Engineer' 'Validation Engineer (based in Letterkenny, Ireland)'\n",
      " 'Product Builder - Permanent - All Shifts' 'Sr Tech Support Engineer- UX'\n",
      " 'DevOps Engineer - Hybrid' 'Data Centre Specialist'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Service Engineer – Dublin Airport' 'Reliability Engineer (Hybrid)'\n",
      " 'Python Django Backend Dev / AWS Software Engineer – full time'\n",
      " 'Physical Security Architect Data Center' 'Piping Designer'\n",
      " 'Full Stack Engineer' 'Validations Technician'\n",
      " 'Agile Technical Analyst- Hybrid' 'Site Infrastructure Engineer'\n",
      " 'Process and Product Development Engineer'\n",
      " 'Site Reliability Engineer (SRE) - Platform Infrastructure'\n",
      " 'Manufacturing Process Engineer (Evening Shift)' 'Field Service Engineer'\n",
      " 'Mechanical Quality Lead'\n",
      " 'Engineer, Test Process Development Engineering'\n",
      " 'Senior Software Engineer in Test' 'QC Analyst – QC Viral Vector'\n",
      " 'QC Product Analyst (Day Position) (TEMPORARY)' 'Junior QA Engineer'\n",
      " 'PMO Analyst' 'Process Scientist - Small Scale Flex'\n",
      " 'Medical Scientist - Biochemistry'\n",
      " 'Principal EIA Environmental Consultant Ireland' 'AI/ML/Vision Engineer'\n",
      " 'Continuous Improvement Engineer'\n",
      " 'Mechanical QA/QC Engineer - Leixlip Co. Kildare'\n",
      " 'Research Engineer in CMOS Integrated Circuit Design for Intravascular Echocardiography'\n",
      " 'L1 Data Center Engineer' 'Supply Chain Data Analyst'\n",
      " 'Mechanical Engineer (Commissioning) - Leixlip'\n",
      " 'Senior Geotechnical Engineer / Engineering Geologist'\n",
      " 'Autonomous Systems Software Engineer' 'Critical Facilities Engineer'\n",
      " 'Production Engineer' 'Process and Quality Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 :\n",
      "['Mechanical BIM Engineer'\n",
      " 'Analytics Engineer, Digital Applications & Analytics'\n",
      " 'Google Cloud Platform - GCP Technical Architect'\n",
      " 'BIM Engineer / Technician - Data Centres' 'BIM Engineer'\n",
      " 'Principal Electrical Design Engineer - Data Centres'\n",
      " 'Senior Technical Support Manager-Cloud'\n",
      " 'Senior Software Engineer - Database Specialist (DT17890)'\n",
      " 'BIM Lead -Industrial Projects' 'Senior Automation Engineer- BioPharma'\n",
      " 'BIM Lead' 'Software Engineer - Product Data Quality'\n",
      " 'QA Validation Specialist - Biotech' 'Data Engineer - Web Scraper'\n",
      " 'Data Engineer' 'Database Engineer' 'Grade VII Data Engineer'\n",
      " 'Data Centre Network Engineer'\n",
      " 'Software Engineer, Data Pipelines - Opportunity for Working Remotely Cork,'\n",
      " 'BI Developers, Data Architects, Data Engineers - Data Analytics, Digital'\n",
      " 'Data Centre Engineer' 'Cloud Engineer ( Data Analytics) Hybrid'\n",
      " 'Big Data Engineer' 'Software Development Engineer (Data Science Team)'\n",
      " 'Quantitative Data Engineer' 'Senior Data Engineer- Hybrid'\n",
      " 'Data Engineer - QuantumBlack' 'Legal Data Specialist' 'Data engineer II'\n",
      " 'Data Core technical authority engineer' 'Data Engineer (AWS)'\n",
      " 'Cloud Operations Engineer (AWS) - Ireland (Remote)'\n",
      " 'Data Cabling Engineer Data Cabling Engineer' 'DevOps Engineer (Data)'\n",
      " 'Platform Data Engineer' 'Data Analytics Engineer'\n",
      " 'Data Engineer (C#, Python, Elastic)' 'Data Engineer (Azure)'\n",
      " 'Data Engineer II' 'Senior Data Engineer' 'Junior Data Engineer'\n",
      " 'NLP Data Engineer' 'Principal data engineer'\n",
      " 'Software Engineer - Big Data Devops' 'Principal Data Engineer'\n",
      " 'Senior Software Development Engineer - Big Data Service'\n",
      " 'Data Center Engineering Operation Technician (Shift)'\n",
      " 'Data Engineer/Analyst - Actuarial'\n",
      " 'BizOps Engineer (DevOps | Cloud Infrastructure | Site Reliability) All Experience Levels'\n",
      " 'Infrastructure Market Data Engineer' 'Data Engineer, Mid to Staff'\n",
      " 'BIM Coordinator'\n",
      " 'Electrical Project Engineer (Data Centre Projects, Europe)'\n",
      " 'Senior BIM Engineer' 'Mechanical QAQC Engineer (Data Centre in Europe)'\n",
      " 'GCP Cloud DevOps Engineer I Shared Technology'\n",
      " 'Senior Software Development Engineer, VMware Cloud Services - Opportunity for Working Remotely Cork,'\n",
      " 'Test Development Engineer (Biosensors)' 'Cloud Data Engineer'\n",
      " 'Data Centre Specialist' 'Physical Security Architect Data Center'\n",
      " 'L1 Data Center Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.12 Israel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = \"Israel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 :\n",
      "['Data Engineer' '\\uf4ccData Engineer\\uf4cc(Hybrid' 'Junior Data Engineer'\n",
      " 'Cloud Data Engineer' 'Junior Data Engineer לארגון מוביל'\n",
      " 'Data Engineer - Data Assets Group' 'Storage Analytics - Data Engineer'\n",
      " 'Deep Learning Engineer' 'Fullstack Engineer'\n",
      " 'Junior Windows Low Level Engineer - Engine team'\n",
      " 'Full-Stack Software Engineer III' 'Experienced Data Engineer'\n",
      " 'Silicon Reliability Engineer' 'Manual QA Engineer'\n",
      " 'Senior Data Engineer'\n",
      " 'לחברת סטראט-אפ בתחום הסייבר דרוש\\\\ה Data Engineer' 'Big Data Engineer'\n",
      " 'Senior Frontend Engineer' 'Data Platform Engineer' 'DATA ENGINEER'\n",
      " 'Data Scientist - Thetaray' 'Data Tools Engineer'\n",
      " 'Data Solutions Engineer' 'Application Developer'\n",
      " 'Software Engineer, Infrastructure' 'QA Engineer'\n",
      " 'Senior Big Data Engineer' 'Senior/Junior Data Engineer'\n",
      " 'Data Engineer Team Leader' 'Integration Engineer Student'\n",
      " 'Data Engineer לארגון מסווג' 'Automation Engineer - Data Group'\n",
      " 'Back End Engineer' 'Data Engineer, Mobile Identity'\n",
      " 'Software Engineer Tel-Aviv' 'Data Engineer!'\n",
      " 'Remote Technical Support Engineer'\n",
      " 'חברה מצליחה ומובילה מגייסת BI data engineer'\n",
      " 'Full stack Engineer (Frontend Oriented)'\n",
      " 'Costing and Master data Expert'\n",
      " 'חברת טלדור מגייסת data Engineer לארגון מסווג במרכז הארץ'\n",
      " 'Data Engineer לחברה פיננסית' 'Data Engineer Lead'\n",
      " 'Quality Assurance Engineer' 'Data Engineer לחברת סטארטאפ'\n",
      " 'Data Engineer לחברת סטארט -אפ מובילה בכפר סבא'\n",
      " 'חברה טכנולוגית מצליחה מגייסת Data Engineer'\n",
      " 'חברת SU מדהימה מגייסת Data Engineer'\n",
      " 'חברת Gaming מוכרת מגייסת Big Data QA Engineer'\n",
      " 'חברת Gaming מגייסת Data Engineer'\n",
      " 'חברת הייטק גלובלית מחפשת Data Engineer'\n",
      " 'ארגון מוכר ומצליח מחפש Data Engineer'\n",
      " 'DATA ENGINEER לתפקיד מעניין ומאתגר'\n",
      " 'Data Engineer לחברת סטארטאפ מצליחה בדרום'\n",
      " 'Data Engineer לחברה מובילה בשרון'\n",
      " 'חברה מדהימה בדרום מגייסת Data Engineer' 'Data Engineer לסטארט-אפ מצליח'\n",
      " 'בנק מוביל מגייס Data Engineer'\n",
      " 'Data Engineer לסטארט-אפ בתחום המשחקים הסוציאלים'\n",
      " 'Data Engineer לחברת סטארטפ משגשגת !'\n",
      " 'ארגון פיננסי נחשב מגייס Data Engineer מנוסה'\n",
      " 'Data Engineer לחברה מדהימה'\n",
      " 'מגייסים Data Engineer לחברת סטארט-אפ בצמיחה'\n",
      " 'Data Engineer לחברה פיננסית מובילה ומוכרת'\n",
      " 'Software Engineer - Data Science Team'\n",
      " 'לחברת ביטוח יציבה ומבוססת דרוש/ה Data Engineer' 'Data QA Engineer'\n",
      " 'Data Platforms Engineer' 'Big Data Engineer לחברת גיימינג עם גב כלכלי'\n",
      " 'Data Engineering Tech Lead'\n",
      " 'לחברה מובילה בצפון דרוש/ה Research Data Engineer'\n",
      " 'לחברה גלובלית הממוקמת בתל אביב דרוש/ה Data Support Engineer'\n",
      " 'דרוש/ה Data Engineer לחברת גיימינג ותיקה ומבוססת הממוקמת במרכז הארץ'\n",
      " 'לארגון רפואי מוביל דרוש/ה Data Engineer'\n",
      " 'לארגון פיננסי מהמובילים בארץ דרוש/ה Data Engineer'\n",
      " 'Data Engineer בחברת סטארטאפ מרשימה!' 'Data Engineer Tech Lead'\n",
      " 'Support Engineer - Tier 1'\n",
      " 'לחברת ביטוח מובילה במרכז דרוש/ה Data Engineer'\n",
      " 'לחברת ONLINE מובילה דרוש/ה Machine Learning Engineer'\n",
      " 'לחברת Gaming בינלאומית דרוש/ה Big Data QA Engineer'\n",
      " 'לחברת סטארטאפ מעולה דרוש/ה DATA Engineer'\n",
      " 'דרוש/ה Data Engineer לחברת סטארט אפ מבוססת'\n",
      " 'לחברת סטארט אפ ממומנת היטב דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה ישראלית גלובלית טכנולוגית שמוערכת במלעלה ממיליארד דולר!'\n",
      " 'לחברת גימיינג מוכרת ומצליחה באיזור השרון מאוד דרוש/ה Data Infrastructure Engineer'\n",
      " 'Software Engineer' 'לחברה בתחום הרובוטיקה בצפון דרוש\\\\ה Data Engineer'\n",
      " 'לחברה מצליחה במרכז דרוש/ה Data Engineer'\n",
      " 'לארגון פיננסי מוביל במרכז דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה גלובלית מדהימה'\n",
      " 'Data Engineer לחברת הייטק גלובלית בתחום הE-Commerce'\n",
      " 'עולם ההייטק ממשיך לגייס! דרוש/ה תוכניתנ/ית BI או DATA Engineer'\n",
      " 'חברה מדהימה מגייסת Data Engineer' 'דרוש/ה Data Enginner מנוסה'\n",
      " 'Software Engineer for Data Intelligence Group'\n",
      " 'דרוש/ה Big Data Software Engineer' 'Finout-Data Engineer'\n",
      " 'לחברה מובילה ומוכרת בעולם דרוש/ה Big Data DevOps Engineer'\n",
      " 'לסטארטאפ מצליח בתל אביב דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Big Data Infrastructure Engineer לחברה מובילה'\n",
      " 'לחברה פיננסית במרכז הארץ דרוש/ה Data Scientist'\n",
      " 'לארגון רפואי גדול בתל אביב דרוש/ה Data Engineer'\n",
      " 'לחברת הייטק מובילה דרוש/ה BI Team Lead'\n",
      " 'דרוש/ה Data Engineer לחברה מצליחה באזור המרכז'\n",
      " 'לחברה Start Up העוסקת בפיתוח סנסורים לנשקים דרוש/ה Data / Machine Learning Engineer'\n",
      " 'לחברה ששמה הולך לפניה בתעשייה המפתחת פלטפורמת Machine Learning ו-Deep Learning דרוש/ה Algorithms Engineer/ Data Scientist'\n",
      " 'Data Engineer לחברה מצליחה ברעננה המפתחת פלטפורמת עסקים'\n",
      " 'Networking Software Engineer לחברת מובילה בשפלה'\n",
      " 'QA Engineer לחברה גלובלית'\n",
      " 'חברת Gaming גדולה ומצליחה מגייסת Data Engineer'\n",
      " 'Software Engineer לחברה ביטחונית מובילה'\n",
      " 'Java Big Data Infrastructure Engineer לחברה גלובלית מובילה בתחום הStorage'\n",
      " 'לחברת סייבר מובילה דרוש/ה Design Verification Engineer'\n",
      " 'Senior Backend/ Data Engineer'\n",
      " 'Senior Software Engineer - Data Flows Team'\n",
      " 'לחברה טכנולוגית מובילה דרוש/ה BI Developer'\n",
      " 'Computer Vision Engineer לחברת סטארטאפ טכנולוגית ממומנת העוסקת בתחום הימאות'\n",
      " 'ראש/ת צוות Big Data לחברת אשראי מובילה ומבוססת'\n",
      " 'Data Engineer לארגון פיננסי מוביל בת\"א!'\n",
      " 'Data Engineer לחברת ביטוח לסיכוני סייבר אשר ממומנת ב50 מליון דולר'\n",
      " 'Software Engineer Intern - Caesarea, Israel' 'C# Software Engineer'\n",
      " 'לארגון בריאות מוביל דרוש/ה Big Data Engineer'\n",
      " 'לארגון פיננסי מהגדולים בארץ דרוש/ה Data Engineer'\n",
      " 'Data Engineer Team Leader לחברת הייטק גלובלית'\n",
      " 'Data Engineer לחברת סטארט אפ ישראלית' 'Junior Software Engineer'\n",
      " 'Software Engineer Data Pipelines And Infrastructure - Tel Aviv'\n",
      " 'Data Engineer לחברת סייבר מצליחה בתל אביב'\n",
      " 'דרוש/ה Python DEV Engineer, Data Analyst' 'Data Integration QA Engineer'\n",
      " 'לחברת הייטק מובילה דרוש/ה Data Architect'\n",
      " 'Data Test Automation Engineer'\n",
      " 'Big Data Software Engineer לחברת סטארט-אפ עם סניף בראשון לציון ובתל אביב - להעדפתך!'\n",
      " 'Junior Full Stack Engineer לחברה בינלאומית מצליחה'\n",
      " 'Data Engineer לסטארטאפ בת\"א' 'דרוש/ה Data Engineer / ETL Developer'\n",
      " 'Big Data Engineer לארגון פיננסי מוביל בת\"א!'\n",
      " 'Python Data Engineer בחברה מעולה בתל אביב'\n",
      " 'דרוש/ה Big Data Principal Engineer לחברה אמריקאית מובילה באזור המרכז'\n",
      " 'Data & MLOps Engineer'\n",
      " 'Data Engineer לחברת סטארטאפ מרשימה העוסקת בתחום ה-Cyber'\n",
      " 'לחברה מרשימה בתחום הE-Commerce בתל אביב דרוש/ה Machine Learning Engineer'\n",
      " 'Data Engineer לחברת סטארטאפ העוסקת בתחום הIOT לRetail'\n",
      " 'Senior SAS Data Engineer' 'Computer Vision Deep Learning Engineer'\n",
      " 'לחברת הייטק בתחום Big Data דרוש/ה System Integration Engineer'\n",
      " 'BI/Data Engineer לחברה חלוצה בתחום הבינה המלאכותית בתחום המשפטי!'\n",
      " 'Data Engineer לחברה המפתחת פלטפורמת SaaS דיגיטאלית וחכמה בתחום ה-AdTech'\n",
      " 'לחברה מצליחה העוסקת בתחום התשלומים מגייסת Data Engineer'\n",
      " 'Data Engineer לחברה בתחום הAutomotive בהרצליה'\n",
      " 'Data Engineer לחברת סטארטאפ בתחום ה-Data Marketing'\n",
      " 'Data Engineer לחברת סטארטאפ יוניקורן העוסקת בתחום הFintech'\n",
      " 'Data Engineer לחברת סטארטאפ ממומנת בתחום הIOT'\n",
      " 'Data Engineer לחברת סטארטאפ מצליחה בתחום הiOT לRetail'\n",
      " 'Senior Data Engineer בחברת סטארט-אפ מרשימה בתחום ה-Cloud'\n",
      " 'סטארטאפ צעיר וממומן היטב העוסק בתחום הראיה הממוחשבת לשוק הבטחוני מגייס Data Engineer'\n",
      " 'Data Engineer לחברת סטארטאפ בתחום ה- insurance-tech'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הסייבר ההגנתי בתל אביב'\n",
      " 'Data Engineer לחברה בתחום ה-Cyber ההגנתי'\n",
      " 'Data Engineer לחברה רווחית ומדהימה עם איזון בית ועבודה'\n",
      " 'Data Engineer לפורטל מידע פיננסי בתל אביב'\n",
      " 'Data Engineer לחברת סטארטאפ בעולם התשלומים המקוונים'\n",
      " 'Data Engineer לחברת סייבר בתחום הביטוח אשר גייסה מעל 50 מליון דולר ממשקיעים מובילים!'\n",
      " 'Big Data Engineer לחברת סטארטאפ בתחום הפיננסי'\n",
      " 'Data Engineer Team Lead לסטארט-אפ בתחום המשחקים הסוציאלים'\n",
      " 'C++ Software Engineer לחברה בתחום ה-Cyber Security ו-Big Data'\n",
      " 'חברת סטארטאפ מצליחה העוסקת בתחום ה iOT לRetail מגייסת Data Engineer'\n",
      " 'QA Web Engineer'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הCloud וInsure-Tech בתל אביב'\n",
      " 'חברה מובילה בתחום ה-Fintech מגייסת Senior Data Engineer'\n",
      " 'לחברת סטארטאפ המפתחת פלטפורמה לניהול תמונות ומדיה בAWS דרוש/ה Data Engineer'\n",
      " 'לחברת סטארטאפ לפני הנפקה Data Engineer'\n",
      " 'Backend Software Engineer חברה יציבה גלובאלית ויציבה בתחום ה-Data'\n",
      " 'Data Engineer לחברה גלובלית מצליחה בתחום הTravel Tech'\n",
      " 'דרוש/ה Backend Engineer לחברת הייטק מובילה'\n",
      " 'Machine Learning/Data Engineer לסטארט-אפ מבטיח מתחת לרדאר בתחום נוירו-טכנולוגיה לאחר גיוס סיד'\n",
      " 'Machine Learning Infra Engineer'\n",
      " 'חברת סטארטאפ מרשימה והמובילה בארץ בתחום הBI וBig Data Analytics מגייסת Fullstack Engineer'\n",
      " 'הזדמנות פורצת דרך להשתלב בחברת E-Commerce ענקית והמצליחות בעולם! מגייסת Machine Intelligence Data science'\n",
      " 'Machine Learning Engineer לחברה המפתחת פלטפורמת Marketing חכמה בתל אביב'\n",
      " 'Machine Learning Engineer'\n",
      " 'Software Data Engineer לחברה גלובלית המפתחת מוצר מבוסס NLP וMachine Learning'\n",
      " 'Data Engineer לעבודה עם Machine Learning בחברת סייבר'\n",
      " 'Deep Learning and Computer Vision Engineer'\n",
      " 'Data Engineer לסטארטאפ בתל אביב' 'Computer Vision Algorithm Engineer'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה- AI'\n",
      " 'Data Engineer לחברה מצליחה בתחום האפליקציות'\n",
      " 'Data Engineer לחברת סטארטאפ לאחר רכישה'\n",
      " 'אחת מהחברות הגדולות בישראל מתחום השיווק הדיגיטלי מגייסת Big Data Engineer'\n",
      " 'משרת פיתוח Machine Learning Engineer לחברת סטארטאפ שגייסה 60 מליון דולר בזמן קצר!'\n",
      " 'חברת סייבר מצליחה הממוקמת במרכז הארץ מגייסת System Validation Engineer'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה-Fast BI וה-AI'\n",
      " 'Data Engineer לחברה המתמחה בתחום ה-Big Data והתשתיות'\n",
      " 'Data Engineer למעבדת innovation בתחום התקשורת'\n",
      " 'Technical Support Engineer (Tier 3)'\n",
      " 'Data Engineer לחברה העוסקת ברכישות מזון מקוונות דרך הMobile בשוק האמריקאי'\n",
      " 'Big Data Engineer לחברה בתחום המובייל!'\n",
      " 'Machine Learning Team Lead בחברת Start Up מרשימה בתחום ה-Automated Predictive Analytics'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה-Medical Device'\n",
      " 'Data Engineer להצטרפות לצוות חדש העוסק בטכנולוגיות Big Data בחברה ממומנת היטב בתחום הMedical'\n",
      " 'חברת סטארט-אפ מרשימה בתחום הBI והBig Data Analytics מגייסת Backend Engineer'\n",
      " 'Algorithms Engineer לחברה מצליחה בתל אביב'\n",
      " 'Backend/Machine Learning Software Engineer בחברת סטארט-אפ'\n",
      " 'סטארט אפ מבטיח בדרום מגייס QA Engineer'\n",
      " 'Machine Learning and Data Engineer'\n",
      " 'Senior Software Engineer - Data Path' 'Software Infrastructure Engineer'\n",
      " 'BI Developer' 'Site Reliability Engineer - Hybrid'\n",
      " '\\uf4ccData Engineer Lead\\uf4cc(Hybrid'\n",
      " 'Senior Data Engineer לחברה מעולה באזור חיפה!'\n",
      " 'Senior Data Engineer לחברת סייבר בשרון'\n",
      " 'Senior Data Engineer לסטארטאפ צעיר ומוביל'\n",
      " 'Software Engineer- Image Processing'\n",
      " 'חברת Gaming מצליחה ברמת החייל (תל אביב) מגייסת Data Engineer'\n",
      " 'Senior BI Engineer' 'Image Processing System Engineer'\n",
      " 'חברת Gaming מחפשת Data Engineer' 'Application Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה בינלאומית מוכרת ויציבה'\n",
      " 'דרוש/ה Data Engineer לחברת הייטק גלובאלית באזור השפלה'\n",
      " 'Software Engineer - Production'\n",
      " 'דרוש/ה Senior Software Engineer לחברה גלובלית בתחום הסקיוריטי'\n",
      " 'Senior Data Engineer לבנק מוביל' 'Analytics Engineer (BI Developer)'\n",
      " 'QA Engineer לחברת סטארט-אפ העוסקת בתחום הData'\n",
      " 'Software Engineer לחברת סטארט-אפ בעולם הסקיוריטי בתל אביב'\n",
      " 'Machine Learning Engineer/ Data Scientist בחברת סטארט אפ מובילה'\n",
      " 'חברת סטארט-אפ רווחית בתחום הימאות מגייסת Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ מדהימה ומצליחה בתחום הרכבים האוטונומים'\n",
      " 'C++ Software Engineer בחברת סטארט-אפ מרשימה בתחום ה- Storage!'\n",
      " 'משרת Data Engineer ראשונה בחברת סטארט-אפ מדהימה אשר גייסה 62 מליון דולר - תנאים מצוינים!'\n",
      " 'מגייסים Senior Backend Engineer לחברת סטארט-אפ מרשימה הממוקמת במרכז'\n",
      " 'Fullstack Engineer בחברת סטארט-אפ מדהימה העוסקת בתחום ה- Data בטכנולוגיית AI!'\n",
      " 'Senior Backend Engineer לחברת סטארט-אפ ממומנת - מובילה בתחום הBIG DATA!'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הLegal-Tech'\n",
      " 'Software Developer (Python)'\n",
      " 'Senior Backend Engineer - Payments App Team' 'Ecosystem QA Engineer'\n",
      " 'Hardware System Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ שעוסקת בתחום של AI'\n",
      " 'סטארט-אפ ישראלי מגייס Big Data Engineer'\n",
      " 'Backend\\\\Machine Learning Software Engineer בחברת סטארט-אפ משגשגת!'\n",
      " 'Senior Software Engineer, Control Path' 'Python Software Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ בתחום ה-AI'\n",
      " 'משרת Senior Software Engineer בחברת סטארט-אפ יוניקורן ישראלית העוסקת בפיתוח תשתיתי בעולמות ה-Big Data'\n",
      " 'Program QA Engineer' 'Data Engineer לסטארט-אפ צומח וממומן!'\n",
      " 'FullStack Software Engineer, Archiver' 'Hardware Specialist Engineer'\n",
      " 'Senior Data Infrastructure Engineer (Architecture Group)'\n",
      " 'Big Data Engineer לחברת סטארט-אפ לאחר סבב גיוס שני משמעותי'\n",
      " 'משרת Data Engineer בחברת סטארט- אפ מובילה!'\n",
      " 'Manual QA Engineer בחברת Data מדהימה בתחום ה- Mobile!'\n",
      " 'Tier 1 Support Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ בתחום שוק הביטוח הדיגיטלי'\n",
      " 'Deep Learning Engineer בחברת סטארט-אפ בתחום הנוירו-טכנולוגיה'\n",
      " 'Mechanical Engineer (Engineering)' 'C++ Software Developer- NGAV team'\n",
      " 'Senior Fullstack Software Engineer'\n",
      " 'Senior QA Automation Engineer לחברת סטארט-אפ ישראלית המנתחת Data'\n",
      " 'Senior Deep Learning Algorithm Engineer'\n",
      " 'Senior Data Engineer לסטארטאפ מוביל'\n",
      " 'Senior Data Engineer לארגון גדול ומוביל'\n",
      " 'Computer Vision Engineer בסטארט-אפ מרשים בתחום המכשור הרפואי!'\n",
      " 'Senior Data Engineer לסטארטאפ חדשני'\n",
      " 'Data Engineer בחברת סטארט-אפ חסרת תקדים בתחום ה- AI & Health care!'\n",
      " 'Senior Big Data Engineer לחברת סטארט-אפ שעוסת בפיתוח פלטפורמה שמעבירה תקשורת ואבטחת מידע על הענן (תל אביב)'\n",
      " 'Data Engineer בחברת סטארט-אפ מדהימה בתחום ה- Data!'\n",
      " 'Big Data Engineer בחברת Start Up מצליחה בתחום ה-Cyber'\n",
      " 'AI Engineer - Data Scientist בחברת סטארט-אפ מצליחה!'\n",
      " 'Software Engineer- בחברת סטארט-אפ מדהימה בתחום ה- AI!'\n",
      " 'Software Engineer בחברת סטארט אפ מצליחה בתחום הCyber Storage'\n",
      " 'Data Engineer בחברת סטארט-אפ ותיקה ויציבה בתחום פלטפורמת הסחר החכם והשיווק!'\n",
      " 'ארגון פיננסי מוביל באזור המרכז מגייס מפתח/ת Big data'\n",
      " 'Data Science Engineer בחברת Start Up בתחום ה-AI'\n",
      " 'Data Engineer-BIG DATA לחברה מצליחה בעולמות הפינטק'\n",
      " 'Data Science Engineer בחברת סטארט-אפ מדהימה בתחום ה- Fintech!'\n",
      " 'Machine Learning Engineer בחברת Storage בתחום ה-AI'\n",
      " 'Data Scientist בחברת סטארט-אפ מדהימה בתחום מניעת הונאות!'\n",
      " 'Data Engineer בחברת סטארט-אפ מדהימה!'\n",
      " 'Big Data Engineer קוד בScala בחברת סטארטאפ מצוינת המפתחת מוצר בעולם הלוגים'\n",
      " 'חברת הייטק מצליחה מגייסת Senior Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ מצליחה'\n",
      " 'QA Engineer בחברת סטארט-אפ בתחום השיווק והפרסום הדיגיטלי'\n",
      " 'QA Automation Engineer בחברת סטארט-אפ בתחום ה-AI'\n",
      " 'Senior Software Engineer- Big Data בחברת סטארט אפ מצליחה וצעירה'\n",
      " 'חברת סטראט-אפ המפתחת פלטפורמה לניתוח Big Data מגייסת QA Engineer'\n",
      " 'משרת Data Engineer בחברת יוניקורן משגשגת'\n",
      " 'C++ Software Engineer לסטארטאפ העוסק בפיתוח טכנולוגיה חדשנית בתחום המציאות הרבודה - תל אביב'\n",
      " 'Data Engineer בחברת Online Gaming מצליחה!'\n",
      " 'QA & Automation Engineer בחברה מובילה בתחום ה-Cyber Security וה-Big Data'\n",
      " 'Frontend QA Engineer' 'יוניקורן בעולם הסייבר מגייס Data Engineer'\n",
      " 'Big Data Engineer לחברת סטארטפ-אפ ברמת גן' 'QA Engineer (manual)'\n",
      " 'Data Engineer בחברת סטארטאפ שנרכשה על ידי חברה מולטידיסיפלינארית מצליחה ביותר'\n",
      " 'Data Engineer לחברה בתחום הפינטק - עבודה על מערכות ביג דאטה מתקדמות'\n",
      " 'Big-Data Software Engineer לפתחת פלטפורמה חדשנית המיועדת לקרנות השקעות, קרנות גדור, פנסיה וביטוח'\n",
      " 'QA Engineer בחברת סטארט-אפ מדהימה!'\n",
      " 'Head of Data סטארט-אפ ממומנת ויציבה בתחום ה- AI & Health care'\n",
      " 'Data Engineer בחברת סטארט-אפ'\n",
      " 'Front-End Engineer בחברת סטארט אפ מצליחה בתחום ה Data Integrity'\n",
      " 'Backend Engineer לחברת סטארטאפ בתחום אבטחת מידע בתל אביב'\n",
      " 'חברת ניתוח נתונים Big Data מגייסת QA Engineer'\n",
      " 'חברת הייטק בתחום הימאות מגיסת Big Data Engineer לצוות הCore של הData'\n",
      " 'חברת סטארט-אפ בצמיחה משמעותית מגייסת Research Data Engineer'\n",
      " 'חברת סטארטאפ חדשנית בתחום הData Warehouse מגייסת Data Engineer'\n",
      " 'חברת סטארטאפ המפתחת פלטפורמת אבטחה אוטונומית בתל אביב מגייסת Principal Big Data Engineer'\n",
      " 'QA Automation Engineer'\n",
      " 'חברת סטארטאפ בתחום ה- Data Marketing מגייסת Fullstack Engineer'\n",
      " 'חברת סייבר אשר נמצאת לאחר אקזיט מוצלח מגייסת Big Data Engineer'\n",
      " 'Automation Engineer לחברת סטארט-אפ המפתחת פלטפורמה אוטומטית לAI'\n",
      " 'Senior Machine Learning Engineer בחברה אמריקאית מרשימה המפתחת מוצר Data AI'\n",
      " 'BI Data Engineer בחברת Start Up מבטיחה בעולם הלוגים'\n",
      " 'חברה מצוינת מובילה עולמית בתחום הApplication Delivery Networking מגייסת Software Engineer'\n",
      " 'חברה המפתחת מוצר דיגיטליזציה לעולם ה-Retail מגייסת Data Engineer'\n",
      " 'Data Engineer בסביבת Big Data ב- R&D Center של אחת החברות הרווחיות בעולם'\n",
      " 'NLP and Text Analytics Software Engineer'\n",
      " 'חברה המפתחת מוצר SaaS שפונה לעולם הEcommerce מגייסת Python Data Engineer'\n",
      " 'Full-Stack Engineer לחברה אשר פיתחה מוצר אונליין Job Marketplace מבוסס Machine Learning ו-NLP'\n",
      " 'חברה מדיקל מובילה מגייסת Big Data Software Engineer'\n",
      " 'לסטארט אפ מצליח הממוקם בתל אביב דרוש/ה Python Data Engineer'\n",
      " 'חברת גיימינג מוכרת מגייסת Big Data QA Engineer'\n",
      " 'חברת הייטק גלובאלית בשרון מגייסת Data Engineer מנוסה'\n",
      " 'Data Engineer בחברת סטארט-אפ מרשימה בתחום ה-Cloud']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 :\n",
      "['Data Engineer' '\\uf4ccData Engineer\\uf4cc(Hybrid' 'Junior Data Engineer'\n",
      " 'Cloud Data Engineer' 'Junior Data Engineer לארגון מוביל'\n",
      " 'Data Engineer - Data Assets Group' 'Storage Analytics - Data Engineer'\n",
      " 'Experienced Data Engineer' 'Senior Data Engineer'\n",
      " 'לחברת סטראט-אפ בתחום הסייבר דרוש\\\\ה Data Engineer' 'Big Data Engineer'\n",
      " 'Data Platform Engineer' 'DATA ENGINEER' 'Data Tools Engineer'\n",
      " 'Senior Big Data Engineer' 'Senior/Junior Data Engineer'\n",
      " 'Data Engineer Team Leader' 'Data Engineer לארגון מסווג'\n",
      " 'Automation Engineer - Data Group' 'Data Engineer, Mobile Identity'\n",
      " 'Data Engineer!' 'חברה מצליחה ומובילה מגייסת BI data engineer'\n",
      " 'חברת טלדור מגייסת data Engineer לארגון מסווג במרכז הארץ'\n",
      " 'Data Engineer לחברה פיננסית' 'Data Engineer Lead'\n",
      " 'Data Engineer לחברת סטארטאפ'\n",
      " 'Data Engineer לחברת סטארט -אפ מובילה בכפר סבא'\n",
      " 'חברה טכנולוגית מצליחה מגייסת Data Engineer'\n",
      " 'חברת SU מדהימה מגייסת Data Engineer' 'חברת Gaming מגייסת Data Engineer'\n",
      " 'חברת הייטק גלובלית מחפשת Data Engineer'\n",
      " 'ארגון מוכר ומצליח מחפש Data Engineer'\n",
      " 'DATA ENGINEER לתפקיד מעניין ומאתגר'\n",
      " 'Data Engineer לחברת סטארטאפ מצליחה בדרום'\n",
      " 'Data Engineer לחברה מובילה בשרון'\n",
      " 'חברה מדהימה בדרום מגייסת Data Engineer' 'Data Engineer לסטארט-אפ מצליח'\n",
      " 'בנק מוביל מגייס Data Engineer'\n",
      " 'Data Engineer לסטארט-אפ בתחום המשחקים הסוציאלים'\n",
      " 'Data Engineer לחברת סטארטפ משגשגת !'\n",
      " 'ארגון פיננסי נחשב מגייס Data Engineer מנוסה'\n",
      " 'Data Engineer לחברה מדהימה'\n",
      " 'מגייסים Data Engineer לחברת סטארט-אפ בצמיחה'\n",
      " 'Data Engineer לחברה פיננסית מובילה ומוכרת'\n",
      " 'Software Engineer - Data Science Team'\n",
      " 'לחברת ביטוח יציבה ומבוססת דרוש/ה Data Engineer'\n",
      " 'Data Platforms Engineer' 'Big Data Engineer לחברת גיימינג עם גב כלכלי'\n",
      " 'Data Engineering Tech Lead'\n",
      " 'לחברה מובילה בצפון דרוש/ה Research Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברת גיימינג ותיקה ומבוססת הממוקמת במרכז הארץ'\n",
      " 'לארגון רפואי מוביל דרוש/ה Data Engineer'\n",
      " 'לארגון פיננסי מהמובילים בארץ דרוש/ה Data Engineer'\n",
      " 'Data Engineer בחברת סטארטאפ מרשימה!' 'Data Engineer Tech Lead'\n",
      " 'לחברת ביטוח מובילה במרכז דרוש/ה Data Engineer'\n",
      " 'לחברת סטארטאפ מעולה דרוש/ה DATA Engineer'\n",
      " 'דרוש/ה Data Engineer לחברת סטארט אפ מבוססת'\n",
      " 'לחברת סטארט אפ ממומנת היטב דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה ישראלית גלובלית טכנולוגית שמוערכת במלעלה ממיליארד דולר!'\n",
      " 'לחברת גימיינג מוכרת ומצליחה באיזור השרון מאוד דרוש/ה Data Infrastructure Engineer'\n",
      " 'לחברה בתחום הרובוטיקה בצפון דרוש\\\\ה Data Engineer'\n",
      " 'לחברה מצליחה במרכז דרוש/ה Data Engineer'\n",
      " 'לארגון פיננסי מוביל במרכז דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה גלובלית מדהימה'\n",
      " 'Data Engineer לחברת הייטק גלובלית בתחום הE-Commerce'\n",
      " 'עולם ההייטק ממשיך לגייס! דרוש/ה תוכניתנ/ית BI או DATA Engineer'\n",
      " 'חברה מדהימה מגייסת Data Engineer'\n",
      " 'Software Engineer for Data Intelligence Group'\n",
      " 'דרוש/ה Big Data Software Engineer' 'Finout-Data Engineer'\n",
      " 'לחברה מובילה ומוכרת בעולם דרוש/ה Big Data DevOps Engineer'\n",
      " 'לסטארטאפ מצליח בתל אביב דרוש/ה Data Engineer'\n",
      " 'דרוש/ה Big Data Infrastructure Engineer לחברה מובילה'\n",
      " 'לארגון רפואי גדול בתל אביב דרוש/ה Data Engineer'\n",
      " 'לחברת הייטק מובילה דרוש/ה BI Team Lead'\n",
      " 'דרוש/ה Data Engineer לחברה מצליחה באזור המרכז'\n",
      " 'Data Engineer לחברה מצליחה ברעננה המפתחת פלטפורמת עסקים'\n",
      " 'חברת Gaming גדולה ומצליחה מגייסת Data Engineer'\n",
      " 'Java Big Data Infrastructure Engineer לחברה גלובלית מובילה בתחום הStorage'\n",
      " 'Senior Backend/ Data Engineer'\n",
      " 'Senior Software Engineer - Data Flows Team'\n",
      " 'לחברה טכנולוגית מובילה דרוש/ה BI Developer'\n",
      " 'Data Engineer לארגון פיננסי מוביל בת\"א!'\n",
      " 'Data Engineer לחברת ביטוח לסיכוני סייבר אשר ממומנת ב50 מליון דולר'\n",
      " 'לארגון בריאות מוביל דרוש/ה Big Data Engineer'\n",
      " 'לארגון פיננסי מהגדולים בארץ דרוש/ה Data Engineer'\n",
      " 'Data Engineer Team Leader לחברת הייטק גלובלית'\n",
      " 'Data Engineer לחברת סטארט אפ ישראלית'\n",
      " 'Software Engineer Data Pipelines And Infrastructure - Tel Aviv'\n",
      " 'Data Engineer לחברת סייבר מצליחה בתל אביב'\n",
      " 'דרוש/ה Python DEV Engineer, Data Analyst'\n",
      " 'לחברת הייטק מובילה דרוש/ה Data Architect'\n",
      " 'Data Test Automation Engineer'\n",
      " 'Big Data Software Engineer לחברת סטארט-אפ עם סניף בראשון לציון ובתל אביב - להעדפתך!'\n",
      " 'Data Engineer לסטארטאפ בת\"א' 'דרוש/ה Data Engineer / ETL Developer'\n",
      " 'Big Data Engineer לארגון פיננסי מוביל בת\"א!'\n",
      " 'Python Data Engineer בחברה מעולה בתל אביב'\n",
      " 'דרוש/ה Big Data Principal Engineer לחברה אמריקאית מובילה באזור המרכז'\n",
      " 'Data & MLOps Engineer'\n",
      " 'Data Engineer לחברת סטארטאפ מרשימה העוסקת בתחום ה-Cyber'\n",
      " 'Data Engineer לחברת סטארטאפ העוסקת בתחום הIOT לRetail'\n",
      " 'Senior SAS Data Engineer'\n",
      " 'לחברת הייטק בתחום Big Data דרוש/ה System Integration Engineer'\n",
      " 'BI/Data Engineer לחברה חלוצה בתחום הבינה המלאכותית בתחום המשפטי!'\n",
      " 'Data Engineer לחברה המפתחת פלטפורמת SaaS דיגיטאלית וחכמה בתחום ה-AdTech'\n",
      " 'לחברה מצליחה העוסקת בתחום התשלומים מגייסת Data Engineer'\n",
      " 'Data Engineer לחברה בתחום הAutomotive בהרצליה'\n",
      " 'Data Engineer לחברת סטארטאפ בתחום ה-Data Marketing'\n",
      " 'Data Engineer לחברת סטארטאפ יוניקורן העוסקת בתחום הFintech'\n",
      " 'Data Engineer לחברת סטארטאפ ממומנת בתחום הIOT'\n",
      " 'Data Engineer לחברת סטארטאפ מצליחה בתחום הiOT לRetail'\n",
      " 'Senior Data Engineer בחברת סטארט-אפ מרשימה בתחום ה-Cloud'\n",
      " 'סטארטאפ צעיר וממומן היטב העוסק בתחום הראיה הממוחשבת לשוק הבטחוני מגייס Data Engineer'\n",
      " 'Data Engineer לחברת סטארטאפ בתחום ה- insurance-tech'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הסייבר ההגנתי בתל אביב'\n",
      " 'Data Engineer לחברה בתחום ה-Cyber ההגנתי'\n",
      " 'Data Engineer לחברה רווחית ומדהימה עם איזון בית ועבודה'\n",
      " 'Data Engineer לפורטל מידע פיננסי בתל אביב'\n",
      " 'Data Engineer לחברת סטארטאפ בעולם התשלומים המקוונים'\n",
      " 'Data Engineer לחברת סייבר בתחום הביטוח אשר גייסה מעל 50 מליון דולר ממשקיעים מובילים!'\n",
      " 'Big Data Engineer לחברת סטארטאפ בתחום הפיננסי'\n",
      " 'Data Engineer Team Lead לסטארט-אפ בתחום המשחקים הסוציאלים'\n",
      " 'C++ Software Engineer לחברה בתחום ה-Cyber Security ו-Big Data'\n",
      " 'חברת סטארטאפ מצליחה העוסקת בתחום ה iOT לRetail מגייסת Data Engineer'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הCloud וInsure-Tech בתל אביב'\n",
      " 'חברה מובילה בתחום ה-Fintech מגייסת Senior Data Engineer'\n",
      " 'לחברת סטארטאפ המפתחת פלטפורמה לניהול תמונות ומדיה בAWS דרוש/ה Data Engineer'\n",
      " 'לחברת סטארטאפ לפני הנפקה Data Engineer'\n",
      " 'Backend Software Engineer חברה יציבה גלובאלית ויציבה בתחום ה-Data'\n",
      " 'Data Engineer לחברה גלובלית מצליחה בתחום הTravel Tech'\n",
      " 'Machine Learning/Data Engineer לסטארט-אפ מבטיח מתחת לרדאר בתחום נוירו-טכנולוגיה לאחר גיוס סיד'\n",
      " 'Software Data Engineer לחברה גלובלית המפתחת מוצר מבוסס NLP וMachine Learning'\n",
      " 'Data Engineer לעבודה עם Machine Learning בחברת סייבר'\n",
      " 'Data Engineer לסטארטאפ בתל אביב'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה- AI'\n",
      " 'Data Engineer לחברה מצליחה בתחום האפליקציות'\n",
      " 'Data Engineer לחברת סטארטאפ לאחר רכישה'\n",
      " 'אחת מהחברות הגדולות בישראל מתחום השיווק הדיגיטלי מגייסת Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה-Fast BI וה-AI'\n",
      " 'Data Engineer לחברה המתמחה בתחום ה-Big Data והתשתיות'\n",
      " 'Data Engineer למעבדת innovation בתחום התקשורת'\n",
      " 'Data Engineer לחברה העוסקת ברכישות מזון מקוונות דרך הMobile בשוק האמריקאי'\n",
      " 'Big Data Engineer לחברה בתחום המובייל!'\n",
      " 'Machine Learning Team Lead בחברת Start Up מרשימה בתחום ה-Automated Predictive Analytics'\n",
      " 'Data Engineer בחברת סטארטאפ בתחום ה-Medical Device'\n",
      " 'Data Engineer להצטרפות לצוות חדש העוסק בטכנולוגיות Big Data בחברה ממומנת היטב בתחום הMedical'\n",
      " 'חברת סטארט-אפ מרשימה בתחום הBI והBig Data Analytics מגייסת Backend Engineer'\n",
      " 'Machine Learning and Data Engineer'\n",
      " 'Senior Software Engineer - Data Path' 'BI Developer'\n",
      " '\\uf4ccData Engineer Lead\\uf4cc(Hybrid'\n",
      " 'Senior Data Engineer לחברה מעולה באזור חיפה!'\n",
      " 'Senior Data Engineer לחברת סייבר בשרון'\n",
      " 'Senior Data Engineer לסטארטאפ צעיר ומוביל'\n",
      " 'חברת Gaming מצליחה ברמת החייל (תל אביב) מגייסת Data Engineer'\n",
      " 'Senior BI Engineer' 'חברת Gaming מחפשת Data Engineer'\n",
      " 'דרוש/ה Data Engineer לחברה בינלאומית מוכרת ויציבה'\n",
      " 'דרוש/ה Data Engineer לחברת הייטק גלובאלית באזור השפלה'\n",
      " 'Senior Data Engineer לבנק מוביל' 'Analytics Engineer (BI Developer)'\n",
      " 'חברת סטארט-אפ רווחית בתחום הימאות מגייסת Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ מדהימה ומצליחה בתחום הרכבים האוטונומים'\n",
      " 'משרת Data Engineer ראשונה בחברת סטארט-אפ מדהימה אשר גייסה 62 מליון דולר - תנאים מצוינים!'\n",
      " 'Senior Backend Engineer לחברת סטארט-אפ ממומנת - מובילה בתחום הBIG DATA!'\n",
      " 'Data Engineer לחברת סטארט-אפ בתחום הLegal-Tech'\n",
      " 'Data Engineer בחברת סטארט אפ שעוסקת בתחום של AI'\n",
      " 'סטארט-אפ ישראלי מגייס Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ בתחום ה-AI'\n",
      " 'משרת Senior Software Engineer בחברת סטארט-אפ יוניקורן ישראלית העוסקת בפיתוח תשתיתי בעולמות ה-Big Data'\n",
      " 'Data Engineer לסטארט-אפ צומח וממומן!'\n",
      " 'Senior Data Infrastructure Engineer (Architecture Group)'\n",
      " 'Big Data Engineer לחברת סטארט-אפ לאחר סבב גיוס שני משמעותי'\n",
      " 'משרת Data Engineer בחברת סטארט- אפ מובילה!'\n",
      " 'Data Engineer בחברת סטארט אפ בתחום שוק הביטוח הדיגיטלי'\n",
      " 'Senior QA Automation Engineer לחברת סטארט-אפ ישראלית המנתחת Data'\n",
      " 'Senior Data Engineer לסטארטאפ מוביל'\n",
      " 'Senior Data Engineer לארגון גדול ומוביל'\n",
      " 'Senior Data Engineer לסטארטאפ חדשני'\n",
      " 'Data Engineer בחברת סטארט-אפ חסרת תקדים בתחום ה- AI & Health care!'\n",
      " 'Senior Big Data Engineer לחברת סטארט-אפ שעוסת בפיתוח פלטפורמה שמעבירה תקשורת ואבטחת מידע על הענן (תל אביב)'\n",
      " 'Data Engineer בחברת סטארט-אפ מדהימה בתחום ה- Data!'\n",
      " 'Big Data Engineer בחברת Start Up מצליחה בתחום ה-Cyber'\n",
      " 'Data Engineer בחברת סטארט-אפ ותיקה ויציבה בתחום פלטפורמת הסחר החכם והשיווק!'\n",
      " 'Data Science Engineer בחברת Start Up בתחום ה-AI'\n",
      " 'Data Engineer-BIG DATA לחברה מצליחה בעולמות הפינטק'\n",
      " 'Data Science Engineer בחברת סטארט-אפ מדהימה בתחום ה- Fintech!'\n",
      " 'Data Engineer בחברת סטארט-אפ מדהימה!'\n",
      " 'Big Data Engineer קוד בScala בחברת סטארטאפ מצוינת המפתחת מוצר בעולם הלוגים'\n",
      " 'חברת הייטק מצליחה מגייסת Senior Big Data Engineer'\n",
      " 'Data Engineer בחברת סטארט אפ מצליחה'\n",
      " 'Senior Software Engineer- Big Data בחברת סטארט אפ מצליחה וצעירה'\n",
      " 'משרת Data Engineer בחברת יוניקורן משגשגת'\n",
      " 'Data Engineer בחברת Online Gaming מצליחה!'\n",
      " 'QA & Automation Engineer בחברה מובילה בתחום ה-Cyber Security וה-Big Data'\n",
      " 'יוניקורן בעולם הסייבר מגייס Data Engineer'\n",
      " 'Big Data Engineer לחברת סטארטפ-אפ ברמת גן'\n",
      " 'Data Engineer בחברת סטארטאפ שנרכשה על ידי חברה מולטידיסיפלינארית מצליחה ביותר'\n",
      " 'Data Engineer לחברה בתחום הפינטק - עבודה על מערכות ביג דאטה מתקדמות'\n",
      " 'Big-Data Software Engineer לפתחת פלטפורמה חדשנית המיועדת לקרנות השקעות, קרנות גדור, פנסיה וביטוח'\n",
      " 'Head of Data סטארט-אפ ממומנת ויציבה בתחום ה- AI & Health care'\n",
      " 'Data Engineer בחברת סטארט-אפ'\n",
      " 'חברת הייטק בתחום הימאות מגיסת Big Data Engineer לצוות הCore של הData'\n",
      " 'חברת סטארט-אפ בצמיחה משמעותית מגייסת Research Data Engineer'\n",
      " 'חברת סטארטאפ חדשנית בתחום הData Warehouse מגייסת Data Engineer'\n",
      " 'חברת סטארטאפ המפתחת פלטפורמת אבטחה אוטונומית בתל אביב מגייסת Principal Big Data Engineer'\n",
      " 'חברת סייבר אשר נמצאת לאחר אקזיט מוצלח מגייסת Big Data Engineer'\n",
      " 'BI Data Engineer בחברת Start Up מבטיחה בעולם הלוגים'\n",
      " 'חברה המפתחת מוצר דיגיטליזציה לעולם ה-Retail מגייסת Data Engineer'\n",
      " 'Data Engineer בסביבת Big Data ב- R&D Center של אחת החברות הרווחיות בעולם'\n",
      " 'NLP and Text Analytics Software Engineer'\n",
      " 'חברה המפתחת מוצר SaaS שפונה לעולם הEcommerce מגייסת Python Data Engineer'\n",
      " 'חברה מדיקל מובילה מגייסת Big Data Software Engineer'\n",
      " 'לסטארט אפ מצליח הממוקם בתל אביב דרוש/ה Python Data Engineer'\n",
      " 'חברת הייטק גלובאלית בשרון מגייסת Data Engineer מנוסה'\n",
      " 'Data Engineer בחברת סטארט-אפ מרשימה בתחום ה-Cloud']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.13 Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Italy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 :\n",
      "['IT Data Engineer' 'Digital ASIC Design Engineer'\n",
      " 'Senior Application Engineer M/F' 'Progettista elettrico'\n",
      " 'Network Engineer - Data Center Network Specialist'\n",
      " 'SENIOR ENGINEER - SVILUPPATORE WEB / E-COMMERCE'\n",
      " 'MECHANICAL PROPOSAL ENGINEER'\n",
      " 'Stage progettista e consulente sistemi energetici'\n",
      " 'Cerchiamo Architetti a Lucca' 'Sviluppatore software C++'\n",
      " 'Data Engineer / ETL Developer' 'Impiegato amministrativo contabile'\n",
      " 'Data Engineer - Powercenter' 'Process Control Engineering M/F'\n",
      " 'Mechanical Design Engineer - Progettista Meccanico'\n",
      " 'SiC Process Engineering - Process Integration M/F'\n",
      " 'PMO Junior -Neolaureato' 'Software Developer' 'Project Sourcing'\n",
      " 'Backend Engineer' 'DATA ARCHITECT/FULL STACK DEVELOPER'\n",
      " 'Software Engineer/Developer (C++)' 'Backend Developer | JEE'\n",
      " 'Responsabile Compliance' 'Stagista/Tirocinante' 'Data Analyst'\n",
      " 'Senior System Engineer Evangelist' 'Senior Piping Engineer'\n",
      " 'RISORSA PER AREA TECNICA' 'Software Developer Junior' 'Data Engineer'\n",
      " 'INGEGNERE PROGETTISTA MECCANICO SENIOR' 'Cerchiamo Architetti a Bergamo'\n",
      " 'Ingegnere Civile nel settore delle costruzioni residenziali'\n",
      " 'PROGETTISTA FIRMWARE / ELECTRONIC ENGINEER'\n",
      " 'Direttore Tecnico di Cantiere con esperienza Impiantistica e Edilizia'\n",
      " 'Tecnico Commerciale Senior e/o Junior - Perito meccanico/elettrotecnico'\n",
      " 'Ingegneri e Diplomati Tecnici' 'DataOps Engineer' 'PHP web developer'\n",
      " 'Tecnico Informatico a procedura / rollout'\n",
      " 'Ingegnere/Architetto - energie rinnovabili'\n",
      " 'R&D ELECTRIC ENGINEER / PROGETTISTA ELETTRICO' 'Help Desk Informatico'\n",
      " 'Tecnico commerciale' 'Senior Data Engineer'\n",
      " 'Ing. Elettronico/Elettrotecnico'\n",
      " 'Tecnico Automazione/meccatronico per installazione impianti Automatizzati'\n",
      " 'Responsabile Tecnico Divisione verifica progetto e controllo tecnico'\n",
      " 'Progettista di Nuovi Prodotti/ Engineer of New Products'\n",
      " 'Responsabile Controllo e Project Manager' 'Direttore Tecnico CTO'\n",
      " 'Project Manager' 'Rookie Data Engineer' 'PROJECT MANAGER'\n",
      " 'BNL – DIT – Junior Data Engineer (m/f/x)' 'SAFETY OFFICER'\n",
      " 'ENERGY ENGINEER - Impianti Elettrici/Fotovoltaici' 'Senior Planner'\n",
      " 'INGEGNERE MECCANICO' 'Responsabile di cantiere impianti'\n",
      " 'INGEGNERE PROGETTISTA'\n",
      " 'laureati in scienze naturali, ingegneria meccanica, ingegneria elettronica,'\n",
      " 'Pianificatore della Produzione Junior'\n",
      " 'ARCHITETTO O INGEGNERE PER UFFICIO GARE - FACILITY MANAGEMENT'\n",
      " 'Responsabile Ricerca e Sviluppo' 'Software Engineer'\n",
      " 'QUALITY ENGINEER (ANCHE SENZA ESPERIENZA)'\n",
      " 'PQ: In-Line Technician for Check and test of mechanical components'\n",
      " 'ANALISTA / PROGETTISTA STRUTTURALE'\n",
      " 'PROPOSAL ENGINEER / PROJECT ENGINEER - Remote Working'\n",
      " 'INDUSTRIAL CONSULTANT - DIVISIONE VALUTAZIONI & ADVISORY'\n",
      " 'IT APPLICATION BUSINESS ANALYST' 'Servizio clienti Settore Meccanico'\n",
      " 'JUNIOR QUALITY ENGINEER'\n",
      " 'PROJECT MANAGER / DEPUTY AFTER SALES MANAGER - CHIMICO | OIL & GAS'\n",
      " 'sales account impianti fotovoltaici'\n",
      " 'INGEGNERE AREA CALCOLI / CAE ENGINEER SENZA ESPERIENZA'\n",
      " 'IMPIEGATO UFFICIO TECNICO' 'ESG Team Leader - Senior Financial Analyst'\n",
      " 'INGEGNERE ELETTROMECCANICO' 'IMPIEGATO COMMERCIALE / SALES ENGINEER'\n",
      " 'SAP BUSINESS ANALYST (Finance FI/CO)'\n",
      " 'Ingegnere progettista Impianti energetici rinnovabili'\n",
      " 'INGEGNERE MECCATRONICO'\n",
      " 'TECHNICAL ADVISOR - VALUTAZIONE IMPIANTI E MACCHINARI'\n",
      " 'JUNIOR TEST ENGINEER' 'INGEGNERE ELETTRICO'\n",
      " 'Addetto Junior Facility Management - ingegnere, archietto, geometra'\n",
      " 'PROCESS MAPPING AND IMPROVEMENT SPECIALIST - SPECIALISTA ORGANIZZAZIONE'\n",
      " 'INGEGNERE ELETTROTECNICO - AUOTOMAZIONE INDUSTRIALE'\n",
      " 'HEAD OF PRODUCT MANAGEMENT'\n",
      " 'RESPONSABILE UFFICIO TECNICO - SETTORE IDRAULICO'\n",
      " 'FIELD SERVICE TECHNICIAN' 'TIROCINANTE UFFICIO TECNICO'\n",
      " 'RESPONSABILE DI PRODUZIONE E MAGAZZINO'\n",
      " 'ESPERTI VALUTATORI - PRAXI Loan Valuations'\n",
      " 'REAL ESTATE ANALYST - VALUTAZIONI IMMOBILIARI'\n",
      " 'Intellectual Property Manager' 'ASSOCIATE CONSULTANT – SENIOR ANALYST'\n",
      " 'RESPONSABILE DI MAGAZZINO' 'PROGETTISTA MECCANICO'\n",
      " 'EXPORT AREA MANAGER / AREA MANAGER ESTERO - PAESI FRANCOFONI'\n",
      " 'PROGETTISTA/INGEGNERE MECCANICO' 'DISEGNATORE MECCANICO'\n",
      " 'ADDETTO/A ALLA PIANIFICAZIONE DI PRODUZIONE JUNIOR'\n",
      " 'RESPONSABILE DI PRODUZIONE E LOGISTICA' 'JUNIOR DEVELOPER'\n",
      " 'IMPIEGATO/A UFFICIO TECNICO' 'Disegnatore meccanico'\n",
      " 'ASSISTENTE DI CANTIERE' 'INGEGNERE CIVILE/EDILE - INGLESE FLUENTE'\n",
      " 'TECNICO COMMERCIALE (LINGUA FRANCESE)' 'INGEGNERE ENERGETICO'\n",
      " 'PROPOSAL ENGINEERING – OFFERTE EPC'\n",
      " 'Technical Support Engineer - Pneumatica'\n",
      " 'TECHNICAL SUPPORT ENGINEER FLUIDICA' 'SVILUPPATORE DIAGNOSI JUNIOR'\n",
      " 'MECHANICAL ENGINEER - AUTOMAZIONE INDUSTRIALE' 'INGEGNERE DI PROCESSO'\n",
      " 'CONSTRUCTION MANAGER - IMPIANTI TRATTAMENTO ACQUE' 'IMPIEGATO TECNICO'\n",
      " 'TECNICO DI MANUTENZIONE JUNIOR'\n",
      " '2 PLANT MANAGER COORDINATOR - AREA PLANT MANAGER CENTRO E SUD ITALIA'\n",
      " 'TECNICO SERVICE' 'IMPIEGATO/A COMMERCIALE JUNIOR (SETTORE IT)'\n",
      " 'Software Tester' 'Solution Engineer'\n",
      " 'IMPIEGATO/A TECNICO COMMERCIALE JR'\n",
      " 'QUALITY & MAINTENANCE MANAGER - Centrali Termoelettriche'\n",
      " 'PRODUCTION PLANNER JUNIOR' 'DISEGNATORE TECNICO' 'TEST ENGINEER'\n",
      " 'Progettista Meccanico R&D' 'RESPONSABILE OPERATIONS (settore edile)'\n",
      " 'Material Planner'\n",
      " 'DIRETTORE OPERATIVO - DIRETTORE OPERATIONS - SETTORE BIOMEDICALE - FARMACEUTICO'\n",
      " 'PROGETTISTA MECCANICO JUNIOR'\n",
      " 'PLANNING & PURCHASING MANAGER/SUPPLY PLANNING MANAGER - COSMETICA'\n",
      " 'OPERATIONS MANAGER / PLANT MANAGER' 'Ingegnere Ambientale'\n",
      " 'SERVICE ENGINEER ESTERO' 'RESPONSABILE QUALITA’ - COSMETICA'\n",
      " 'SUPPLIER QUALITY ENGINEER'\n",
      " 'ASSET CONSTRUCTION AND MAINTENANCE DIRECTOR - Alberghiero'\n",
      " 'COMPLIANCE MONITORING OFFICER'\n",
      " 'SITE MANAGER - RESPONSABILE DI MANUTENZIONE'\n",
      " 'KEY ACCOUNT MANAGER - Vernici – Rivestimenti - Edilizia'\n",
      " 'Lead Database Developer'\n",
      " 'RESPONSABILE DELLA PIANIFICAZIONE DELLA PRODUZIONE'\n",
      " 'PROGETTISTA ELETTRO-STRUMENTALE' 'CONTROLLER / BUSINESS ANALYST'\n",
      " 'PREVENTIVISTA - SETTORE EDILE' 'IMPIEGATO/A PIANIFICAZIONE PRODUZIONE'\n",
      " 'Data Engineer (Junior)' 'BACK OFFICE REAL ESTATE'\n",
      " 'Quality Control Manager' 'COST CONTROLLER'\n",
      " 'Progettista e Supporto Tecnico Cantieri'\n",
      " 'JOB COORDINATOR (SETTORE VALVOLE OIL&GAS)'\n",
      " 'IMPIEGATO/A UFFICIO PRODUZIONE' 'FEM Design Engineer'\n",
      " 'PROJECT ENGINEER ELETTRICO (SETTORE HVAC)' 'Tender Specialist'\n",
      " 'Manutentore' 'Sales Engineer' 'PROGRAMMATORE PLC'\n",
      " 'Stageur area Product Management e Marketing'\n",
      " 'PROJECT MANAGER (settore edilizia)' 'ADDETTO ALLA LOGISTICA'\n",
      " 'Delivery Manager' 'Stage-Back Office Commerciale'\n",
      " 'IMPIEGATO ADDETTO ALLA LOGISTICA' 'Jr Controller' 'Junior Data Analyst'\n",
      " 'DATA ENGINEER' 'Animatore centri estivi a tema Lego® e tema S.T.E.M'\n",
      " 'PROPOSAL MANAGER' 'Progettista Meccanico Senior'\n",
      " 'ELECTRICAL SPECIALIST LEAD' 'SOFTWARE DEVELOPER C#'\n",
      " 'SENIOR PE (SETTORE HVAC)' 'PROGETTISTA ELETTRICO'\n",
      " 'FIRMWARE ENGINEER - AUTOMAZIONE INDUSTRIALE'\n",
      " 'Data Engineer & Analyst Junior' 'MANUTENTORE'\n",
      " 'STAGE SETTORE ANALISI PROCESSI DI PRODUZIONE'\n",
      " 'Software Engineer - Big Data' 'Manufacturing Data Engineer'\n",
      " 'CAPOCANTIERE' 'JUNIOR DATA ANALYST' 'Junior Data Scientist'\n",
      " 'PROGETTISTA TERMOTECNICO' 'Sr. Software Engineer, Front End'\n",
      " 'Cryptography Engineer' 'Junior Data Scientist - Automotive'\n",
      " 'Junior Data Engineer - Data Integration' 'Mechatronic & Data Engineer'\n",
      " 'Junior Data Engineer' 'DATA ARCHITECT JUNIOR'\n",
      " 'Junior Data Scientist / Machine Learning Engineer'\n",
      " '1 Specialist Gestione Sistemi'\n",
      " 'Junior Functional Analyst/IT Data Engineer ambito Sistemi di Pagamento'\n",
      " 'Big Data Engineer, Machine Learning Specialist, Big Data Analyst'\n",
      " 'Data Engineer Junior' 'Data Scientist & Machine Learning Engineer'\n",
      " 'ICT Data Engineer'\n",
      " 'Specialist Software Engineer (Data Protection Service)'\n",
      " '1 Specialist Analista di Sistemi' 'Data engineer'\n",
      " 'JR DATA SCIENCE ENGINEER'\n",
      " 'DATA WAREHOUSE ENGINEER | BIG DATA ENGINEER | DATA ENGINEER'\n",
      " 'Microsoft Data Engineer' 'Data Engineer - Data Office'\n",
      " 'Software Engineer – Data Scientist'\n",
      " 'Data Analyst - Collection strategies' 'Data Engineer (Junior/Middle)'\n",
      " 'JUNIOR DATA SCIENTIST - CLAIMS' 'Junior SQL Engineer' 'Process engineer'\n",
      " 'JUNIOR DATA SCIENTIST ARTIFICIAL INTELLIGENCE ENTRY LEVEL'\n",
      " 'Stage Data Engineer'\n",
      " 'ANALISTA SVILUPPATORE SOFTWARE - AZIENDA PROV. MILANO'\n",
      " 'Addetto/a affari regolatori zona Albano Laziale'\n",
      " 'Junior Field Application Engineer – Telecom Power Solutions'\n",
      " 'DATA SCIENTIST'\n",
      " 'Computer Vision and Machine Learning Engineer – Settore Industry'\n",
      " 'Azure Data Engineer' 'Machine Learning / Automation Engineer'\n",
      " 'HSE ENGINEER JUNIOR' 'PROPOSAL ENGINEER' 'STAGE JR DATA ENGINEER'\n",
      " 'Data Intelligence Research Engineer' 'Quality Engineer I'\n",
      " 'Data Engineer - Data Science' 'Data Scientist II (Remote)'\n",
      " 'Account Sales Engineer' 'Data Scientist' 'Machine Learning Engineer'\n",
      " 'INGEGNERE DI MANUTENZIONE JUNIOR' 'Customer Support Specialist'\n",
      " 'Big Data Engineer - Sede di Napoli'\n",
      " 'Big Data & Machine Learning Engineer'\n",
      " 'Junior Propulsion System Engineer (Aerospace)'\n",
      " 'Senior Data Engineer (m/f/x)' 'Aerospace Data Analyst Engineer'\n",
      " 'COMPUTER VISION ENGINEER | AI | DATA SCIENTIST' 'JUNIOR DATA SCIENTIST'\n",
      " 'MACHINE LEARNING ENGINEER'\n",
      " 'ANALISTA PROGRAMMATORE C# TSQL - ZONA ALBA (CN)'\n",
      " 'Senior ML data/ops Engineer' 'BIG DATA ENGINEER(MI017)'\n",
      " 'Data Engineer e Data Analyst' 'STAGE - Data Analyst' 'BI Analyst'\n",
      " 'Big Data Engineer' 'Network Engineer/Data Center - TELECOMMUNICATION'\n",
      " 'LINUX Tecnico Informatico System Specialist Sistemista Server'\n",
      " 'SENIOR DATA NETWORK ENGINEER'\n",
      " 'Data Engineer - Integrazione dati e sviluppo data warehouse'\n",
      " 'System Engineer vehicle' 'Data Engineer Full Remote'\n",
      " 'FEM Analyst Aerospace' 'Data Engineer - 14401' 'Data Warehouse Analyst'\n",
      " 'Physical Appliances Engineer (Onsite)' 'BIG DATA ENGINEER'\n",
      " 'SENIOR DATA ENGINEER' 'Signalling System Engineer Junior'\n",
      " 'Data Analyst_Roma' 'DIGITAL FORENSICS INVESTIGATOR INTERNSHIP'\n",
      " 'MACHINE LEARNING | BIG DATA | DATA SCIENTIST'\n",
      " 'Geospatial Data Scientist con sede a Roma' 'Test Engineer (Maranello)'\n",
      " 'Data Center Network Engineer' 'Junior System Engineer'\n",
      " 'Network & Data Center System Engineer'\n",
      " 'Data Scientist (Divisione Broking)'\n",
      " 'ANALISTA FUNZIONALE ERP_ SETTORE AUTOMOTIVE AFTERMARKET'\n",
      " 'Tirocinio per Ingegnere elettrico impianti'\n",
      " 'Procurement Specialist Rete Tecnici' 'Senior Buyer'\n",
      " 'Ingegnere progettista' 'Back office estero HVAC' 'Tecnico asseveratore'\n",
      " '7948 RESPONSABILE MANUTENZIONE DI STABILIMENTO'\n",
      " 'Test engineer veicolo Macchine Movimento terra'\n",
      " 'Tirocinio formativo società di ingegneria' 'preventivista idraulico']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 :\n",
      "['IT Data Engineer' 'Network Engineer - Data Center Network Specialist'\n",
      " 'Data Engineer / ETL Developer' 'Data Engineer - Powercenter'\n",
      " 'Data Engineer' 'DataOps Engineer' 'Senior Data Engineer'\n",
      " 'Rookie Data Engineer' 'BNL – DIT – Junior Data Engineer (m/f/x)'\n",
      " 'SITE MANAGER - RESPONSABILE DI MANUTENZIONE' 'Lead Database Developer'\n",
      " 'Data Engineer (Junior)' 'DATA ENGINEER' 'Data Engineer & Analyst Junior'\n",
      " 'Software Engineer - Big Data' 'Manufacturing Data Engineer'\n",
      " 'Junior Data Engineer - Data Integration' 'Mechatronic & Data Engineer'\n",
      " 'Junior Data Engineer' 'DATA ARCHITECT JUNIOR'\n",
      " 'Junior Functional Analyst/IT Data Engineer ambito Sistemi di Pagamento'\n",
      " 'Big Data Engineer, Machine Learning Specialist, Big Data Analyst'\n",
      " 'Data Engineer Junior' 'ICT Data Engineer'\n",
      " 'Specialist Software Engineer (Data Protection Service)' 'Data engineer'\n",
      " 'JR DATA SCIENCE ENGINEER'\n",
      " 'DATA WAREHOUSE ENGINEER | BIG DATA ENGINEER | DATA ENGINEER'\n",
      " 'Microsoft Data Engineer' 'Data Engineer - Data Office'\n",
      " 'Data Engineer (Junior/Middle)' 'Stage Data Engineer'\n",
      " 'Azure Data Engineer' 'STAGE JR DATA ENGINEER'\n",
      " 'Data Intelligence Research Engineer' 'Data Engineer - Data Science'\n",
      " 'Big Data Engineer - Sede di Napoli' 'Senior Data Engineer (m/f/x)'\n",
      " 'Aerospace Data Analyst Engineer' 'Senior ML data/ops Engineer'\n",
      " 'BIG DATA ENGINEER(MI017)' 'Data Engineer e Data Analyst'\n",
      " 'Big Data Engineer' 'Network Engineer/Data Center - TELECOMMUNICATION'\n",
      " 'SENIOR DATA NETWORK ENGINEER'\n",
      " 'Data Engineer - Integrazione dati e sviluppo data warehouse'\n",
      " 'Data Engineer Full Remote' 'Data Engineer - 14401' 'BIG DATA ENGINEER'\n",
      " 'SENIOR DATA ENGINEER' 'Data Center Network Engineer'\n",
      " 'Network & Data Center System Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.14 Luxembourg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Luxembourg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 :\n",
      "['Senior DevOps Engineer Elastic Stack' 'Architect Security'\n",
      " 'Full Stack Developer' 'IT Class September 2023' 'Data Scientist'\n",
      " 'Data Engineer' 'Data Engineer (ETL)' 'DATA ENGINEER'\n",
      " 'Data Engineer (H/F)' 'R&D Engineer' 'Data Engineer (m/f/gn)'\n",
      " 'Senior Data Engineer (F/M)' '2023 Data Engineer Internship'\n",
      " 'Senior Data Engineer (f/m/d)' 'Test Engineer'\n",
      " 'Software Engineers Data (with ETL specialization) (M/F)'\n",
      " 'Internship as control systems engineer (F/M)'\n",
      " 'Advanced Analytics & Big data – Senior Data Scientist (M/F)'\n",
      " 'Application Engineer (IT) / Freelance'\n",
      " 'Energy and Decarbonisation engineer – Rolling Mills (H/F)'\n",
      " 'Security Operations Engineer (F/M)'\n",
      " 'Corporate R&D & Industrialization Engineer (m/f)'\n",
      " 'Software Engineer PL/SQL'\n",
      " 'Information Security Data Loss Prevention (DLP) Engineer (m/f)'\n",
      " 'Data Architect/Senior Data Engineer (m/f/n)' 'Engineer Aircraft Systems'\n",
      " 'Senior Software Engineer (Full Stack)' 'Data Development Engineer (M/W)'\n",
      " 'Senior Test Analyst / Test Engineer' 'Developer - C# and Python'\n",
      " 'Java Backend Software Engineer' 'Product Development Engineer'\n",
      " 'M365 Technology Expert'\n",
      " 'Service Manager / Service Delivery Manager (team Service Management) (M/F)'\n",
      " 'Software Engineers Data with ETL specialization – FR/EN (F/M)'\n",
      " 'Financial Information System Manager (M/F)' 'Senior Storage Engineer'\n",
      " 'Soldering Engineer/Expert (f/m/d)' 'System Engineer – EN'\n",
      " 'Problem Management Analyst (team Service Management) (M/F)'\n",
      " 'DevOps Systems Engineer (Medior to Senior)' 'Cloud Engineer / Architect'\n",
      " 'Sr. Technical Program Manager - Cloud' 'Integration Engineer (M/F/D)'\n",
      " 'Technician iPSC NIM (F/M)' 'R&D specialist for MFN group'\n",
      " 'Business Intelligence Engineer M/F' 'Senior Backup Engineer'\n",
      " 'Order Fulfilment Specialist - 2100000T' 'EMC expert (m/f)'\n",
      " 'Unix System Engineer (Solaris/Aix/Linux RedHat) – EN'\n",
      " 'Full Stack Engineer (m/f)' 'Cloud Engineer (m/f)'\n",
      " 'Test Analyst / Test Engineer' 'Software engineer C++ (H/F)'\n",
      " 'System Engineer' 'Senior Backend Engineer (m/w)'\n",
      " 'Software engineer PL/SQL (Fund) M/F' 'Test Engineer (Medior to Senior)'\n",
      " 'Unix/Linux System Engineer (m/f)'\n",
      " 'Internal Services - Test Automation Engineer (M/F) – FR/UK'\n",
      " 'Software Test Automation Engineer Trainee (M/F/D)'\n",
      " 'Research Support Technician in computer vision for space applications (m/f)'\n",
      " 'Application Support Officer (M/F/D)'\n",
      " 'Team leader cloud virtualization engineer'\n",
      " 'R&D Specialist in Embedded Telecommunication Systems and Artificial Intelligence / Machine Learning'\n",
      " 'Software Test Engineer Consultant – EN (m/f)'\n",
      " 'Senior Software-Defined Data Center Engineer'\n",
      " 'Software Project Coordinator (m/f)' 'Senior System Engineer – FR/EN'\n",
      " 'R&D Engineer Hardware & Software Development'\n",
      " '2023 Software Development Engineer Internship'\n",
      " 'Unified Communications Engineer (Microsoft)'\n",
      " 'IM-2313 DATA-DRIVEN OPTIMIZATION OF CHARGED PARTICLE OPTICS INTERNSHIP'\n",
      " 'Software Test Automation Engineer Trainee'\n",
      " 'Software Engineer - .NET Core/Angular'\n",
      " 'Engineer for Homologation/Type Approval of Vehicles'\n",
      " 'Data Centre Engineer' 'SAP PP Functional Support Engineer - EN'\n",
      " 'Software Developer (Python/Java)'\n",
      " 'Business Intelligence (BI) /Data Analyst (DeFi) - Digital Assets'\n",
      " 'C#.NET Developer - LUXEMBOURG M/W'\n",
      " 'Internship - Development Process Engineer (m/f/d)'\n",
      " 'Internship as control systems engineer'\n",
      " 'Customer Solutions Engineer, EMEA' 'Quality Assurance Software Engineer'\n",
      " '(Senior) Windows Engineer' 'IT Network and Security Engineer (m/f/d)'\n",
      " 'IT System Engineer' 'data engineer (m/f)'\n",
      " 'M-2124 PHD STUDENT IN DATA-DRIVEN MODELLING AND DESIGN OF PHOTOELECTROCHEMICAL CELL (PEC)'\n",
      " 'II-2304 WEARABLE IOT TECHNOLOGIES FOR HEALTHCARE INTERNSHIP'\n",
      " 'Software Engineer'\n",
      " 'IM-2312 FDTD SIMULATIONS FOR QUANTUM APPLICATIONS WITH SILICON CARBIDE NANOPHOTONICS INTERNSHIP'\n",
      " '(Junior) IT-Security Administrator/ IT-Security Engineer (m/w/d)'\n",
      " 'System and Network Engineer (M/F)' 'Unified Communications Engineer'\n",
      " '2023 Business Intelligence Engineer Internship'\n",
      " 'Data Engineer Cloud / Lead Dev Senior (H-F)' 'Technical Engineer (M/F)'\n",
      " 'IM-2319 INCORPORATION OF INTUMESCENT FLAME-RETARDANT ADDITIVES INTO POLYAMIDE INTERNSHIP'\n",
      " 'MS System Engineer (Microsoft / Infrastructure)'\n",
      " 'Microsoft System Engineer' 'Analytics Engineer (m/f/d)'\n",
      " 'Electronics Test Design Engineer'\n",
      " 'M-2379 COMPUTATIONAL MECHANICS ENGINEER' 'Security Operations Engineer'\n",
      " 'II-2309 LUXEMVERSE, THE LUXEMBOURGISH METAVERSE INTERNSHIP'\n",
      " 'Senior Frontend Engineer' 'Software Engineer, PHP (MSC team)'\n",
      " 'IT Support Engineer' 'Lead Application Support Engineer'\n",
      " 'IE-2312 BIODISCOVERY OF FUNGAL DERIVED BIOACTIVE COMPOUNDS INTERNSHIP'\n",
      " 'CAD Designer' 'Computer Vision Engineer (Embedded Devices)'\n",
      " 'Quality Engineer' 'Senior Data Engineer (m/f/d)' 'CloudOps Engineer'\n",
      " 'Senior Backend Engineer (m/f)' '(Senior) Engineer, Software'\n",
      " 'M-2366 POST DOC POSITION FOR SURFACE TREATMENT OF BAMBOO FIBERS FOR COMPOSITE APPLICATIONS'\n",
      " 'Senior Backend Developer (PHP) (m/w)' 'SOFTWARE ENGINEER (H/F/X)'\n",
      " 'Windows System Engineer - LUXEMBOURG M/W'\n",
      " 'Research Support Technician in computer vision for space applications'\n",
      " 'Internship - EAGLE-1 Key Management System Software Development'\n",
      " 'Software Engineer (H/F)'\n",
      " 'M-2376 POST-DOC IN NOVEL, NON LINEAR LASER DIAGNOSTIC DEVELOPMENT'\n",
      " 'IM-2320 IMPROVING THE ADHESION BETWEEN NATURAL FIBER AND POLYMERIC MATRIX FOR ADDITIVE MANUFACTURING'\n",
      " 'IM-2325 SYNTHESIS OF VITRIMERS, UPSCALING AND ONLINE MONITORING INTERNSHIP'\n",
      " 'Long term IT contracts at the European Institutions'\n",
      " 'CTS Plates (144237)' 'ETL/Data Engineer (m/w/d)'\n",
      " 'Security Engineer - LUXEMBOURG M/W'\n",
      " 'IM-2322 POROUS MATERIAL EFFECTIVE PROPERTIES THROUGH FINITE ELEMENT SIMULATION INTERNSHIP'\n",
      " 'Network Engineer'\n",
      " 'CIX Management G.I.E. - System and Application Engineer'\n",
      " 'Microsoft Cloud Engineer' 'Senior Software Engineer'\n",
      " 'Senior Cloud Engineer (m/f/x)' 'Network and Security MS Engineer'\n",
      " 'Cloud on Prem Engineer - O'\n",
      " 'I-2327 RESEARCH ENGINEER ON LOW-CODE FOR BOT-BASED SYSTEMS'\n",
      " 'Lead Informatica MDM Engineer'\n",
      " 'System Engineer (m/f/d) Telecommunications - NOC - Network Technology'\n",
      " 'Solution Engineer - Materials & ICME' 'Analytics Engineer'\n",
      " 'Software Architect' 'IT Security Engineer'\n",
      " 'AV Support Engineer, IT Services' 'Senior Informatica MDM Engineer'\n",
      " 'Wireless Software Engineer (L2/L3)' 'Senior DSP Engineer'\n",
      " 'Senior Engineer, Ground Systems' 'Senior Data Privacy Consultant'\n",
      " '(Junior) Specialist, Technical Facility Management (Benelux)'\n",
      " 'SOLUTION ARCHITECT JAVA'\n",
      " 'Digital Electronics Engineer / Embedded Hardware Architect - Space Avionics Units OBC & OBDH (m/f/o) - Luxembourg Space / Satellite Industry'\n",
      " 'I-2328 RESEARCH ENGINEER IN THE FIELD OF DIGITAL TWIN FOR MOBILITY'\n",
      " 'SENIOR WINDOWS SYSTEM ENGINEER'\n",
      " 'Senior Windows Client Systems Engineers'\n",
      " 'M-2318 POST DOC IN MEMS FABRICATION FOR ION OPTICAL DEVICES'\n",
      " 'I-22016 – SYSTEM ADMINISTRATOR FOR IT INFRASTRUCTURE'\n",
      " 'OFFICE 365 / SYSTEM ENGINEER'\n",
      " 'EKXEL - Cloud Engineer Microsoft Azure - EU Public Institution Luxembourg'\n",
      " 'M-2021 – SCIENTIFIC AND TECHNOLOGY EXPERT IN MICRO-ELECTROMECHANICAL SYSTEM'\n",
      " 'I-2215 - POST-DOCTORAL POSITION IN SOFTWARE SECURITY'\n",
      " 'GCP Data Solution Architect' 'PHP Developer'\n",
      " 'Head of Data Engineering / Lead Database Engineer'\n",
      " 'Microsoft Systems Engineer' 'MAINFRAME SYSTEM ENGINEER (F/M)'\n",
      " 'Senior Test Analyst /Test Engineer in Luxembourg'\n",
      " 'I-2314 POSTDOCTORAL RESEARCHER IN MULTILAYER NETWORK VISUALIZATION ON LARGE DISPLAYS'\n",
      " 'Digital Workplace Senior Engineer'\n",
      " 'E-2326 LCA ANALYST AND SOFTWARE DEVELOPER']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 :\n",
      "['Data Engineer' 'Data Engineer (ETL)' 'DATA ENGINEER'\n",
      " 'Data Engineer (H/F)' 'Data Engineer (m/f/gn)'\n",
      " 'Senior Data Engineer (F/M)' '2023 Data Engineer Internship'\n",
      " 'Senior Data Engineer (f/m/d)'\n",
      " 'Software Engineers Data (with ETL specialization) (M/F)'\n",
      " 'Information Security Data Loss Prevention (DLP) Engineer (m/f)'\n",
      " 'Data Architect/Senior Data Engineer (m/f/n)'\n",
      " 'Data Development Engineer (M/W)'\n",
      " 'Software Engineers Data with ETL specialization – FR/EN (F/M)'\n",
      " 'Cloud Engineer / Architect' 'Sr. Technical Program Manager - Cloud'\n",
      " 'Cloud Engineer (m/f)' 'Team leader cloud virtualization engineer'\n",
      " 'Senior Software-Defined Data Center Engineer' 'Data Centre Engineer'\n",
      " 'data engineer (m/f)' 'Data Engineer Cloud / Lead Dev Senior (H-F)'\n",
      " 'Analytics Engineer (m/f/d)' 'Senior Data Engineer (m/f/d)'\n",
      " 'CloudOps Engineer' 'ETL/Data Engineer (m/w/d)'\n",
      " 'Microsoft Cloud Engineer' 'Senior Cloud Engineer (m/f/x)'\n",
      " 'Cloud on Prem Engineer - O' 'Analytics Engineer'\n",
      " 'Senior Data Privacy Consultant'\n",
      " 'EKXEL - Cloud Engineer Microsoft Azure - EU Public Institution Luxembourg'\n",
      " 'GCP Data Solution Architect'\n",
      " 'Head of Data Engineering / Lead Database Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.12 Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Netherlands'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 :\n",
      "['Lead Data Engineer' 'Data Engineer' 'Supply Chain Engineer'\n",
      " 'Services Engineer Networking' 'QHSE Manager' 'DevOps Engineer'\n",
      " 'ILS Engineer' 'Application Engineer - portal beheerder'\n",
      " 'Senior AWS Cloud Engineer' 'Production Engineer'\n",
      " 'R&D Energie in Gebouwen' 'Data Engineer DWH & BI'\n",
      " 'Senior Database Specialist' 'Medior/Senior Embedded Systems Engineer'\n",
      " 'Maintenance Engineer'\n",
      " 'Geo-ICT Traineeship, open voor alle achtergronden' 'Engineer'\n",
      " '(Senior) MES Engineer (Relocation)' '(Junior) Presales Engineer - Breda'\n",
      " 'Cloud Workspace Engineer' 'Data engineer' 'Systeembeheerder'\n",
      " 'Data engineer toezicht, bezwaar en klantinteractie' 'DataHub architect'\n",
      " 'Slimme modelleur van digitale woningen'\n",
      " 'IAM Security Engineer Trust Services' 'Security Engineer'\n",
      " 'Service Engineer Waterstof' 'Thermal Systems Engineer Heat Pump system'\n",
      " 'Proces Engineer Tilburg' 'Lead Software Engineer'\n",
      " 'Helicopter Engineer AW-139' 'Monteur Elektrotechniek'\n",
      " 'Platform Data Engineer' 'service engineers'\n",
      " 'Werkvoorbereider machinebouw' 'Service Engineer'\n",
      " 'Cybersecurity CIAM Engineer' 'Data DevOps Engineer'\n",
      " 'IT - OT System Engineer' 'Python Software Engineer - Metrology'\n",
      " 'Senior Project-/proces Engineer' 'Commissioning Engineer Paneelbouw'\n",
      " 'Engineering Service Technician / Fietsenmaker / Monteur (E-Bike)'\n",
      " 'Medior Java ontwikkelaar (Team Tokamak)' 'DevOps Engineer - Java (NL)'\n",
      " 'Marketing Technologist' 'Supply Chain Planner' 'Sales Engineer'\n",
      " 'System Engineer' 'SW Design Engineer for Metrology Diagnostics'\n",
      " 'Repair Technician' 'Process engineer (Master)' 'Systeem Data Engineer'\n",
      " 'Engineering Manager - Big Data' 'ICT Service Engineer op locatie'\n",
      " 'Practice lead Testing' 'Software Applications Engineer'\n",
      " 'Kwaliteitscoördinator' 'Social innovator'\n",
      " 'Senior Software Design Engineer' 'Java Developer' 'Test Engineer'\n",
      " 'Technician Extreme Ultraviolet'\n",
      " 'Software Engineer Industriële Automatisering' 'Design Engineer'\n",
      " 'Data modelleur' 'Reliability Engineer' 'Business Engineer'\n",
      " 'Execution (Train) Architect' 'Assistent Chef Technisch Bureau'\n",
      " 'Senior Front-end developer' 'Process Engineer' 'Software Engineer PLC'\n",
      " 'Product owner' '.NET Software Engineer' 'Infra & Automation Engineer'\n",
      " 'IT-coördinator' 'Engineering Manager - Developer Productivity Tooling'\n",
      " 'Logistics Engineer bol.com' 'Test automation engineer (team Ammon)'\n",
      " 'Monteur / Engineer' 'Technisch Proces Engineer'\n",
      " 'Senior Data Engineer Klantinteractie'\n",
      " 'Engineering Manager - Site Reliability, FinTech'\n",
      " 'Frontend Senior Software Engineer' 'Monteur machinebouw'\n",
      " 'EUV System Qualification Test engineer'\n",
      " 'Engineering Manager - Core Infrastructure' 'Logistic Engineer'\n",
      " 'Master Data Specialist' 'Engineering Manager - Trips -Insurance'\n",
      " 'Fullstack Software Developer' 'Full Stack Software Developer'\n",
      " 'BI-consultant' 'Technisch Tekenaar' 'BI Developer'\n",
      " 'Business Intelligence Developer – Breda, Consultancy'\n",
      " 'Senior Java Developer'\n",
      " 'Operationeel/Technisch netwerk monitoring specialist'\n",
      " 'Business Informatie Analist'\n",
      " 'Field Application Engineer Lighting Systems' 'Datawarehouse developer'\n",
      " 'Rubber Characterization - Thesis Project'\n",
      " 'Software Engineer C/Python/C++' 'Engineering Manager - Cloud Security'\n",
      " 'Energy Engineer' 'Ingenieur Elektrisch' 'Jr. Instrument Engineer'\n",
      " 'Lead Security infrastructure Engineer'\n",
      " 'Group Leader - Technical Support' 'Software programmeur'\n",
      " 'SecOps Engineer' 'Technical Outsource Manager Data Management'\n",
      " 'Senior Software Engineer (C++/C#)'\n",
      " 'Engineering Manager - Trip Foundations'\n",
      " 'Engineering Manager - Observability'\n",
      " 'Engineering Manager - Identity Platform' 'Embedded Test Engineer'\n",
      " 'Medior Data Engineer' 'Data Scientist' 'Junior Data Engineer'\n",
      " 'Junior Data engineer' 'C++ Software Design Engineer - Machine Control'\n",
      " 'Specialist AVEVA System platform' 'Conversational AI Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 :\n",
      "['Lead Data Engineer' 'Data Engineer' 'Senior AWS Cloud Engineer'\n",
      " 'Data Engineer DWH & BI' 'Senior Database Specialist'\n",
      " 'Cloud Workspace Engineer' 'Data engineer'\n",
      " 'Data engineer toezicht, bezwaar en klantinteractie' 'DataHub architect'\n",
      " 'Platform Data Engineer' 'Data DevOps Engineer'\n",
      " 'Engineering Service Technician / Fietsenmaker / Monteur (E-Bike)'\n",
      " 'Systeem Data Engineer' 'Engineering Manager - Big Data'\n",
      " 'Senior Data Engineer Klantinteractie'\n",
      " 'Engineering Manager - Site Reliability, FinTech'\n",
      " 'Master Data Specialist' 'BI-consultant' 'BI Developer'\n",
      " 'Operationeel/Technisch netwerk monitoring specialist'\n",
      " 'Datawarehouse developer' 'Engineering Manager - Cloud Security'\n",
      " 'Technical Outsource Manager Data Management'\n",
      " 'Engineering Manager - Observability' 'Medior Data Engineer'\n",
      " 'Junior Data Engineer' 'Junior Data engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.13 Norway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Norway'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 :\n",
      "['Cloud Services Engineer (Based in Germany)'\n",
      " 'Contracts Officer (Based in Germany)'\n",
      " 'Data Governance Engineer (m/f/d) 80% Homeoffice' 'Data Engineer'\n",
      " 'Sommerjobb for data scientists / data engineers — Oslo'\n",
      " 'Data Engineer (Java)' 'Data Engineer (m/f/d)'\n",
      " 'Data Engineer – Java Developer'\n",
      " 'Data Engineer | NorgesGruppen | Skøyen, Oslo'\n",
      " 'Software Support Engineer' 'Support Engineer'\n",
      " 'Senior Full Stack Engineer GIS (remote)'\n",
      " 'VIE Program_FLOW INSSURANCE ENGINEER (M/F)_NORWAY'\n",
      " 'Data Center ICT Engineer / ICT Engineer Data Center'\n",
      " 'Customer Support Engineer (All Levels)'\n",
      " 'Operations Engineer (Stavanger)'\n",
      " 'Senior Engineer for our global frontend engineering team at the HISP Centre.'\n",
      " 'ICT Engineer / ICT Technician' 'DATA ENGINEER'\n",
      " 'Machine Learning / Software Engineer'\n",
      " 'We are looking for a Senior Data Engineer to join Cognizant!'\n",
      " 'Senior Hydrodynamic Engineer' 'Petroleum Engineer'\n",
      " 'Data Scientist Biodiversity Informatics'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Løsningsarkitekt Analytics / Data Engineer'\n",
      " 'Lead Partner Technology Strategist - Norway' 'Naval Package engineer'\n",
      " 'DevOps Engineer' 'Cloud Engineer' 'Data Engineer - Ocean'\n",
      " 'Maritime Autonomous Vessel Engineer' 'Chemical Process Engineer'\n",
      " 'Technical Sales Engineer - Summer internship (Norway)'\n",
      " 'EICT package engineer' 'Infrastructure Engineer | SWIFT'\n",
      " 'Field Service Engineer (m/f/d)' 'Lifecycle Engineer'\n",
      " 'Azure Data Engineer with DataBricks - Oslo' 'Cloud engineer'\n",
      " 'MAC Engineer (Freelancer)' 'Data/ML Engineer' 'Data Scientist'\n",
      " 'Technical Support Engineer' 'Lead Software Engineer, Global Bill Pay'\n",
      " 'Cyber Security Control System Engineer' 'Project Engineer'\n",
      " 'System Engineer Network'\n",
      " 'Software Engineer - Ubuntu Build Infrastructure' 'Special engineer'\n",
      " 'Summer Job programme 2023 - Technology' 'Software Engineer - Launchpad'\n",
      " 'Critical Environment Program Manager (CEFSE)'\n",
      " 'Graduate Cloud Native Platform Engineer - Norway'\n",
      " 'DevOps Engineer Network Cloud' 'Inspection Engineer'\n",
      " 'Bruker du data til å forstå verden?' 'Operational Technology Engineer'\n",
      " 'Data Management-konsulent' 'Senior Systems Engineer'\n",
      " 'Senior Completion Engineer' 'Senior Engineer SURF Asset Integrity'\n",
      " 'Senior Process Engineer – Cell Assembly & Formation (Norway)'\n",
      " 'Reservoir Engineer' 'Client Relationship & Technical Lead'\n",
      " 'Senior Process Engineer – Cell Manufacturing (Norway)'\n",
      " 'Network Consulting Engineer' 'Specialist Engineer - Survey'\n",
      " 'Senior Instrument Engineer' 'Drilling Engineer' 'Marine Engineer'\n",
      " 'PLC Software Engineer' 'Welding Engineer' 'Completion Engineer'\n",
      " 'Sr. Completion Engineer' 'Engineer'\n",
      " 'Integrated Planning Engineer - Drilling and Completions'\n",
      " 'Operations Engineer' 'IT Operations Cloud Engineer'\n",
      " 'Lead Electrical Engineer' 'Senior Cloud Engineer' 'Instrument Engineer'\n",
      " 'Principle Engineer Automation'\n",
      " '«Great opportunity in clean energy technology development :'\n",
      " 'Mobile Core Network Engineer'\n",
      " 'NORDIC AVANADE TRAINEE PROGRAM 2023 – Software Engineering – Avanade Norway'\n",
      " 'Application Engineer' 'Applications Engineering Intern'\n",
      " 'Senior DevOps Engineer' 'Engineer - Survey'\n",
      " 'Operations Engineer – Smart hands'\n",
      " 'Integrated Operations Specialist Engineer'\n",
      " 'Machine Learning Ops Engineer'\n",
      " '2nd Engineer| Crewed Robotic Multi Purpose Offshore Vessel / Norway'\n",
      " 'Core Network Engineer' 'Marketing Specialist (fluent in Norwegian)'\n",
      " 'Field Service Engineer - Industrial Control & Safety (SAS) systems'\n",
      " 'Integration Engineer - Cloud Solutions to Ericsson in Fornebu Oslo.'\n",
      " 'Method Engineer' 'Senior Drilling Engineer'\n",
      " 'Dimensional Control Engineer' 'ASIC Engineer, Design' 'Lead DevOps'\n",
      " 'WARRANTY ENGINEER - OSLO TRAM'\n",
      " 'Join the green revolution as a PLC Software Engineer specializing in battery systems!'\n",
      " \"Klar for å løfte BI's skyplattformer til nye høyder i rollen som Cloud Engineer?\"\n",
      " 'We are looking for Power BI experts in Oslo!'\n",
      " 'Database Reliability Engineer - ElasticSearch' 'Field Engineer Norway'\n",
      " 'Cable Installation Engineer (freelancer)' 'Critical Services Shift Lead']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 :\n",
      "['Cloud Services Engineer (Based in Germany)'\n",
      " 'Data Governance Engineer (m/f/d) 80% Homeoffice' 'Data Engineer'\n",
      " 'Data Engineer (Java)' 'Data Engineer (m/f/d)'\n",
      " 'Data Engineer – Java Developer'\n",
      " 'Data Engineer | NorgesGruppen | Skøyen, Oslo'\n",
      " 'Data Center ICT Engineer / ICT Engineer Data Center' 'DATA ENGINEER'\n",
      " 'We are looking for a Senior Data Engineer to join Cognizant!'\n",
      " 'Løsningsarkitekt Analytics / Data Engineer' 'Cloud Engineer'\n",
      " 'Data Engineer - Ocean' 'Azure Data Engineer with DataBricks - Oslo'\n",
      " 'Cloud engineer' 'Data/ML Engineer'\n",
      " 'Lead Software Engineer, Global Bill Pay'\n",
      " 'Graduate Cloud Native Platform Engineer - Norway'\n",
      " 'DevOps Engineer Network Cloud' 'IT Operations Cloud Engineer'\n",
      " 'Senior Cloud Engineer' 'Mobile Core Network Engineer'\n",
      " 'Integration Engineer - Cloud Solutions to Ericsson in Fornebu Oslo.'\n",
      " \"Klar for å løfte BI's skyplattformer til nye høyder i rollen som Cloud Engineer?\"]\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.14 Poland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Poland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 :\n",
      "['Backend Product Software Engineer' 'Front End Product Software Engineer'\n",
      " 'Infrastructure Software Engineer'\n",
      " 'Senior Backend Product Software Engineer'\n",
      " 'Senior Infrastructure Software Engineer'\n",
      " 'Senior Front End Product Software Engineer'\n",
      " 'Cloud Services Engineer (Based in Germany)'\n",
      " 'Electro Mechanical Design Engineer'\n",
      " 'Technical Reporting & Compliance Lead' 'Data Engineer (DWH) (m/f/d)'\n",
      " 'Lead .NET Software Engineer (Remote)'\n",
      " 'Advanced Software Engineer - AI (remote)'\n",
      " 'Senior Software Engineer - AI (remote)'\n",
      " '.NET Senior Software Engineer (Remote or Hybrid)'\n",
      " 'Advanced Software Engineer - AI'\n",
      " 'Advanced Software Engineer (Enrichment)'\n",
      " '.NET Senior Software Engineer (Remote)'\n",
      " '.NET Software Engineer - Search Excellence'\n",
      " 'Senior Software Engineer with Elasticsearch'\n",
      " 'Senior Fullstack Software Engineer (.NET) - Commodity Insights'\n",
      " 'Contracts Officer (Based in Germany)' 'Full Stack Developer'\n",
      " 'Lead Software Engineer with Elasticsearch'\n",
      " 'Senior Fullstack Software Engineer (Java) - Commodity Insigths'\n",
      " 'SPECJALISTA DS. BADAWCZO-ROZWOJOWYCH I OBSŁUGI DOFINANSOWANIA'\n",
      " 'Manual QA Engineer' 'Data Engineer' 'Data Engineer - Summer Internship'\n",
      " 'QA Engineer - Bielsko-Biala HYBRID'\n",
      " 'Sr. Data Engineer (Snowflake, Matillion ETL, Python)'\n",
      " 'Snr QA Engineer (Automation) - Payments' 'Senior QA Automation Engineer'\n",
      " 'Automation Project Engineer'\n",
      " 'Principal / Associate Director Software Engineer'\n",
      " 'Data Engineer for Machine Learning in Computer Vision (Robotics)'\n",
      " 'Backend Engineer - Internship' 'Data Analyst/Engineer Internship'\n",
      " 'Data Scientist (ML/AI) Internship Poland 2023'\n",
      " 'Data Scientist (Trainee)' 'Data Engineer Intern'\n",
      " 'Crypto Data Engineer Intern (Remote)' 'Data Analysis Engineer'\n",
      " 'DATA ENGINEER' 'Junior DWH Data Engineer (get the DWH skill)'\n",
      " 'Junior Data Analyst' 'Data Quality Engineer'\n",
      " 'Process Data Engineer - Subsurface' 'Junior Process Data Engineer'\n",
      " 'Frontend Software Engineer (React)' 'Senior .NET Developer'\n",
      " 'Senior Android Developer' 'Senior Electrical Service Engineer'\n",
      " 'NLP Engineer I' 'Power Platform Engineer' 'Software Engineer Intern'\n",
      " 'Backend Engineer' 'Data Engineer (Hybrid)'\n",
      " 'Data Engineer with Scala - Anti-Money Laundering'\n",
      " 'Software Engineer - App Stores Backend (Remote)' 'Senior Data Engineer'\n",
      " 'Snowflake Data Engineer (zdalnie)' 'Product Analyst'\n",
      " 'Cloud Data Engineer' 'Junior Software Engineer'\n",
      " 'Software Engineer - Launchpad' 'DevOps Engineer' 'Test Engineer'\n",
      " 'C++ Software Developer (Summer Internship)' 'Data Engineer (Microsoft)'\n",
      " 'Data Engineer (Databricks)' 'Software Engineer Nebula AI (Remote)'\n",
      " '.NET Software Engineer' 'Software Engineer, Front End'\n",
      " 'IT Data Lake Engineer' 'Software Engineer Advanced - JAVA'\n",
      " 'Senior Data Engineer - Oracle' 'Big Data Engineer'\n",
      " 'Engineer II, Software Development' 'Lead Data Engineer (remote Europe)'\n",
      " 'Product Data Management Engineer' 'Data & BI Developer' 'Data engineer'\n",
      " 'Data Scientist (Predictive Maintenance)' 'Backend Software Engineer'\n",
      " 'Python Data Engineer (Snowflake)' 'AI Engineer'\n",
      " 'Data Engineer - NeoXam (financial sector)'\n",
      " 'Senior/Staff Software Engineer - Scientific Pipelines'\n",
      " 'Reference Data Engineer 1' 'Senior Data Engineer with Python'\n",
      " '[VMB] Front-end Software Engineer (React)'\n",
      " 'Senior Software Engineer Data Integr.' 'Software Engineer'\n",
      " 'Data Engineer (Allegro Pay)' 'Senior Python/ Data Engineer'\n",
      " 'Data Engineer - Tech Lead' 'Data Analytics Engineer'\n",
      " 'Staff Data Engineer' 'Software Engineer – Data Fabric'\n",
      " 'Junior Cloud Data Platform Engineer' 'Python Data Engineer'\n",
      " 'Senior Data Engineer in Finance Analytics' 'AWS Data Engineer'\n",
      " 'Data Engineer with DataBricks' 'Data Solution Engineer'\n",
      " 'Lead Python Developer - Data Engineering (Possible Remote)'\n",
      " 'Splunk Data Engineer' 'DATA ENGINEER (ETL/GCP)'\n",
      " 'ETL Data Engineer (Databricks)' 'Senior Python Data Engineer'\n",
      " 'Head of Data Science' '[GRI] Data Engineer with Python'\n",
      " 'Data Engineer (REGULAR)' 'Senior Data Engineer (EU Candidates)'\n",
      " 'Senior Data Scientist/ Engineer in Transaction Monitoring Optimisation Analytics'\n",
      " 'Lead Data Engineer (Poland - remote)' 'Big Data Engineer (Retailer)'\n",
      " 'Senior Support Engineer (Data Lake)'\n",
      " 'Senior Data Engineer (Poland - remote)' 'Data Engineer (AI Engineer)'\n",
      " 'Senior Data Engineer (Spark)'\n",
      " 'Graphics Formal Verification Engineer / Tech Lead (Fully Remote / Hybrid @ Katowice/Gdańsk)'\n",
      " 'Data Platform Engineer' 'Data Engineer Oracle'\n",
      " 'Cloud Data Center Escalation Engineer - Tier 2'\n",
      " 'Senior Data Analytics Engineer' 'Big Data Engineer (Scala, Spark)'\n",
      " 'Data Engineer – Compliance' 'Azure Data Engineer'\n",
      " 'Cloud Engineer (Infrastructure/Administration)'\n",
      " 'MLOps Engineer - Data Analytics Platform'\n",
      " 'Inżynier ds. baz danych / Data & Business Intelligence Engineer'\n",
      " 'Industrial Software Engineer'\n",
      " 'Senior Python Data Engineer (Microservices)' 'Python Software Engineer'\n",
      " 'Data Engineer (Python)' 'Software Engineer/Data Engineer (Python)'\n",
      " 'Senior Data Engineer (AWS)' 'Data Platform Engineer - Datalake'\n",
      " 'Tech MES Solution Engineer'\n",
      " 'Data Platform Engineer - Data Analytics Platform'\n",
      " 'Associate Software Engineer (Java or Python) - Carfax'\n",
      " 'SQL Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 :\n",
      "['Cloud Services Engineer (Based in Germany)'\n",
      " 'Data Engineer (DWH) (m/f/d)' 'Data Engineer'\n",
      " 'Data Engineer - Summer Internship'\n",
      " 'Sr. Data Engineer (Snowflake, Matillion ETL, Python)'\n",
      " 'Data Engineer for Machine Learning in Computer Vision (Robotics)'\n",
      " 'Data Analyst/Engineer Internship' 'Data Engineer Intern'\n",
      " 'Crypto Data Engineer Intern (Remote)' 'Data Analysis Engineer'\n",
      " 'DATA ENGINEER' 'Junior DWH Data Engineer (get the DWH skill)'\n",
      " 'Process Data Engineer - Subsurface' 'Junior Process Data Engineer'\n",
      " 'Data Engineer (Hybrid)'\n",
      " 'Data Engineer with Scala - Anti-Money Laundering' 'Senior Data Engineer'\n",
      " 'Snowflake Data Engineer (zdalnie)' 'Cloud Data Engineer'\n",
      " 'Data Engineer (Microsoft)' 'Data Engineer (Databricks)'\n",
      " 'IT Data Lake Engineer' 'Senior Data Engineer - Oracle'\n",
      " 'Big Data Engineer' 'Lead Data Engineer (remote Europe)'\n",
      " 'Product Data Management Engineer' 'Data & BI Developer' 'Data engineer'\n",
      " 'Python Data Engineer (Snowflake)'\n",
      " 'Data Engineer - NeoXam (financial sector)'\n",
      " 'Senior/Staff Software Engineer - Scientific Pipelines'\n",
      " 'Reference Data Engineer 1' 'Senior Data Engineer with Python'\n",
      " 'Senior Software Engineer Data Integr.' 'Data Engineer (Allegro Pay)'\n",
      " 'Senior Python/ Data Engineer' 'Data Engineer - Tech Lead'\n",
      " 'Data Analytics Engineer' 'Staff Data Engineer'\n",
      " 'Software Engineer – Data Fabric' 'Junior Cloud Data Platform Engineer'\n",
      " 'Python Data Engineer' 'Senior Data Engineer in Finance Analytics'\n",
      " 'AWS Data Engineer' 'Data Engineer with DataBricks'\n",
      " 'Data Solution Engineer'\n",
      " 'Lead Python Developer - Data Engineering (Possible Remote)'\n",
      " 'Splunk Data Engineer' 'DATA ENGINEER (ETL/GCP)'\n",
      " 'ETL Data Engineer (Databricks)' 'Senior Python Data Engineer'\n",
      " 'Head of Data Science' '[GRI] Data Engineer with Python'\n",
      " 'Data Engineer (REGULAR)' 'Senior Data Engineer (EU Candidates)'\n",
      " 'Lead Data Engineer (Poland - remote)' 'Big Data Engineer (Retailer)'\n",
      " 'Senior Data Engineer (Poland - remote)' 'Data Engineer (AI Engineer)'\n",
      " 'Senior Data Engineer (Spark)' 'Data Platform Engineer'\n",
      " 'Data Engineer Oracle' 'Cloud Data Center Escalation Engineer - Tier 2'\n",
      " 'Senior Data Analytics Engineer' 'Big Data Engineer (Scala, Spark)'\n",
      " 'Data Engineer – Compliance' 'Azure Data Engineer'\n",
      " 'Cloud Engineer (Infrastructure/Administration)'\n",
      " 'MLOps Engineer - Data Analytics Platform'\n",
      " 'Inżynier ds. baz danych / Data & Business Intelligence Engineer'\n",
      " 'Senior Python Data Engineer (Microservices)' 'Data Engineer (Python)'\n",
      " 'Software Engineer/Data Engineer (Python)' 'Senior Data Engineer (AWS)'\n",
      " 'Data Platform Engineer - Datalake'\n",
      " 'Data Platform Engineer - Data Analytics Platform' 'SQL Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.15 Portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Portugal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525 :\n",
      "['Senior AWS DevOps Engineer' 'Full Stack Developer' 'Data Engineer'\n",
      " 'Software Engineer, Data Architecture'\n",
      " 'Ingestion Senior Data Engineer (f/m/d)' 'SQL Engineer'\n",
      " 'Senior Data Engineer- Analytics' 'Software Integration Engineers'\n",
      " 'Aptiv.io - Data Engineer - Remote' 'Software Engineer (SAP PLM)'\n",
      " 'Software Q&A Engineer/Technician' 'Lead Data Engineer'\n",
      " 'Technical Support Engineer'\n",
      " 'Senior Data Engineer - LatAm, Western or Eastern Europe'\n",
      " 'Senior Backend Software Engineer - AWS / APIs / Databases / Home Office (m/f/d)'\n",
      " 'Data Engineer Trainee (m/f/d)' 'Business Analyst - (Remote)'\n",
      " 'Back-End Engineer (Home-Based Portugal)' 'Data Engineer | AWS'\n",
      " 'Release & Code Analyst' 'Machine Learning Engineer'\n",
      " 'Data Engineer - Azure Datafactory' 'DATA ANALYST ENGINEER (M/F/D)'\n",
      " 'Microsoft Dynamics 365 (CRM) integration Developer'\n",
      " 'Cloud Data Engineer' 'Data Engineer (M/F)'\n",
      " 'Senior Software Engineer (SAP PLM)' 'DATA ENGINEER'\n",
      " 'DATA ENGINEER - POWERBI & AZURE'\n",
      " 'DATA ENGINEER SNOWFLAKE/ AZURE DATABRICKS' 'TECH LEAD DATA ENGINEER'\n",
      " 'SENIOR DATA ENGINEER' 'Data Scientist (Trainee)'\n",
      " 'SQL DATA ENGINEER (F/M/D)' 'TECH LEAD/DATA ENGINEER-TALEND'\n",
      " 'SENIOR INGESTION DATA ENGINEER (F/M/D)' 'Senior Data Engineer'\n",
      " 'DATA ENGINEER (M/F/D)' 'SAP ABAP Software Engineer (m/f/d)'\n",
      " 'Senior Cloud/Support Engineer' 'INGESTION DATA ENGINEER (F/M/D)'\n",
      " 'QUALITY ENGINEER - CARREGADO' 'DevOps Engineer'\n",
      " 'Quality Automation Engineer' 'Data Engineer - (Viator)'\n",
      " 'Full Stack Engineer' 'ETL Data Engineer'\n",
      " 'DevOps Engineer - AWS / Testing / Agile Development / Home Office (m/f/d)'\n",
      " 'PRODUCTION ENGINEER'\n",
      " 'Senior Frontend Software Engineer - JavaScript / HTML / CSS / Home Office (m/f/d)'\n",
      " 'Full-stack Engineer' 'SQL Data Engineer'\n",
      " 'Data Software Engineer - Senior'\n",
      " 'Data Engineer/ Data Scientist Graduate - Lisbon'\n",
      " 'Frontend Software Developer' 'Azure Data Engineer'\n",
      " 'DEVOPS ENGINEER (M/F/D)' 'Data Engineer (Spark/Databricks)'\n",
      " 'SUPPLIER DEVELOPMENT ENGINEER - QUALITY'\n",
      " 'TECH LEAD SNOWFLAKE/AZURE DATABRICKS' 'Software Engineer I'\n",
      " 'Data Engineer: Azure Data Factory'\n",
      " 'Data Engineer: Synapse SQL - Lisboa ou Porto/Híbri'\n",
      " 'Software Engineer - Frontend' 'Data Engineer Dynatrace'\n",
      " 'Junior Software Engineer' 'Software Engineer II'\n",
      " 'Data Engineer (Python)' 'Data Scientist'\n",
      " 'Junior Front-end Web Developer' 'Google Cloud Platform Data Engineer'\n",
      " 'Full Stack (Node.js + React.js) Engineer'\n",
      " 'QA AUTOMATION ENGINEER (M/F/D)' 'FIRMWARE ENGINEER (M/F/D)'\n",
      " 'Senior Backend Engineer' 'SOFTWARE ENGINEER (SAP PLM) (M/F/D)'\n",
      " 'Software Engineer - Backend' 'EMBEDDED SYSTEMS ENGINEER INNOVATIONS'\n",
      " 'Data Analyst (m/f/d)' 'MATERIAL PLANNER'\n",
      " 'SENIOR QA AUTOMATION ENGINEER (M/F/D)' 'Backend Software Developer'\n",
      " 'RELEASE AND CODE ANALYST (F/M/D)' 'Software Engineer - Data Platform'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'TECH LEAD- QLIK/SQL/REDSHIFT'\n",
      " 'SENIOR SOFTWARE ENGINEER (SAP PLM) (M/F/D)'\n",
      " 'Senior Backend Engineer | BASE'\n",
      " 'SOFTWARE ENGINEER FOR SAP COMMERCE CLOUD (M/F/D)'\n",
      " 'Software Engineer Trainee' 'Field Service Engr II (Sines)'\n",
      " 'ESRI Frontend Developer' 'PLANT EHS OFFICER - AUTOMOTIVE INDUSTRY'\n",
      " 'Backend Software Engineer' 'Data Engineer for Business Intelligence'\n",
      " 'Big Data Engineer' 'Software Engineer- Engineering Enablement'\n",
      " 'Staff Data Engineer (m/f/d)' 'Tech Lead Data Engineer (M/F)'\n",
      " 'Senior Frontend Engineer' 'Junior .NET developer (m/f)'\n",
      " 'AWS Data Engineer' 'Software Engineer - Ubuntu Build Infrastructure'\n",
      " 'Java Software Engineer M/F Portugal'\n",
      " 'Python Software Engineer (M/F) Portugal' 'Senior Software Engineer'\n",
      " 'Azure Senior Data Engineer' 'Software Engineer - Launchpad'\n",
      " 'Quant Engineer Trainee' 'Data Scientist - Artificial Intelligence'\n",
      " 'Software Engineer - ThousandEyes' 'Data Engineer / BI Developer'\n",
      " 'Operations System Engineer for LogTech Scale-Up'\n",
      " 'Software Engineer (Graduate) – Supply Chain' 'On-Prem Data Engineer'\n",
      " 'Golang System Software Engineer - Containers / Virtualisation'\n",
      " 'Software Engineer (JavaScript)' 'Junior Full Stack Developer'\n",
      " 'Cloud Engineer' 'Software Engineer (m/f/d)' 'Software Engineer III'\n",
      " 'Frontend Engineer' 'Data Scientist - Search'\n",
      " 'Data Engineer – PowerBI & Azure (M/F)' 'Staff Data Engineer'\n",
      " 'SD-WAN Engineer (W/M)' 'Software Developer / Engineer'\n",
      " 'Frontend Engineer, Analytics and BI interface'\n",
      " 'Data Engineer (Microsoft)' 'Systems Engineer'\n",
      " 'Data Engineer: Synapse SQL' 'Hardware Engineer (f/m/d)'\n",
      " 'Software Engineer, Machine Learning'\n",
      " 'Data Engineer - LatAm, Western or Eastern Europe' 'Data Engineer II'\n",
      " 'Leading R&D Engineer (Sustainable Energy and Software) (m/f/d)'\n",
      " 'Data Engineer Senior (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Software Engineer - Web Developer NMS' 'Frontend Developer'\n",
      " 'IT Operations Professional / DevOps Engineer (m/f/d)'\n",
      " 'Full Stack Software Engineer'\n",
      " 'Senior Software Engineer, Cloud Microservices'\n",
      " 'Senior Software Engineer in Test' 'Core Engineer' 'Software Engineer'\n",
      " 'Data Analyst' 'Data Engineer - Customer Service' 'Cloud DevOps Engineer'\n",
      " 'Data Engineers Splunk' 'UX/UI Designer' 'Tech Support Engineer Tier II'\n",
      " 'Computer Science Trainee' 'Frontend Software Engineer'\n",
      " 'Full-Stack Engineer' 'QA Automation Engineer' 'Backend Engineer'\n",
      " 'QA Engineer' 'Web Backend Engineer' 'Data Developer | GCP'\n",
      " 'Data Engineer - AWS' 'Computer and systems engineer'\n",
      " 'Test Automation Engineer - TPS (m/f/d)'\n",
      " 'Machine Learning Engineer II (NLP)' 'Software Engineer, Porto'\n",
      " 'Data & Systems Integration Engineer' 'Junior/Mid FrontEnd Engineer'\n",
      " 'Backend Developer & Operation Engineer' 'DevOps Engineer (M/F)'\n",
      " 'Software Engineer, Cloud and Enterprise Agents - ThousandEyes'\n",
      " 'Software Development Engineer' 'Intern Systems Engineer (m/f/x)'\n",
      " 'Backend Software Engineer: Security Center Team' 'Data Architect (M/F)'\n",
      " 'Customer Support Agent' 'Senior Developer' 'Software Engineer, WAF'\n",
      " 'Implementation Engineer – Data Center & Multi Cloud'\n",
      " 'Senior Full Stack Engineer' 'Software Engineer Manager'\n",
      " 'Principal Frontend Engineer' 'Backend Developer (M/F)'\n",
      " 'Cable Installation Engineer (freelancer)' 'Senior Integration Engineer'\n",
      " 'Database Engineer'\n",
      " 'Senior Backend Software Engineer | Revenue Assurance'\n",
      " 'Site Reliability Engineer' 'Kotlin/Java Developer'\n",
      " '(Senior) UI/UX Developer (m/f/d) JavaScript / Angular'\n",
      " '(Senior) Full Stack UI/UX Developer - Angular/Rest/Spring'\n",
      " 'Senior Kotlin/Java Developer' 'Web systems engineer'\n",
      " 'Systems Engineer (Full Remote)' 'Network Operations Engineer, Mid'\n",
      " 'Vehicle Integration Engineer (f/m/d) | IEFP Internship'\n",
      " 'Junior Visual Designer'\n",
      " 'Machine Learning Engineer (100% remote working)'\n",
      " 'Junior Project Manager' 'Test Engineer, Senior' 'Analytics Engineer'\n",
      " 'Software Engineer - Finalta' 'React Software Engineer'\n",
      " 'Subsea Engineer (PLSV Engineering)' 'Systems Engineer - Database'\n",
      " 'Computer Vision Engineer' 'Renewable Energy Performance Lead Engineer'\n",
      " 'Platform Engineer' 'Azure Data Engineer (m/f)'\n",
      " 'Installation Engineer - Automation (M/F)'\n",
      " 'ADAS – Radar Perception Engineer' 'DevOps Lead' 'Devops Engineer'\n",
      " 'Middle DevOps Engineer' 'Mid Java Developer'\n",
      " 'Maintenance Engineer - Electronics/Electrotechnical/Mechatronics (m/f)'\n",
      " 'Azure DevOps Engineer' 'Senior Frontend Software Engineer'\n",
      " 'Data Engineering' 'Silicon Alliances Business Development Lead'\n",
      " 'DATA ENGINEER - EMEA/REMOTE (Portugal)'\n",
      " 'Space Surveillance and Tracking engineer' 'Data Engineer (m/f)'\n",
      " 'Power BI Consultant (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Senior Software Engineer-Back End'\n",
      " 'Manufacturing Process Development Engineer – Laser Systems (m/f)'\n",
      " 'Project Engineer' 'Manufacturing Engineer SMT (M/F/D)'\n",
      " 'Junior Manual Test Engineer'\n",
      " 'Computer Graphics and UI Software Engineer'\n",
      " 'DC Engineering & Delivery Unit Engineer'\n",
      " 'CSV & Automation Engineer (M/F)' 'Senior Data Engineer and Analyst'\n",
      " 'Data Engineer/Architect' 'DATA ENGINEER (M/F)'\n",
      " 'Embedded Software Engineer (C, C++)' 'Senior Big Data Engineer'\n",
      " 'Senior Automation Test Engineer' 'Monitoring Engineer - Trainee'\n",
      " 'Development and Test Engineer for Hydraulics in Heat Pump development (M/F)'\n",
      " 'React Developer (M/F) - Porto'\n",
      " 'Technical Consulting Engineer - Server Virtualization'\n",
      " 'Full Stack Engineer - Lisbon' 'Data Engineer - Pyspark'\n",
      " 'Hackers wanted – Senior Software Engineer (m/f/d)'\n",
      " 'Global TAC Expert Network Engineer'\n",
      " 'Junior Project Installation Engineer' 'Data Analyst- Porto or Lisbon'\n",
      " 'Machine Learning Engineer (m/f) - Remote or Hybrid | Porto'\n",
      " 'Process Engineer (F/M/D)' 'Software Architect (M/F)'\n",
      " 'Manufacturing Quality Engineer' 'Cybersecurity Engineer'\n",
      " 'Software Engineer - ClienTech' 'Junior Web Backend Engineer'\n",
      " 'Voleon - Senior DevOps Engineer'\n",
      " 'Lead Back-end & DevOps Engineer (Remote EU - fluent)'\n",
      " 'Embedded Software Engineer' 'C++ Software Engineer'\n",
      " 'Senior Embedded Systems Engineer (f/m/d)'\n",
      " 'Embedded Software Engineer | C++'\n",
      " 'Software Engineer - Integration Specialist'\n",
      " 'Embedded Systems - Software Engineer' 'Senior Hardware Engineer'\n",
      " 'O&M Bid Engineer - Trainee M/F' 'Computer Scientist'\n",
      " 'Senior Full Stack Engineer (TypeScript)' 'Software Developer Trainee'\n",
      " 'Senior Front-end Engineer (React)'\n",
      " 'Data Architect Manager (Azure) - Europe West Lisbon Tech Hub'\n",
      " '.NET Core Software Engineer'\n",
      " 'Data Engineer Qlik - Consulting/Global Projects + Lisbon/Hybrid + Effective Contract with our client (m/d/f)'\n",
      " 'Senior Consultant - Cyber Security Engineer | Lisbon Office'\n",
      " 'Senior Software Engineer - Portugal' 'Mid - Senior Software Engineer'\n",
      " 'C# .Net Backend Developer (Work from Home/Fully Remote)'\n",
      " 'Software Engineer, Decision Intelligence Solutions'\n",
      " 'Data Engineer – Azure – Lisboa (M/F)' 'Cloud Engineer - AWS and .NET'\n",
      " 'Hackers wanted – Software Engineers (m/f/d)'\n",
      " 'Professional Software Engineer (Integration)' 'Cloud Network Engineer'\n",
      " 'Site Reliability Engineer (f/m/d)' 'Front-End Developer'\n",
      " 'Professional Software Engineer' 'Cloud Engineer / DevOps Engineer'\n",
      " 'Automation Industrial Engineer (M/F)'\n",
      " 'Cost Engineer / Design to Cost Engineer' 'Data Engineer (m/f/d)'\n",
      " 'Software Engineering Manager - Container and Virtualisation Infrastructure'\n",
      " 'SQL Data Engineer – Porto (M/F)' 'Data Tech Lead'\n",
      " 'Software Engineer - SAP CX/Hybris (all genders)'\n",
      " 'Embedded Software Test Engineer' 'Senior Data Governance Engineer'\n",
      " 'Software Engineering Squad Leader - Container/Virtualisation - LXD'\n",
      " 'Junior Requirements Engineer' 'Solar Photovoltaic Design Engineer'\n",
      " 'Full-Stack Developer'\n",
      " 'Professional Internship: Heat Pump Simulation Model Development (M/F)'\n",
      " 'Backend Engineer, Video Business Intelligence'\n",
      " '.Net Full Stack Developer' 'Cloud Security Engineer'\n",
      " 'Google Cloud Architect (M/F)' 'Cloud infrastructure senior engineer'\n",
      " 'Miniclip - Cloud Engineer' 'Senior Data Scientist'\n",
      " 'Senior .NET Developer 100% Remote'\n",
      " 'Cloud, Docker and Container Ecosystem Research'\n",
      " 'Business Intelligence Data Engineer (m,f,d)' 'Senior Cloud Engineer'\n",
      " 'Senior Software Engineer (Backend), Cloud and Enterprise Agents'\n",
      " 'Senior Software Engineer - Digital Workplace'\n",
      " 'Senior Cloud Security Engineer'\n",
      " 'DevOps Engineer Automation & Cloud Operations'\n",
      " 'Software Engineer - Social, Healthcare and Public Entities (SHaPE)'\n",
      " 'Performance/Load Test Engineer'\n",
      " 'Senior Cloud Solutions Architect (fully remote)'\n",
      " 'Installation Analysis Engineer' 'Quality Automation Engineer (m/f/d)'\n",
      " 'Lead Software Engineer, Cloud Crypto Microservices'\n",
      " 'Director, Software Engineering' 'SAP BTP Cloud Engineer (M/F/D)'\n",
      " 'Master Data Specialist for Material & Product (M/F)'\n",
      " 'Senior Big Data Engineer – Lisboa'\n",
      " 'Mechanical Development Engineer within Heat Pump development (M/F)'\n",
      " 'Fullstack QA Engineers (Functional/Automation) (M/F)'\n",
      " 'Design Engineer (Automotive)' 'Electronics/Software Engineer'\n",
      " 'Group Infrastructure Engineer M/F' 'Data Engineer - IT Corporate'\n",
      " 'IoT Engineer (M/F)' 'Mid Software Engineer - Back End'\n",
      " 'Senior Software Engineer, Full-stack' 'Operations systems engineer'\n",
      " 'Senior Backend Software Engineer' 'QA Engineer (Backend)'\n",
      " 'IT Network Tools & Automation Engineer'\n",
      " 'Field Services Deskside Engineer' 'PHP Developer' 'C# Engineer'\n",
      " 'Senior DevOps Engineer (m/f/x)'\n",
      " 'Test & Commissioning Engineer - Trainee (Porto) M/F'\n",
      " 'Robotic Process Automation Dev'\n",
      " 'Senior Data Engineer (m/f) - hybrid | Porto'\n",
      " '.NET Full-stack Developer - Junior' 'Senior Frontend Engineer (Porto)'\n",
      " 'DevOps Engineer Kubernetes' 'Software Engineer II - Full Stack'\n",
      " 'Be proactive & Apply! Marine Engineer opportunities in Portugal (local contract)'\n",
      " 'Junior Software Engineer Back End - Farfetch.com - Marketplace Web'\n",
      " 'Be proactive & apply! Naval Architect/Engineer opportunities in Portugal (local contract)'\n",
      " 'Be proactive & apply! OIPOC Engineer opportunities in Portugal (local contract)'\n",
      " 'Customer Success Engineer (M/F)' 'Senior Frontend Developer'\n",
      " 'Tech Lead Software Developer C# (m/f/d)' 'FRONTEND ENGINEER (M/F)'\n",
      " 'Energy Storage Engineer - Trainee - Porto' 'Infrastructure Engineer'\n",
      " 'Be proactive & apply! Rotating Equipment Engineer opportunities in Portugal (local contract)'\n",
      " 'Senior .Net Developer' 'Project Development Associate'\n",
      " 'Principal SRE/DevOps Engineer' 'Procurement Process Engineer'\n",
      " 'Product Owner' 'SQL Data Engineer (f/m/d)' 'QA Engineer (Porto)'\n",
      " 'BASE Software Developer - Porto' 'Senior Software Engineer, Porto'\n",
      " 'Senior JavaScript Engineer - Porto' 'DevOps Engineering'\n",
      " 'Engineering Lead' 'VP, Engineering'\n",
      " 'Engineering Manager, Data processing'\n",
      " 'DevOps Engineer @ Kubernetes Certification'\n",
      " 'Engineering Manager (m/w/d)'\n",
      " 'Application Support Engineer (Porto/Portugal)'\n",
      " 'Senior Security Engineer (Application Security, Containers, CI/CD and Kubernetes)'\n",
      " 'Android Developer (Engineering Team)' 'Recruiter (m/f/d)'\n",
      " 'Senior Application Support Engineer (m/f)'\n",
      " 'Software Engineer - Front End' 'Performance Engineer'\n",
      " 'Senior Backend Engineer (Engineering Team)'\n",
      " 'Project Engineer with Hydrogen Experience' 'Embedded Systems Engineer'\n",
      " 'Sr. Software Engineer (DevOps)' 'Junior QA Specialist'\n",
      " 'QA Automation Tester' 'Junior QA Engineer (Part time possible)'\n",
      " 'SQL Developer' 'Graduate and Academy Program' 'QA Specialist'\n",
      " 'Senior Software Engineer - Data Platform'\n",
      " 'Junior Performance/Load Test Engineer' 'AZURE DATA LAKE EXPERT (M/F)'\n",
      " 'React JS Engineer' 'Data Analyst Intern - Lisbon - French Fluent'\n",
      " 'Cloud Engineer Azure' 'Data Engineer – Azure'\n",
      " 'Senior QA Engineer (all genders) Augmented Reality'\n",
      " 'AWS DevOps Engineer - Remote - $66,000' 'IT Network NCO Manager'\n",
      " 'System Engineer' 'Senior UNIX Systems Admin'\n",
      " 'Assistant Lead Engineer - Process and Tool Development'\n",
      " 'Professional Internship: Demand Planner Spare Parts (M/F)'\n",
      " 'Senior Software Engineer, Mobile Full Stack'\n",
      " 'Technical Project Manager (Web & Cloud) (m/f/d)'\n",
      " 'Senior Java Software Engineer' 'DBA Engineer'\n",
      " 'Senior Database Administrator' 'Service Now Business Analyst'\n",
      " 'UI/UX Designer' 'Java Developer' 'Data Scientist - Search Marketplace'\n",
      " 'Senior Java Software Engineer – Flight Products'\n",
      " 'Device Security Engineer' 'Senior DevOps Engineer (m/f/*)'\n",
      " 'Senior Customer Technical Support Engineer - Tier 4 Optical DWDM Networks'\n",
      " 'Arabic Search Language Specialist' 'Data Analyst Marketing Intelligence'\n",
      " 'Operations System Engineer -Porto-Portugal-Hybrid'\n",
      " 'C++ Embedded Software Engineer for Europe (Full-Time onsite)'\n",
      " 'Software Developer Java (all genders)'\n",
      " 'Senior Embedded Engineer (All Genders)'\n",
      " 'Lead Data Consultant (All Genders)'\n",
      " 'Azure Requirements Engineer (m/f/d)' 'Diller - Senior .NET Developer'\n",
      " 'Technical Architect' 'Site Reliability Engineer (Lisbon)'\n",
      " 'Manual Tester'\n",
      " 'Cybersecurity - Senior IAM Engineer - Europe West Lisbon Tech Hub'\n",
      " 'Linux Engineer' 'Software Engineer (Lisbon)' 'Senior Civil Engineer'\n",
      " 'Senior Software Engineer (Go)' 'Senior Fullstack Developer (m/f/d)'\n",
      " 'Senior Software Engineer - MAAS' 'Senior Front-End Developer'\n",
      " 'Senior Fullstack Engineer' 'Infrastructure Security Engineer'\n",
      " 'Software Engineer - Distributed Key Value Store'\n",
      " 'Quality Assurance Engineer'\n",
      " 'Data Engineer: Azure Datafactory e Azure Databricks (remoto Portugal)'\n",
      " 'Java Software Engineer'\n",
      " 'Senior Software Engineer — Java (EMEA — Remote)' 'Java Expert'\n",
      " 'Java Engineer' 'JAVA TECH LEAD' 'NOX Health - Java Engineer'\n",
      " 'Software Engineer - Java/Backend'\n",
      " 'Senior Software Engineer / Tech Lead (Java, AWS)' 'Blade test engineer'\n",
      " 'Site Reliability Engineer - Mobile'\n",
      " 'Data Engineer - Scala/Python/Java/Hadoop/Spark/Kafka/Azkaban/Luigi for Europe (Full-Time onsite)'\n",
      " 'Backend Developer (Java)'\n",
      " 'Software Engineer Java - Network Management Systems'\n",
      " 'Data Engineer Senior' 'Backend Software Engineer - Node.js'\n",
      " 'Java Spring Developer' 'Content Manager'\n",
      " 'Senior Software Engineer - Portugal remote' 'Quality Engineer'\n",
      " 'Environmental Engineer - LCA Expert (m/f/d)'\n",
      " 'Safety Assurance Engineer (m/f)' 'Senior Data Engineer (mf/d)'\n",
      " 'Senior Product Manager (Payments) - Remote working model'\n",
      " '3D Front End Engineer'\n",
      " 'Machine Learning Architect (100% remote working)'\n",
      " 'Quality Process Engineer'\n",
      " 'Power Platform and SharePoint Online Development Service'\n",
      " 'Power BI Senior (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Senior Software Engineer - Ingestion Team (Remote)'\n",
      " 'Senior Flexible Pipes & Analyses Engineer'\n",
      " 'Site Reliability Engineer - Lisbon' 'Java Software Engineer Senior'\n",
      " 'GNSS Engineer' 'Senior Embedded Systems Engineer'\n",
      " 'Chief Technology Officer (CTO)'\n",
      " 'HVDC Commissioning Engineer (f/m/d) - SCADA'\n",
      " 'Software Engineer II (Performance Marketing) Backend'\n",
      " 'Production Engineer (m/f)' 'Quality Assurance'\n",
      " 'Senior Software Engineer - Durable Objects'\n",
      " 'Staff Engineer Backend (Typescript or Golang)'\n",
      " 'Technical Support Engineer - Zero Trust Focused'\n",
      " 'Senior Backend Node.js Developer' 'Environmental Compliance Engineer'\n",
      " 'Release and Code Analyst (f/m/d)'\n",
      " 'Software Engineer (Node.js, MongoDB, React.js)' 'Data Analyst Intern'\n",
      " 'Team Lead Frontend Engineer' 'Senior Full Stack Software Engineer'\n",
      " 'Analytics Engineer - DBT/Looker' 'iOS Software Engineer'\n",
      " 'Energy Storage Engineer - Trainee - Oliveira de Frades M/F'\n",
      " 'Energy Operational Reporting & Data Quality Engineer (m/f)'\n",
      " 'Value Engineer - Finance Sector' 'Engineer Sustainability'\n",
      " 'Industrial Engineer' 'Senior Backend Software Developer' 'UX Designer'\n",
      " 'DevOps Senior Engineer' 'Snowflake Data Analyst (m/f/d)'\n",
      " 'Senior Data Architect' 'Full-stack Principal Engineer'\n",
      " 'Composite Design and Tooling Engineer'\n",
      " 'Salesforce Developer - Salesforce Platform team'\n",
      " 'Sr/Compliance Manager (QA, Real World Data), Some EMEA locations'\n",
      " 'Software QA Engineer (L4)'\n",
      " 'Senior Software Engineer - Internal Integrations'\n",
      " 'Senior Backend Software Engineer (Python/Django)'\n",
      " 'Senior Software Engineer – Platform'\n",
      " 'Release Manager for E-Commerce - Omnichannel (f/m/x)'\n",
      " 'Backend Developer - Django'\n",
      " 'Lead Software Engineer, Full Stack Observability'\n",
      " 'Consultor de Engenharia de Dados' 'SharePoint Developer'\n",
      " 'GIS/Graphics Software Engineer' 'Software Architect'\n",
      " 'Senior Software Engineer (PHP) - Connectors Engineering'\n",
      " 'Senior Web Developer - Workplace Engineering'\n",
      " 'Sr. Software Engineer - SIEM Platform' 'Electronics Engineer'\n",
      " 'Senior DSP/Image Processing Designer'\n",
      " 'Team Lead Analytics Marketing Intelligence']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 :\n",
      "['Data Engineer' 'Software Engineer, Data Architecture'\n",
      " 'Ingestion Senior Data Engineer (f/m/d)'\n",
      " 'Senior Data Engineer- Analytics' 'Aptiv.io - Data Engineer - Remote'\n",
      " 'Lead Data Engineer'\n",
      " 'Senior Data Engineer - LatAm, Western or Eastern Europe'\n",
      " 'Senior Backend Software Engineer - AWS / APIs / Databases / Home Office (m/f/d)'\n",
      " 'Data Engineer Trainee (m/f/d)' 'Data Engineer | AWS'\n",
      " 'Data Engineer - Azure Datafactory' 'DATA ANALYST ENGINEER (M/F/D)'\n",
      " 'Cloud Data Engineer' 'Data Engineer (M/F)' 'DATA ENGINEER'\n",
      " 'DATA ENGINEER - POWERBI & AZURE'\n",
      " 'DATA ENGINEER SNOWFLAKE/ AZURE DATABRICKS' 'TECH LEAD DATA ENGINEER'\n",
      " 'SENIOR DATA ENGINEER' 'SQL DATA ENGINEER (F/M/D)'\n",
      " 'TECH LEAD/DATA ENGINEER-TALEND' 'SENIOR INGESTION DATA ENGINEER (F/M/D)'\n",
      " 'Senior Data Engineer' 'DATA ENGINEER (M/F/D)'\n",
      " 'INGESTION DATA ENGINEER (F/M/D)' 'Data Engineer - (Viator)'\n",
      " 'ETL Data Engineer' 'SQL Data Engineer' 'Data Software Engineer - Senior'\n",
      " 'Azure Data Engineer' 'Data Engineer (Spark/Databricks)'\n",
      " 'TECH LEAD SNOWFLAKE/AZURE DATABRICKS'\n",
      " 'Data Engineer: Azure Data Factory'\n",
      " 'Data Engineer: Synapse SQL - Lisboa ou Porto/Híbri'\n",
      " 'Data Engineer Dynatrace' 'Data Engineer (Python)'\n",
      " 'Google Cloud Platform Data Engineer' 'Software Engineer - Data Platform'\n",
      " 'SOFTWARE ENGINEER FOR SAP COMMERCE CLOUD (M/F/D)'\n",
      " 'Data Engineer for Business Intelligence' 'Big Data Engineer'\n",
      " 'Staff Data Engineer (m/f/d)' 'Tech Lead Data Engineer (M/F)'\n",
      " 'AWS Data Engineer' 'Azure Senior Data Engineer'\n",
      " 'Data Engineer / BI Developer' 'On-Prem Data Engineer' 'Cloud Engineer'\n",
      " 'Data Engineer – PowerBI & Azure (M/F)' 'Staff Data Engineer'\n",
      " 'Data Engineer (Microsoft)' 'Data Engineer: Synapse SQL'\n",
      " 'Data Engineer - LatAm, Western or Eastern Europe' 'Data Engineer II'\n",
      " 'Data Engineer Senior (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Senior Software Engineer, Cloud Microservices'\n",
      " 'Data Engineer - Customer Service' 'Cloud DevOps Engineer'\n",
      " 'Data Engineers Splunk' 'Data Developer | GCP' 'Data Engineer - AWS'\n",
      " 'Data & Systems Integration Engineer'\n",
      " 'Software Engineer, Cloud and Enterprise Agents - ThousandEyes'\n",
      " 'Data Architect (M/F)'\n",
      " 'Implementation Engineer – Data Center & Multi Cloud' 'Database Engineer'\n",
      " 'Analytics Engineer' 'Systems Engineer - Database'\n",
      " 'Azure Data Engineer (m/f)' 'Data Engineering'\n",
      " 'DATA ENGINEER - EMEA/REMOTE (Portugal)' 'Data Engineer (m/f)'\n",
      " 'Power BI Consultant (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Senior Data Engineer and Analyst' 'Data Engineer/Architect'\n",
      " 'DATA ENGINEER (M/F)' 'Senior Big Data Engineer'\n",
      " 'Monitoring Engineer - Trainee' 'Data Engineer - Pyspark'\n",
      " 'O&M Bid Engineer - Trainee M/F'\n",
      " 'Data Architect Manager (Azure) - Europe West Lisbon Tech Hub'\n",
      " 'Data Engineer Qlik - Consulting/Global Projects + Lisbon/Hybrid + Effective Contract with our client (m/d/f)'\n",
      " 'Data Engineer – Azure – Lisboa (M/F)' 'Cloud Engineer - AWS and .NET'\n",
      " 'Cloud Network Engineer' 'Cloud Engineer / DevOps Engineer'\n",
      " 'Data Engineer (m/f/d)' 'SQL Data Engineer – Porto (M/F)'\n",
      " 'Data Tech Lead' 'Senior Data Governance Engineer'\n",
      " 'Cloud Security Engineer' 'Google Cloud Architect (M/F)'\n",
      " 'Cloud infrastructure senior engineer' 'Miniclip - Cloud Engineer'\n",
      " 'Business Intelligence Data Engineer (m,f,d)' 'Senior Cloud Engineer'\n",
      " 'Senior Software Engineer (Backend), Cloud and Enterprise Agents'\n",
      " 'Senior Cloud Security Engineer'\n",
      " 'DevOps Engineer Automation & Cloud Operations'\n",
      " 'Senior Cloud Solutions Architect (fully remote)'\n",
      " 'Lead Software Engineer, Cloud Crypto Microservices'\n",
      " 'SAP BTP Cloud Engineer (M/F/D)'\n",
      " 'Master Data Specialist for Material & Product (M/F)'\n",
      " 'Senior Big Data Engineer – Lisboa' 'Data Engineer - IT Corporate'\n",
      " 'Senior Data Engineer (m/f) - hybrid | Porto' 'SQL Data Engineer (f/m/d)'\n",
      " 'Engineering Manager, Data processing'\n",
      " 'Senior Software Engineer - Data Platform' 'Cloud Engineer Azure'\n",
      " 'Data Engineer – Azure' 'Senior Database Administrator'\n",
      " 'Arabic Search Language Specialist' 'Lead Data Consultant (All Genders)'\n",
      " 'Data Engineer: Azure Datafactory e Azure Databricks (remoto Portugal)'\n",
      " 'Data Engineer - Scala/Python/Java/Hadoop/Spark/Kafka/Azkaban/Luigi for Europe (Full-Time onsite)'\n",
      " 'Data Engineer Senior' 'Senior Data Engineer (mf/d)'\n",
      " 'Analytics Engineer - DBT/Looker' 'Engineer Sustainability'\n",
      " 'Senior Data Architect'\n",
      " 'Sr/Compliance Manager (QA, Real World Data), Some EMEA locations'\n",
      " 'Team Lead Analytics Marketing Intelligence']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'], \n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.16 Romania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Romania'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 :\n",
      "['Data Engineer' 'Functional Safety Engineer (m/f/d)'\n",
      " 'Cloud Services Engineer (Based in Germany)'\n",
      " 'Sr. Backend Software Engineer - Python, Go, OpenSearch a.k.a Elasticsearch, MySQL, AWS (Romania)'\n",
      " 'Backend Software Engineer (Go, AWS, Cassandra) - Cloud Security (Remote or Hybrid, Romania)'\n",
      " 'Software Backend Engineer (Discover), Remote or Hybrid'\n",
      " 'Software Development Engineer in Test (Java) - Sandbox | Romania (Remote)'\n",
      " 'Frontend Engineer (Discover), (Remote)'\n",
      " 'Cloud Developer | Discover | Golang, Kafka, Cassandra, AWS - Remote or Hybrid'\n",
      " 'Software Test Engineer' 'OutSystems Software Engineer'\n",
      " 'Back-End Software Engineer' 'Data Engineer - Talend'\n",
      " 'Frontend Engineer - Partner Data' 'Data Lake Engineer'\n",
      " 'Python Data Engineer' 'Big Data Engineer (f/m/d)'\n",
      " 'Fixed Data Configuration 2nd Line Engineer'\n",
      " 'CSA SIEM Engineer (Chronicle)'\n",
      " 'Tableau/Elastic Search/Hadoop ( Data lake) Engineer for...'\n",
      " 'IPB Agile - L2/RM engineer' 'Senior Consultant (f/m) Data Engineer'\n",
      " 'Data Engineer with Python' 'Junior Tester'\n",
      " 'WebCenter Data Integration Tech Support Engineer'\n",
      " 'IBD-SS FPS Field Service Technician/Engineer, Journeyman - Secret Clearance Required'\n",
      " 'IBD-SS FPS Field Service Technician/Engineer, Junior- Secret Clearance Required'\n",
      " 'IBD-SS Senior FPS Field Service Technician/Engineer - Secret Clearance Required'\n",
      " 'Software Test Engineer, FitbitOS Release Testing'\n",
      " 'Data Engineer (C#, Python, Elastic)'\n",
      " 'Signalling Engineer - remote (f/m/d), Bucharest - Romania'\n",
      " 'Data Engineer Manager' 'Quality Assurance Engineer'\n",
      " 'Data Warehouse Engineer' 'Azure Data Engineer'\n",
      " 'ETL Engineer|Unite Data Management @ ING Hubs Romania'\n",
      " 'Engineer (m/f/x) - Digital Affluent Proposition' 'Data Engineer (F/M)'\n",
      " 'Test Environment Management Engineer' 'Manual QA Tester'\n",
      " 'Business Intelligence Engineer' 'Technical Support Engineer'\n",
      " 'Data Engineer (Python)'\n",
      " 'Java Engineer - DBNL Data Management @ING Hubs Romania'\n",
      " 'Software Test Engineer, Fitbit Activity' 'Data Engineer and Analyst'\n",
      " 'Data Solution Engineer' 'ICT DevOps Engineer'\n",
      " 'Global Support Engineer – Mobile Data Network Romania'\n",
      " 'Senior AWS Cloud Infrastructure Engineer'\n",
      " 'Junior Backend Engineer (Scala) IRC178380' 'System Engineer'\n",
      " 'Junior DevOps Engineer' 'Online System Engineer'\n",
      " 'Mid-Senior Data Engineer' 'Data Integration Engineer'\n",
      " 'DevOps Cloud Engineer (f/m/d)' 'Number Portability Engineer'\n",
      " 'PHP Data Engineer' 'Big Data Engineer - Kafka & Java'\n",
      " 'Staff Software Engineer' 'Machine Learning Engineer/Data Scientist'\n",
      " 'Big Data Engineer with Hadoop'\n",
      " 'QA & Testing Engineer (m/f/x) - Digital Affluent Proposition'\n",
      " 'Big Data Engineer M/F' 'Junior Cloud Engineer'\n",
      " 'Junior Software Development Engineer' 'Data Quality Engineer'\n",
      " 'Software Development Engineer for CarDataBox (f/m/d)' 'DevOps Engineer'\n",
      " 'Digitalization Specialist in IT' 'Data Engineer IRC177741'\n",
      " 'Cloud Security Engineer' 'UDM Solution Architect' 'Junior QA Engineers'\n",
      " 'Data DevOps Engineer' 'ServiceDesk Engineer @ING Hubs Romania'\n",
      " 'Data Platform Engineer |Data Analytics Platform @ING Hubs Romania'\n",
      " 'Early Engagement - Survey Management Technical Support Engineer - OFS'\n",
      " 'Backhaul Design Data Production Engineer'\n",
      " 'Production Planning & Materials Management (m/f/d)'\n",
      " 'Graduate IoT Software Support Engineer'\n",
      " 'Software Engineer - Data Integration'\n",
      " 'Data Engineer (Google Cloud Platform)'\n",
      " 'DevOps Engineer - Data Streaming Service @ING Hubs Romania'\n",
      " 'Staff Software Engineer, Fitbit'\n",
      " 'Suppliers Quality Data Analyst & Business Process Engineer (F/M)'\n",
      " 'UDM Integration Engineer' 'IoT IP Network Engineer'\n",
      " 'Junior Penetration Tester' 'Devops Engineer' 'Software Engineer Intern'\n",
      " 'IVVQ Engineer' 'Professional Services Engineer'\n",
      " 'Infrastructure Engineer' 'Senior Data Engineer'\n",
      " 'Lead Data Engineer IRC182509' 'JUNIOR SOFTWARE ENGINEER'\n",
      " 'Digital Verification Engineer (f/m/div)*'\n",
      " 'IPB Agile - Solution Architect - Data' 'Data Architect'\n",
      " 'Staff Engineer- Data Management Architect'\n",
      " 'Cyber Hygiene Solution Architect' 'Jr Data Engineer'\n",
      " 'Front-end Engineer' 'Cloud Engineer' 'ITSM Data Architect'\n",
      " 'Security Engineer with Python' 'UAT Analyst' 'ETL engineer @ING Hubs'\n",
      " 'Data Lake Dev-Ops Engineer @ING Hubs Romania'\n",
      " 'Mobile QA Tester (Remote)' 'Cloud Engineer– Bucharest'\n",
      " 'DevOps Cloud Engineer' 'Big Data Engineer' 'DevOps Engineer with AWS'\n",
      " 'DevOps Engineer |Data Lake - Core team @ING Hubs Romania'\n",
      " 'Cloud DevOps Lead Engineer'\n",
      " 'Site Reliability Engineer - Adobe Experience Platform'\n",
      " 'Junior Software Engineer' 'DevOps ENGINEER' 'Informatica Data Engineer'\n",
      " 'Tech Operations Support Engineer' 'Junior Test Engineer'\n",
      " 'Technical Lead Devops Engineer' 'Cloud Azure Engineer (Remote)'\n",
      " 'Concepteur moyens du montage/ Process Design Engineer (F/M)'\n",
      " 'Data Visualization Engineer (Power BI)'\n",
      " 'Projects - Contractor Installers - Romania'\n",
      " 'Senior Software Engineer, Data Platform'\n",
      " 'Site Reliability Engineer | Data Analytics Platform @ING Hubs Romania'\n",
      " 'Data Security Engineer - fully remote' 'Cyber Security Engineer'\n",
      " 'Software Engineer - Launchpad'\n",
      " 'Senior DevOps Engineer - Autonomous Database'\n",
      " '#9035 Test Automation Engineer- Remote across Romania'\n",
      " 'Early Engagement - Drill Bits Applications Engineer (OFS)'\n",
      " 'Hardware Test Engineer' 'DevOps Engineer / Metro Campaign Management'\n",
      " 'Data Engineer - Customer Lifecycle Recommender'\n",
      " 'Application Security Engineer'\n",
      " 'Hardware Test Development Engineer for Space Projects'\n",
      " 'Service Engineer' 'DevOps Engineer - (Hybrid in Romania only)'\n",
      " 'Technology Summer Internship Programme' 'Automated Test Engineer'\n",
      " 'Junior System Engineer IRC176210' 'Data Analyst - Arabic language'\n",
      " 'Dev/DevOps engineer IRC177134'\n",
      " 'Summer Internship Program - Bucharest Global Technology Center'\n",
      " 'Solar Design Engineer' 'Cloud Security Architect'\n",
      " 'Data Center Engineer (On Demand Contract)'\n",
      " 'Frontend Development Engineer'\n",
      " 'Early Engagement - DCF Application Engineer (OFS)'\n",
      " 'Senior Data & AI DevOps Engineer'\n",
      " 'HRI Solution Engineer - Intern (French)' 'Data Modeler'\n",
      " 'Junior Automation Engineer - Digital Channels CeandEe'\n",
      " 'Infrastructure Engineer @ING Hubs Romania'\n",
      " 'Software Engineer - Ubuntu Build Infrastructure'\n",
      " 'Junior C++ Engineer IRC182157' 'BUSINESS INTELLIGENCE ENGINEER'\n",
      " 'Hardware Power Management Application Engineer I'\n",
      " 'SIEM Security Engineer'\n",
      " 'Site Reliability Engineer - Summer 2023 (Remote, ROU)'\n",
      " 'Senior Network Engineer (m/f/d)' 'Senior Software Engineer'\n",
      " 'Cloud Security Engineer - fully remote'\n",
      " 'Tableau or Elastic Search or Hadoop (Data lake) Engineer for Datawarehouse'\n",
      " 'Senior Android Software Engineer, Fitbit Device Management'\n",
      " 'Development engineer in the field of thermal management (m/f/d)'\n",
      " 'Private Cloud Network Engineer' 'Junior Automotive Software Engineer'\n",
      " 'L2 Cloud Engineer' 'Testing Engineer' 'Junior QNX Developer IRC181694'\n",
      " 'Tools Software Engineer' 'DevOps Engineer IRC178542'\n",
      " 'Data Engineer /ML Engineer' 'Senior DevOps Engineer (AWS)'\n",
      " 'Platform Systems Engineer / SQL DB' 'Senior DevOps Engineer'\n",
      " 'Technology Graduate Programme' 'Talent Acquisition Partner - Technology'\n",
      " 'Application Design Engineer'\n",
      " 'Advanced Threat Researcher Internship - Summer 2023 (Remote, ROU)'\n",
      " 'Application Support Engineer' 'Software Engineer: Marketing Technology'\n",
      " 'Power Management Principal Application Engineer - PMIC'\n",
      " 'Senior Application Support Engineer' 'Software Test Engineer I'\n",
      " 'Director - Technology Risk'\n",
      " 'Paid Internship - Frostbite Software Engineer'\n",
      " 'Embedded Software Engineer for Automotive - Ethernet / Power Management - Junior'\n",
      " 'Paid internship: C++ Software Engineer' 'Solution Architect & RPA'\n",
      " 'Test Automation Engineer'\n",
      " 'ING International Talent Programme– IT Track @ING Hubs Romania'\n",
      " 'Design Engineer - Body in White'\n",
      " 'DevOps Engineer – STW FM Digitalization @ING Hubs Romania' 'Engineer I'\n",
      " 'Facilities Technician' 'In Vehicle Network Engineer'\n",
      " 'Graduate Distribution Technical Engineer'\n",
      " 'Field Engineer - Next Gen Graduate Program'\n",
      " 'Associate Solutions Engineer - Bachelor/Master (Graduate) - Romania'\n",
      " 'ING International Talent ProgrammeaEUR\" IT Track @ING Hubs Romania'\n",
      " 'Feature Engineer @ ING Hubs Romania'\n",
      " 'Platform Ops Engineer AP Squad @ING Hubs Romania'\n",
      " 'Chapter Lead Software Engineer - DBNL Data Management @ING Hubs Romania'\n",
      " 'Development Engineer Assisted Channels @ING Hubs Romania'\n",
      " 'Ops Engineer – IPA Ops @ING Hubs Romania-3'\n",
      " 'Java Software Engineer IPA MI @ING Hubs Romania' 'Arhitect Sisteme IT'\n",
      " 'Java Software Engineer - Touchpoint Platform OnePAM 2 @ING Hubs Romania'\n",
      " 'Senior Java Engineer Broadway | Trading Platforms @ING Hubs Romania'\n",
      " 'Java Touchpoint Software Engineer - Global Authentication Platform @ING Hubs Romania'\n",
      " 'Paid internship: C++ Software Engineer - Electronic Arts Romania'\n",
      " 'PDM Engineer Romania' 'IT Engineer'\n",
      " 'Software Frontend Engineer - TPA Global Authentication Platform @ING Hubs Romania'\n",
      " 'IT Technical Support Engineer' 'Systems Support Engineer'\n",
      " 'Remote across Romania'\n",
      " 'Senior Ops Engineer – IPA Ops {multiple openings} @ING Hubs Romania'\n",
      " 'DevOps Engineer | BPM Product @ ING Bank'\n",
      " 'DevOps Engineer | Customer Interactions @ING Bank'\n",
      " 'IT Lead Test Engineer - Edge Computing'\n",
      " 'Electrical Engineer (MK Airbase in Constanta, Romania)'\n",
      " '#9037 Software Tester/Agile Tester- Remote across Romania'\n",
      " 'Graduate Distribution Technical Engineer (m/f/d)'\n",
      " 'Data Analyst - Spanish/Portuguese language'\n",
      " 'Junior User Acceptance Testing (UAT) Analyst'\n",
      " 'Internship - Software Engineering - N-able'\n",
      " 'Digital Customer Support Engineer' 'IT Infrastructure Engineer'\n",
      " 'Senior Infrastructure Engineer' 'Frontend Software Engineer'\n",
      " 'Frontend Developer' 'Senior AEM Engineer - Frontend'\n",
      " 'Network Engineer | Infrastructure as a Service @ ING Bank'\n",
      " 'DevOps Engineer | Digital @ ING Bank' 'Software Engineer III, Fitbit'\n",
      " 'IPB Agile - L2/RM Support Engineer - Data'\n",
      " 'Maintenance Technical Support Engineer' 'Junior Infrastructure Engineer'\n",
      " 'Senior IT Risk & Cyber Security Engineer'\n",
      " 'Frontline Support Engineer with French'\n",
      " 'Java Software Engineer Touchpoint Global Authentication @ING Hubs'\n",
      " 'Senior DevOps Engineer - Romania' 'IT Operations Engineer - IFS'\n",
      " 'Early Career Program - Field Engineering 2023 (Romania)'\n",
      " 'Information Security Engineer - Software'\n",
      " 'Software Engineer (Python) IRC177750'\n",
      " 'Scala Software Engineer - DriverApp'\n",
      " 'Senior Software Engineer - Web Services' 'QNX Software Engineer'\n",
      " 'Principal Software Engineer' 'Rotational Program - Development Engineer'\n",
      " 'IPB Agile - DevOps - Investments' 'Java Software Engineer'\n",
      " 'Java Software Engineer Specialist' 'Java Software Engineer II'\n",
      " 'Platform Engineer (Java) IRC182125' 'Senior Software Engineer - Java'\n",
      " 'Java Software Engineering opportunities'\n",
      " 'Java Software Engineer - Touchpoint platform OnePAM 1 @ING Hubs Romania'\n",
      " 'OMS - Software Engineer (Java)' 'Software Engineer'\n",
      " 'Software Python Engineer Continuous Integration Delivery Platform'\n",
      " 'Java Dev Engineer Broadway | Trading Platforms @ING Hubs Romania'\n",
      " 'Site Reliability Engineer' 'Early Engagement - CWI Reliability Engineer'\n",
      " 'Early Engagement - Drilling Services - Reliability Engineer (OFS)'\n",
      " 'System Admin and Cyber Security Engineer' 'Senior Dev Ops Engineer'\n",
      " 'IPB Agile - Dev (back-end) - Lending'\n",
      " 'Staff Engineer Component Verification (f/m/div)*'\n",
      " 'Senior UI engineer - Digital Asset Custody'\n",
      " 'System Analyst | Finance & Risk @ ING Bank'\n",
      " 'Database Senior Support Engineer - High Availability'\n",
      " 'Test Engineer (f/m/div*)' 'Senior Software Engineer (Python)'\n",
      " 'Java Back End Engineer IRC182511'\n",
      " 'Software Engineer for Microsoft ERP Solution (Dynamics 365) (f/m/d)'\n",
      " '2023 Software Development Engineer Internship - Amazon Dev Ctr (Romania) S.R.L'\n",
      " 'Java Engineer IRC180390' 'Consultant QNX BSP Engineer IRC179025'\n",
      " 'Completion Engineer' 'IPB Agile - Dev (back-end) - Platform Services'\n",
      " 'Windows Infrastructure Engineer' 'Feature Engineer@ING Hubs'\n",
      " 'Senior Java Engineer' 'Senior Customer Support Engineer'\n",
      " 'Operations Engineer CASY @ ING Hubs RO'\n",
      " 'Cloud Infrastructure Senior Engineer'\n",
      " 'Senior Software Engineer – Onboarding'\n",
      " 'Senior Network Security Engineer' 'Data Analyst - German language'\n",
      " 'Product Support Engineer' 'SDN - CFTS Technical Support Engineer'\n",
      " 'C++ Software Engineer - EA Sports FC™'\n",
      " 'Staff Software Engineer / Ontology and Knowledge Graphs (m/f/d)'\n",
      " 'Senior Analytics Engineer' 'L1 Signal Processing Junior Engineer'\n",
      " '(Junior/Senior) Network Security Engineer' 'Resident Engineer'\n",
      " 'Research Analyst – Portuguese language'\n",
      " 'HIL Test Engineer for Automotive with EXAM (m/f/d)'\n",
      " '2023 Intern aEUR\" C++ Software Engineer'\n",
      " 'Aero Simulation Engineer (Engines)' 'Remote'\n",
      " 'FPGA Software integration engineer'\n",
      " 'Middle Automation Test Engineer (C#) IRC179327'\n",
      " 'Software Development Engineer'\n",
      " 'Senior Network Engineer - Lifecycle Management and Evolution'\n",
      " 'Software Development Engineer, SDO Privacy, Data Access, SDO Privacy - AEDU team'\n",
      " 'Junior Automation Test Engineer (C#) IRC177821'\n",
      " 'Touchpoint Software Engineer - Global Authentication Platform @ING Hubs Romania'\n",
      " 'Senior Test Engineer' 'Senior Digital Analytics Engineer'\n",
      " 'Software Engineer – Power and Performance Benchmarking'\n",
      " 'Exadata Support Engineer (DBA/Sys Admin)'\n",
      " '(Senior) Digital Analytics Engineer' 'Founding Full Stack Engineer'\n",
      " 'Planning Engineer' 'IT Test Engineer (MyHealth project)'\n",
      " 'Software Engineer (C++, C#)' 'ETL Development Engineer'\n",
      " 'Middle Automation Test Engineer (C#) IRC179406'\n",
      " 'Software Development Engineer for Braking System'\n",
      " 'Java QA automation engineer IRC180792' 'QA Automation Engineer'\n",
      " 'Senior Design Automation Engineer (f/m/div*)' 'Automation Testing'\n",
      " 'Hardware Development Engineer - Sibiu AN R&D'\n",
      " 'Cloud Infrastructure Automation Developer'\n",
      " 'Network Automation Engineer Juniper'\n",
      " '#9063 Senior Automation Test Engineer - Remote' 'BI Tester'\n",
      " 'Senior Software Engineer (C++, C#)' 'Senior QA Engineer'\n",
      " 'Senior QA Automation Engineer (Python)' 'QA Engineer'\n",
      " 'Senior QA Automation Engineer' 'Senior Manual QA Engineer (All Genders)'\n",
      " 'Manual QA Engineer IRC178279'\n",
      " 'Senior QA Engineer (Manual and Automation)' 'Mobile QA Engineer'\n",
      " 'QA Data Engineer' 'QA Engineer - Full Remote'\n",
      " 'Senior Java Engineer, Remote' 'QA Manual testing'\n",
      " 'DLP System Engineer M/F' 'Software Engineer - Data Platform'\n",
      " 'Software Engineer (C#, Python, Elastic)' 'CIM/PFA/SRL - Remote'\n",
      " 'Core Networks (Switching & Routing) – R01523961'\n",
      " 'QA Manual Engineer with DWH'\n",
      " 'Senior Embedded C++ Software Development Engineer'\n",
      " 'Test Engineer with German' 'C++ and Qt Software Developer'\n",
      " 'Senior Software Engineer (C++/Qt) IRC181598' 'Senior C++ Engineer'\n",
      " 'C++ Software Engineer - Hybrid or Remote'\n",
      " 'Embedded Software Engineer for HMI-Touch sense developer'\n",
      " 'Embedded Software Development Engineer, Ring'\n",
      " 'Embedded Software Engineer for Automotive'\n",
      " 'Technical Support Engineer with German (SCCM & Intune)'\n",
      " 'Associate Architect - C++' 'C/C++ Software Engineer IRC170097'\n",
      " 'C/C++ Engineer' 'Automation Solution Engineer - HR Technology'\n",
      " 'QA Manual Engineer' 'Senior Software Engineer (UiPath Robot)'\n",
      " 'Software Engineer II - DevSecOps' 'Chip Architect Engineer- Remote'\n",
      " 'Senior Remote Access Engineer' 'Senior C++ Software Engineer IRC178849'\n",
      " 'Senior DevOps Engineer - Automation'\n",
      " 'C++ Developer (Cross-platform Apps) IRC178862'\n",
      " 'Senior Web Engineer, Remote'\n",
      " 'Senior Software Java Engineer - (full-remote EU)'\n",
      " 'Senior Software Engineer (Oracle, PL/SQL)' 'Software Test Analyst II'\n",
      " 'Supplier Quality Engineer – Key Expert for Digitalisation'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Senior JavaScript UI Software Engineer'\n",
      " '2023 Intern aEUR\" JavaScript Software Engineer - Adobe'\n",
      " 'Senior DevOps Cloud Engineer- AWS'\n",
      " 'Software Release Train Architect - Sibiu AN R&D'\n",
      " 'Testing & Validation Engineer' 'Testing & Commissioning Engineer'\n",
      " 'Junior QNX Developer IRC181695'\n",
      " 'Student Job Automotive Functional Safety Engineer (f/m/div)*'\n",
      " 'DC Protection Engineer for Designing and Testing of HVDC Software Solutions'\n",
      " 'Global Software Development Frontend Engineer (Business Intelligence) - Sibiu Industrial Engineering Plant'\n",
      " 'Data Engineer for Data Science Applications - IT (TM)'\n",
      " 'Staff Software Engineer, Big Data' 'Product Engineer (f/m/div*)'\n",
      " 'JavaScript Engineer' 'Soldering Process QA Engineer'\n",
      " 'Internship_Full Stack Developer__TSN' 'Engineer - Design'\n",
      " 'Senior Software Development Backend Engineer - Sibiu IE Plant'\n",
      " 'Test Engineer for IC Product Development (f/m/div)*'\n",
      " 'Junior Technical Sales Engineer with Italian' 'BI Engineer'\n",
      " 'Reliability Testing Engineer' 'ITSM AI and Automation Engineer'\n",
      " 'ERTMS Verification & Validation Engineer' 'Audio and Video Engineer'\n",
      " 'Test Engineer' 'PLM Release Support Engineer (seats)'\n",
      " 'Sales Support with German' 'Quality Test Engineer (CWS)'\n",
      " 'Sr Staff Build & Release Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 :\n",
      "['Data Engineer' 'Cloud Services Engineer (Based in Germany)'\n",
      " 'Backend Software Engineer (Go, AWS, Cassandra) - Cloud Security (Remote or Hybrid, Romania)'\n",
      " 'Cloud Developer | Discover | Golang, Kafka, Cassandra, AWS - Remote or Hybrid'\n",
      " 'Data Engineer - Talend' 'Data Lake Engineer' 'Python Data Engineer'\n",
      " 'Big Data Engineer (f/m/d)' 'Fixed Data Configuration 2nd Line Engineer'\n",
      " 'Tableau/Elastic Search/Hadoop ( Data lake) Engineer for...'\n",
      " 'Senior Consultant (f/m) Data Engineer' 'Data Engineer with Python'\n",
      " 'Software Test Engineer, FitbitOS Release Testing'\n",
      " 'Data Engineer (C#, Python, Elastic)' 'Data Engineer Manager'\n",
      " 'Data Warehouse Engineer' 'Azure Data Engineer'\n",
      " 'ETL Engineer|Unite Data Management @ ING Hubs Romania'\n",
      " 'Data Engineer (F/M)' 'Data Engineer (Python)'\n",
      " 'Java Engineer - DBNL Data Management @ING Hubs Romania'\n",
      " 'Software Test Engineer, Fitbit Activity' 'Data Engineer and Analyst'\n",
      " 'Data Solution Engineer' 'Senior AWS Cloud Infrastructure Engineer'\n",
      " 'Mid-Senior Data Engineer' 'Data Integration Engineer'\n",
      " 'DevOps Cloud Engineer (f/m/d)' 'Number Portability Engineer'\n",
      " 'PHP Data Engineer' 'Big Data Engineer - Kafka & Java'\n",
      " 'Big Data Engineer with Hadoop' 'Big Data Engineer M/F'\n",
      " 'Junior Cloud Engineer'\n",
      " 'Software Development Engineer for CarDataBox (f/m/d)'\n",
      " 'Data Engineer IRC177741' 'Cloud Security Engineer'\n",
      " 'Data DevOps Engineer'\n",
      " 'Data Platform Engineer |Data Analytics Platform @ING Hubs Romania'\n",
      " 'Backhaul Design Data Production Engineer'\n",
      " 'Software Engineer - Data Integration'\n",
      " 'Data Engineer (Google Cloud Platform)'\n",
      " 'DevOps Engineer - Data Streaming Service @ING Hubs Romania'\n",
      " 'Staff Software Engineer, Fitbit'\n",
      " 'Suppliers Quality Data Analyst & Business Process Engineer (F/M)'\n",
      " 'Senior Data Engineer' 'Lead Data Engineer IRC182509'\n",
      " 'IPB Agile - Solution Architect - Data' 'Data Architect'\n",
      " 'Staff Engineer- Data Management Architect' 'Jr Data Engineer'\n",
      " 'Cloud Engineer' 'ITSM Data Architect' 'ETL engineer @ING Hubs'\n",
      " 'Data Lake Dev-Ops Engineer @ING Hubs Romania'\n",
      " 'Cloud Engineer– Bucharest' 'DevOps Cloud Engineer' 'Big Data Engineer'\n",
      " 'DevOps Engineer |Data Lake - Core team @ING Hubs Romania'\n",
      " 'Cloud DevOps Lead Engineer' 'Informatica Data Engineer'\n",
      " 'Cloud Azure Engineer (Remote)' 'Data Visualization Engineer (Power BI)'\n",
      " 'Senior Software Engineer, Data Platform'\n",
      " 'Data Security Engineer - fully remote'\n",
      " 'Senior DevOps Engineer - Autonomous Database'\n",
      " 'Early Engagement - Drill Bits Applications Engineer (OFS)'\n",
      " 'Data Engineer - Customer Lifecycle Recommender'\n",
      " 'Cloud Security Architect' 'Data Center Engineer (On Demand Contract)'\n",
      " 'Senior Data & AI DevOps Engineer'\n",
      " 'Cloud Security Engineer - fully remote'\n",
      " 'Tableau or Elastic Search or Hadoop (Data lake) Engineer for Datawarehouse'\n",
      " 'Senior Android Software Engineer, Fitbit Device Management'\n",
      " 'Private Cloud Network Engineer' 'L2 Cloud Engineer'\n",
      " 'Data Engineer /ML Engineer'\n",
      " 'Paid Internship - Frostbite Software Engineer'\n",
      " 'Chapter Lead Software Engineer - DBNL Data Management @ING Hubs Romania'\n",
      " 'Software Engineer III, Fitbit' 'Cloud Infrastructure Senior Engineer'\n",
      " 'Senior Analytics Engineer'\n",
      " 'Software Development Engineer, SDO Privacy, Data Access, SDO Privacy - AEDU team'\n",
      " 'Senior Digital Analytics Engineer' '(Senior) Digital Analytics Engineer'\n",
      " 'ETL Development Engineer' 'Hardware Development Engineer - Sibiu AN R&D'\n",
      " 'Cloud Infrastructure Automation Developer' 'QA Data Engineer'\n",
      " 'Software Engineer - Data Platform' 'Senior DevOps Cloud Engineer- AWS'\n",
      " 'Software Release Train Architect - Sibiu AN R&D'\n",
      " 'Data Engineer for Data Science Applications - IT (TM)'\n",
      " 'Staff Software Engineer, Big Data'\n",
      " 'Senior Software Development Backend Engineer - Sibiu IE Plant'\n",
      " 'BI Engineer' 'Reliability Testing Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.17 Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Spain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 :\n",
      "['Data Engineer'\n",
      " 'Project Manager / Engineer Smart Cities & Smart Grids (m/f/x)'\n",
      " 'Technical Service Engineer' 'Senior IT Architect'\n",
      " 'Systems Engineer Future Combat Air System Relocation to Germany (f/m/d)'\n",
      " 'Research Engineer (m/f/x/d)' 'M3 Functional Analyst (Remote Role)'\n",
      " 'Senior Data Engineer - BCG X' 'Data Engineer, 100% En remoto'\n",
      " 'Python Data Engineer (Remote)' '(Mid) Data Engineer'\n",
      " 'Data Engineer entry level (20 - 30K)' 'JUNIOR DATA ENGINEER'\n",
      " 'DevOps Engineer - Work from home'\n",
      " 'Mobility Analytics Product Engineer & Data Scientist'\n",
      " 'Java R&D Engineer *REMOTE* f/m' 'Head of Product Data - 100% Remote'\n",
      " 'Junior Data Engineer'\n",
      " 'Data Engineer Spark - B2C1 - Remoto 100%, 100% En remoto'\n",
      " 'Data Engineer PowerCenter, 100% En remoto'\n",
      " 'Data Engineer exp +2 (30-40K)' 'Data Analyst, GeoAnalytics – BCG X'\n",
      " 'EMEA Data Scientist- Remote (open to candidates across Europe)'\n",
      " 'Data Engineer (Experimentation)'\n",
      " 'Lead Data Engineer (Google Cloud, Scala, CI/CD)'\n",
      " 'Senior Cloud R&D Engineer f/m' 'Software Engineer'\n",
      " 'Construction & Development Intern' 'Product Manager AI/ML f/m'\n",
      " 'RCE ML Engineer' 'Data Engineer (I&D)'\n",
      " 'Software Development Senior Specialist' 'Systems Engineer'\n",
      " 'Data Engineer/Power BI Developer, Barcelona'\n",
      " 'Director, Go to Market Strategy and Transformation - REMOTE or FLEX'\n",
      " 'Senior Data Engineer - SCALA' 'Sales & Order Operations Specialist f/m'\n",
      " 'Senior Data Engineer' 'Google Professional Data Engineer'\n",
      " 'Data Visualization Engineer'\n",
      " 'Data Engineer _ Analytics and Modeling analyst'\n",
      " 'Senior Data Engineer (40k-50k)'\n",
      " 'Junior Data Scientist - Hybrid Intelligence' 'Data Engineer - PagoNxt'\n",
      " 'Campaign Data Analyst Intern' 'Data Engineer AWS, Madrid'\n",
      " 'Data Engineer Intern' 'Data Engineer AWS'\n",
      " 'IA and Data Engineer (Hybrid)' 'Junior Data Engineer (OF0523)'\n",
      " 'Data Software Engineer' 'Data Engineer (m/f/d)'\n",
      " 'Big Data Engineer – Data Lake - Openbank (Openhub Valencia)'\n",
      " 'Data Analyst/ Engineer Junior (Beca)'\n",
      " 'GDC Process Mining Data Engineer (f/m/d)'\n",
      " 'Data Engineer ETL PowerCenter (Remoto)' 'Data Engineer (Valencia)'\n",
      " 'Advanced Driver Assistance Systems (ADAS) Test Engineer, Spain'\n",
      " 'Data Engineer Python y Cloud, Madrid'\n",
      " 'Intern Data Scientist / Software Engineer - Spanish Speaking (End-of-studies)'\n",
      " 'Cloud Data Engineer' 'Staff Quality Engineer (ETL-Data Engineer Squad)'\n",
      " 'Data Engineer (H/M)' 'Data Engineer junior (H/M)'\n",
      " 'Data Engineer MicroStrategy (Madrid)' 'Analyst - GeoAnalytics'\n",
      " 'Data Engineer Junior' 'CAD Engineer Intern' 'Big Data Engineer'\n",
      " 'Satec se encuentra en búsqueda de un Data Engineer' 'Data Analyst'\n",
      " 'AIRWORTHINESS TECHNICAL ENGINEER FOR OPERATIONAL SUITABILITY DATA (OSD)'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Data Scientist / Deep Learning Engineer'\n",
      " 'Data Engineer/Analista Funcional 100%TT, 100% En remoto'\n",
      " 'Data Engineer Senior Consultant STEM'\n",
      " 'Senior Data Software Engineer (AWS/Databricks/PySpark)'\n",
      " 'Senior Data Software Engineer - (Malaga or Madrid)'\n",
      " 'Data Engineer (Shared Tech)' 'Software Engineer (m/f/d)' 'DATA ENGINEER'\n",
      " 'Data Engineer for HR (f/m/d)' 'Digital Analyst'\n",
      " 'Sales & Order Operations Specialist'\n",
      " 'Data Engineering Trainer - Freelance, Remote'\n",
      " 'Lead Data Software Engineer (Spark/Scala/Databricks)'\n",
      " 'Clinical Data Engineer' 'Software Engineer (Leadership) - Metaworks'\n",
      " 'Market Data Engineer (f/m/d)' '(Senior) Data Engineer - Link (Remote)'\n",
      " 'Consultoría | Senior Associate - Data Governance'\n",
      " 'Software Engineer Internship in Big Data & Data Science tools'\n",
      " 'Fullstack Software Engineer – Physical Meeting Points, Group Digital, Madrid'\n",
      " 'Junior Engineer - Opentech' 'Data engineers welcomed'\n",
      " 'Big Data Engineer / Analista Funcional - Remoto'\n",
      " 'Data Platform Engineer' 'Specialist Big Data and AI Engineer MSH'\n",
      " 'Platform Engineer (f/m/d)' 'Senior Data Engineer Azure / Spark'\n",
      " 'Senior Software R&D Engineer' 'Data engineer AWS y pyspark'\n",
      " 'C++ Senior Software Engineer' 'Data Engineer M/F'\n",
      " 'TECHNICAL EXPERT (PROJECT ENGINEER)'\n",
      " 'Junior Field Service Engineer (m/f/d)'\n",
      " 'Senior Machine Learning Engineer for Computer Vision'\n",
      " 'Azure Data Engineer'\n",
      " 'Interoperability and Tactical Data Links Systems Engineer'\n",
      " 'Software Engineer for data systems of satellite missions'\n",
      " 'Business Intelligence & Big Data Software Engineer'\n",
      " 'Data Modelling Especialist'\n",
      " 'Data Engineer - Energy Trading Optimisation (f/m/d)'\n",
      " 'Data Platform Engineer - Link'\n",
      " 'Automation Software Engineer - Full Time Internship'\n",
      " 'Data and Analytics - IT Product Owner' 'Software Engineer - Launchpad'\n",
      " 'Big Data Engineer - Datalake - Openbank (Openhub Bilbao)'\n",
      " 'Data Engineer Analyst' 'Software Engineer Internship'\n",
      " 'Data Engineer (Google Cloud)' 'Senior Software Engineer - Telco'\n",
      " 'Senior Engineer for Remote Sensing applications'\n",
      " 'Software Engineer - Capabilities - Malaga or Madrid'\n",
      " 'Data Security Engineer - fully remote'\n",
      " 'Automation Software Engineer Internship Madrid'\n",
      " '.NET Senior Software Engineer'\n",
      " 'Senior Software Engineer, Microservices (Java)'\n",
      " 'Senior Backend Software Engineer (Java)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 :\n",
      "['Data Engineer' 'Senior Data Engineer - BCG X'\n",
      " 'Data Engineer, 100% En remoto' 'Python Data Engineer (Remote)'\n",
      " '(Mid) Data Engineer' 'Data Engineer entry level (20 - 30K)'\n",
      " 'JUNIOR DATA ENGINEER' 'Head of Product Data - 100% Remote'\n",
      " 'Junior Data Engineer'\n",
      " 'Data Engineer Spark - B2C1 - Remoto 100%, 100% En remoto'\n",
      " 'Data Engineer PowerCenter, 100% En remoto'\n",
      " 'Data Engineer exp +2 (30-40K)' 'Data Engineer (Experimentation)'\n",
      " 'Lead Data Engineer (Google Cloud, Scala, CI/CD)'\n",
      " 'Senior Cloud R&D Engineer f/m' 'Data Engineer (I&D)'\n",
      " 'Data Engineer/Power BI Developer, Barcelona'\n",
      " 'Senior Data Engineer - SCALA' 'Senior Data Engineer'\n",
      " 'Google Professional Data Engineer' 'Data Visualization Engineer'\n",
      " 'Data Engineer _ Analytics and Modeling analyst'\n",
      " 'Senior Data Engineer (40k-50k)' 'Data Engineer - PagoNxt'\n",
      " 'Data Engineer AWS, Madrid' 'Data Engineer Intern' 'Data Engineer AWS'\n",
      " 'IA and Data Engineer (Hybrid)' 'Junior Data Engineer (OF0523)'\n",
      " 'Data Software Engineer' 'Data Engineer (m/f/d)'\n",
      " 'Big Data Engineer – Data Lake - Openbank (Openhub Valencia)'\n",
      " 'Data Analyst/ Engineer Junior (Beca)'\n",
      " 'GDC Process Mining Data Engineer (f/m/d)'\n",
      " 'Data Engineer ETL PowerCenter (Remoto)' 'Data Engineer (Valencia)'\n",
      " 'Data Engineer Python y Cloud, Madrid' 'Cloud Data Engineer'\n",
      " 'Data Engineer (H/M)' 'Data Engineer junior (H/M)'\n",
      " 'Data Engineer MicroStrategy (Madrid)' 'Data Engineer Junior'\n",
      " 'Big Data Engineer' 'Satec se encuentra en búsqueda de un Data Engineer'\n",
      " 'AIRWORTHINESS TECHNICAL ENGINEER FOR OPERATIONAL SUITABILITY DATA (OSD)'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Data Engineer/Analista Funcional 100%TT, 100% En remoto'\n",
      " 'Data Engineer Senior Consultant STEM'\n",
      " 'Senior Data Software Engineer (AWS/Databricks/PySpark)'\n",
      " 'Senior Data Software Engineer - (Malaga or Madrid)'\n",
      " 'Data Engineer (Shared Tech)' 'DATA ENGINEER'\n",
      " 'Data Engineer for HR (f/m/d)'\n",
      " 'Data Engineering Trainer - Freelance, Remote'\n",
      " 'Lead Data Software Engineer (Spark/Scala/Databricks)'\n",
      " 'Clinical Data Engineer' 'Market Data Engineer (f/m/d)'\n",
      " '(Senior) Data Engineer - Link (Remote)'\n",
      " 'Software Engineer Internship in Big Data & Data Science tools'\n",
      " 'Data engineers welcomed'\n",
      " 'Big Data Engineer / Analista Funcional - Remoto'\n",
      " 'Data Platform Engineer' 'Specialist Big Data and AI Engineer MSH'\n",
      " 'Senior Data Engineer Azure / Spark' 'Data engineer AWS y pyspark'\n",
      " 'Data Engineer M/F' 'Azure Data Engineer'\n",
      " 'Interoperability and Tactical Data Links Systems Engineer'\n",
      " 'Software Engineer for data systems of satellite missions'\n",
      " 'Business Intelligence & Big Data Software Engineer'\n",
      " 'Data Modelling Especialist'\n",
      " 'Data Engineer - Energy Trading Optimisation (f/m/d)'\n",
      " 'Data Platform Engineer - Link'\n",
      " 'Big Data Engineer - Datalake - Openbank (Openhub Bilbao)'\n",
      " 'Data Engineer Analyst' 'Data Engineer (Google Cloud)'\n",
      " 'Software Engineer - Capabilities - Malaga or Madrid'\n",
      " 'Data Security Engineer - fully remote']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'], \n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.18 Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Sweden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 :\n",
      "['Kreativ BI-utvecklare med passion för dataanalys'\n",
      " 'Data Governance Engineer (m/f/d) 80% Homeoffice'\n",
      " 'Examensarbete - Inverkan av materialstruktur på livslängd hos polykristallina superlegeringar'\n",
      " 'DevOps Engineer to Zebware'\n",
      " 'Manager Parts Business - STG Steam Turbines, Finspång fleet'\n",
      " 'Service Application Engineer' 'Head of Azure till Bitlog'\n",
      " 'Network Data Engineer' 'Battery Module Safety Engineer'\n",
      " 'Process Manager (m/f/d) 80% Homeoffice'\n",
      " 'Data/ cloud engineer for a global telecom company!'\n",
      " 'Senior Software Engineer / Architect for biotech company'\n",
      " 'Public Cloud Engineer Azure' 'SIEM Engineer' 'Data Engineer'\n",
      " 'Junior Data Engineer' 'Backend engineer with a love for gaming'\n",
      " 'Data Engineer (Stockholm)'\n",
      " 'Data Engineer (Power BI/SQL) - Health Analytics'\n",
      " 'Azure AI/Data Engineer' 'Junior Engineer - JavaScript'\n",
      " 'Frontend Engineer' 'Lead CI Engineer till Tutus Data AB!'\n",
      " 'Junior Systemutvecklare till Axis i Lund!'\n",
      " 'Ingenjörer / Produkt testingenjörer till Sigma Connectivity'\n",
      " 'Junior software engineer till Husqvarna'\n",
      " 'Data Engineer to Business Intelligence & Analytics...'\n",
      " 'Data Engineer (Google Looker) - Fully Remote (LATAM)'\n",
      " 'Senior Data Engineer' 'Data Engineer to Vetfamily'\n",
      " 'Offertingenjör till Valmet' 'Electronics Engineer to Envirotainer!'\n",
      " 'BI-utvecklare till Sweco' 'Quality Engineer till Olink'\n",
      " 'DATA ENGINEER AHLSELL' 'Fullstack Engineer - Commercial Tech'\n",
      " 'Data Infrastructure Engineer' 'IT Support Engineer'\n",
      " 'Machine Learning Engineer som vill skapa framtida historia'\n",
      " 'Data Engineer - Nyfiken på nya utmaningar?' 'Data Science Engineer'\n",
      " 'Data Engineer – Data platform & device telemetry'\n",
      " 'PostNord Graduate Program 2023'\n",
      " 'IT solution engineer / Cloud engineer till QD'\n",
      " 'Junior Systemutvecklare till Axis!'\n",
      " 'Future Talent Programme - Business Analyst' 'Purchasing Data Engineer'\n",
      " 'Autumn Technology Internships 2023' 'Software Engineer - Data Platform'\n",
      " 'Software Engineer- Planning & Forecasting' 'Junior Backend Engineer'\n",
      " 'Software Engineer - Launchpad' 'Junior Engineer - Gothenburg'\n",
      " 'Junior Engineer - Java' 'Obsolescence Expert'\n",
      " 'Data Engineer for Deep Learning within Autonomous Driving'\n",
      " 'Swescan söker GIS-ingenjör!' 'Data Engineer och Tech Lead'\n",
      " 'Lead Python Engineer - Lund' 'Software Engineer'\n",
      " 'Future Talent Programme - Data Analyst, Facility & Real Estate'\n",
      " 'Data Engineer (Engagement team)' 'Software Engineer (Rust)'\n",
      " 'Data Engineer to Data Platform and Innovation team' 'GCP Data Engineer'\n",
      " 'Azure Data Engineer till vårt team' 'Software Engineer, Algorithms'\n",
      " 'Är du en syftesdriven Data Engineer?' 'Data Center Intern'\n",
      " 'Software Engineer Intern (Front-end)'\n",
      " 'Samhällsnytta: BI konsult/Data Engineer'\n",
      " 'Junior Infrastructure Engineer' 'DevOps / Data engineer'\n",
      " 'Software Engineer (Leadership) - Metaworks'\n",
      " 'R&D Engineer - 2023 Graduate' 'Backend Software Engineer - Sweden'\n",
      " 'Junior R&D Engineer to cleantech company Epishine' 'DATA ENGINEER'\n",
      " 'GIS ingenjör' 'Data engineer to Mpya Sci & Tech'\n",
      " 'Machine Learning Engineer' 'Design Engineer' 'Network Engineer'\n",
      " 'Deep Learning Engineer (Python)' 'Data Engineer to Avinode!'\n",
      " 'Order Engineer' 'Hive Student Lab - Internship' 'Mechanical Engineer'\n",
      " 'Software Development Engineer to Elvaco' 'Test Engineer'\n",
      " 'Data Engineer till Sogeti i Skåne!'\n",
      " 'Nyexaminerad civilingenjör - se hit!'\n",
      " 'Graduate Performance Analysis Engineer'\n",
      " 'GIS-ingenjör till Plan- & exploateringsenheten'\n",
      " 'Data Engineer som verkligen gillar att bygga data pipelines'\n",
      " 'Validation Engineer Internship'\n",
      " 'Data Engineer som vill skapa framtida historia' 'Release Train Engineer'\n",
      " 'Molnleverantör på 10 vårar söker likasinnad' 'Data Engineers'\n",
      " 'Sourcing Specialist' 'Software Engineer to AlgoDx'\n",
      " 'Analytics and Engineering Tech Lead' 'Tech Lead'\n",
      " 'Legal & Compliance Specialist / GDPR & informationssäkerhet till Hailey HR'\n",
      " 'Tech Lead (working with Machine Learning Engineers)'\n",
      " 'Web Analytics Engineer' 'Data Engineer till Data Edge'\n",
      " 'Tech Lead - Data Engineering (MarTech)'\n",
      " 'Product Engineer, Machine Vision & Scanner Measurements /...'\n",
      " 'Software Engineer to join Surveillance' 'Data Engineer – Google Cloud'\n",
      " 'Compute and Data Platform Engineer' 'Data Engineer to XLN Audio'\n",
      " 'Software engineer to Mpya Sci & Tech'\n",
      " 'Machine learning ingenjör för analys of patientdata'\n",
      " 'Lead Data Engineer' 'Data Engineer som är datadriven och strukturerad'\n",
      " 'Brädspelare och Data Engineer önskas som kollega'\n",
      " 'Product Stewardship Engineer' 'Data Engineer (f/m/d)'\n",
      " 'Process Engineer/Process System Owner'\n",
      " 'Data Engineer in findability - Tietoevry Create' 'Database Engineer'\n",
      " 'R&D Engineer'\n",
      " 'Senior Software Engineer, Autonomous and data-driven capabilities'\n",
      " 'BI & DATA ANALYTICS MANAGER' 'Analytics Engineer'\n",
      " 'Machine Learning Engineer within analytics PostNord'\n",
      " 'Tech Lead Data & Analytics – Stockholm' 'Principal Data Engineer'\n",
      " 'AWS Data Engineer - Fully Remote'\n",
      " 'Senior Data Governance Engineer (m/f/d)'\n",
      " 'Data Engineer / Developer to IT at Scania' 'Deep Learning Engineer'\n",
      " 'Data Engineer till Stena Metall Group IT'\n",
      " 'DevOps Engineer | CI Triage and Release' 'Data Warehouse-ansvarig'\n",
      " 'Azure Data Engineer- Insights & Data, Stockholm'\n",
      " 'Data Engineer | Relex Promotions' 'DevOps Cloud Engineer for AWS'\n",
      " 'Specialist Network Engineer' 'SOFTWARE ENGINEER (WEB)'\n",
      " 'ABB Robotics is looking for IS Application Specialist!'\n",
      " 'Data Engineer (Master Data) (f/m/d)' 'Junior Consultant'\n",
      " 'Games Streaming Software Engineer' 'Forward-deployed Lead Data Engineer'\n",
      " 'Data Developer/Engineer' 'Senior Data Engineer - SWE/UK (Remote)'\n",
      " 'Kickstarta din karriär som ledare inom IT/produktutveckling'\n",
      " 'Security Data Scientist / Data Engineer' 'GSI Systems Engineer, EMEA'\n",
      " 'Senior Machine Learning Engineer' 'Cloud Engineer'\n",
      " 'Software Engineer - Ubuntu Build Infrastructure'\n",
      " 'Mobile Engineer - Utilities' 'Senior Cloud Software Engineer Azure'\n",
      " 'RAN Engineer' 'Systemutvecklare' 'Senior Electronics Engineer'\n",
      " 'Machine Learning Engineer, Solna' 'Senior Data Engineer/Cloud Architect'\n",
      " 'Datadriven Sälj- & Verksamhetsplanerare'\n",
      " 'Lead Software Engineer, Data Analytics'\n",
      " 'Junior QA-engineer till Axis Communications i Lund!'\n",
      " 'Software Test Engineer (m/w/d)'\n",
      " 'Data Engineer at Volvo Group Digital and IT'\n",
      " 'QA Engineer - Post Purchase Support'\n",
      " 'Cloud Engineer - Data & Analytics Platform' 'Head of engineering'\n",
      " 'Software Engineer - DICE Quality Engineering'\n",
      " '(Senior) Data Engineer(s) - The future of banking with...'\n",
      " 'Data engineers till Sveriges riksbank' 'Embedded Software Engineers'\n",
      " 'AFRY X söker Data Engineer' 'CTO / Engineering Director - Full remote'\n",
      " 'Area Lead Engineer Cloud & Data Center Engineering'\n",
      " 'Senior ML data/ops Engineer' 'Data & Analytics Manager'\n",
      " 'Director: Area Lead Engineer - Data Transformation'\n",
      " 'Frontend React Developer' 'Data Engineer_Principal'\n",
      " 'Associate Principal AI Engineer'\n",
      " 'Intelligent Manufacturing, Data and AI Engineer, fluent in English']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 :\n",
      "['Data Governance Engineer (m/f/d) 80% Homeoffice'\n",
      " 'Manager Parts Business - STG Steam Turbines, Finspång fleet'\n",
      " 'Head of Azure till Bitlog' 'Network Data Engineer'\n",
      " 'Data/ cloud engineer for a global telecom company!'\n",
      " 'Senior Software Engineer / Architect for biotech company'\n",
      " 'Public Cloud Engineer Azure' 'Data Engineer' 'Junior Data Engineer'\n",
      " 'Data Engineer (Stockholm)'\n",
      " 'Data Engineer (Power BI/SQL) - Health Analytics'\n",
      " 'Azure AI/Data Engineer' 'Lead CI Engineer till Tutus Data AB!'\n",
      " 'Data Engineer to Business Intelligence & Analytics...'\n",
      " 'Data Engineer (Google Looker) - Fully Remote (LATAM)'\n",
      " 'Senior Data Engineer' 'Data Engineer to Vetfamily'\n",
      " 'DATA ENGINEER AHLSELL' 'Data Infrastructure Engineer'\n",
      " 'Data Engineer - Nyfiken på nya utmaningar?' 'Data Science Engineer'\n",
      " 'Data Engineer – Data platform & device telemetry'\n",
      " 'IT solution engineer / Cloud engineer till QD'\n",
      " 'Purchasing Data Engineer' 'Software Engineer - Data Platform'\n",
      " 'Data Engineer for Deep Learning within Autonomous Driving'\n",
      " 'Data Engineer och Tech Lead' 'Data Engineer (Engagement team)'\n",
      " 'Data Engineer to Data Platform and Innovation team' 'GCP Data Engineer'\n",
      " 'Azure Data Engineer till vårt team'\n",
      " 'Är du en syftesdriven Data Engineer?'\n",
      " 'Samhällsnytta: BI konsult/Data Engineer' 'DevOps / Data engineer'\n",
      " 'DATA ENGINEER' 'Data engineer to Mpya Sci & Tech'\n",
      " 'Data Engineer to Avinode!' 'Data Engineer till Sogeti i Skåne!'\n",
      " 'Data Engineer som verkligen gillar att bygga data pipelines'\n",
      " 'Data Engineer som vill skapa framtida historia' 'Data Engineers'\n",
      " 'Analytics and Engineering Tech Lead' 'Web Analytics Engineer'\n",
      " 'Data Engineer till Data Edge' 'Tech Lead - Data Engineering (MarTech)'\n",
      " 'Data Engineer – Google Cloud' 'Compute and Data Platform Engineer'\n",
      " 'Data Engineer to XLN Audio' 'Lead Data Engineer'\n",
      " 'Data Engineer som är datadriven och strukturerad'\n",
      " 'Brädspelare och Data Engineer önskas som kollega'\n",
      " 'Data Engineer (f/m/d)' 'Data Engineer in findability - Tietoevry Create'\n",
      " 'Database Engineer'\n",
      " 'Senior Software Engineer, Autonomous and data-driven capabilities'\n",
      " 'BI & DATA ANALYTICS MANAGER' 'Analytics Engineer'\n",
      " 'Tech Lead Data & Analytics – Stockholm' 'Principal Data Engineer'\n",
      " 'AWS Data Engineer - Fully Remote'\n",
      " 'Senior Data Governance Engineer (m/f/d)'\n",
      " 'Data Engineer / Developer to IT at Scania'\n",
      " 'Data Engineer till Stena Metall Group IT'\n",
      " 'Azure Data Engineer- Insights & Data, Stockholm'\n",
      " 'Data Engineer | Relex Promotions' 'DevOps Cloud Engineer for AWS'\n",
      " 'Data Engineer (Master Data) (f/m/d)'\n",
      " 'Forward-deployed Lead Data Engineer' 'Data Developer/Engineer'\n",
      " 'Senior Data Engineer - SWE/UK (Remote)' 'Cloud Engineer'\n",
      " 'Senior Cloud Software Engineer Azure'\n",
      " 'Senior Data Engineer/Cloud Architect'\n",
      " 'Lead Software Engineer, Data Analytics'\n",
      " 'Data Engineer at Volvo Group Digital and IT'\n",
      " 'Cloud Engineer - Data & Analytics Platform'\n",
      " '(Senior) Data Engineer(s) - The future of banking with...'\n",
      " 'Data engineers till Sveriges riksbank' 'AFRY X söker Data Engineer'\n",
      " 'Area Lead Engineer Cloud & Data Center Engineering'\n",
      " 'Senior ML data/ops Engineer' 'Data & Analytics Manager'\n",
      " 'Director: Area Lead Engineer - Data Transformation'\n",
      " 'Data Engineer_Principal'\n",
      " 'Intelligent Manufacturing, Data and AI Engineer, fluent in English']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.19 Switzerland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Switzerland'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 :\n",
      "['Data Engineer (w/m) | 80% oder mehr'\n",
      " 'Leiter Data Analytics & Data Science (a)'\n",
      " 'Big Data & Platform Engineer 80-100%'\n",
      " 'Data Engineer Messtechnik (m/w/d)' 'Data Scientist (60%-100%)'\n",
      " 'Platform-Engineer Information Management (w/m/d)'\n",
      " 'Software Systems Engineer - Client Services Technology'\n",
      " 'Data Engineer / Analyst (m/f/d)'\n",
      " 'Manufacturing Engineer - Medical Device'\n",
      " 'Software Systems Engineer – Business Support Services Technology'\n",
      " 'System Engineer (RFID)'\n",
      " 'Global Student Internship in Innovation & Sustainability - Drive Sustainable Change through Robotic Solubility Determination (m/f/d; starting in July 2023 for 3 months, with a possibility of extension)'\n",
      " 'Product Engineer - ArcGIS JavaScript API'\n",
      " 'Senior Power BI Engineer (m/w/d)' 'Data- & Integration Engineer (w/m/d)'\n",
      " 'Cold Chain Packaging Engineer - temporary for 12 months'\n",
      " 'Development Reliability Engineer (contract)'\n",
      " 'Automation Engineer (m/w/d)' 'Senior Quality Engineer (m/f/d)'\n",
      " 'IT Test Engineer'\n",
      " 'Global Student Internship in Innovation & Sustainability - Fill Level Calibration without Water Consumption (m/f/d; starting in July 2023 for 3 months, with a possibility of extension)'\n",
      " 'M365 Engineer (w/m/d) (8666)'\n",
      " 'Roche Postdoctoral Fellow (RPF) in Oncology Discovery – Phenotypic CRISPR Library Screens for Cell Therapy'\n",
      " 'Postdoctoral Fellow, Retinal Organoid Bioengineering (m/f/d)'\n",
      " 'Data Management Engineer (m/f)' 'DevOps Data Engineer 100%'\n",
      " 'Data Engineer / Analyst'\n",
      " 'Belle opportunité pour un/e Data Engineer Junior - Urgent - Banque'\n",
      " 'Scientific Area Lead Computational Pathology'\n",
      " 'Mobile Software Engineer Flutter' 'Software Engineer Microscopy'\n",
      " 'Data Management Engineer (w/m/d)' 'IT-Trainee'\n",
      " 'Scientist/Engineer at the cryoEM facility at Biozentrum'\n",
      " 'Azure Data Engineer - Spark' 'System Engineer' 'Process / MSAT Engineer'\n",
      " 'Product Engineering Lead - 3D Content' 'IT Data Engineer'\n",
      " 'System Development Engineer'\n",
      " 'Head Software Solutions Competence Center (100%)'\n",
      " 'Engineer/Scientist in electrochemical method development'\n",
      " 'PLM Polarion Engineer (50% Home Office)' 'Electrical System Engineer'\n",
      " \"Manager de l'équipe Data Engineer\"\n",
      " 'Biotechnologist - Technical Support Laboratory Instruments (m/f/d)'\n",
      " 'Graduate Talent Program 2023, UBS RiskLab Data Science Model Services'\n",
      " 'C++ / Python Software Engineer - SEM'\n",
      " 'Data Engineer mit mehrjähriger Erfahrung im SQL Bereich (m/w/d)'\n",
      " 'Senior Data Science Engineer & Business Intelligence Developer (Power BI) (m/w/d)'\n",
      " 'Project Scheduler (m/f/d)'\n",
      " 'ICT System Engineer (m/w/d) Datacenter und Cloud'\n",
      " 'Big Data - (Senior) Data Scientist and Machine Learning Engineer (m/w)'\n",
      " 'Senior Software Engineer' 'Big Data Engineer (m/w/d)'\n",
      " 'Lead Data Scientist - Shipping Analytics'\n",
      " 'E2E Network Performance Engineer Radio'\n",
      " 'IT System Engineer - Data Center (m/w/d)'\n",
      " 'Embedded Software Engineer (f/m/d)' 'Data Engineer (m/w/d)'\n",
      " 'Data Engineer - Sustainability (m/w/d)'\n",
      " 'System Engineer Data Services (m/w/d)'\n",
      " 'IT System Engineer - Data Services (m/w/d)'\n",
      " 'System Engineer StorageGRID'\n",
      " '(Senior) Big Data Software Entwickler (m/w/d) - Bern'\n",
      " 'Data Engineer/Data Analyst Business Intelligence (m/w/d)'\n",
      " 'Senior System Engineer Data Services (m/w/d)' 'System Engineer - NetApp'\n",
      " 'Angular Expert Software Engineer'\n",
      " 'IT System-Engineer 80-100% (m/w/d) mit Fokus Microsoft Cloud-Services (55981)'\n",
      " 'Senior Data Scientist / Engineer' 'Senior Data Scientist'\n",
      " 'Data Center Engineer' 'System Engineer / System Specialist'\n",
      " 'Data Science Lead (m/w/d)' 'Cloud Data Infrastructure Engineer (m/w/d)'\n",
      " 'IT Service Desk Agent' 'IT Engineer - Automatisierung'\n",
      " 'Applied Researcher/Data Scientist position in time series / forecasting'\n",
      " 'Junior Data Engineer (m/f/d)' 'Cloud & Data Engineer'\n",
      " 'Software / System Engineer' 'Data Scientist / AI Engineer (w/m/d)'\n",
      " 'Engineer – Structural Dynamics Specialist (80-100%)'\n",
      " 'Application Owner & Process Engineer SAP PP & QM (80%-100%)'\n",
      " 'Machine Learning Engineer / KI Software Developer Public Sector'\n",
      " 'Senior Business Analyst for Process Testing'\n",
      " 'IT Systems and Network Specialist'\n",
      " '(Junior) Data Architect / Data Engineer'\n",
      " 'Data Center Infrastructure Engineer (m/w/d)'\n",
      " 'IT System Engineer Production IT (m/w/d)' 'Network Engineer (m/w/d)'\n",
      " 'IT System Engineer Operational Technology (m/w/d)'\n",
      " 'Senior Java Developer' 'System Engineer (m/w/d)'\n",
      " 'System Engineer - SCCM Spezialist (m/w/d)' 'Manual Test Engineer'\n",
      " 'Senior System Engineering mit Führungsverantwortung (m/w/d)'\n",
      " 'System Engineer Windows (m/w/d)'\n",
      " 'Internship in Research & Development Materials Engineering - August / September 2023 for 6 months'\n",
      " 'Azure Data Engineer / Developer' 'Data & Analytics Specialist'\n",
      " 'Datenbank Developer (m/w/d)' 'IT Network Engineer (m/w/d)'\n",
      " 'Junior System Engineer ICT (m/w/d)' 'JAVA Engineer (Senior) (m/w/d)'\n",
      " 'System Engineer - Data Mining(56)' 'BI Data Engineer'\n",
      " 'IT Data Engineer (m/w/d)' 'Data Engineer' 'IBM Data Power Engineer(56)'\n",
      " 'Senior Software Backend Engineer (m/w/d)'\n",
      " 'Automation/Hybrid Test Engineer'\n",
      " 'Senior Data Engineer (Technisch) (m/w/d)' 'Database Engineer (m/w/d)'\n",
      " 'IT System Engineer mit Schwerpunkt Industrie (m/w/d)'\n",
      " 'Professional Azure Data Engineer (m/w/d)'\n",
      " 'MSSQL Database Engineer (m/w/d)' 'Java Software Engineer (m/w/d)'\n",
      " 'SharePoint Online Architect / Solutions Engineer'\n",
      " 'ICT Senior System Engineer (m/w/d)'\n",
      " 'Digital Biomarker Biomedical Engineer' 'System Cloud Engineer (m/w/d)'\n",
      " 'Software Engineer Java (m/w/d)'\n",
      " 'TELEMATIKER/IN (Solution Engineer Data/Voice)'\n",
      " 'Software Engineer - Blockchain' 'BPM Engineer Senior Appian Developer'\n",
      " 'ICT System Engineer - Microsoft Technologies (m/w/d)'\n",
      " 'Big Data Engineer' 'Professional Storage System Engineer (m/w/d)'\n",
      " 'Cloud Data Architect mit Flair für AI' 'Senior System Engineer (m/w/d)'\n",
      " 'Internship in Mechanical Development Hydrogen (m/f/x)'\n",
      " '.NET Software Engineer – Zurich' 'Senior Network Specialist (m/w/d)'\n",
      " 'Global Automation Lead Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 :\n",
      "['Data Engineer (w/m) | 80% oder mehr'\n",
      " 'Big Data & Platform Engineer 80-100%'\n",
      " 'Data Engineer Messtechnik (m/w/d)' 'Data Engineer / Analyst (m/f/d)'\n",
      " 'Senior Power BI Engineer (m/w/d)' 'Data- & Integration Engineer (w/m/d)'\n",
      " 'Postdoctoral Fellow, Retinal Organoid Bioengineering (m/f/d)'\n",
      " 'Data Management Engineer (m/f)' 'DevOps Data Engineer 100%'\n",
      " 'Data Engineer / Analyst'\n",
      " 'Belle opportunité pour un/e Data Engineer Junior - Urgent - Banque'\n",
      " 'Mobile Software Engineer Flutter' 'Data Management Engineer (w/m/d)'\n",
      " 'Scientist/Engineer at the cryoEM facility at Biozentrum'\n",
      " 'Azure Data Engineer - Spark' 'IT Data Engineer'\n",
      " \"Manager de l'équipe Data Engineer\"\n",
      " 'Data Engineer mit mehrjähriger Erfahrung im SQL Bereich (m/w/d)'\n",
      " 'Senior Data Science Engineer & Business Intelligence Developer (Power BI) (m/w/d)'\n",
      " 'ICT System Engineer (m/w/d) Datacenter und Cloud'\n",
      " 'Big Data Engineer (m/w/d)' 'IT System Engineer - Data Center (m/w/d)'\n",
      " 'Data Engineer (m/w/d)' 'Data Engineer - Sustainability (m/w/d)'\n",
      " 'System Engineer Data Services (m/w/d)'\n",
      " 'IT System Engineer - Data Services (m/w/d)'\n",
      " 'Data Engineer/Data Analyst Business Intelligence (m/w/d)'\n",
      " 'Senior System Engineer Data Services (m/w/d)'\n",
      " 'IT System-Engineer 80-100% (m/w/d) mit Fokus Microsoft Cloud-Services (55981)'\n",
      " 'Data Center Engineer' 'Data Science Lead (m/w/d)'\n",
      " 'Cloud Data Infrastructure Engineer (m/w/d)'\n",
      " 'Junior Data Engineer (m/f/d)' 'Cloud & Data Engineer'\n",
      " '(Junior) Data Architect / Data Engineer'\n",
      " 'Data Center Infrastructure Engineer (m/w/d)'\n",
      " 'Azure Data Engineer / Developer' 'Data & Analytics Specialist'\n",
      " 'System Engineer - Data Mining(56)' 'BI Data Engineer'\n",
      " 'IT Data Engineer (m/w/d)' 'Data Engineer' 'IBM Data Power Engineer(56)'\n",
      " 'Senior Data Engineer (Technisch) (m/w/d)' 'Database Engineer (m/w/d)'\n",
      " 'Professional Azure Data Engineer (m/w/d)'\n",
      " 'MSSQL Database Engineer (m/w/d)' 'System Cloud Engineer (m/w/d)'\n",
      " 'TELEMATIKER/IN (Solution Engineer Data/Voice)' 'Big Data Engineer'\n",
      " 'Cloud Data Architect mit Flair für AI']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.20 Turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Turkey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 :\n",
      "['Key Account Engineer (w/m/x)' 'Technology Service Engineer (Turkey)'\n",
      " 'DevOps Engineer / IT Support (m/w/d) - Remote/(Home) Office'\n",
      " 'Service Engineer - Turbocharging / Servis Mühendisi - Turbocharging (80-100%)'\n",
      " 'DevOps Engineer - Remote' 'Data Engineer - Remote'\n",
      " 'Machine Learning Engineer With Data Science Experience' 'Data Engineer'\n",
      " 'Big Data & AI Engineer' 'Network Data Solutions Delivery Engineer'\n",
      " 'Engineer, Field Applications'\n",
      " '2023 Bilişim Vadisi Stajyer İstihdam Programı' 'Stajyer'\n",
      " 'Analytics Engineer' 'Continuous Improvement Engineer'\n",
      " 'Customer Engineer, Data Analytics, Google Cloud'\n",
      " 'Software Test Engineer' 'QA Engineer' 'Artificial Intelligence Engineer'\n",
      " 'Üretim Sorumlusu' 'Computer Vision Engineer' 'Civil Engineer (m/f/d)'\n",
      " 'HSE Engineer' 'Senior Data Engineer'\n",
      " 'Full stack Developer - Angular deneyimli önemli - Remote'\n",
      " 'DWH ETL Engineer - Remote' 'Platform Engineer - Devops - Remote'\n",
      " 'Engineer - Product Definition - Hybrid' 'Jr DevOps Engineer'\n",
      " 'Smart Start Production Engineer | Industry 4.0'\n",
      " '【Remote IT Engineer】Global Project Manager(English and IT Project Manage)'\n",
      " 'Developer in Test - Remote' 'Cut&Sew Engineer'\n",
      " 'Manufacturing Process Engineer'\n",
      " 'Field Service Engineer (eMobility Turkey)'\n",
      " 'Software & Big Data Engineer' 'Mechanical Maintenance Engineer'\n",
      " 'Artificial Intelligence - Machine Learning Engineer'\n",
      " 'Database Engineer - Remote' 'Factory Planning Engineer'\n",
      " 'Software Engineer - Data Platform'\n",
      " 'Autonomous Driving Software Engineer' 'Smart Start Project Engineer'\n",
      " 'Application Engineer - Downstream' 'IT Infrastructure Engineer'\n",
      " 'Project Engineer' 'DevOps Engineer' 'EDA Support Engineer'\n",
      " 'Software Engineer - IT'\n",
      " 'Smart Start Automation Engineer @Special Machinery'\n",
      " 'Software Engineer, Javascript'\n",
      " 'Early Career Program: LEAD - Field Engineer (Turkey)2023'\n",
      " 'Full Stack Software Engineer'\n",
      " 'Software Development Engineer (.NET Full Stack)'\n",
      " 'Smart Start Purchasing Engineer'\n",
      " 'Structural Engineer – Istanbul, Turkey' 'data engineer'\n",
      " 'Front-End Developer' 'Smart Start Logistics Planning Engineer' 'Intern'\n",
      " 'Software Engineer: Marketing Technology'\n",
      " 'Solution Engineer - Turkey (Istanbul/ Ankara)'\n",
      " 'Software Development Engineer - Frontend'\n",
      " 'Software Development Engineer (C#/.Net)' 'Devops Engineer'\n",
      " 'Welding Process Expert / Continuous Improvement Engineer'\n",
      " 'Quality Control Engineer' 'Automation Engineer'\n",
      " 'AWS DevOps Engineer - Remote - $66,000'\n",
      " 'Technology Engineer - Site Reliability Engineer(SRE)'\n",
      " 'Industrial Communication Test Engineer' 'Platform Engineer'\n",
      " 'Test Engineer' 'Web Tasarımı (Front - End)'\n",
      " 'Design Engineer, RF Design Engineering' 'Full Stack Engineer'\n",
      " 'R&D Process Engineer' 'Devops System Engineer'\n",
      " 'Electronic Hardware Design Engineer' 'Field Service Engineer'\n",
      " 'Angular deneyimli Full Stack Web Developer Aranıyor - Remote'\n",
      " 'Service Specialist I - Logging and Perf'\n",
      " 'Planning Engineer @Siemens Mobility Türkiye'\n",
      " 'Smart Start Logistics Engineer'\n",
      " 'Research & Development Biomedical Engineer'\n",
      " 'Software Developer/ Engineer' 'SRE / Site Reliability Engineer'\n",
      " 'C++ Software Development Engineer' 'Environmental Engineer'\n",
      " 'Service Engineer' 'Production Engineer'\n",
      " 'Senior Software Engineer – CargoWise Customs, Turkey'\n",
      " 'Mid Level Site Reliability Engineer' 'Product Reliability Engineer'\n",
      " 'IT support & Ops Engineer'\n",
      " 'Frontend Software Development Engineer (Industrial Computing)'\n",
      " 'Software Engineer' 'Embedded Software Engineer'\n",
      " 'Senior Software development Engineer' 'Senior React Engineer (Remote)'\n",
      " 'Site Installation Engineer @Siemens Mobility Türkiye' 'Data Engineer II'\n",
      " 'Research And Development Engineer' 'Cloud Engineer (Kubernetes)'\n",
      " 'Security and Personal Data Protection Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 :\n",
      "['Data Engineer - Remote' 'Data Engineer' 'Big Data & AI Engineer'\n",
      " 'Network Data Solutions Delivery Engineer' 'Analytics Engineer'\n",
      " 'Customer Engineer, Data Analytics, Google Cloud' 'Senior Data Engineer'\n",
      " 'DWH ETL Engineer - Remote' 'Software & Big Data Engineer'\n",
      " 'Database Engineer - Remote' 'Software Engineer - Data Platform'\n",
      " 'data engineer' 'Planning Engineer @Siemens Mobility Türkiye'\n",
      " 'Site Installation Engineer @Siemens Mobility Türkiye' 'Data Engineer II'\n",
      " 'Cloud Engineer (Kubernetes)'\n",
      " 'Security and Personal Data Protection Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.21 United_Kingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'United_Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 :\n",
      "['Data Engineer' 'Data Engineer - Submarines' 'Data Engineer (Junior)'\n",
      " 'Senior Data Engineer' 'Machine Learning Data Engineer'\n",
      " 'Data Engineer (100% Remote)' 'Prover BI Engineer'\n",
      " 'Geospatial Data Engineer' 'Trainee Data Engineer'\n",
      " 'Data Engineer - London' 'Data Engineer/ ETL Engineer'\n",
      " 'Junior Data Engineer' 'Lead Data Engineer' 'Graduate Data Engineer'\n",
      " 'Data Pipeline Engineer' 'Financial Crime Data Engineer' 'BI Engineer'\n",
      " 'Power BI Developer' 'Customer Lead Data Engineer' 'Data Engineer (Java)'\n",
      " 'Azure Data Engineer' 'Data Developer Engineer'\n",
      " 'Junior Software Engineer (C#, SQL)'\n",
      " 'Analyst, Data Engineer, AI & Data, 12 Week Placement (FTC / Secondment)'\n",
      " 'GCP Data engineer' 'Azure Data Engineer - 6 month contract'\n",
      " 'UK Internship Programme - Data Engineering'\n",
      " 'Software Engineer - Data Platform' 'Data Engineer - Snowflake'\n",
      " 'Data Engineer - Outside IR35 - Python, Spark, SQL'\n",
      " 'Data DevOps Engineer' 'Data Engineer - Geospatial'\n",
      " 'Graduate Data Engineer - Bristol' 'Junior SQL engineer'\n",
      " 'Forward Deployed Data Engineer' 'Data Platform Engineer'\n",
      " 'Data Engineer - Remote' 'Data Engineer - UK Remote'\n",
      " 'Power BI Developer - (Berkshire / Remote)'\n",
      " 'Data Specialist - Power BI Developer' 'Junior SQL Data Engineer'\n",
      " 'SQL Data Engineer' 'Data Engineer | UK, Wide'\n",
      " 'Spark / Scala Data Engineer' 'CMA1828 Data Engineer'\n",
      " 'Data Engineer - Outside IR35 - Python, Spark, PySpark'\n",
      " 'Senior Azure Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 :\n",
      "['Data Engineer' 'Data Engineer - Submarines' 'Data Engineer (Junior)'\n",
      " 'Senior Data Engineer' 'Machine Learning Data Engineer'\n",
      " 'Data Engineer (100% Remote)' 'Prover BI Engineer'\n",
      " 'Geospatial Data Engineer' 'Trainee Data Engineer'\n",
      " 'Data Engineer - London' 'Data Engineer/ ETL Engineer'\n",
      " 'Junior Data Engineer' 'Lead Data Engineer' 'Graduate Data Engineer'\n",
      " 'Data Pipeline Engineer' 'Financial Crime Data Engineer' 'BI Engineer'\n",
      " 'Power BI Developer' 'Customer Lead Data Engineer' 'Data Engineer (Java)'\n",
      " 'Azure Data Engineer' 'Data Developer Engineer'\n",
      " 'Analyst, Data Engineer, AI & Data, 12 Week Placement (FTC / Secondment)'\n",
      " 'GCP Data engineer' 'Azure Data Engineer - 6 month contract'\n",
      " 'UK Internship Programme - Data Engineering'\n",
      " 'Software Engineer - Data Platform' 'Data Engineer - Snowflake'\n",
      " 'Data Engineer - Outside IR35 - Python, Spark, SQL'\n",
      " 'Data DevOps Engineer' 'Data Engineer - Geospatial'\n",
      " 'Graduate Data Engineer - Bristol' 'Forward Deployed Data Engineer'\n",
      " 'Data Platform Engineer' 'Data Engineer - Remote'\n",
      " 'Data Engineer - UK Remote' 'Power BI Developer - (Berkshire / Remote)'\n",
      " 'Data Specialist - Power BI Developer' 'Junior SQL Data Engineer'\n",
      " 'SQL Data Engineer' 'Data Engineer | UK, Wide'\n",
      " 'Spark / Scala Data Engineer' 'CMA1828 Data Engineer'\n",
      " 'Data Engineer - Outside IR35 - Python, Spark, PySpark'\n",
      " 'Senior Azure Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "            job_title=row['Job_title']\n",
    "        ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.22 USA 🦅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'United_States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 :\n",
      "['Junior Data Engineer (Remote)' 'AWS Data Engineer' 'Data Engineer'\n",
      " 'Azure Data Engineer' 'Data Engineer - REMOTE' 'Data Engineer - Flink'\n",
      " 'Snowflake Data Engineer' 'Software/Data Engineer (Entry'\n",
      " 'Data Analytics Engineer - GCP' 'MLOps Engineer / Data Scientist'\n",
      " 'Data Pipeline Engineer' 'Big Data Engineer'\n",
      " 'Azure Data Engineer (US citizens or GC holders) - Locals ONLY'\n",
      " 'Data Analytics Engineer' 'GCP Data Engineer'\n",
      " 'Big Data Developer (Data Engineer)' 'Big Data Operations Engineer'\n",
      " 'Healthcare AWS Data Engineer' '100% REMOTE // GCP Data Engineer'\n",
      " 'Data Engineer I' 'Sr. Data Engineer' 'Senior Data Engineer'\n",
      " 'Google Cloud Data Engineer' 'Data Engineer II' 'Network Data Engineer'\n",
      " 'Data Engineer with Analytics Background'\n",
      " 'Data Engineer with PL/SQL Developer' 'Senior Data Engineer, Analytics'\n",
      " 'Data Migration Engineer' 'Data Engineer Level 3'\n",
      " 'Data Engineer (Mid/Jr)' 'Data Integration Engineer(Banking)'\n",
      " 'Data Visualization Engineer' 'Software Engineer (Data)'\n",
      " 'Jr. Data Engineer' 'Big Data Engineer - PySpark'\n",
      " 'Snowflake Data Engineer (Azure)' 'Python Data Engineer'\n",
      " 'AWS Data engineer' 'Engineer, Data' 'Azure Data engineer (Remote)'\n",
      " 'Data Engineer (REMOTE NATIONWIDE)' 'Data/ETL Engineer'\n",
      " 'Data Solutions Engineer I' 'Data Engineer- Remote'\n",
      " 'Data Engineer Associate' 'Data Engineer/Data Analyst'\n",
      " 'Data Engineer, Hybrid, Local Texas only'\n",
      " 'Senior Azure Data Bricks Engineer' 'Azure Data Engineer - Healthcare'\n",
      " 'Data Engineer on W2' 'Data Engineer 2' 'Azure Data Engineer architect'\n",
      " 'Remote Data Engineer' 'Sr Cloud Data Engineer' 'Data Solutions Engineer'\n",
      " 'Data Engineer - Remote' 'Sr. Cloud Data Engineer'\n",
      " 'GCP Data Engineer Onsite' 'Sr. Data Engineer (Hybrid)'\n",
      " 'Software Engineer, Data Engineering' 'DATA ENGINEER'\n",
      " 'Cloud Data Engineer' 'Software/Data Engineer' 'Data Engineer- Strategy'\n",
      " 'Data Engineer (Azure)' 'Data Engineer III' 'Junior Data Engineer'\n",
      " 'Data Engineer - Onsite' 'Data Center Engineer'\n",
      " 'Software and Data Junior Engineer' 'Associate Data Engineer'\n",
      " 'Data Insights Engineer' 'Azure Data engineer' 'Data Recon Engineer'\n",
      " 'Data Engineer - Data Analytics' 'Data Engineer - Jr/Mid Level (Remote)'\n",
      " 'Data Software Engineer' 'Data Engineer (P)'\n",
      " 'Snowflake Data Engineer pipeline' 'Data Warehouse Engineer'\n",
      " ': Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 :\n",
      "['Junior Data Engineer (Remote)' 'AWS Data Engineer' 'Data Engineer'\n",
      " 'Azure Data Engineer' 'Data Engineer - REMOTE' 'Data Engineer - Flink'\n",
      " 'Snowflake Data Engineer' 'Software/Data Engineer (Entry'\n",
      " 'Data Analytics Engineer - GCP' 'Data Pipeline Engineer'\n",
      " 'Big Data Engineer'\n",
      " 'Azure Data Engineer (US citizens or GC holders) - Locals ONLY'\n",
      " 'Data Analytics Engineer' 'GCP Data Engineer'\n",
      " 'Big Data Developer (Data Engineer)' 'Big Data Operations Engineer'\n",
      " 'Healthcare AWS Data Engineer' '100% REMOTE // GCP Data Engineer'\n",
      " 'Data Engineer I' 'Sr. Data Engineer' 'Senior Data Engineer'\n",
      " 'Google Cloud Data Engineer' 'Data Engineer II' 'Network Data Engineer'\n",
      " 'Data Engineer with Analytics Background'\n",
      " 'Data Engineer with PL/SQL Developer' 'Senior Data Engineer, Analytics'\n",
      " 'Data Migration Engineer' 'Data Engineer Level 3'\n",
      " 'Data Engineer (Mid/Jr)' 'Data Integration Engineer(Banking)'\n",
      " 'Data Visualization Engineer' 'Software Engineer (Data)'\n",
      " 'Jr. Data Engineer' 'Big Data Engineer - PySpark'\n",
      " 'Snowflake Data Engineer (Azure)' 'Python Data Engineer'\n",
      " 'AWS Data engineer' 'Engineer, Data' 'Azure Data engineer (Remote)'\n",
      " 'Data Engineer (REMOTE NATIONWIDE)' 'Data/ETL Engineer'\n",
      " 'Data Engineer- Remote' 'Data Engineer Associate'\n",
      " 'Data Engineer/Data Analyst' 'Data Engineer, Hybrid, Local Texas only'\n",
      " 'Senior Azure Data Bricks Engineer' 'Azure Data Engineer - Healthcare'\n",
      " 'Data Engineer on W2' 'Data Engineer 2' 'Azure Data Engineer architect'\n",
      " 'Remote Data Engineer' 'Sr Cloud Data Engineer' 'Data Engineer - Remote'\n",
      " 'Sr. Cloud Data Engineer' 'GCP Data Engineer Onsite'\n",
      " 'Sr. Data Engineer (Hybrid)' 'Software Engineer, Data Engineering'\n",
      " 'DATA ENGINEER' 'Cloud Data Engineer' 'Software/Data Engineer'\n",
      " 'Data Engineer- Strategy' 'Data Engineer (Azure)' 'Data Engineer III'\n",
      " 'Junior Data Engineer' 'Data Engineer - Onsite' 'Data Center Engineer'\n",
      " 'Software and Data Junior Engineer' 'Associate Data Engineer'\n",
      " 'Data Insights Engineer' 'Azure Data engineer' 'Data Recon Engineer'\n",
      " 'Data Engineer - Data Analytics' 'Data Engineer - Jr/Mid Level (Remote)'\n",
      " 'Data Software Engineer' 'Data Engineer (P)'\n",
      " 'Snowflake Data Engineer pipeline' 'Data Warehouse Engineer'\n",
      " ': Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[salary_type].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.23 Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Japan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 :\n",
      "['【rinna Summer Internship】技術職・Technical Role (Software Engineer Intern)'\n",
      " 'Internship (Frontend Engineer)' 'Data Engineer/Scientist'\n",
      " 'Technical Support Engineer - Fukuoka' 'Application Engineer'\n",
      " 'GCP Professional Data Engineerの通信講座講師' 'IT Support engineer'\n",
      " 'Principal Test Engineer' 'Mechanical (Physical Simulation) Engineer'\n",
      " 'Data/Business Intelligence Engineer opening!'\n",
      " 'データサイエンティスト / Data Scientist' 'VED_Vehicle Integration CAN HMI Engineer'\n",
      " 'Internship (AI Engineer)' 'Wind Turbine Load Engineer'\n",
      " 'Data Center Engineer' 'Electrical Engineer I'\n",
      " 'Bilingual IT Support Engineer (Desktop & Network Infra)'\n",
      " 'JR network engineer' 'Software Engineer - Data Platform' 'Data Engineer'\n",
      " 'IT system engineer 1' 'Technical Support Engineer - Nagoya'\n",
      " 'Desktop Support' 'データ・エンジニア / Data Engineer, Search Engine Analysis'\n",
      " 'Data Center Engineer (Remote Hands)'\n",
      " 'Staff Software Engineer - Applications Backend' 'Project Engineer'\n",
      " 'データエンジニア (リアルタイムデータパイプライン) / Data Engineer (Realtime Data Pipiline)'\n",
      " 'Global software engineer' 'Data Center Customer Operations Engineer III'\n",
      " 'Quality Engineer' 'IT Support Engineer - Tokyo'\n",
      " 'Service Engineer, Nagakute'\n",
      " 'Field Services Engineer / Technical Customer Support'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Deployment Engineer (Core Application)' 'Azure data engineer (DTS-180)'\n",
      " 'Customer Quality Engineer'\n",
      " 'Data and Digital Engineer Staff/Manager, Customer Experience Group, BIAHJ'\n",
      " 'Senior Software Engineer (English Speaking job)'\n",
      " 'データ サイエンティスト / データ エンジニア Data Scientist / Data Engineer'\n",
      " 'Technical Support Engineer' 'Senior Software Engineer - Data Platform'\n",
      " 'IT Support Engineer' 'CW CS HMI JP Field Service Engineer'\n",
      " 'Automation Data Governance Engineer / オートメーション データガバナンス エンジニア'\n",
      " 'Snr Electrical Engineer II' 'Computer Vision Engineer' 'MRP Controller'\n",
      " 'Civil Engineer' 'Software Engineer (CFE)' 'Local ISO'\n",
      " 'Data Platform Engineer' 'Embedded software development engineer'\n",
      " 'FT: Data Engineer' 'FY23 Graduate Engineer, Field Applications'\n",
      " 'Application engineer (off highway)'\n",
      " 'ADAS Test Operator / Engineer - ADASテストエンジニア'\n",
      " 'Staff Engineer, Field Applications' 'Engineer, Field Applications'\n",
      " 'Advanced Application Development Engineer – Polymer & Electronics Additive'\n",
      " 'Senior Backend Software Engineer'\n",
      " 'Software development engineer for embedded devices'\n",
      " 'Internship (Secure Computing Engineer)' 'Desktop Network Support'\n",
      " 'Incoming Quality Engineer'\n",
      " '【中途採用】プロセスエンジニア_福島会津工場_Manufacturing Process Engineer (GaN)'\n",
      " 'Senior Principal Layout Engineer'\n",
      " 'Associate Manager, Development Engineering Packaging'\n",
      " 'Field service engineer Yokkaichi Japan'\n",
      " 'IT Technician Help Desktop Support Visa Support Available'\n",
      " 'Software Engineer, Mercari Summer Internship 2023 (Internship)'\n",
      " 'Quality Supervisor'\n",
      " 'Network Engineer - Electronic Trading Systems Infra'\n",
      " 'VED_ESS_Customer Project Manager_Regident Engineer' 'Operations Manager'\n",
      " 'Radar MMIC System / Application engineer for automotive applications'\n",
      " 'Sr Engineer, Quality'\n",
      " 'ソフトウェアエンジニア (機械学習) / Software Engineer (Machine Learning)'\n",
      " 'Assistant Facilities Manager'\n",
      " 'New grads Recruitment 2024(Data Center Critical Facilities II) / 2024年卒 新卒採用 (クリティカルファシリティエンジニア)'\n",
      " 'Senior Client Engineer, Tools & Pipeline - Lightspeed'\n",
      " 'Field Service Engineer (m/f/x) Japan'\n",
      " 'データ分析基盤エンジニア/Data Analysis Infrastructure Engineer'\n",
      " 'HPC and Research Computing Engineer' 'Cloud Technical Support Engineer'\n",
      " 'DBRE(Database Reliability Engineer)'\n",
      " 'IT Lab Data Mgmt Senior System Engineer'\n",
      " 'Mechanical Systems Engineer / 機械システムズエンジニア'\n",
      " 'SRE(Site Reliability Engineer)'\n",
      " 'IBX Critical Facilities Engineer IV (Tokyo) ファシリティエンジニア4(東京)'\n",
      " 'Service Engineer CP'\n",
      " '【新卒採用】フィールドアプリケーションエンジニア_Field Applications Engineer'\n",
      " 'IT Field Engineer' 'Energy Efficiency Senior Engineer'\n",
      " 'Power Generation Senior Application Engineer - Electrical'\n",
      " '(1)【正社員】データサイエンティスト(Data Scientist)(2)【正社員】システムエンジニア(System Engineer)'\n",
      " 'Senior Field Systems Engineering'\n",
      " 'Director, Yield and Defect Engineering (CFM) - Japan'\n",
      " 'Senior Staff Engineer, Firmware Validation'\n",
      " 'Senior Geotechnical Engineer – Offshore Wind' 'Finance Supervisor'\n",
      " '[XC/NE-JP] DevOps エンジニア / DevOps Engineer'\n",
      " 'アプリ運用保守エンジニア/ Application Operation & Maintenance Engineer'\n",
      " 'Routing and Switching セールス・エンジニア' 'Senior Frontend Software Engineer'\n",
      " 'Senior Web Application Engineer (Frontend)'\n",
      " 'SSG Premium Services Support - Technical Support Engineer - Japan'\n",
      " 'Quality Japan 品質保証カスタマーサービスエンジニア' 'Web Application Engineer'\n",
      " 'データソリューションズエンジニア/ Data Solutions Engineer'\n",
      " 'Quality Japan 品質保証カスタマーサービスエンジニア ホンダ・スバル'\n",
      " 'ファシリティエンジニア/ Facility Engineer' 'Sales Engineer, IND - Japan'\n",
      " 'Senior Backend Engineer(Relocate to Japan/Visa support)'\n",
      " 'Google Cloud Platform認定トレーニングの講師'\n",
      " 'Senior Support & Software Engineer, Tokyo (Remote)'\n",
      " 'Production Services Engineer (Data Services)'\n",
      " 'AI Research Engineer at Data Security Startup' 'フルスタック エンジニア'\n",
      " 'ネットワークエンジニア / Network Engineer'\n",
      " 'Collaboration and Messaging Technical Support Engineer'\n",
      " 'Senior Staff Engineer' 'Lead, Network Engineer'\n",
      " 'ソフトウェアエンジニア (SEO) / Software Engineer (SEO)'\n",
      " '【Inspection Robot】Mechanical Engineer' 'Facility Engineering Specialist'\n",
      " 'Bilingual IT Support Engineer (Junior/Mid-level) - Financial Services Industry'\n",
      " 'Specialist Engineer, Technical Support'\n",
      " 'IBX Critical Facilities Engineer Ⅳ (Osaka) ファシリティエンジニア4(大阪)'\n",
      " 'SRE/SecOps Engineer_EN [23a-008]'\n",
      " 'Data Engineer, Data/インターネット/Webサービス・ASP'\n",
      " 'Site Reliability/DevOps Engineer' 'CAD Designer'\n",
      " 'IBX Critical Facilities Engineer Ⅴ(Saito) / IBXクリティカルファシリティーエンジニアⅤ(彩都)'\n",
      " 'PLM Project Manager (Any PLM experience is okay)'\n",
      " 'Software Engineer (Backend, Kotlin), Fukuoka'\n",
      " '[XC/NE-JP] Expert for Technology Scouting / 新規技術スカウティング・エキスパート'\n",
      " '游戏后台开发工程师 (Backend Development Senior Engineer)'\n",
      " 'construction management and maintenance work'\n",
      " 'Seat Product Design Engineer & Customer Interface'\n",
      " '【Small Satellite】RF/Electrical Engineer'\n",
      " 'Cybersecurity Delivery Engineer' 'データエンジニア ・ Data Engineer'\n",
      " 'IT Systems Development and Operation Engineer (Front & Back End Process)'\n",
      " 'MBF_Manager, Business Intelligence and Risk Management'\n",
      " 'Engineering Manager, Platform Engineering – Mercari' 'QA Engineer'\n",
      " 'Senior Engineer, Firmware Validation'\n",
      " 'IBX Critical Facilities Engineer Ⅴ(Inzai) / IBXクリティカルファシリティーエンジニアⅤ(印西)'\n",
      " 'Ruby on Rails Engineer'\n",
      " '【Blockchain Engineer】 NO NEED JLPT Visa Support Available!!'\n",
      " 'New opening - SRE - Senior Site Reliability Engineer!'\n",
      " 'Data Reliability Engineer/データリライアビリティエンジニア'\n",
      " 'Senior Site Reliability Engineer' 'CAD Engineer'\n",
      " 'Data Site Reliability Engineer' 'ML Engineer/モバイル/アプリサービス'\n",
      " 'Full-stack Engineer (FinTech Software Company - Moneytree)'\n",
      " 'Data Engineer/ データエンジニア' 'Senior Python Software Engineer'\n",
      " 'Satellite Operations Engineer/衛星運用エンジニア'\n",
      " '【Small Satellite】Mechanical Engineer'\n",
      " 'Software Engineer, Frontend / ソフトウェアエンジニア (Frontend) – Merpay'\n",
      " 'Senior DevOps Engineer' 'バックエンド エンジニア'\n",
      " 'Mission Planning Systems Engineer (MPSE)' 'Cyber Security Engineer'\n",
      " 'Engineer Stuff' 'Software Engineer / LINE Account Platform'\n",
      " 'Sr. Site Reliability Engineer - U.S. Search Engine Platform SRE'\n",
      " '【SRE】/モバイル/アプリサービス' 'Senior Software Engineer'\n",
      " 'Backend Engineer / バックエンドエンジニア' 'クラウドデータエンジニア / Cloud Data Engineer'\n",
      " 'Senior Game Client Engineer' 'フルスタックエンジニア / Full Stack Engineer'\n",
      " 'Ruby Senior Software Engineer - Cutting Edge Tech Company'\n",
      " 'DevOps Engineer' 'AI Engineer at Japanese IT Servicer' 'Data engineer'\n",
      " 'Backend Engineer - Japan'\n",
      " 'Desk-side IT Support Engineer [Finance Industry]' 'Voice Engineer'\n",
      " '【NEW! 】TaxTeam-Engineer (Client Technology Hub)' 'アナリティクスエンジニア'\n",
      " '前橋オープンポジション - テクノロジー コンサルティング本部' 'Solutions Engineer, Japan'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " 'Software Engineer, Backend / ソフトウェアエンジニア (Backend) – Mercoin'\n",
      " 'iOS Engineer/モバイル/アプリサービス' 'Hardware Support Engineer'\n",
      " 'Field Application Engineer / System Engineer/NAS業界のリーディングカンパ'\n",
      " 'Network Engineer' 'Senior Client Engineer/Graphics Engineer, Lightspeed'\n",
      " 'Senior Backend Engineer' 'Business Development Manager' 'PHPエンジニア'\n",
      " 'Software Engineer - Workflow' 'Developer Solutions Engineer, Japan'\n",
      " 'Senior Account Manager IT Software (B2B)'\n",
      " 'Senior Data Centre Network Engineer -(~1150万)' 'Scalaエンジニア'\n",
      " 'DevOps Engineer for Telecom domain project'\n",
      " 'Software Engineer, Data Platform / ソフトウェアエンジニア (Data Platform) – Merpay'\n",
      " '【Small Satellite】Ground System Backend Engineer'\n",
      " '【Warehouse】Field Engineer【Autonomous Transportation Robot】'\n",
      " '2024 新卒採用 通販・ネット販売' 'Full Stack Software Engineer - Up to 10M/Yr.'\n",
      " 'Security Engineer' 'Network Engineer - Wireless Networks'\n",
      " 'Analog Design Engineer for High Speed SerDes'\n",
      " 'Security Engineer (Threat Detection Engineering) – Mercari'\n",
      " 'Valve Design Engineer'\n",
      " 'インテリジェント\\u3000オートメーション\\u3000コンサルタント / エンジニア - テクノロジー コンサルティング本部 (TfLS)'\n",
      " 'ソリューション・エンジニア - テクノロジー コンサルティング本部 (ITS-AIF)'\n",
      " 'Oracleコンサルタント / エンジニア - テクノロジー コンサルティング本部 (BIO・ITS-Oracle)'\n",
      " 'Bilingual IT End User Support Engineer (on-site & remote)'\n",
      " 'Support Engineer, Tier-3' 'AI Engineer【Publishing Industry】'\n",
      " 'PLM Technical Lead (Teamcenter)'\n",
      " '[Automated System] Robot/Vision Engineer *No Japanese okay' 'データエンジニア'\n",
      " 'Big Data Engineer' 'SAP Data Migration Lead'\n",
      " '【Strength in 3D】Computer Vision Engineer【Venture】'\n",
      " 'Software Engineer, Backend / ソフトウェアエンジニア (Backend) – Merpay'\n",
      " 'Datacenter Network Technician L2' 'Sales Engineer (URGENT Role)'\n",
      " 'Android Engineer/モバイル/アプリサービス'\n",
      " 'Senior Client Engineer, Game Engine - Lightspeed'\n",
      " 'Field Specialist - Turbomachinery'\n",
      " 'Infrastructure Security Engineer / セキュリティセンター'\n",
      " '[Machine learning, AI utilization] AI engineer'\n",
      " 'リードエンジニア/lead engineer ~台湾・香港国籍の方々活躍中!~*N3 Japanese needed'\n",
      " 'CRMシニアコンサルタント' 'Network Engineer III'\n",
      " 'CDP・BIエンジニア / CDP・BIコンサルタント - ソング本部'\n",
      " 'クラウドアーキテクト / エンジニア / コンサルタント – テクノロジー コンサルティング本部(ICE)'\n",
      " 'Electronics System Engineer (On-Board Computer)/ OBCエンジニア'\n",
      " 'Remote Sensing Specialist【Technical Sales】'\n",
      " 'データマネジメントスペシャリスト / Data Management Specialist' 'Back End Engineer'\n",
      " 'MLOps Engineer【Publishing Industry】' '【Image Processing】iOS Engineer'\n",
      " 'アナログ半導体世界シェアNo.1' 'Senior Network Engineer'\n",
      " 'Telecom Engineer - Earn up to 12M/Yr.'\n",
      " 'Machine Learning Engineer - Business level Japanese' 'ITコンサルタント(アプリ)'\n",
      " '【バックエンドエンジニア(データエンジニア)】マネーフォワードケッサイ(Data Forward Group)_東京(田町)'\n",
      " 'Bilingual IT Support Engineer (Junior/Mid-level)'\n",
      " 'Business Solutions Architect'\n",
      " 'セキュリティインフラエンジニア / Security Infrastructure Engineer'\n",
      " '未経験不可/Natural Language Processing Engineer' 'Storage/Server Engineer'\n",
      " 'Senior Applications Engineer (Ruby)'\n",
      " '[No onsite / work in Kobe ] System engineer, programmer (server / application)'\n",
      " 'AIOps Engineer at Data Security Startup'\n",
      " 'Analytics Service Full Stack Engineer (Cloud Integration)'\n",
      " 'Shared Services Engineer' 'DevOps Engineer【Robot Venture】'\n",
      " 'Backend Engineer【Robot Venture】'\n",
      " '[Issuance/distribution optimization solution]AI Engineer'\n",
      " 'Java開発者/Javaエンジニア Java Developer/Java Engineer'\n",
      " '【Inspection Robot】Sr. Mechanical Engineer - Production'\n",
      " '[Communication Robot]\\u3000Software Engineer'\n",
      " 'Data Engineer【Big Data・Fintech・IoT・AI】'\n",
      " 'Network Delivery Engineer - ネットワークエンジニア' 'Software Engineer – Mercari'\n",
      " 'その他コンピュータ関連職' 'Software Engineer, Platform Engineering – Mercari'\n",
      " 'Ruby Engineer @Fast-Growing Mobile App Development Company!'\n",
      " 'ビッグデータ部 サーチ課:サーチデータサイエンティスト(BDD)'\n",
      " '【Aquaculture】IoT SW Engineer【Full-Stack】'\n",
      " 'Travel Development部:開発エンジニア (大阪 TDD)'\n",
      " '【Satellite Imagery Processing Pipeline】DevOps Engineer'\n",
      " '【Satellite】Web Backend Engineer'\n",
      " 'WMS Product Manager | Top Global E-Commerce Company'\n",
      " 'English Only - Data Engineer'\n",
      " 'G0016_【AI・データサイエンティスト】AI・機械学習を用いたビジネスモデル創出' 'データサイエンティスト・エンジニアリング'\n",
      " 'Webサービスエンジニア(ネットワーク・サーバー・データベース)'\n",
      " 'Project Manager-big e-commerce company'\n",
      " 'Cloud Architect/Engineer/Consultant'\n",
      " 'Backend Engineer Multinational IT Company' 'データサイエンティスト・アナリスト'\n",
      " '[Issuance/distribution optimization solution] MLOps Engineer'\n",
      " 'Webフロントエンドエンジニア'\n",
      " 'English Speaking Network engineer (conversational Japanese)'\n",
      " '【クラウドエンジニア(AWS)】【業務委託(準委任)】データレイクエンジニア'\n",
      " 'コマースカンパニービジネスサポート開発部 ECビジネスサポート課(大阪):Salesforceエンジニア(BSD)'\n",
      " 'Business Intelligence (BI) Engineer' '【Urgent】Infrastructure Engineer'\n",
      " 'レジャープロダクト開発部 オートエコサービス開発課:プロデューサー(LPD大阪支社)'\n",
      " 'Senior Technical Support Engineer - Japan'\n",
      " 'Servicenowエンジニア N I DRIVE株式会社'\n",
      " 'エンジニア・アーキテクト::アプリケーションエンジニア / Application Engineer'\n",
      " 'Windchill Project Manager' 'DBエンジニア(DXデータ分析基盤構築/SOMPOホールディングス)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 :\n",
      "['Data Engineer/Scientist' 'GCP Professional Data Engineerの通信講座講師'\n",
      " 'Data/Business Intelligence Engineer opening!'\n",
      " 'Wind Turbine Load Engineer' 'Data Center Engineer'\n",
      " 'Software Engineer - Data Platform' 'Data Engineer'\n",
      " 'データ・エンジニア / Data Engineer, Search Engine Analysis'\n",
      " 'Data Center Engineer (Remote Hands)'\n",
      " 'データエンジニア (リアルタイムデータパイプライン) / Data Engineer (Realtime Data Pipiline)'\n",
      " 'Data Center Customer Operations Engineer III'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Azure data engineer (DTS-180)'\n",
      " 'Data and Digital Engineer Staff/Manager, Customer Experience Group, BIAHJ'\n",
      " 'Senior Software Engineer - Data Platform'\n",
      " 'Automation Data Governance Engineer / オートメーション データガバナンス エンジニア'\n",
      " 'Data Platform Engineer' 'FT: Data Engineer'\n",
      " 'Senior Client Engineer, Tools & Pipeline - Lightspeed'\n",
      " 'データ分析基盤エンジニア/Data Analysis Infrastructure Engineer'\n",
      " 'IT Lab Data Mgmt Senior System Engineer'\n",
      " 'Production Services Engineer (Data Services)'\n",
      " 'AI Research Engineer at Data Security Startup'\n",
      " 'Data Engineer, Data/インターネット/Webサービス・ASP'\n",
      " 'Site Reliability/DevOps Engineer' 'データエンジニア ・ Data Engineer'\n",
      " 'Data Engineer/ データエンジニア' 'クラウドデータエンジニア / Cloud Data Engineer'\n",
      " 'Data engineer' 'Senior Data Centre Network Engineer -(~1150万)'\n",
      " 'Software Engineer, Data Platform / ソフトウェアエンジニア (Data Platform) – Merpay'\n",
      " 'Big Data Engineer' 'SAP Data Migration Lead'\n",
      " 'データマネジメントスペシャリスト / Data Management Specialist'\n",
      " 'AIOps Engineer at Data Security Startup'\n",
      " 'Data Engineer【Big Data・Fintech・IoT・AI】'\n",
      " 'Ruby Engineer @Fast-Growing Mobile App Development Company!'\n",
      " '【Satellite Imagery Processing Pipeline】DevOps Engineer'\n",
      " 'English Only - Data Engineer' 'Cloud Architect/Engineer/Consultant'\n",
      " 'Business Intelligence (BI) Engineer']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.24 Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Singapore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 :\n",
      "['Data Engineer, ETL (SG)'\n",
      " 'SAS Data Engineer, Data & Analytics, Technology Consulting'\n",
      " 'Data Engineer' 'Junior Data Engineer'\n",
      " 'Associate Data Centre Engineer (fresh graduates are welcome to apply)'\n",
      " 'Data Centre Cabling & Hardware Support Engineer'\n",
      " 'Data Centre Engineer, Analyst' 'Machine Learning / Data Engineer'\n",
      " 'Data Center Engineer'\n",
      " 'Associate Data Engineer, Data & Analytics - Technology Consulting'\n",
      " 'Machine Learning Engineer - Data Solutions Center'\n",
      " '[LTA-ITCD] DATA ENGINEER / SCIENTIST'\n",
      " 'Data Center Critical Facilities Engineer' 'Data Scientist - Payments'\n",
      " 'Data Engineer – Data Analytics, Global Wholesale Banking (1 year contract)'\n",
      " 'Desktop Support Engineer (Desktop Support L2 + Data Centre)'\n",
      " 'Data Analyst Intern, Regional BI & Planning (Summer 2023)'\n",
      " 'Data Engineer (Taiwan)'\n",
      " 'Data Analyst Intern - Ops Projects, Business Development (Summer 2023)'\n",
      " 'Data Engineer (AWS)' 'Data Engineer (Adobe Tag Management)'\n",
      " 'DevOps Engineer (Data Warehouse, Backend Systems) - 2023 Start'\n",
      " 'Data Engineer (Remote)' 'Data Center Customer Operations Engineer II'\n",
      " 'Informatica/Python Data Engineer' 'Data Centre Engineer #FreshGraduate'\n",
      " 'Cloud Operations Engineer, Associate/Senior, Technology Consulting, Data & Analytics'\n",
      " 'AWS with ETL Data Engineer' 'AI Data Engineer'\n",
      " 'Cloudera Data Platform Engineer (Data Migration)'\n",
      " 'Informatica Data Platform Engineer'\n",
      " 'Technical Data Engineer (1-Year Direct Contract)'\n",
      " 'Data Analytics Engineer'\n",
      " 'Frontend Software Engineer (Aeolus), Data Platform'\n",
      " 'AVP, Big Data Engineer - Spark Development, Data Technology, Technology & Operations'\n",
      " 'Senior Data Engineer' 'Data Management Analyst/Engineer'\n",
      " 'Informatica Data Engineer' 'Data Engineer - ETL Informatica Developer'\n",
      " 'Data Security Engineer (Fresh Grad)'\n",
      " 'Data Engineer (EDW), Group Operations & Technology'\n",
      " 'Data Center Support Engineer' 'Data Science Engineer'\n",
      " 'Data Engineer (12 months contract)' 'Support Engineer (Data Centre)'\n",
      " 'AVP, Big Data Engineer - Spark Development and Support, Middle Office Technology, Technology & Operations'\n",
      " 'Data Engineer - Singapore' 'QA Engineer'\n",
      " 'Lead Data Center Customer Operations Engineer'\n",
      " 'Data Engineer, Data Platform' 'Data Engineer (Python)'\n",
      " 'APAC Data Center Management Engineer'\n",
      " 'End User and VIP Support Engineer, GO&T' 'Data Engineer - TikTok'\n",
      " 'Engineer (Data Center)'\n",
      " 'VP, Big Data Engineer, Middle Office Technology, Technology & Operations'\n",
      " 'Data Production Engineer'\n",
      " 'AVP, Site Reliability Engineer – CORE Banking, Group Consumer Banking and Big Data Analytics Technology, Technology and Operations)'\n",
      " 'Data Engineer (DART), Data Science & AI Division'\n",
      " 'Associate, Data Engineer (Private Market Solutions), Technology Group'\n",
      " 'VP/AVP, Site Reliability Engineer (Infrastructure), Group Consumer Banking and Big Data Analytics Technology, Technology and Operations)'\n",
      " 'Customer Network Support Engineer-PACO' 'Big Data Engineer'\n",
      " 'Data Scientist, SAPMENA' 'Big Data Support Engineer'\n",
      " 'Analytics Consultant / Data Engineer / Business Intelligence'\n",
      " 'Consultant / Snr Consultant (Data Engineer / ETL / BI)' 'Data Analyst'\n",
      " 'Full-Time Intern: Data Analytics'\n",
      " 'Tech Refresh Engineer (fresh graduates are welcometo apply)'\n",
      " 'Data Center Facilities Engineer'\n",
      " 'VP/ AVP, Data Quality Engineer/ Lead, Middle Office Technology, Technology & Operations'\n",
      " 'VP/AVP, Site Reliability Engineer, Group Consumer Banking and Big Data Analytics Technology, Technology & Operations'\n",
      " 'Senior Data Engineer (Big Data)'\n",
      " 'Software Engineer (Big Data Analysis Applications)'\n",
      " 'IT Support Engineer (1-Year contract) – yearly renewable'\n",
      " 'Cloud Data Engineer' 'Senior Big Data Engineer'\n",
      " 'VP, Data Engineer, Team Lead, Data Technology, Technology & Operations'\n",
      " 'Data Center Operation Engineering Engineer, AWS Infrastructure Operations'\n",
      " 'ACI -Data Center Engineer' 'Cloud Support Engineer'\n",
      " 'Lead IT Support Engineer' 'Data Engineer - Security Engineering'\n",
      " 'Big Data Engineer (Financial Services) Consultant/Senior Consultant, Technology Consulting'\n",
      " 'Systems Engineer (Data and BI)'\n",
      " 'Project Support, Data & Analytics - Technology Consulting (6 months contract)'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Data Platform Engineer'\n",
      " 'PL/SQL, Oracle, Unix Production Support Engineer' 'Data Crawl Engineer'\n",
      " 'Junior Data Consultant - Singapore'\n",
      " 'VP/ AVP, Machine Learning Engineer, Data Technology, Technology & Operations'\n",
      " 'Data Engineer / Senior Data Engineer' 'Deskside Support Engineer'\n",
      " 'Big Data Engineer (Financial Services) Manager/Senior Manager, Technology Consulting'\n",
      " 'Associate/AVP, Data Engineer, Investment Insights Group'\n",
      " 'Data Center Operations Technician, Data Center Operations'\n",
      " 'Data Engineer / Associate Data Engineer, Data Science'\n",
      " 'Senior Staff Engineer Data Scientist'\n",
      " 'Senior Data Engineer (Cyber Defence System)'\n",
      " 'Data Engineer -Global Payments' 'Senior Data Engineer (Digital)'\n",
      " 'Desktop Support Engineer'\n",
      " 'SRE Lead, Consumer Banking Group Technology, Technology & Operations'\n",
      " 'Technical Support Engineer II - Remote'\n",
      " 'Computer Vision Engineer, Data Monetization Technology'\n",
      " 'Artificial Intelligence and Machine Learning Engineer'\n",
      " 'Senior Engineer, Data Pipeline' 'Technology Operations Engineer (Data)'\n",
      " 'Network Support Engineer'\n",
      " 'Data Engineer/Modeller - Text Analytics [ITE Headquarters] (1 Year Contract)'\n",
      " 'Data Scientist' 'Application Support Engineer (SQL) - Permanent, MNC'\n",
      " 'Data Engineer - Systematic Data Platform'\n",
      " 'Data Center Facilities Engineer, Operations'\n",
      " 'Senior Consultant - Data Science & Analytics'\n",
      " 'AVP/ VP, Data Analyst, Data Management' 'Machine Learning Engineer'\n",
      " 'Cyber Security Engineer' 'Senior Data Engineer (Estate Solutions)'\n",
      " 'SVP / VP, Enterprise Architect, EASRE, Technology & Operations'\n",
      " 'Data Engineer Manager/Senior Manager'\n",
      " 'Senior Data Engineer (Big Data Engineering)' 'Site Reliability Engineer'\n",
      " 'Associate Platform and Data Engineer' 'Data Engineer/ Senior Engineer'\n",
      " 'Information Technology - Senior Data Engineer' 'IT Lead'\n",
      " 'Cloud Engineer - IT Systems & Operation [ITE Headquarters]'\n",
      " 'Service Ops Engineer (IT)' 'Business Intelligence Engineer'\n",
      " 'Lead Data Engineer, Standard Chartered nexus'\n",
      " '[LTA-ITCD] SENIOR / EXECUTIVE DATA ENGINEER / DATA ENGINEER'\n",
      " 'Big Data Engineer, Recommendation Architecture'\n",
      " 'Full Stack Engineer (Video Analytics) , Data Science & Artificial Intelligence'\n",
      " 'Vice President - Data Scientist / Machine Learning Engineer, Data Management Office'\n",
      " 'Backend Software Engineer (Aeolus), Data Platform'\n",
      " 'Big Data R&D Engineer - Game Direction' 'Principal Big Data Engineer'\n",
      " 'Data center engineer' 'Facilities Engineer (Data Center)']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 :\n",
      "['Data Engineer, ETL (SG)'\n",
      " 'SAS Data Engineer, Data & Analytics, Technology Consulting'\n",
      " 'Data Engineer' 'Junior Data Engineer'\n",
      " 'Associate Data Centre Engineer (fresh graduates are welcome to apply)'\n",
      " 'Data Centre Engineer, Analyst' 'Machine Learning / Data Engineer'\n",
      " 'Data Center Engineer'\n",
      " 'Associate Data Engineer, Data & Analytics - Technology Consulting'\n",
      " '[LTA-ITCD] DATA ENGINEER / SCIENTIST'\n",
      " 'Data Center Critical Facilities Engineer'\n",
      " 'Data Engineer – Data Analytics, Global Wholesale Banking (1 year contract)'\n",
      " 'Data Engineer (Taiwan)' 'Data Engineer (AWS)'\n",
      " 'Data Engineer (Adobe Tag Management)'\n",
      " 'DevOps Engineer (Data Warehouse, Backend Systems) - 2023 Start'\n",
      " 'Data Engineer (Remote)' 'Data Center Customer Operations Engineer II'\n",
      " 'Informatica/Python Data Engineer' 'Data Centre Engineer #FreshGraduate'\n",
      " 'Cloud Operations Engineer, Associate/Senior, Technology Consulting, Data & Analytics'\n",
      " 'AWS with ETL Data Engineer' 'AI Data Engineer'\n",
      " 'Cloudera Data Platform Engineer (Data Migration)'\n",
      " 'Informatica Data Platform Engineer'\n",
      " 'Technical Data Engineer (1-Year Direct Contract)'\n",
      " 'Data Analytics Engineer'\n",
      " 'AVP, Big Data Engineer - Spark Development, Data Technology, Technology & Operations'\n",
      " 'Senior Data Engineer' 'Data Management Analyst/Engineer'\n",
      " 'Informatica Data Engineer' 'Data Engineer - ETL Informatica Developer'\n",
      " 'Data Security Engineer (Fresh Grad)'\n",
      " 'Data Engineer (EDW), Group Operations & Technology'\n",
      " 'Data Science Engineer' 'Data Engineer (12 months contract)'\n",
      " 'AVP, Big Data Engineer - Spark Development and Support, Middle Office Technology, Technology & Operations'\n",
      " 'Data Engineer - Singapore'\n",
      " 'Lead Data Center Customer Operations Engineer'\n",
      " 'Data Engineer, Data Platform' 'Data Engineer (Python)'\n",
      " 'APAC Data Center Management Engineer' 'Data Engineer - TikTok'\n",
      " 'Engineer (Data Center)'\n",
      " 'VP, Big Data Engineer, Middle Office Technology, Technology & Operations'\n",
      " 'Data Production Engineer'\n",
      " 'Data Engineer (DART), Data Science & AI Division'\n",
      " 'Associate, Data Engineer (Private Market Solutions), Technology Group'\n",
      " 'Big Data Engineer'\n",
      " 'Analytics Consultant / Data Engineer / Business Intelligence'\n",
      " 'Consultant / Snr Consultant (Data Engineer / ETL / BI)'\n",
      " 'Data Center Facilities Engineer' 'Senior Data Engineer (Big Data)'\n",
      " 'Software Engineer (Big Data Analysis Applications)'\n",
      " 'Cloud Data Engineer' 'Senior Big Data Engineer'\n",
      " 'VP, Data Engineer, Team Lead, Data Technology, Technology & Operations'\n",
      " 'Data Center Operation Engineering Engineer, AWS Infrastructure Operations'\n",
      " 'ACI -Data Center Engineer' 'Data Engineer - Security Engineering'\n",
      " 'Big Data Engineer (Financial Services) Consultant/Senior Consultant, Technology Consulting'\n",
      " 'Systems Engineer (Data and BI)'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Data Platform Engineer' 'Data Crawl Engineer'\n",
      " 'Junior Data Consultant - Singapore'\n",
      " 'Data Engineer / Senior Data Engineer'\n",
      " 'Big Data Engineer (Financial Services) Manager/Senior Manager, Technology Consulting'\n",
      " 'Associate/AVP, Data Engineer, Investment Insights Group'\n",
      " 'Data Engineer / Associate Data Engineer, Data Science'\n",
      " 'Senior Data Engineer (Cyber Defence System)'\n",
      " 'Data Engineer -Global Payments' 'Senior Data Engineer (Digital)'\n",
      " 'Senior Engineer, Data Pipeline' 'Technology Operations Engineer (Data)'\n",
      " 'Data Engineer/Modeller - Text Analytics [ITE Headquarters] (1 Year Contract)'\n",
      " 'Data Engineer - Systematic Data Platform'\n",
      " 'Data Center Facilities Engineer, Operations'\n",
      " 'Senior Consultant - Data Science & Analytics'\n",
      " 'Senior Data Engineer (Estate Solutions)'\n",
      " 'Data Engineer Manager/Senior Manager'\n",
      " 'Senior Data Engineer (Big Data Engineering)'\n",
      " 'Associate Platform and Data Engineer' 'Data Engineer/ Senior Engineer'\n",
      " 'Information Technology - Senior Data Engineer'\n",
      " 'Cloud Engineer - IT Systems & Operation [ITE Headquarters]'\n",
      " 'Lead Data Engineer, Standard Chartered nexus'\n",
      " '[LTA-ITCD] SENIOR / EXECUTIVE DATA ENGINEER / DATA ENGINEER'\n",
      " 'Big Data Engineer, Recommendation Architecture'\n",
      " 'Backend Software Engineer (Aeolus), Data Platform'\n",
      " 'Big Data R&D Engineer - Game Direction' 'Principal Big Data Engineer'\n",
      " 'Data center engineer' 'Facilities Engineer (Data Center)']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.25 New Zealand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'New_Zealand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 :\n",
      "['Data Engineer' 'Senior Data Engineer'\n",
      " 'Data Engineer (Auckland or Wellington)'\n",
      " 'Senior Data Operations Engineer' 'Data Engineer Chapter Member'\n",
      " 'Process Control Engineer' 'Junior Systems Engineer'\n",
      " 'Data Engineer - Entry/Senior level' 'Data Engineer - QuantumBlack'\n",
      " 'Data Engineer (Flexible, Hybrid or Full Remote)'\n",
      " 'Engineer Innovation and Development'\n",
      " 'Senior Data Engineer (Auckland or Wellington)' 'Lead Engineer'\n",
      " 'Data Science Trainee'\n",
      " 'Data Engineer/Integration Specialist - Fully remote'\n",
      " 'Junior/Intermediate Software Engineer'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Quality Assurance Engineer - New Zealand/Canada' 'Data Scientist'\n",
      " 'Senior Analytics Engineer' 'Enterprise Wireless Engineer'\n",
      " 'Expressions of Interest- Work in Data & Analytics Consultant/Senior Consultant/Manager'\n",
      " 'SNOC Engineer' 'Senior Data Analyst' 'IT Operations Engineer'\n",
      " 'Data Scientist - QuantumBlack'\n",
      " 'Kaipūhanga Pūnaha Mātāmua | Senior Systems Engineer/Systems Engineer'\n",
      " 'L2 Onsite Desktop Engineer' 'Senior Software Engineer, Evo - Remote'\n",
      " 'Machine Learning Engineer' 'Senior Infrastructure Dev Engineer'\n",
      " 'Product Development Engineer' 'Senior Network Engineer'\n",
      " 'Senior Electronic Engineer' 'Senior Front-end Engineer'\n",
      " 'Lead / Senior Data Engineer - Christchurch'\n",
      " 'Project Engineer (Bridge Structures)'\n",
      " 'Expressions of Interest | Senior Data Engineers' 'Reliability Engineer'\n",
      " 'Engineering Internship in Sweden!'\n",
      " 'Industrial / Process Engineer | Auckland NZ' 'Site Reliability Engineer'\n",
      " 'Senior Product Development Engineer' 'Project Engineer'\n",
      " 'Hardware Reliability Engineer'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Senior Systems Engineer (VMware)' 'IT EDI Support Engineer'\n",
      " 'Rust Engineer (Greenfield project)'\n",
      " 'Business Analysts/Senior Business Analysts' 'Digital Product Designer'\n",
      " 'Principal Front-End Engineer' 'Infrastructure and Operations Manager'\n",
      " 'Technical Lead' 'Sr. Front End Engineer'\n",
      " 'Reliability Engineer - Reporoa' 'Managed Network Engineer'\n",
      " 'Graduate Engineer' 'Desktop Network Support' 'Senior Platform Engineer'\n",
      " 'Quality Engineer' 'Senior Cloud Engineer' 'Frontend Software Engineer'\n",
      " 'Estimator' 'Site Engineer' 'Environmental Engineer | Wellington'\n",
      " 'Linux System Engineer' 'Network Engineer' 'Data Engineer - Azure Cloud'\n",
      " 'Transportation Engineer or Planner - Abley'\n",
      " 'VIE Equipment/Mechanical Engineer - New Zealand'\n",
      " 'Program for Aspiring Vertiv Engineers' 'Cybersecurity Engineer'\n",
      " 'Controls & Automation Engineer' 'Civil Engineers (Multiple jobs)'\n",
      " 'Senior Mechanical Engineer' 'Insights Analyst'\n",
      " 'Intermediate Systems Software Engineer'\n",
      " '2024 Graduate Transport Engineer' 'Digital Test Engineer'\n",
      " 'Road Network Inspector (Network Engineer)' 'Business Analyst'\n",
      " 'Graduate Product Development Engineer (Mechatronics, Software, Electronics)'\n",
      " 'Software Engineer / Developer' 'Azure DevOps Engineer'\n",
      " 'Staff Software Engineer - Frontend - Performance Platform'\n",
      " 'Software Engineer (React)'\n",
      " 'Embedded & Desktop Linux Systems Engineer - Optimisation'\n",
      " 'Junior Test Automation Engineer' 'Information Technology Tutor'\n",
      " 'Research & Development Chemist' 'Customer Engineer - Thermal, NZ'\n",
      " 'Wireless Power - Test and Automation Engineer'\n",
      " 'Senior Cloud PHP / JS Software Engineer'\n",
      " 'Water Utility Monitoring Technician' 'GIS Analyst'\n",
      " 'Virtualisation Engineer' 'Verification Engineer (Mechanical)'\n",
      " 'Product Manager' 'Power Systems Engineer' 'Software Engineers - 7 roles'\n",
      " 'Database Performance Consultant - MySQL (Remote)' 'Reservoir Engineer'\n",
      " 'Software Engineer - Platform Services' 'Database Engineer'\n",
      " 'Senior Software Engineer' 'Platform Engineer'\n",
      " 'Associate Director – Quality Engineering Practice Lead'\n",
      " 'Senior Electrical/Mechanical Engineer'\n",
      " 'Product Development Engineer - Laundry'\n",
      " 'Product Development Engineer - Refrigeration'\n",
      " 'Graduate Electrical Engineer' 'Formulation Development Engineer'\n",
      " 'Senior Developer (Backend)' 'Graduate Geotechnical Engineer'\n",
      " 'Staff Software Engineer - Backend (User Platform)'\n",
      " 'Sr. Software Development Engineer' 'Senior Python Developer (NZ)'\n",
      " 'Senior Frontend Developer' 'Engineer I - Mechanical Development & Test'\n",
      " 'Electronics Test Engineer'\n",
      " 'Graduate Electrical Engineer (Building Services)'\n",
      " 'Movio Graduate Software Engineer (Jan 2024 start)'\n",
      " 'Design Research Lead - Product Growth'\n",
      " 'Product Development Engineer - Dishwashing'\n",
      " 'Senior Python Software Engineer' 'Software Support Engineer'\n",
      " 'Tech Lead, Software Development' 'Product Development Test Engineer'\n",
      " '2023-2024 Graduate Opportunities - Water' 'Technical Security Engineer'\n",
      " 'Cyber Security Engineer / Analyst'\n",
      " 'Senior Software Engineer | Typescript' 'Java Software Engineer'\n",
      " 'Kaipūhanga Pūnaha Mātāmua | Senior Systems Engineer'\n",
      " 'MS Engineer (L3) - Cyber Security'\n",
      " 'Principal Engineer - Drinking Water - Network Engineering'\n",
      " '(Remote) Senior Software Engineer, APAC'\n",
      " 'Technical Product Analyst iNFX' 'ELETRICAL TECHNICIAN'\n",
      " 'Express of Interest - Junior Blockchain Developer'\n",
      " 'Team Lead für Data Analytics' 'Mobile Engineer - iOS and Android'\n",
      " 'Senior Embedded Systems Engineer' 'Senior Infrastructure Engineer'\n",
      " 'Product Design Group Lead - Collaboration'\n",
      " 'Azure Cloud Solutions Architect' 'Sport : Development Operations'\n",
      " 'Application and Technical Support Specialist'\n",
      " 'Application Support Engineer (Tier2-3)' 'Product Management Specialist'\n",
      " 'Senior Application Support Engineer (Tier4)'\n",
      " 'JO-1809-063Desktop EUC Support Specialist (Multiple Openings L1, L2, L3) AKL004'\n",
      " 'Technical Support Engineer - SIP / VOIP (UC)' 'IT Support Engineer'\n",
      " 'Support Engineer - AV Specialist (CONTRACT)'\n",
      " 'Scrum Master and Product Lead'\n",
      " 'Technical Director – Electrical Engineer' 'Automation Engineer'\n",
      " 'Software Specialist - Testing Chapter - Transactional Banking Platform, Treasury & Trading'\n",
      " 'Data Migration Technical Lead' 'Systems Engineer - Hamilton'\n",
      " 'Communications Systems Architect' 'Configuration Analyst'\n",
      " 'Road Maintenance Engineer'\n",
      " 'Senior Distribution / Asset and Standards Engineer -...'\n",
      " 'Field Technician - Onehunga'\n",
      " 'Silicon Alliances Business Development Lead'\n",
      " 'Planview Functional/ Solution Lead'\n",
      " 'Tradesperson / Trades Assistant -Locomotive Servicing'\n",
      " 'Maintenance Technician' 'Senior Business Analyst'\n",
      " 'Transpower 2024 Graduate Operations Planning Engineers' 'Surveyor'\n",
      " 'Engineering Manager - SaaS Software & Platform'\n",
      " 'Automation Engineer (Intermediate)' 'Integration Cloud Engineer'\n",
      " 'Senior iOS Engineer' 'Project / Mechanical Design Engineer'\n",
      " 'Project Coordinator' 'Control Systems Integrator'\n",
      " 'Propulsion Analyst, In-Space Propulsion'\n",
      " 'Senior Software Engineer (C++)' 'Molecular Pathology Technician - Scion'\n",
      " 'Senior Backend/Full Stack Developer' 'Regional Accountant'\n",
      " 'Senior Systems Engineer (Cloud)'\n",
      " 'Network Design Estimator - Waipa Networks'\n",
      " 'Propulsion Engineer - Spaceplane'\n",
      " 'System Delivery and Modernisation - Senior Consultant/Manager'\n",
      " 'Cloud Consultant'\n",
      " 'Kaiwhakahaere Horanga Hangarau | Manager – Technology Delivery'\n",
      " 'Senior Content Designer - Design Systems'\n",
      " 'Product Design Lead - Expression of Interest' 'LAME Avionics'\n",
      " 'Principal Engineer - Stormwater - Network Engineering'\n",
      " 'Hydrologist and Flood Modeller' 'Full Stack PHP/JS Engineer Team Lead'\n",
      " 'Supervisor - Preventative and Corrective Gas Maintenance'\n",
      " 'Mechanical Test Engineer | Structures'\n",
      " 'Senior Test Engineer - Mobile and API' 'Senior Automation Test Engineer'\n",
      " 'Intermediate Full Stack Engineer'\n",
      " 'Junior to Intermediate Fullstack Developer - Auckland or singapore'\n",
      " 'Performance & Test Automation Engineer'\n",
      " 'Technical Engineering Lead (FE) - Pro Growth (NZ remote)'\n",
      " 'Intermediate Geotechnical Engineer / Engineering Geologist - Wellington'\n",
      " 'Automation Engineer (Intermediate or Senior)'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " 'Senior Engineering Geoscientist' 'HEAVY TRANSPORT ENGINEER'\n",
      " 'Intermediate Back-end Engineer' 'Intermediate .NET Developer'\n",
      " 'Senior Infrastructure Development Engineer' 'Civil Project Engineer'\n",
      " 'Staff / Lead Engineer' 'Senior DevOps Engineer - Residential'\n",
      " 'Security Engineer – Infrastructure & Cloud' 'Partnership Manager'\n",
      " 'Sales Engineer, NZ' 'Sales Engineer'\n",
      " 'Environmental Scientist | Wellington'\n",
      " 'Senior Technical Security Specialist' 'Project Engineer- Wellington'\n",
      " 'Product Owner - Clinical Platform'\n",
      " 'Senior Front End Developer - Remote/Flex JOB ID - 127'\n",
      " 'Senior Security Engineer' 'Geothermal Geoscientist'\n",
      " 'Senior Full Stack Engineers - International Candidates Welcome'\n",
      " 'Full Stack Software Engineer - Platform Services'\n",
      " 'Geotechnical Engineer, Auckland' 'Test Automation Practice Lead'\n",
      " 'Platform Engineer / DevOps Engineer' 'Associate Systems Engineer | TOC'\n",
      " 'Sales Support Manager - Auckland, New Zealand Lead our Team of Technical Sales Engineers to Co'\n",
      " 'System Engineer - Transport' 'Flight Software Engineer II'\n",
      " 'HSEQ Advisor' 'Environmental Engineer | Auckland'\n",
      " 'Programme Test Manager'\n",
      " 'Asset Manager, Northern Region (Auckland and Whangārei)'\n",
      " 'Senior Full Stack Developer - Budgeting & Forecasting'\n",
      " 'Senior Hydrologist and Flood Modeller' 'NLP & Algorithm Engineer'\n",
      " 'Field Engineer | Wellington' '2024 Graduate GIS Consultant'\n",
      " 'Electrical Building Services Engineer' 'Senior Algorithm Developer'\n",
      " 'Security Engineering Manager - Security Partnerships (NZ remote)'\n",
      " 'Digital Designer - McKinsey Digital' 'Technician - Napier'\n",
      " 'Fire Protection Engineer' 'Cloud / Modern Workplace Engineer - Contract'\n",
      " 'Quality Assurance Technician' 'Geotechnical Engineer'\n",
      " 'GNC Engineer II - Neutron' 'Maintenance Planner / Scheduler'\n",
      " 'Technical Services Administrator' 'Data Centre Operator'\n",
      " 'Drafting & BIM Coordinator' 'Front End Principal Engineer'\n",
      " 'Transport Planner' 'Digital Executive - FMCG'\n",
      " 'Senior Transport Planner, Community Shaping (Tauranga)'\n",
      " 'Project Management Assistant (6 Months Fixed Term)'\n",
      " '(Sr) DevOps Engineer - New Zealand, Fiji (GMT+12)'\n",
      " 'Java Developer (3429)' 'Demand Planner'\n",
      " 'Senior/Intermediate Security Technician'\n",
      " 'Senior DevOps Engineer - New Zealand' 'Supervisor - Mining'\n",
      " 'Security Architect (software engineering focused)'\n",
      " 'Senior Cybersecurity Engineer | Wellington' 'Engineering Geologist'\n",
      " 'Senior Site Reliability Engineer' 'Research Chemist Team Leader'\n",
      " 'GNC Engineer II - Mission Design'\n",
      " 'Compliance Analyst - Water Services Team'\n",
      " 'Senior & Lead Data Engineer / Consultant'\n",
      " 'Senior System Engineer - Transport'\n",
      " 'Expression of Interest | Senior Salesforce Developer'\n",
      " 'Senior Full Stack TypeScript Engineer'\n",
      " 'Building Services Commissioning Engineer'\n",
      " 'Senior or Principal Ecologist' 'Principal Carbon & Net Zero Advisor'\n",
      " 'Senior Roading Asset Engineer' 'Environmental Engineer | Waikato'\n",
      " 'Principal Engineer'\n",
      " 'Expression of Interest | Senior Salesforce Functional Consultant'\n",
      " 'Sr. Oracle Database Administrator'\n",
      " '2 x Technical Business Analysts - Contract' 'Cloud Engineer | Contract'\n",
      " 'Business Intelligence Analyst' 'Project Manager (Plant Projects)'\n",
      " 'Environmental Scientist / Contaminated Land Consultant'\n",
      " 'Platform Engineer | Wellington'\n",
      " 'Associate, Strategic Transport Consultant (Tauranga)'\n",
      " 'Projects and Maintenance Technician' 'Design Architect'\n",
      " 'Enterprise Architect' 'Digital Executive - Toys'\n",
      " 'C++ Software Engineer II']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 :\n",
      "['Data Engineer' 'Senior Data Engineer'\n",
      " 'Data Engineer (Auckland or Wellington)'\n",
      " 'Senior Data Operations Engineer' 'Data Engineer Chapter Member'\n",
      " 'Data Engineer - Entry/Senior level' 'Data Engineer - QuantumBlack'\n",
      " 'Data Engineer (Flexible, Hybrid or Full Remote)'\n",
      " 'Senior Data Engineer (Auckland or Wellington)'\n",
      " 'Data Engineer/Integration Specialist - Fully remote'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Senior Analytics Engineer'\n",
      " 'Expressions of Interest- Work in Data & Analytics Consultant/Senior Consultant/Manager'\n",
      " 'Lead / Senior Data Engineer - Christchurch'\n",
      " 'Expressions of Interest | Senior Data Engineers' 'Senior Cloud Engineer'\n",
      " 'Data Engineer - Azure Cloud' 'Senior Cloud PHP / JS Software Engineer'\n",
      " 'Database Performance Consultant - MySQL (Remote)' 'Database Engineer'\n",
      " 'Team Lead für Data Analytics' 'Azure Cloud Solutions Architect'\n",
      " 'Data Migration Technical Lead' 'Integration Cloud Engineer'\n",
      " 'Senior Systems Engineer (Cloud)' 'Cloud Consultant'\n",
      " 'Senior Test Engineer - Mobile and API'\n",
      " 'Security Engineer – Infrastructure & Cloud'\n",
      " 'Cloud / Modern Workplace Engineer - Contract'\n",
      " 'Drafting & BIM Coordinator' 'Senior & Lead Data Engineer / Consultant'\n",
      " 'Sr. Oracle Database Administrator' 'Cloud Engineer | Contract']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.26 Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Australia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 :\n",
      "['Data Centre Cabling & Hardware Support Engineer' 'Data Engineer'\n",
      " 'Expressions of Interest - Data Engineer, Data & Analytics'\n",
      " 'Data Engineer (Melbourne)' 'Senior Data Engineer' 'Graduate Engineer'\n",
      " 'Data Engineer - ANZ Plus' 'Field Engineer - APAC' 'Engineer'\n",
      " 'NAB Technology Internship Program July 2023'\n",
      " 'Junior Support Engineer – Remote Australia' 'Mechanical Engineer'\n",
      " 'Data & Control Systems Engineer' 'Specialist Data engineer'\n",
      " 'Mining Engineer' 'Informatica Data Engineer'\n",
      " 'Production Improvement Engineer' 'Data Engineer/Snowflake Consultant'\n",
      " 'Data/Systems Engineer – Vehicle Monitoring'\n",
      " 'Associate Engineer Intern - Data Centre Solutions'\n",
      " 'Data Engineer - All Australia Locations'\n",
      " 'Data Engineer - Spark and PySpark | Data and Analytics' 'DATA ENGINEER'\n",
      " 'Senior Software Engineer, Data Safety' 'Data Engineer Roles'\n",
      " 'Storage Engineer | Data Centre Projects'\n",
      " 'Data Analytics/ Data Governance/ Data Engineer Expression of Interest'\n",
      " 'Data Platform Engineer' 'Data Engineer - Data Portfolio'\n",
      " 'Data Engineer - FinOps' 'Desktop Support Engineer'\n",
      " 'Senior - Drill & Blast Engineer' 'Data & AI Solutions Engineer'\n",
      " 'Data Center Customer Operations Engineer IV' 'Data Engineer - Quantexa'\n",
      " 'Big Data Engineer' 'Data Science and Software Engineer Analyst'\n",
      " 'Data Analyst/Engineer/Scientist' 'Azure Data Engineer'\n",
      " 'Data Engineer / Data Analyst' 'Lead Data Engineer'\n",
      " 'AWS Data Engineer - Sydney'\n",
      " 'HV Electrical Design Engineer - Data Centers, APAC Data Center Design Engineering'\n",
      " 'Process Engineer' '2023 Software Engineer Intern'\n",
      " 'Backend Software Engineer (TikTok Live) - 2023 Start'\n",
      " 'Data Center Technical Operations Engineer, Infraops DCEO'\n",
      " 'Operations Data Infrastructure Engineer, Global Operations Engineering'\n",
      " 'Machine Learning Engineer' 'Senior Shield Engineer'\n",
      " 'Data Analytics Engineer' 'R&D Algorithms Engineer'\n",
      " 'Graduate Systems Engineer' 'Graduate Civil Engineer'\n",
      " 'Water Engineer - Tailings & Water' 'Telecommunications Field Engineer'\n",
      " 'Data Engineers - Technology Consulting - Data & Analytics'\n",
      " 'Junior Software Engineer - Identity & Access'\n",
      " 'Software Engineering Internship - Summer 2023/2024 (Sydney)'\n",
      " 'Graduate Mining Engineer' 'Associate/Graduate Engineer' 'Field Engineer'\n",
      " 'Application Support Engineers | Data Centre Projects'\n",
      " 'Data Engineering Manager - ANZ Plus'\n",
      " 'Software Engineer - All Australia Locations'\n",
      " '2024 ANZ Plus Graduate Program' 'Data Engineer (All levels)'\n",
      " 'Project Engineer' 'ASD 4, 5, 6 Data Professionals'\n",
      " 'Engineer Reliability - Signals & Systems' 'Data Analyst'\n",
      " 'Support Engineer' 'Process Improvement Engineer'\n",
      " 'Associate Process Engineer Plating' 'Data engineer - Azure cloud'\n",
      " 'Raw Materials Process Engineer'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Junior Identity Engineer' 'Engineer - ANZPlus'\n",
      " 'Data Engineer - AWS Redshift'\n",
      " '2023 Summer Internship - Software Engineering - Sydney'\n",
      " 'Expressions of Interest - Work in Data & Analytics'\n",
      " 'Power BI Data Engineer and Manager']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 :\n",
      "['Data Engineer'\n",
      " 'Expressions of Interest - Data Engineer, Data & Analytics'\n",
      " 'Data Engineer (Melbourne)' 'Senior Data Engineer'\n",
      " 'Data Engineer - ANZ Plus' 'Data & Control Systems Engineer'\n",
      " 'Specialist Data engineer' 'Informatica Data Engineer'\n",
      " 'Data Engineer/Snowflake Consultant'\n",
      " 'Data/Systems Engineer – Vehicle Monitoring'\n",
      " 'Associate Engineer Intern - Data Centre Solutions'\n",
      " 'Data Engineer - All Australia Locations'\n",
      " 'Data Engineer - Spark and PySpark | Data and Analytics' 'DATA ENGINEER'\n",
      " 'Senior Software Engineer, Data Safety' 'Data Engineer Roles'\n",
      " 'Storage Engineer | Data Centre Projects'\n",
      " 'Data Analytics/ Data Governance/ Data Engineer Expression of Interest'\n",
      " 'Data Platform Engineer' 'Data Engineer - Data Portfolio'\n",
      " 'Data Engineer - FinOps' 'Data Center Customer Operations Engineer IV'\n",
      " 'Data Engineer - Quantexa' 'Big Data Engineer'\n",
      " 'Data Science and Software Engineer Analyst'\n",
      " 'Data Analyst/Engineer/Scientist' 'Azure Data Engineer'\n",
      " 'Data Engineer / Data Analyst' 'Lead Data Engineer'\n",
      " 'AWS Data Engineer - Sydney'\n",
      " 'HV Electrical Design Engineer - Data Centers, APAC Data Center Design Engineering'\n",
      " 'Data Center Technical Operations Engineer, Infraops DCEO'\n",
      " 'Operations Data Infrastructure Engineer, Global Operations Engineering'\n",
      " 'Data Analytics Engineer'\n",
      " 'Data Engineers - Technology Consulting - Data & Analytics'\n",
      " 'Data Engineering Manager - ANZ Plus' 'Data Engineer (All levels)'\n",
      " 'Engineer Reliability - Signals & Systems' 'Data engineer - Azure cloud'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Data Engineer - AWS Redshift' 'Power BI Data Engineer and Manager']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.27 Hong Kong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Hong_Kong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 :\n",
      "['AI/ML Engineer' 'Data Engineer'\n",
      " 'Data Scientist / Big Data Engineer (Fresh Graduate and Interim Welcome)'\n",
      " 'Data Engineer (BigData)' 'Graduate Trainee Engineer Program 2023'\n",
      " 'Data Engineer (Financial/ E-commerce)'\n",
      " 'Graduate Machine Learning Engineer 2022-2023' 'IECC Engineer'\n",
      " 'Remote Data Engineer' 'Data Support Engineer' 'Deep Learning Engineer'\n",
      " 'Software Engineer – Intern' 'Data Engineer - Mox'\n",
      " 'Senior Data Engineer - IC' 'Machine Learning Engineer'\n",
      " 'Junior Frontend Engineer (Remote)' 'Data Engineer (Hong Kong)'\n",
      " 'Data Centre Engineer'\n",
      " 'Data Engineer/ Scientist/ Analyst/ Governance (30K-85K)'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'AIML - Siri Language Engineer for Hong Kong Cantonese'\n",
      " 'Cybersecurity Engineer (English+Cantonese speaking)'\n",
      " 'Assistant Engineer' 'Data Scientist'\n",
      " 'IT Technical Engineer / Support Engineer (HKD 23 - 35k / month)'\n",
      " 'Senior Engineer/Consultant (Video Analytics / IoT / Big Data) - IC'\n",
      " 'AI Engineer'\n",
      " 'Regional Data Scientist - MNC | Machine Learning & Data Mining | TH-8464'\n",
      " 'Data Engineer / Data Architect / Data Scientist (E-commerce | Market place | APAC start up)'\n",
      " 'Data Engineer - Conglomerate' 'Junior Biology Engineer'\n",
      " 'Graduate Engineer' 'Project Engineer'\n",
      " '17229-Senior Data Mining Engineer' 'Machine Learning Engineer x 1'\n",
      " 'Graduate Trainee'\n",
      " 'Duty Engineer (Sha Tin Data Centre) [Ref no.: 20010058]'\n",
      " 'Web Engineer (React)'\n",
      " 'AI/ML - AI/ML - Siri Language Engineer for Hong Kong Cantonese'\n",
      " 'Network Engineer (Voice & Data Team)'\n",
      " 'Application Support Engineer / Consultant (Fresh graduate is also welcome)'\n",
      " 'Process Engineer (Fresh Graduate / IANG Holder Is Welcome )'\n",
      " 'Remanufacturing Process Engineer'\n",
      " 'Junior Programmer / Graduate Software Engineer (Welcome Fresh Graduate)'\n",
      " 'APAC Global Expansion Engineer, Data Center Design Engineering'\n",
      " 'Computer Vision ML Engineer' 'Backend Engineer (Fintech)'\n",
      " 'Web Frontend Engineer' 'Python Backend Engineer'\n",
      " '(PERM /URGENT) Data Engineer' 'Engineering Systems Engineer'\n",
      " 'Azure Data Engineer x 1' 'Data Engineer - Public Cloud Exposure'\n",
      " 'Assistant Resident Engineer 駐地盤助理工程師 (Job ID# 10238)'\n",
      " 'Assistant Business Operation Engineer'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Senior Engineer - Data Engineering & Analytics'\n",
      " 'Senior Data Engineer, Data & Analytics - Hong Kong'\n",
      " 'Hong Kong - Data Engineering Graduate Programme'\n",
      " 'Data Engineer/50-60K/Real Estate' 'AI / Computer Vision Engineer'\n",
      " 'Electrical Engineer | Data Center | Kwai Chung'\n",
      " 'HVAC Engineer | Data Center | Kwai Chung'\n",
      " 'Data & Analytics Engineer - Actuarial Technology'\n",
      " 'Engineer , Polymers & Silicon based Mater'\n",
      " 'Software Engineer — AI Industry' 'Technology - Data Engineer'\n",
      " 'Graduate Engineer - Logistics Automation' 'Data Center Engineer'\n",
      " 'Business Intelligence Engineer (Power BI)'\n",
      " 'Senior Data Engineer (Data & Analytics)' 'Cloud Data Engineer'\n",
      " 'DevOps Engineer(12 Months Contract, 41K)'\n",
      " 'Data Architect + Data Team for leading IT solutions in APAC'\n",
      " 'Data Engineer (New Media Business)'\n",
      " 'Electronics Engineer (Science park/14 Month Package/ 5days work)'\n",
      " 'Senior Data Engineer' 'Backend Engineer - IC' 'IT Support Engineer'\n",
      " 'Senior Machine Learning Ops Engineer (MLOps) -Strong Python (Up to 65K)'\n",
      " 'B1 Engineer (Defect Solution)' 'Data Architect/Engineer x 1'\n",
      " 'Sales Engineer (AI Solutions)'\n",
      " 'Senior Consultant, Machine Learning Engineer, Technology Consulting'\n",
      " 'Software Engineer - Big Data (Java/Scala/Spark) - Trading'\n",
      " 'Data Center Operator/ NOC Engineer (Upto 40K X13 guarantee)'\n",
      " 'Maintenance Support Engineer - Control and Communication Systems'\n",
      " 'Senior Data Scientist / Senior Data Engineer, Data & Analytics - Hong Kong'\n",
      " 'IT Engineer (Fresh Graduates welcomed)'\n",
      " '[Fintech] Junior DevOps Engineer'\n",
      " 'Machine Learning Engineer (Up to HKD 1 mil)'\n",
      " 'BI Architect (Luxury Retail)'\n",
      " 'Thermal Engineer (Fresh Graduate / IANG Holder Is Welcome )'\n",
      " '(Senior) CAE Engineer (Fresh Graduate / IANG Holder Is Welcome)'\n",
      " 'Senior Backend Engineer' 'Senior data engineer'\n",
      " 'Senior Data Scientist, Data and Analytics - Hong Kong'\n",
      " 'Data Engineer /Top Financial Company – HK$35K – HK$65K/m'\n",
      " 'Data Engineer - New Media Division'\n",
      " 'Testing & Commissioning Engineer | Data Center | Kwai Chung'\n",
      " 'DigitalBCG|GAMMA AI Software Engineer'\n",
      " 'DevOps Engineer (Intern/graduate), Financial Technologies'\n",
      " 'Assistant Engineer / Graduate Engineer / Trainee'\n",
      " 'Data Center Engineer (Cabling)'\n",
      " 'Mechanical Engineer (Fresh Graduate / IANG Holder Is Welcome )'\n",
      " 'Senior Data Center Engineer (Facility Management / Electrical) - CK'\n",
      " 'Project Engineer (Mechanical)' 'Support Engineer – Network'\n",
      " 'Data Center Customer Operations Engineer II' 'Engineer, IP Networking'\n",
      " 'Senior Service Engineer - Automation and Digitalization'\n",
      " 'Electronic Engineer / Senior Electronic Engineer in Motion Control (Fresh Graduate is welcome)'\n",
      " 'Assistant Manager, Customer Analytics & Data Engineering'\n",
      " 'Associate / Senior Associate - Data Analyst / Engineer - Actuarial - Hong Kong'\n",
      " 'Data Engineer/50'\n",
      " '(Senior) Vibration Engineer (Fresh Graduate / IANG Holder Is Welcome )'\n",
      " 'Planning Engineer' 'Senior Programming Engineer (Capital Works)'\n",
      " 'Data Analyst / Engineer' 'Senior Data Scientist, Payment Optimization'\n",
      " 'Senior Electronic Engineer/ Electronic Engineer for IoT Applications (R&T Development)'\n",
      " 'IT System engineer' 'Deskside Support Engineer'\n",
      " 'Facilities Engineers, BMS Engineer, Cabling Engineer - Data Center'\n",
      " 'SENIOR MIXED-SIGNAL ENGINEER' 'Data Engineer (ETL)'\n",
      " 'Project Engineer - Data Center' 'Engineer, BMS Data Center'\n",
      " 'Radio Network Optimization Engineer'\n",
      " 'Data Engineer (Business Intelligence Projects)' 'Component Engineer'\n",
      " 'Solutions Engineer, AVP' 'Assistant Engineer, Innovative Solutions'\n",
      " 'Solutions Consultant Investment Solutions' 'Sr Electronic Engineer'\n",
      " 'Engineer, Backend - Mox' 'Data Engineer / Lead Data Engineer x 2'\n",
      " 'Data Center Operator – on shift'\n",
      " 'Senior Software Engineer / Software Engineer'\n",
      " 'Data Engineer (IoT / Innovations / Data Streaming)'\n",
      " 'Engineer (Production Planning)'\n",
      " 'Data Centre Engineer (Electrical License Grade A) (No Shift Duty) - CK'\n",
      " 'Sr. Electronic Engineer / Electronic Engineer (A global award winning co)'\n",
      " 'Frontend Engineer - Mox' 'Facilities Engineer'\n",
      " 'Big Data Engineer (AM level, individual contributor, Up to 55k)'\n",
      " '(Senior) Network Engineer (English+Cantonese speaking)'\n",
      " 'Software Engineer (Big Data, Business Intelligent)'\n",
      " 'Process Engineer (Fresh Graduate / IANG Holder Is Welcome)'\n",
      " 'Product Engineer/ Product Support Engineer'\n",
      " 'Critical Facilities Engineer' 'IT Engineer'\n",
      " 'H.264/H.265 Algorithm Engineer' 'Infrastructure Engineer'\n",
      " 'Market Data Integration Engineer'\n",
      " 'Senior BIM Engineer / BIM Engineer / Asst BIM Engineer'\n",
      " 'XAI R&D Engineer' 'Network Engineer - CK'\n",
      " 'C++ Lead Software Engineer, Equities Technology'\n",
      " 'Technician / Senior Technician'\n",
      " 'Technician/Senior Technician (Builder)/Duty Engineer'\n",
      " 'Engineer or Technician (Double pay/Medical/Bank holiday)'\n",
      " 'Assistant Engineer / Engineer, Switching Network'\n",
      " 'Telecom Operation Engineer (NOC) (Shift duty) - IC'\n",
      " 'Avaya Voice Engineer' 'Software Engineer'\n",
      " 'Research Engineer / Senior Engineer (Ref. No.: AiDLab/RP3-5/R070)'\n",
      " '2023 Graduate Engineer (Building)' 'Engineer'\n",
      " 'Senior Software Engineer (Fintech)' 'Sr. Unix Engineer(HK)'\n",
      " 'Engineer - All levels (Electrical/HVAC/P&D/Fire) (REF: BS)'\n",
      " 'Cloud Engineer' 'Data Center Operation Engineer'\n",
      " 'Data Centre Operations Engineer' 'IDC Electrical Engineer'\n",
      " 'Research Engineer'\n",
      " 'Digital Electronic Engineer in Computer Vision (Fresh Graduate is welcome)'\n",
      " 'Electrical Engineer'\n",
      " 'Mid or Senior Rust Backend Developer (Multiple openings - remote possible)'\n",
      " 'Senior Data Centre Engineer (No Shift)'\n",
      " 'Software Engineer - AI and Machine Learning'\n",
      " 'Embedded System Software Engineer (Fresh Graduate / IANG Holder Is Welcome)'\n",
      " 'Data Engineer (35-50k, financial service)'\n",
      " 'Data Engineer (up to 50k, venture capital)'\n",
      " 'Senior Analyst, Data Analytics'\n",
      " 'IoT Project Engineer - Smart City Solutions'\n",
      " '(Remote) Senior Software Engineer, APAC'\n",
      " 'Lead Engineer (HV Systems) - Data Center' 'Hardware Engineer'\n",
      " 'Systems Engineer - Technology and Application Support - GTS - Hong Kong'\n",
      " 'GM APS Electronic Equities Support Engineer'\n",
      " 'Technical Services Engineer' 'Assistant Operation Engineer (SAC)'\n",
      " 'Software Engineer (Testing Process and Automation Development / Fresh Graduate is welcome)'\n",
      " 'Failure Analysis Engineer (Electronic)'\n",
      " '2023 Graduate Engineer (Building Services / Electrical / Mechanical / Building Sustainability / Fire)'\n",
      " 'Graduate Trainee - Building Services (Scheme “A” - 2023 Intake) (REF: GT)'\n",
      " 'IDC HVAC Engineer' 'Building Engineer' 'Night Shift Engineer'\n",
      " 'Assistant Electronic Engineer'\n",
      " 'Assistant Mechanical Engineer / Technician'\n",
      " 'IDC Testing & Commissioning Engineer'\n",
      " 'IT Trainee/ Data Analyst/ Network Engineer/ Programmer (Fresh Graduates are welcome)'\n",
      " 'Assistant Building Services Engineer'\n",
      " 'Assistant Engineer (Building Services)'\n",
      " 'Software Engineer (Computer Vision / Fresh Graduate is welcome)'\n",
      " 'Sales Specialist, Enterprise Data Solutions' 'Technical Assistant 工程助理'\n",
      " 'Software Engineer (System Programming Development / Fresh Graduate is welcome)'\n",
      " 'Frontend Software Engineer I/II'\n",
      " 'Network Engineer(Fresh Graduates are welcome)'\n",
      " '2023 Graduate Engineer (Building Services)' 'Solution Engineer'\n",
      " 'Technician (5-day work Week)'\n",
      " 'Customer Service Support Engineer (Fresh Graduates are welcome)'\n",
      " 'Integrated Photonics Engineer (Apply-HK-2204-IPE)'\n",
      " 'Software Engineer - Research & Development'\n",
      " 'Technician - Network Operations Centre'\n",
      " 'Graduate Engineer / Associate Engineer' 'Assistant Engineer, Robotics'\n",
      " 'Engineer Trainee / Assistant Engineer' 'Assistant Mechanical Engineer'\n",
      " 'Associate Engineer (Electrical / Mechanical / Building Services/ Energy/ Computer and Electronic)'\n",
      " 'Engineer – All levels (Electrical/HVAC/MVAC/P&D/Fire) (REF: SER)'\n",
      " 'Engineering Trainee / Engineer - Network Operation Center'\n",
      " 'Java Backend Engineer - Data Services'\n",
      " 'QA Asst. Engineer / Engineer Trainee'\n",
      " 'Project Engineer (Mechanical / Electrical / Building Services)'\n",
      " 'Mechanical Engineer (Fresh Graduate / IANG Holder Is Welcome)'\n",
      " 'Field Services Engineer' 'Engineering Trainee'\n",
      " 'Engineer Trainee - Restricted Substance (Testing Lab/Fresh grad welcome)'\n",
      " 'Engineer Trainee / Engineer - Softlines (Report Checking)'\n",
      " 'Trainee -Web / Programmer / Business System Analyst / Data Analyst / System Engineer (Welcome Fresh)'\n",
      " 'Mechanical Engineer' 'Technical Services Engineer – Powerplant/Process'\n",
      " 'Assistant Network Engineer (Fresh Graduate Welcome)'\n",
      " 'Assistant Chief Engineer (5-day work)' 'Engineer / Assistant Engineer'\n",
      " 'Assistant / Technical Support Engineer (20-25K)'\n",
      " 'Assistant Engineer – Maintenance Services (Welcome Fresh Graduate)'\n",
      " 'Facility Engineer/ Technician (Upto $25-45K X 14)'\n",
      " 'Computer Technician/ Assistant Technical Engineer (25K+)'\n",
      " 'Aircraft Engineering Licence Trainee' 'QA Engineer (Fresh grad welcome)'\n",
      " 'Service Engineer (Contract-Based)' 'Asst. Engineer'\n",
      " 'Project Engineer 項目工程師' 'Thermal Engineer'\n",
      " 'Assistant Ground System Engineer' 'Network Engineer'\n",
      " 'Assistant Chief Engineer' 'Mechanical Design Engineer'\n",
      " 'Software engineer' 'EMC Engineer'\n",
      " 'Network Engineer, HK IT Managed Services Provider- Startup, +$416K HKD'\n",
      " 'Computer Operator (Network/Server Support) (Day + night shift) x 1'\n",
      " 'Project Engineer / Project Consultant / Engineer / Consultant (Sustainability)'\n",
      " 'System Engineer' '(Field Service) Engineer – Lifts and Escalators'\n",
      " 'Senior Computer Vision Engineer / Data Science - Software Developer'\n",
      " 'Backend Software Engineer I/II' 'IT Assistant Support Engineer'\n",
      " 'Field Service Engineer (Printer Maintenance)' 'Data Center Operator'\n",
      " 'Assistant Engineer – Tender Support, BMS / ELV Solutions (REF: CESS/BMS)'\n",
      " 'Assistant / Chief Engineer (5-Day Work Week)' 'Field Test Engineer'\n",
      " 'Linux System Engineer'\n",
      " 'Assistant Engineer / Graduate Engineer (Ref.: AE/Civil/JD)'\n",
      " 'IT Infrastructure Engineer' 'Hardware Engineer (Automotive Projects)'\n",
      " 'Technical Support Engineer(技術支持工程師 )' 'Desktop Support Engineer'\n",
      " 'Assistant Engineer / Engineer - PMSD' 'Data Engineer #3788 (Telecom)'\n",
      " 'Software Engineer – FinTech'\n",
      " 'Service Engineer / service Engineer Trainee'\n",
      " 'Assistant Engineer, Software Development' 'R&D Engineer' 'PLC Engineer'\n",
      " 'Graduate Software Engineer' 'Electronic Engineer'\n",
      " 'Analyst/ Associate, Software Engineer (Backend), IT Department'\n",
      " 'Assistant IT Officer (Assistant IT Support Engineer)'\n",
      " 'Lead Technical Services Engineer - eOperations & Avionics Projects'\n",
      " 'Assistant Test Engineer/Trainee Engineer'\n",
      " 'Assistant Engineer / Technician'\n",
      " 'Lead/ Senior/ Research Engineer (Ref No. AC0007)'\n",
      " 'Assistant Engineer / Engineer (Materials)'\n",
      " 'Controls Engineer(Audio/Video)' 'IT System Engineer'\n",
      " 'Assistant System Engineer / System Engineer (valid until 31/03/2023)'\n",
      " 'IT Officer / Technical Support Engineer (Ref: TSE)'\n",
      " 'IT - System Engineer' 'Devops Engineer (Middle)'\n",
      " 'Helpdesk Engineer (IT Support Team)' 'Network Engineer (Night Shift)'\n",
      " 'Cloud Support Engineer' 'Backend Engineer - PM Tools'\n",
      " 'Associate - System Engineer - Global Technology Solutions - Hong Kong'\n",
      " 'IT Service Desk Engineer' 'Assistant Network Engineer'\n",
      " 'System Administrator' 'Engineer (Network Configuration)'\n",
      " 'Blockchain Security DevOps Engineer'\n",
      " 'Resident Building Services Engineer'\n",
      " 'Technical Officer (Electrical) (Ref: PD-ELEC-TO-CT)'\n",
      " 'System Engineer (Systems)'\n",
      " 'Network Engineer/ Senior Network Engineer (IT System Integration)'\n",
      " 'Engineer, Transmission Network'\n",
      " 'Engineer, Firmware Design (Job Ref. No. Apply-HK-2209-EFD)'\n",
      " 'Associate Network Engineer' 'Backend engineer'\n",
      " 'Electronics Engineer (Robotics) (Ref CLR2023-017)'\n",
      " 'Cloud DevOps Engineer'\n",
      " 'Software Engineer, AI & Robotics programming design'\n",
      " 'Security Operation Engineer (KL)' 'Research Support Staff'\n",
      " 'Technical Engineer' 'NOC Assistant Engineer (Kwun Tong)'\n",
      " 'Resident Engineer (Mechanical / Electrical / Building Services)'\n",
      " 'Engineer - Building (Ref.: EB/Bldg/JD)' 'Software Engineer (Paneron)'\n",
      " 'Field Service Engineer' 'Field Service Engineer - Medical Device'\n",
      " 'Network and System Engineer' 'Engineer (ICT Solution)'\n",
      " 'IT Security Engineer' 'Software Engineer (Frontend), IT Department'\n",
      " 'Systematic Operations Engineer' 'Assistant Engineer / Graduate Engineer'\n",
      " 'Technical Support Engineer' 'Software Test Engineer - IC'\n",
      " 'Project Engineer/ Assistant Project Engineer (Double-pay)'\n",
      " 'Service Engineer' 'System Engineer (Infrastructure & Applications)'\n",
      " 'Engineer I-IT Systems' 'Engineer (IP Backbone Network)'\n",
      " 'APS Electronic Equities / Listed Derivative Support Engineer(HK)'\n",
      " 'Software Engineer (R&T Development)'\n",
      " 'Software Engineer (C / C++ Preferred) (Fresh Graduate / IANG Holder Is Welcome )'\n",
      " 'Project Engineer (Multimedia Display Systems) - IC' 'IT Project Manager'\n",
      " 'Senior Engineer / Senior Consultant/ Project Consultant/ Engineer/ Consultant (Technology Consulting / Digital Services)'\n",
      " 'Electrical Project Engineer (MVAC / BMS/ E&M)'\n",
      " 'Senior Engineer / Project Engineer / Engineer - Mechanical (Building Services)'\n",
      " 'Project Engineer (ICT Project)'\n",
      " 'Project Engineer / Assistant Project Engineer'\n",
      " 'Engineer / Project Engineer / Senior Engineer (Electrical / Mechanical / Building Services)'\n",
      " 'Assistant Service Engineer / Service Engineer (valid until 31/03/2023)'\n",
      " '2023 Graduate Engineer (Digital Services)'\n",
      " 'Assistant / Technical Support Engineer (20-25K) 5-day per week'\n",
      " 'System Support Engineer / Desktop Support Engineer'\n",
      " 'Assistant Service Engineer - Sleep Service' 'DevOps Engineer'\n",
      " 'Senior Building Services Engineer / Building Services Engineer'\n",
      " 'Engineer/Project Engineer/Senior Engineer (Building Services/Electrical/Fire Services/Mechanical/Plumbing & Drainage/Public Health)'\n",
      " 'Senior Associate(Cloud DevSecOps Engineer) - Global Technology Solutions - Firmwide Corporate Services - Hong Kong'\n",
      " 'Senior Project Engineer (ELV/ BMS/ Automation)'\n",
      " 'Product Research Engineer'\n",
      " 'Installation Engineer / Specialist (AV Display) (Airport) - IC'\n",
      " 'Assistant Geologist'\n",
      " 'System Administrator / System Engineer (Linux) - IC'\n",
      " 'Assistant Research Engineer' 'Graduate Engineer 見習工程師'\n",
      " 'Software Quality Assurance Engineer' 'Software Engineer - IC'\n",
      " 'Chief Programming Engineer' 'Technical Services Engineer (Avionics)'\n",
      " 'Assistant Data Science Manager'\n",
      " 'System & Software Engineer (Digital Transformation, DX) - IC'\n",
      " 'Controls Engineer(Show Control Systems)' 'Assistant BIM Engineer'\n",
      " 'Graduate Electrical Engineer' 'MVAC - Engineer / Assistant Engineer'\n",
      " 'Project Manager/Engineer/Electronic Engineering Manager/Engineer(Toys/electrical household)Urgent'\n",
      " 'MR Collaboration Research Scientist' 'Calibration Technician'\n",
      " 'Senior Site Reliability Engineer (SRE)'\n",
      " 'Systems Engineer / Senior Systems Engineer'\n",
      " 'Software Engineer (Media Solutions) - IC' '3D Computer Vision Engineer'\n",
      " 'Assistant Planning Engineer'\n",
      " 'Senior Hardware Quality Assurance Engineer'\n",
      " 'Product Engineer (Welcome Fresh graduates)'\n",
      " 'Assistant / E&M / Building Services Engineer (18-30K)'\n",
      " 'Engineer / Planning Engineer' 'Project Building Engineer'\n",
      " 'Assistant Engineer (Ref: GEN-MD-MP-AE-CT)'\n",
      " 'Engineer - Project Management and Service Access'\n",
      " 'Senior System Engineer'\n",
      " '(Senior) Systems Engineer (Banking support) - IC'\n",
      " 'Product/ Industrial Designer Engineer'\n",
      " 'Network Engineer / Senior Network Engineer (Circa 25k – 45k / month)'\n",
      " 'Technical Analyst/System Engineer' 'Technical Business Analyst'\n",
      " 'IT Infrastructure Support Engineer (Cloud) – Top Insurance Company HK$50K-HK$60K/M'\n",
      " 'Controls Engineer' 'Line Maintenance Engineer'\n",
      " 'Senior System Engineer - CK'\n",
      " 'Desktop Support Engineer - In-house enterprise'\n",
      " 'Assistant Line Maintenance Engineer'\n",
      " 'CLOUD ENGINEER - Wealth Management'\n",
      " 'Trainee Satellite Network Operations Engineer'\n",
      " 'Inspection & Technical Support Engineer II (Ref: CS-CI-ITSEII-CT)'\n",
      " 'Database Support Engineer' 'Cloud Data Engineer, AWS - Insurance'\n",
      " 'Project Engineer / Assistant Project Engineer / Technician'\n",
      " 'Technical Services Assistant Engineer (Component Engineering) - Avionics'\n",
      " 'Senior Systems Engineer - Cyber Fusion Center Analyst - Global Technology Solutions - Firmwide Corporate Services - Hong Kong'\n",
      " 'Senior Systems Engineer (VMware and Storage)'\n",
      " 'System Engineer (Ref: JDB/GETS/SE)'\n",
      " 'System Engineer / System Support (1 Year Contract)'\n",
      " 'Engineer (Buiding Services)' 'Technical Officer (Ref: PD-C&A-G2-TO-CT)'\n",
      " 'Assistant Engineer (助理工程師) / Engineer (工程師)'\n",
      " 'Analyst Programmer I (Cyber Info Security)' 'system engineer'\n",
      " 'Customer Engineer - Google Workspace'\n",
      " 'Senior Duty Engineer (5 Days Work Week)' '(Senior) Network Engineer'\n",
      " 'Cloud Application Support Engineer (Fresh Graduates are welcome)'\n",
      " 'Engineer/ Assistant Engineer ( Audio Visual System)'\n",
      " 'Mechatronics / Mechanical Engineer'\n",
      " 'Engineer – Overhead Line (Capital Works)'\n",
      " 'Support Engineer – Communications' 'System Support Engineer (Linux)'\n",
      " 'Engineer I/ II/ III / Assistant Engineer'\n",
      " 'Senior Customer Service Engineer/ Customer Service Engineer (Healthcare)'\n",
      " 'IT Assistant Support Engineer (Temporary) (Job ref: ITASE-2329-CT)'\n",
      " '2023 Graduate Engineer (Transport / Logistics)'\n",
      " 'PROJECT ENGINEER / SENIOR PROJECT ENGINEER (25K-39K) - Europe MNC elevator/escalator (Kwun Tong/5 days)'\n",
      " 'Linux Engineer' 'System Engineer – Infrastructure' 'OPERATIONS ENGINEER'\n",
      " '(Junior) System Engineer/ Security and Network Engineer/ IT Support/ Project Trainee (Welcome Fresh)'\n",
      " 'Engineer III (Loss Prevention & Training) (Ref. No. TND-TS-EIII-LPT-CT)'\n",
      " 'Cloud System Support Engineer (R&T Development)' 'Senior QA Engineer'\n",
      " 'Assistant Engineer / Engineer (Tunnelling – Geotechnical)'\n",
      " 'Engineer/Assistant Engineer' 'QA Engineer'\n",
      " 'Duty Engineer 當值工程師 (5-day Work Week 5天工作週)'\n",
      " '(Electrical & Mechanical) Senior Engineer - Mission Critical Focused'\n",
      " 'Engineer (Building Services) (Ref: TND-CM-EBS-CT)'\n",
      " 'SMT Engineer / Technician' 'Senior Building Engineer'\n",
      " 'Optical System R&D Engineer' 'Infrastructure Architect'\n",
      " 'Design Support Engineer - Rolling Stock Engineering (Mechanical)'\n",
      " 'Site Reliability Engineer - Blockpour'\n",
      " 'Senior Technician (5 Days Work Week)' 'PreSales Engineer (R)'\n",
      " 'QA QUALITY ASSURANCE ENGINEER (25K-30K) – Europe MNC toys trading/sourcing office (TST/5 days)'\n",
      " 'L2 Support Engineer' 'IT QA & Testing Services Lead'\n",
      " '2023 Graduate Engineer (Structural)' 'Software Test Engineer'\n",
      " 'Engineer (Sensors & Instrumentation)'\n",
      " 'Senior Maintenance Standards Engineer'\n",
      " 'System Infrastructure Support Specialist'\n",
      " 'Trainee Satellite Control Engineer' 'UI/UX Engineer'\n",
      " 'System Analyst, Data Scientist / Machine Learning Engineer'\n",
      " 'Data Centre Engineer - Swift Duty']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 :\n",
      "['Data Engineer' 'Data Engineer (BigData)'\n",
      " 'Data Engineer (Financial/ E-commerce)' 'Remote Data Engineer'\n",
      " 'Data Engineer - Mox' 'Senior Data Engineer - IC'\n",
      " 'Data Engineer (Hong Kong)' 'Data Centre Engineer'\n",
      " 'Data Engineer/ Scientist/ Analyst/ Governance (30K-85K)'\n",
      " 'Senior Engineer/Consultant (Video Analytics / IoT / Big Data) - IC'\n",
      " 'Data Engineer - Conglomerate' 'Junior Biology Engineer'\n",
      " '17229-Senior Data Mining Engineer'\n",
      " 'Duty Engineer (Sha Tin Data Centre) [Ref no.: 20010058]'\n",
      " 'Network Engineer (Voice & Data Team)'\n",
      " 'APAC Global Expansion Engineer, Data Center Design Engineering'\n",
      " '(PERM /URGENT) Data Engineer' 'Azure Data Engineer x 1'\n",
      " 'Data Engineer - Public Cloud Exposure'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Senior Engineer - Data Engineering & Analytics'\n",
      " 'Senior Data Engineer, Data & Analytics - Hong Kong'\n",
      " 'Hong Kong - Data Engineering Graduate Programme'\n",
      " 'Data Engineer/50-60K/Real Estate'\n",
      " 'HVAC Engineer | Data Center | Kwai Chung'\n",
      " 'Data & Analytics Engineer - Actuarial Technology'\n",
      " 'Technology - Data Engineer' 'Data Center Engineer'\n",
      " 'Business Intelligence Engineer (Power BI)'\n",
      " 'Senior Data Engineer (Data & Analytics)' 'Cloud Data Engineer'\n",
      " 'Data Architect + Data Team for leading IT solutions in APAC'\n",
      " 'Data Engineer (New Media Business)' 'Senior Data Engineer'\n",
      " 'Data Architect/Engineer x 1'\n",
      " 'Software Engineer - Big Data (Java/Scala/Spark) - Trading'\n",
      " 'Data Center Operator/ NOC Engineer (Upto 40K X13 guarantee)'\n",
      " 'BI Architect (Luxury Retail)' 'Senior data engineer'\n",
      " 'Data Engineer /Top Financial Company – HK$35K – HK$65K/m'\n",
      " 'Data Engineer - New Media Division'\n",
      " 'Testing & Commissioning Engineer | Data Center | Kwai Chung'\n",
      " 'Data Center Engineer (Cabling)'\n",
      " 'Senior Data Center Engineer (Facility Management / Electrical) - CK'\n",
      " 'Data Center Customer Operations Engineer II'\n",
      " 'Assistant Manager, Customer Analytics & Data Engineering'\n",
      " 'Associate / Senior Associate - Data Analyst / Engineer - Actuarial - Hong Kong'\n",
      " 'Data Engineer/50' 'Data Analyst / Engineer'\n",
      " 'Facilities Engineers, BMS Engineer, Cabling Engineer - Data Center'\n",
      " 'Data Engineer (ETL)' 'Project Engineer - Data Center'\n",
      " 'Engineer, BMS Data Center'\n",
      " 'Data Engineer (Business Intelligence Projects)'\n",
      " 'Data Engineer / Lead Data Engineer x 2'\n",
      " 'Data Engineer (IoT / Innovations / Data Streaming)'\n",
      " 'Data Centre Engineer (Electrical License Grade A) (No Shift Duty) - CK'\n",
      " 'Big Data Engineer (AM level, individual contributor, Up to 55k)'\n",
      " 'Software Engineer (Big Data, Business Intelligent)'\n",
      " 'Market Data Integration Engineer'\n",
      " 'Senior BIM Engineer / BIM Engineer / Asst BIM Engineer' 'Cloud Engineer'\n",
      " 'Data Center Operation Engineer' 'Data Centre Operations Engineer'\n",
      " 'Senior Data Centre Engineer (No Shift)'\n",
      " 'Data Engineer (35-50k, financial service)'\n",
      " 'Data Engineer (up to 50k, venture capital)'\n",
      " 'Lead Engineer (HV Systems) - Data Center'\n",
      " '2023 Graduate Engineer (Building Services / Electrical / Mechanical / Building Sustainability / Fire)'\n",
      " 'IT Trainee/ Data Analyst/ Network Engineer/ Programmer (Fresh Graduates are welcome)'\n",
      " 'Sales Specialist, Enterprise Data Solutions'\n",
      " 'Java Backend Engineer - Data Services'\n",
      " 'Trainee -Web / Programmer / Business System Analyst / Data Analyst / System Engineer (Welcome Fresh)'\n",
      " 'Project Engineer / Project Consultant / Engineer / Consultant (Sustainability)'\n",
      " 'Data Engineer #3788 (Telecom)' 'Cloud DevOps Engineer'\n",
      " 'Engineer/Project Engineer/Senior Engineer (Building Services/Electrical/Fire Services/Mechanical/Plumbing & Drainage/Public Health)'\n",
      " 'Senior Associate(Cloud DevSecOps Engineer) - Global Technology Solutions - Firmwide Corporate Services - Hong Kong'\n",
      " 'Assistant Data Science Manager' 'Assistant BIM Engineer'\n",
      " 'CLOUD ENGINEER - Wealth Management'\n",
      " 'Cloud Data Engineer, AWS - Insurance'\n",
      " 'Data Centre Engineer - Swift Duty']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.28 Taiwan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Taiwan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 :\n",
      "['Data Center Facilities Mechanical Engineer (English, Mandarin)'\n",
      " 'Data Engineer'\n",
      " 'Data Analytics and Supply Chain Management Engineer - 身心障礙人才招募'\n",
      " 'Big Data Engineer : Taiwan' 'Data Scientist Engineer'\n",
      " 'Contract Engineer I' 'Mask Data Preparation Engineer'\n",
      " 'Data Engineer-身心障礙人才招募'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Precision Optics Manufacturing Engineer' 'Clinical Data Engineer II'\n",
      " 'Technical Quality Engineer' 'MTB IE Engineer (Intern)'\n",
      " 'INTERN--Taichung Process Engineer' 'Data Engineer 數據工程師 / BI工程師'\n",
      " '【採線上面談】Data Engineer 資料工程師' 'Service Engineer'\n",
      " 'Manufacturing - Business Engineer - Linkou'\n",
      " '(2023實習) Machine Learning Engineer (時薪260)(可視訊面試及彈性WFH)'\n",
      " 'Configuration Engineer (EPG) II - (E2)' 'Engineer III' 'RD Engineer'\n",
      " 'AI/ML Engineer'\n",
      " 'Gas Sensor Engineer ( Calibration, Experiment Data Analysis )'\n",
      " 'Data Engineer (Python)'\n",
      " '數據工程師 Data Engineer / 資深數據工程師 Senior Data Engineer'\n",
      " 'Data Science Engineer (Application Engineer)'\n",
      " 'Data Engineer, ETL & Data Pipeline' 'Component Engineer - Power Supply'\n",
      " 'Data Center Network Engineer' 'Thermal Engineer熱傳工程師 (智能服務事業群/林口園區)'\n",
      " 'senior data engineer' 'AI Research Engineer (AI Lab)'\n",
      " 'Machine Learning Engineer (Generative Models)' 'AI/ML Engineer-身心障礙人才招募'\n",
      " 'Data Automation Engineer and Data Access Governance'\n",
      " 'Supply Chain Logistic Management Engineer - 身心障礙人才招募'\n",
      " 'RD Process engineer-身心障礙人才招募' '外商 Fintech Data Engineer' 'Engineer'\n",
      " 'Data Engineer/Data Architect(內湖瑞光路)'\n",
      " 'Silicon bringup and validation engineer'\n",
      " 'Shock and Vibration Reliability Engineer'\n",
      " 'Software Engineer, Pixel Touch and Haptics' 'TCAD Engineer'\n",
      " '[TW Pipeline] CSCM - Global Operations Center Data Engineer - Linkou/Taichung'\n",
      " 'RF Project Engineer' 'Junior Site Engineer' 'Project Engineer'\n",
      " 'Senior Data Engineer (Big Data)'\n",
      " 'Hardware Engineer, Devices and Services, Pixel'\n",
      " 'Silicon Engineering Intern, Spring or Summer 2023' 'Data Engineer|資料工程師'\n",
      " 'Lens Module Engineer' 'Backend Engineer (Python) TW'\n",
      " 'Software Development Engineer in Test, Foundation'\n",
      " 'Rust Engineer (Greenfield project)'\n",
      " 'Data Center Platform Application Engineer - Signal Integrity'\n",
      " '工程師-身心障礙人才招募' 'NCR, Foundry Yield Management Engineer'\n",
      " 'Manufacturing Process Engineer' 'Algorithm Engineer - Taiwan'\n",
      " '雲端工程師 - 歡迎身心障礙人才應徵, WFH/In-Office'\n",
      " 'M0079 - Display Component Engineer (ME)' 'IT System Engineer'\n",
      " 'N2 RD Device Engineer' '(台南)【2023研發替代役】資料工程師 Data Engineer'\n",
      " 'BI system engineer' 'Data Center Lead'\n",
      " 'Java Backend Engineer - Data Services'\n",
      " 'TPU Architect, Power and Performance'\n",
      " 'Aerospace - ASIC Sustaining Engineer'\n",
      " 'Bank / 數位金融中台發展ETL Data工程師 / ETL Data Engineer'\n",
      " '[Data Sci] Data Analytics Engineer'\n",
      " 'Data Scientist, APAC / 數據工程師 (亞太地區)' 'R&D Engineer II'\n",
      " 'SMTS Silicon Design Engineer'\n",
      " 'Infrastructure Security Operations Engineer'\n",
      " 'A14/N2 RD Integration Engineer' 'MEMS & 3DIC Product Engineer'\n",
      " '2023 DNA Internship Program (Engineering)'\n",
      " 'BIOS Platform Application Engineer'\n",
      " '2023 Signal Integrity Engineer Summer Intern' '資材管理職位 -身心障礙人才招募'\n",
      " 'Platform Hardware Engineer, Tablet and Smart Home Devices'\n",
      " '2023 Thermal Engineer Summer Intern'\n",
      " 'Software Engineer (Machine Learning)' 'Application Development Engineer'\n",
      " 'Foundry Product Engineer' 'Hardware Electrical Component Engineer'\n",
      " 'Business Analyst - Remote'\n",
      " 'Silicon Physical Design Engineer, Power Grid Methodology'\n",
      " 'AC Power Engineer (1-Year Internship)'\n",
      " '2023 Mechanical Engineer Summer Intern' 'Supply Chain Logistic Engineer'\n",
      " 'Fullstack Engineer' 'Hardware Engineer, Devices and Services'\n",
      " 'Machine Learning Engineer' 'Site Reliability Engineer'\n",
      " 'Silicon Technical Lead, Performance Dashboard'\n",
      " '2023 Electrical Engineer Summer Intern'\n",
      " '2023 Power Firmware Engineer Summer Intern'\n",
      " '設備安裝產品工程師 E&I Product Engineer' 'Electrical Circuit Engineer'\n",
      " 'Regional Customer Application Engineer' 'Industrial Design Specialist'\n",
      " '2023 Hardware Engineer Summer Intern'\n",
      " 'Co-Op/ Intern - Power & Performance System Engineer'\n",
      " '【2023 Campus Recruitment】Taiwan_Materials Management & Risk Management'\n",
      " 'Mask Optical Simulation Research and Development engineer'\n",
      " '2023 Systems Development Engineer Summer Intern'\n",
      " '2023 Software Engineer Summer Intern'\n",
      " '[派遣至外商軟體龍頭] Research and engineering Intern - 軟體設計工程師實習生 - 105SL 3530'\n",
      " '2023 Product Security Engineer Summer Intern' 'Senior Data Engineer'\n",
      " '[Data Sci] Sr. Nodejs backend engineer' '[Data Sci] Data Engineer(JP)'\n",
      " '數據工程師 / Data Engineer / ETL Engineer'\n",
      " '[Data Sci] Site Reliability Engineer (Data)'\n",
      " '[Taiwan RDSS/AO] D&E - Mechanical Design Engineer'\n",
      " 'Data Infrastructure Engineer'\n",
      " 'Campus Hire 2023: Shock/Vibration and Package Engineer'\n",
      " '(2023 實習) Data Engineer - ETL Specialist (時薪260) (可視訊面試及彈性WFH)'\n",
      " 'INTERN--Taoyuan Process Engineer'\n",
      " 'Campus Hire 2023: Power Supply Unit Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 :\n",
      "['Data Engineer'\n",
      " 'Data Analytics and Supply Chain Management Engineer - 身心障礙人才招募'\n",
      " 'Big Data Engineer : Taiwan' 'Mask Data Preparation Engineer'\n",
      " 'Data Engineer-身心障礙人才招募'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Clinical Data Engineer II' 'Data Engineer 數據工程師 / BI工程師'\n",
      " '【採線上面談】Data Engineer 資料工程師'\n",
      " 'Gas Sensor Engineer ( Calibration, Experiment Data Analysis )'\n",
      " 'Data Engineer (Python)'\n",
      " '數據工程師 Data Engineer / 資深數據工程師 Senior Data Engineer'\n",
      " 'Data Science Engineer (Application Engineer)'\n",
      " 'Data Engineer, ETL & Data Pipeline' 'Data Center Network Engineer'\n",
      " 'senior data engineer'\n",
      " 'Data Automation Engineer and Data Access Governance'\n",
      " '外商 Fintech Data Engineer' 'Data Engineer/Data Architect(內湖瑞光路)'\n",
      " '[TW Pipeline] CSCM - Global Operations Center Data Engineer - Linkou/Taichung'\n",
      " 'Senior Data Engineer (Big Data)' 'Data Engineer|資料工程師'\n",
      " 'Data Center Platform Application Engineer - Signal Integrity'\n",
      " '(台南)【2023研發替代役】資料工程師 Data Engineer' 'BI system engineer'\n",
      " 'Data Center Lead' 'Java Backend Engineer - Data Services'\n",
      " 'Bank / 數位金融中台發展ETL Data工程師 / ETL Data Engineer'\n",
      " '[Data Sci] Data Analytics Engineer' 'BIOS Platform Application Engineer'\n",
      " 'Senior Data Engineer' '[Data Sci] Sr. Nodejs backend engineer'\n",
      " '[Data Sci] Data Engineer(JP)' '數據工程師 / Data Engineer / ETL Engineer'\n",
      " 'Data Infrastructure Engineer'\n",
      " '(2023 實習) Data Engineer - ETL Specialist (時薪260) (可視訊面試及彈性WFH)']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.29 South Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'South_Korea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 :\n",
      "['APAC Global Expansion Engineer, Data Center Design Engineering'\n",
      " '[데브시스터즈] Data Engineer - Analytics/BI' 'Data Engineer'\n",
      " 'Data Platform Engineer' 'Data/Machine Learning Engineer (신입)'\n",
      " '[컬리] 데이터플랫폼팀 데이터 엔지니어 (Data Engineer)'\n",
      " '부산본사 개발자 (Data Engineer / Backend Develope)'\n",
      " 'Customer Engineer, Data Analytics, Google Cloud'\n",
      " 'Associate Customer Engineer, Data Analytics, Google Cloud, gReach Program for People with Disabilities (장애인 채용)'\n",
      " '데이터 엔지니어 (Data Engineer)' 'Rust Engineer (Greenfield project)'\n",
      " 'S/W Development Engineer - R&D'\n",
      " 'Machine Learning Engineer Intern - Korea' '[11번가] BI/DW Engineer'\n",
      " '각 부문별' '[IT 부문 대규모 경력 채용] 데이터 엔지니어링[매니저, 책임매니저]'\n",
      " 'Data Platform Kubernetes Engineer' 'Refining Tech Services Engineer'\n",
      " 'Software Engineer Intern - Korea'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " '[데브시스터즈] Machine Learning Engineer (경력)' 'HPC Engineer'\n",
      " 'Data Center Engineering Operations Engineer'\n",
      " 'Regional Safety Engineer - South Korea, Data Center Health and Safety'\n",
      " 'Robotics Engineer [LiDAR(PointCloud) Perception][책임연구원]'\n",
      " '한국은행 경력직 각 부문(IT/금융시장/통계)' 'Ethylene Tech Services Engineer'\n",
      " 'MLOps Engineer' '[카카오커머스] DW/BI 엔지니어'\n",
      " 'Data Specialist (Scientist&&Engineer)' '[카카오커머스] 데이터 분석/엔지니어'\n",
      " 'Back-End Engineer' 'Battery Global 제조(Engineer) 경력'\n",
      " 'Data Engineer(3-10년)'\n",
      " 'Data Center Network Technician, Infrastructure Delivery' '인재풀 등록 (Data)'\n",
      " 'Data Scientist' 'Product Engineer' '2023년 상반기 포스코스틸리온 신입( 연계형 인턴)/경력사원'\n",
      " 'Software Engineer-Phantom Works Global' '플리토 Machine Learning Engineer'\n",
      " '[HYBE] IT PM (사내시스템 구축/운영)' 'AI Engineer (데이터프로덕트팀)'\n",
      " '[Lighthouse] Machine Learning Engineer 정규직 채용'\n",
      " 'Data Center Operations Technician, Data Center Operations'\n",
      " 'Project Engineer- Factory Control System (FCS)' '딥러닝 알고리즘 및 응용 Engineer'\n",
      " 'Packaging engineer' '[ICT] Data Engineering[책임매니저(G3)]'\n",
      " 'Project Engineer - EV Battery Testing Lab' 'Process Engineer'\n",
      " 'Python Backend Engineer' '상시지원 (AI & ML Engineer)'\n",
      " '[메가존클라우드] Learning & Development'\n",
      " 'Display Manufacturing Process Engineer (MPE),' 'Senior Data Engineer'\n",
      " '[KR] Solutions Engineer, TikTok Ads'\n",
      " 'Partner Engineer, YouTube Shopping (English, Korean)'\n",
      " 'Customer Solutions Engineer, Global Client and Agency Solutions'\n",
      " 'Quality Engineer' 'Yeosu Process Engineer' 'Systems Engineer'\n",
      " 'Cloud Support Engineer, Big Data' 'Medical AI Research Engineer(Junior)'\n",
      " 'Data Center Critical Facilities Engineer'\n",
      " '[골라라] BackEnd 개발, FrontEnd 개발, App 개발, Data 엔지니어 경력 정규직'\n",
      " '[카카오커머스] 머신러닝 엔지니어' '[ICT] Network Engineer[매니저(G2), 책임매니저(G3)]'\n",
      " 'Software Engineer, Robot Behavior' '데이터 엔지니어'\n",
      " 'Camera Process Development Engineer (Korea)'\n",
      " 'Customer Engineer, Application Modernization, Google Cloud'\n",
      " '[메가존클라우드] Hybrid Cloud Engineer'\n",
      " 'Vocational Training Program Intern (Research&Innovation) - 2024/2025 Graduates Korea (VT/STAR)'\n",
      " '[쿠키런 킹덤] DevOps Engineer'\n",
      " 'Staff Back-end Engineer (Site Reliability Engineering)'\n",
      " 'Senior Software Engineer (Cloud Native Developer, Data Engineer/Dashboard Developer)'\n",
      " 'Embedded Software Engineer (m/f/d)' '전문연구요원 (병역특례) 수시 (AI / HW 분야)'\n",
      " 'Staff, Back-end Engineer (DNA Engineering)' 'Project Engineer'\n",
      " 'Customer Engineering Manager, Gaming, Google Cloud'\n",
      " 'AI Engineer : 의료 영상 데이터 활용 딥러닝 알고리즘 개발' '[에이모] 자율주행 센서 데이터 수집/정제 엔지니어'\n",
      " 'BI 엔지니어' 'IDC사업 IT인프라 영업부문 인재' 'Junior Cable Engineer'\n",
      " 'Machine Learning Ops Engineer' '[HYBE] HRIS담당'\n",
      " 'Supplier Development Engineer' 'AI 개발 실무교육 및 취업과정 교육생 (수료 후 인턴십, 정규직전환)'\n",
      " '인터넷 전화 기술지원 엔지니어' 'Staff, Back-end Engineer [WMS]'\n",
      " '[Coupang Fulfillment Services] Automation Maintenance Engineer (AGV, 동탄1 FC)'\n",
      " 'Intern - Product Test Engineer' 'SQL, Python 기반 Data Engineer(경력직) 채용'\n",
      " 'Application Engineer (인재DB 등록)' 'Reliability Engineer'\n",
      " '(Seoul) Data Engineer · Lunit INSIGHT'\n",
      " 'Staff Engineer - Software Development'\n",
      " 'Customer Engineer, Infrastructure Modernization, Google Cloud'\n",
      " 'Customer Engineer, Networking, Google Cloud'\n",
      " '보안 엔지니어 (Security Engineer) (2년 이상)' 'Customer Engineer'\n",
      " '[Digital Industries] Customer Service - Motion Control (Service Engineer)'\n",
      " '[Vueron] Application Engineer' 'Front-end Engineer'\n",
      " '외국계 화장품 기업 IT-Information Security Engineer (사원대리급)'\n",
      " '[OK금융그룹] OK데이터시스템 IT직군 경력직 (HR개발/운영)'\n",
      " '[EY한영] 컨설팅본부 Data & Analytics팀 데이터 기술 전문가 모집'\n",
      " 'E-Tester and Automated Optical Inspection Tool Engineer,'\n",
      " 'IT Support Engineer, Associate - Technology'\n",
      " '[TaaS] Frontend Engineer for Mobility[연구원, 책임연구원]'\n",
      " '[컬리] 데이터농장 DW Ops 엔지니어 (DW Ops Engineer)'\n",
      " 'Power and Drive field service engineer'\n",
      " 'Site Reliability Engineer - Korea' 'Principal Application Engineer'\n",
      " '(Icheon/Cheongjoo) Customer Support Engineer (Evergreen/인재DB 등록)'\n",
      " '대외IDC운영 부문 인재' 'Machine Learning Engineer' 'API Gateway Engineer'\n",
      " '(동탄/평택) Customer Support Engineer (인재DB 등록)' 'Telematics engineer'\n",
      " 'Process Support Engineer for Advanced Packaging(Digital Lithography Technology)'\n",
      " 'Staff Full-satck/Back-end Engineer (New Project(Whale))'\n",
      " 'Lab Analyst Yangsan, South Korea Operations'\n",
      " 'Lead Engineer – Electrical Component Engineer'\n",
      " '[ICT] Frontend Developer[매니저(G2), 책임매니저(G3)]'\n",
      " 'Manufacturing Engineering Engineer' 'Technical Support Engineer - APAC'\n",
      " '[메가존클라우드] Cloud Presales Engineer, HashiCorp'\n",
      " '[대기업] 데이터 엔지니어 (BI 개발) 과차장급' 'Technical Sales Engineer (Intern) - Korea'\n",
      " 'Cloud Support Engineer - Linux' 'Engineering Specialist'\n",
      " '드론 데이터 스타트업, 엔젤스윙 엔지니어' '백엔드 개발자 (Backend Engineer)'\n",
      " '프론트엔드 개발자 (Frontend Engineer)'\n",
      " '[서울, 코스닥 상장] (주) KINX IDC 관제, 운영 엔지니어 공개'\n",
      " 'Engineer, Product Applications' '[SPC그룹] 2023년 섹타나인 상반기 각 부문 신입 /경력'\n",
      " 'Project Engineer - PCS' 'CTO 영입' '[크라우드웍스] NLP 엔지니어'\n",
      " 'Technical Engagement Engineer' '[AI/LAB] 머신러닝 리서치 엔지니어 (전문연구요원 가능)'\n",
      " '[ICT] Backend Developer[매니저 (G1), 매니저 (G2), 책임매니저 (G3)]'\n",
      " 'Field Network Engineer-Korea' 'CSCM Disturbance Admin'\n",
      " 'Cloud Support Engineer, People with disabilities (장애인 채용)'\n",
      " 'CI/CD Engineer & Application Architect' '인재풀 등록 (Tech)' '[해외취업] IT 엔지니어'\n",
      " '기계엔지니어(설비기술/Maintenance) 경력직 수시 채용'\n",
      " 'Customer Operations Engineer / 데이터센터 운영 엔지니어'\n",
      " 'Product Installation Engineer'\n",
      " '미국 VC가 인정한 Next Unicorn Startup Tridge 최고의 인재'\n",
      " '이큐브랩 웹 프론트엔드/백엔드 개발자 (신입/경력)'\n",
      " 'Principal Field Applications Engineer (copy)'\n",
      " 'Senior Consultant Engineer for Advanced Process Modelling Work - SPSE'\n",
      " '[해외취업] 전자전기/기계설계/IT 엔지니어 24.04 신졸' '중국 유명 네트워크 회사 한국 지사 네트워크 엔지니어'\n",
      " '[Vueron] 딥러닝 엔지니어 - 자율주행/라이다 처리'\n",
      " 'Media Production Suite Support Specialist - APAC'\n",
      " 'MOBILE PLATFORM ENGINEER, UNREAL ENGINE' 'Engineering Team Leader'\n",
      " '프롭테크 스타트업 Back-end Engineer' 'Machine Learning Engineer(머신런닝 엔지니어, 서울)'\n",
      " 'IT End User Service Desk' '[메가존클라우드] Support Engineer, HashiCorp'\n",
      " 'Data Engineer (Mid/Senior)' 'Battery 제조기술 경력'\n",
      " 'Determined AI - Developer Support Engineer' 'Engineer, Service Delivery'\n",
      " 'Field Network Engineer Lead– Korea (Secret - ability to obtain a TS/SCI)'\n",
      " 'Customer Facing Data Scientist' '2023 국군장병 공개채용'\n",
      " 'Software Engineer, Frontend(프론트엔드 엔지니어, 서울)'\n",
      " 'NIPA 사업기반, 윈드위시와 함께 4차산업 선도할 인재' 'Cloud Support Engineer'\n",
      " '[해외취업] 한국 거주 IT 개발자 대상 온라인 면접회'\n",
      " '[메가존클라우드] Cloud PreSales Engineer, KT-Cloud'\n",
      " 'Manufacturing Engineer (Intern)'\n",
      " 'Software Engineer - App Stores Backend (Remote)'\n",
      " 'Network Security Engineer (5년 이상)' 'PAE'\n",
      " 'Cymer Service Demand Planning Planner'\n",
      " 'Software Engineer(Product Support/Global Software Focal)'\n",
      " '자동화설비 제작 전장엔지니어'\n",
      " '(인천) [아이폰 부품 생산] (주)비에이치 / 3조2교대 / 면접없이 바로출근 / 상여금 / 통근버스'\n",
      " 'Customer Service Engineer' 'Senior Substation Engineer'\n",
      " 'Field Application Engineer Korea' 'SPE Applications Engineering'\n",
      " '[경력] 반도체 양산 테스트 엔지니어 터치개발팀 모집' 'Research Development Engineer'\n",
      " 'Manufacturing Engineer' 'Senior Application Engineer Korean Branch'\n",
      " 'EPI Module Process Engineer' '4월 신입사원 채용[생산기술]'\n",
      " 'Service Engineer - Automation, Marine & Ports, Busan' '패션고 서비스 프론트엔드 개발'\n",
      " 'IT End - User Support Engineer (Contract)'\n",
      " '[IT기업] 화성 모뎀 QA Test Engineer' '[서울/부산/광주] IT 솔루션 엔지니어'\n",
      " 'Sr./ Field Application Engineer - Korea'\n",
      " 'Sr. Manager, Back-end engineering (Coupang Play)'\n",
      " 'Senior Software Engineer (System Software)' '[인텔리전스랩스] 매치메이킹 서버 개발자'\n",
      " 'Senior Software Engineer' '[해외취업] 【2023년 상반기 일본면접회 】 J.인터페이스 _이상'\n",
      " 'PE Field Applications Engineering' 'Lead Technical Support Engineer'\n",
      " '(Seoul) Backend Engineer · AI Platform' '바이오텍기업 IT 담당자'\n",
      " 'Back-end Engineer (Fulfillment and Transportation System)'\n",
      " 'Engineer, Firmware Engineering'\n",
      " '[외국계투자 데이터기반 디지털마케팅기업] Lead Technical Solution Engineer'\n",
      " 'Network Engineer Lead' 'Automation Engineer (PLC/SCADA/OT Network)'\n",
      " 'Staff, Back-end Engineer [FTS: Fulfillment and Transportation]'\n",
      " '무선통신모듈 SW 개발 Engineer 신입/경력'\n",
      " 'Senior Resident Engineer – Systems Integration Engineer (SIE)'\n",
      " 'Software Developer' '[레전더리스] 데이터 엔지니어 영입(경력 3~5년)'\n",
      " 'MES Application Engineer' 'Senior Python Software Engineer'\n",
      " 'Backend Engineer' 'Field Service Engineer South Korea'\n",
      " '[IT/데이터 인프라/솔루션 전문기업] 리눅스 엔지니어 (고객사 대상 기술지원)'\n",
      " 'Radar Vehicle Test Engineer'\n",
      " '마케팅 테크 데이터 엔지니어 (유관경력 2년 이상) 리딩 뷰티 기업(서울 용산 근무)'\n",
      " '머신러닝 엔지니어 자연어처리 (경력 10년 이상, 유관 경력 3년 이상), 리딩 뷰티 기업'\n",
      " 'QA 엔지니어 (QA Engineer)' '지도데이터 전문기업 딥러닝 엔지니어'\n",
      " '[MUSMA] IT/ICT 국책 사업 기획 및 수행 담당자 (경력직)' '[안양] 시스템 엔지니어(경력5년이상)'\n",
      " '[YONEX KOREA] 전산 관리 신입 및 경력직'\n",
      " '효성 인포메이션시스템(주) 2021년 영업 및 시스템 엔지니어 경력직 수시' '(주)싸이버로지텍 2023년 신입 및 경력'\n",
      " '[정규직][경력] 스크린 골프 플랫폼 QA 운영 SW 테스트 엔지니어' 'Sr. Staff Engineer'\n",
      " 'Senior Technical Support Engineer' '[국비무료]인공지능/블록체인 교육생'\n",
      " 'Cloud Solutions Architect'\n",
      " '[Digital Industries] Factory Automation - Product promoter'\n",
      " 'Senior Data Analyst (eCommerce Product Analytics)'\n",
      " '[정규직][경력] 엔터테인먼트 커뮤니티 플랫폼 QA 운영 SW 테스트 엔지니어'\n",
      " '대기업 Ecommerce유통사 -IT개발·데이터(Java, PL/SQL, Py 외 경력 정규직, 계약직'\n",
      " 'IT 시스템 설치/운용/관리 엔지니어' '[IT부문 정규직] 영업/엔지니어/사업관리/영업지원 신입 및 경력직'\n",
      " 'System SW 엔지니어' '안드로이드 PDA 개발자' '[전문연구요원 신규편입 & 전직] Software Engineer'\n",
      " 'Sr. Software Dev Engineer, Skill Builder Learner Experience Team'\n",
      " 'Process Support Engineer for Advanced Packaging(Laser/Plasma)'\n",
      " 'Senior Cybersecurity Engineer' '데이터 엔지니어 경력 정규직' 'SHE Specialist'\n",
      " '파이썬SW 금융분야 특화 챗봇 개발 함께 할 엔지니어' '[메가존클라우드] Solutions Architect, AWS / 대전'\n",
      " '독일계 전기전원장치 제조사 전기엔지니어'\n",
      " '[AI/Lab] Recommendation System 리서치 엔지니어 (전문연구요원 가능)' '머신러닝 엔지니어(자연어처리)'\n",
      " '[GMSJ] Test Operator (Field Testind/Driver) 채용 (일어,영어가능자,해외거…'\n",
      " 'Procurement Coordinator' 'Display(OLED) Evaporation Process Engineer'\n",
      " 'Linux System Engineer - QA, Tooling, Automation'\n",
      " '[TES물류기술연구소] 데이터 엔지니어(DE) 경력사원' '[싸이버원] 보안솔루션 기술지원 엔지니어 (신입/경력)'\n",
      " '특허 조사 분석' 'Field Application Engineer for TV SMPS'\n",
      " 'Snr Electrical Engineer - PECVD' 'SW Test Engineer'\n",
      " 'Front-end Software Engineer(전문연구요원) - Korea'\n",
      " 'Specialist Solutions Architect' 'Frontend Engineer - Map 데이터 관리시스템'\n",
      " 'Cymer Software Engineer' '웹 개발자(웹 엔지니어·웹 프로그래머)'\n",
      " 'Software Engineer (New Grad), R&D' 'Senior Software Engineer, Backend'\n",
      " 'Senior Software Engineer - Cybersecurity' 'Sr Software Engineer'\n",
      " 'Solution Architect - Rapid Portfolio Modernization - Opportunity for Working Remotely Seoul,'\n",
      " 'Software Architecture 설계 엔지니어' 'Sr Appl/Sys Sales Engineer'\n",
      " 'Sr Devops Engineer' 'Software Engineer (QA/Automation)'\n",
      " 'Software Engineers'\n",
      " 'Technical Solution Engineer (A.I. Marketing Technology)'\n",
      " 'Sr Product Manager (대기업 금융사) 인도네시아' '하드웨어 부품 설계/검증 엔지니어'\n",
      " 'Software Engineer, 산업기능요원 Alternative military service'\n",
      " 'Cloud Engineer_DBA' 'Software Engineer - Front-End'\n",
      " '[메가존클라우드] Solutions Architect, K-Cloud' '[DTC 플랫폼] Frontend 개발자 경력채용'\n",
      " '[쿠팡풀필먼트서비스] 물류센터 자동화 설비 및 AGV 보전 관리자 (동탄1)'\n",
      " 'Staff, Back-end Engineer (Coupang Play)'\n",
      " '[메가존클라우드] Solutions Architect, Gaming'\n",
      " '[메가존클라우드] Solutions Architect, AI/ML'\n",
      " 'VED - SYSTEM Mechatronics Engineer'\n",
      " '[메가존클라우드] Solutions Architect, AWS (일본 법인 전담)'\n",
      " '[메가존클라우드] Cloud Presales Engineer, AWS'\n",
      " '[정규직][경력] 트래블테크 플랫폼 QA 운영 SW 테스트 엔지니어' '[상시] 컴퓨터 비전 및 딥러닝 엔지니어'\n",
      " '[메가존클라우드] Advocacy Program Manager'\n",
      " '[메가존클라우드] Solutions Architect, Application Engineer'\n",
      " 'Environmental, Health, & Safety Engineer' '데이터 엔지니어를 모십니다.'\n",
      " 'Vocational Training Program Intern (Engine) - 2024/2025 Graduates Korea (VT/STAR)'\n",
      " '프론트엔드 엔지니어 (웹 퍼블리셔)'\n",
      " 'Engineer / Assistant Engineer (Electrical / Mechanical)'\n",
      " '[DTC 플랫폼] DevOps engineer senior 경력채용'\n",
      " 'Software Engineer(전문연구요원) - Korea'\n",
      " 'ISCO - Mechanical Engineer (HVAC 설계)'\n",
      " '[청년일자리도약장려금] 하드웨어 개발 및 운영관리 담당자 - 경력 신입'\n",
      " 'Supplier Development Engineer, Custom Silicon Foundry Manufacturing'\n",
      " '[DTC 플랫폼] Markup Developer'\n",
      " 'Embedded & Desktop Linux Systems Engineer - Optimisation'\n",
      " 'Site Reliability Engineer' '(Senior) Wind & GIS Engineer'\n",
      " 'Geophysical & Geotechnical Project Engineer'\n",
      " '[메가존클라우드] Solutions Architect, Elastic'\n",
      " '[메가존클라우드] Solutions Architect, AWS'\n",
      " '[메가존클라우드] DevOps Engineer, HashiCorp' '[메가존클라우드] Technical Trainer'\n",
      " '[메가존클라우드] Solutions Architect, Gitlab'\n",
      " '[메가존클라우드] Pre-Sales Engineer (3)' '[메가존클라우드] Solutions Architect, GCP'\n",
      " '[메가존클라우드] Application Developer, IoT Platform'\n",
      " '[메가존클라우드] Solutions Architect, AWS / 인천'\n",
      " '[메가존클라우드] Finance Presales Engineer, AWS'\n",
      " '[메가존클라우드] Solutions Architect, Media'\n",
      " '[메가존클라우드] Solutions Architect, Security'\n",
      " '[메가존클라우드] Security Presales Engineer, AWS'\n",
      " '[메가존클라우드] Solutions Architect, DevOps'\n",
      " '[TaaS] Backend Engineer for Mobility[연구원, 책임연구원]'\n",
      " '[메가존클라우드] Solutions Architect, GCP / DevOps'\n",
      " '세이프닥 앱 개발자와 크라우드 소싱 기반 시스템 개발자를 모집합니다.'\n",
      " 'Site Reliability Engineer (Database Platform)'\n",
      " '[DTC 플랫폼] Backend Developer' 'Production Operator(Time-Limited)'\n",
      " 'Principal Sales Engineer' 'Energy Manager/Resource Efficiency Manager'\n",
      " 'Senior Offshore Wind Engineering Design Manager'\n",
      " 'ENGR SR PRIN, PRODUCT_TL' 'Software Engineer Frontend'\n",
      " '천안근무 화학 QC Operater 및 품질보증 QA 엔지니어 외국계기업'\n",
      " '2022년 메디칼파크 기업부설연구소 연구원 의료장비, 원격의료 외 신입/경력 정규직,계약직' '소프트웨어 제품 개발 신입/경력'\n",
      " 'Vocational Training Program Intern (SAC Planning Development) - 2024/2025 Graduates Korea (VT/STAR)'\n",
      " '좋은 품질의 아름다운 코드를 만들어갈 안드로이드 엔지니어를 찾습니다.' '신입/경력 개발자 프로그래머 및 솔루션 개발(엔지니어)'\n",
      " 'DevOps Engineer' '클라우드 SW엔지니어 양성 과정 SeSAC 영등포 캠퍼스 5기 SW 교육생'\n",
      " '다코타 앱 서비스 안드로이드 개발자 모심' '고속 고정밀 기계설계 산업용로봇 로봇시스템 DIE BONDER Rework 기계'\n",
      " '[메가존클라우드] Presales Engineer' '클라우드 개발 및 엔지니어 직군'\n",
      " 'Android Engineer_업비트 서비스 개발 (시니어)'\n",
      " '[다우데이타][장애인만 프래그램 개발직, 기술직, 관리직(정규직)' '인공지능 로봇개발자 초빙' '추천시스템팀 데이터엔지니어'\n",
      " '[상시] 웹 프론트엔드 엔지니어' '[신입/경력][강서구] SW개발직 엔지니어 모집 (형상관리,요구사항관리)'\n",
      " '레이더 센서 개발 모집[PC Visualizer 엔지니어]'\n",
      " '[메가존클라우드] Solutions Architect, APM (Application Performance'\n",
      " '[토닉 프라이빗 에쿼티] 경영지원, 일반사무 외 신입/경력 정규직, 계약직' '프로덕트부문 서비스 품질엔지니어'\n",
      " '데이터 시스템 엔지니어' 'Sr. Manager, Pricing Operation (Pricing Excellence)'\n",
      " '[(주)디앤인터내셔널] 본사(커튼월 및 외장재 설계 및 엔지니어링 컨설팅, 공사 및 공무) 직원' '시니어 검색 엔지니어'\n",
      " '연구개발 신입/경력 (소프트웨어, 하드웨어, 기계 기구개발)' '프론트엔드 엔지니어 경력자'\n",
      " 'CS EUV SKH Competency Engineer F&T' 'Sr Sales Engineer in PSS'\n",
      " 'Software Engineer']\n"
     ]
    }
   ],
   "source": [
    "show_unique_and_its_len(dfs[salary_type]['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 :\n",
      "['APAC Global Expansion Engineer, Data Center Design Engineering'\n",
      " '[데브시스터즈] Data Engineer - Analytics/BI' 'Data Engineer'\n",
      " 'Data Platform Engineer' '[컬리] 데이터플랫폼팀 데이터 엔지니어 (Data Engineer)'\n",
      " '부산본사 개발자 (Data Engineer / Backend Develope)'\n",
      " 'Customer Engineer, Data Analytics, Google Cloud'\n",
      " 'Associate Customer Engineer, Data Analytics, Google Cloud, gReach Program for People with Disabilities (장애인 채용)'\n",
      " '데이터 엔지니어 (Data Engineer)' '[11번가] BI/DW Engineer'\n",
      " 'Data Platform Kubernetes Engineer'\n",
      " 'Python Engineer - Data Center Hardware Integration (Taipei)'\n",
      " 'Data Center Engineering Operations Engineer'\n",
      " 'Regional Safety Engineer - South Korea, Data Center Health and Safety'\n",
      " 'Robotics Engineer [LiDAR(PointCloud) Perception][책임연구원]'\n",
      " 'Data Specialist (Scientist&&Engineer)' 'Data Engineer(3-10년)'\n",
      " '[ICT] Data Engineering[책임매니저(G3)]' 'Senior Data Engineer'\n",
      " 'Data Center Critical Facilities Engineer'\n",
      " 'Customer Engineer, Application Modernization, Google Cloud'\n",
      " '[메가존클라우드] Hybrid Cloud Engineer'\n",
      " 'Senior Software Engineer (Cloud Native Developer, Data Engineer/Dashboard Developer)'\n",
      " 'Customer Engineering Manager, Gaming, Google Cloud'\n",
      " 'SQL, Python 기반 Data Engineer(경력직) 채용'\n",
      " '(Seoul) Data Engineer · Lunit INSIGHT'\n",
      " 'Customer Engineer, Infrastructure Modernization, Google Cloud'\n",
      " 'Customer Engineer, Networking, Google Cloud'\n",
      " '[메가존클라우드] Cloud Presales Engineer, HashiCorp'\n",
      " 'MOBILE PLATFORM ENGINEER, UNREAL ENGINE' 'Data Engineer (Mid/Senior)'\n",
      " '[메가존클라우드] Cloud PreSales Engineer, KT-Cloud' 'Cloud Solutions Architect'\n",
      " 'Cloud Engineer_DBA' '[메가존클라우드] Solutions Architect, K-Cloud'\n",
      " '[메가존클라우드] Cloud Presales Engineer, AWS'\n",
      " '[TaaS] Backend Engineer for Mobility[연구원, 책임연구원]']\n"
     ]
    }
   ],
   "source": [
    "df = dfs[salary_type]\n",
    "filtered_df = df[df.apply(\n",
    "    lambda row: is_data_engineering_job(\n",
    "        row['Job_title'],\n",
    "        salary_type\n",
    "    ),\n",
    "    axis=1)\n",
    "]\n",
    "\n",
    "show_unique_and_its_len(filtered_df['Job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[salary_type] = filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "del specializations_non_english, data_terms_non_english, invalid_non_eng, filtered_df, salary_type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Split `Location` column into `Region`, `Country`, `State` and `City`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_language_versions = {\n",
    "    'Arabic': [\"عن بعد\", \"مكتب منزلي\", \"العمل عن بعد\", \"مكتب افتراضي\", \"خارج الموقع\", \"العمل من أي مكان\", \"فريق موزع\", \"غير معتمد على الموقع\", \"قوة عمل متنقلة\", \"مكتب سحابي\", \"مساحة عمل عبر الإنترنت\", \"الرحالة الرقمي\", \"موقع مرن\", \"مكتب أي مكان\", \"عمل عن بعد\", \"مكان عمل افتراضي\", \"مكتب متنقل\", \"وظيفة تجوالية\", \"مكتب بلا حدود\", \"مكتب فرعي\", \"ممكّن عن بعد\", \"العمل عن بعد\"],\n",
    "    'Basque': [\"Urruneko\", \"Etxebizitza\", \"Telelan\", \"Birtual\", \"Kanpoaldeko\", \"Lan egitea nondik nahi izan\", \"Banatutako taldea\", \"Kokapenik gabeko\", \"Mugikor lan taldea\", \"Hodeiko bulegoa\", \"Online lan gunea\", \"Nabigatzaile digitala\", \"Kokapen aldaerazpena\", \"Inon edozein bulego\", \"Urruneko lan\", \"Birtual bulego\", \"Mugikor bulegoa\", \"Lan ibiltaria\", \"Mugikor ofizina\", \"Mugikor gunea\", \"Mendebalde ofizina\", \"Urruneko gaitasuna\", \"Urruneko lan\"],\n",
    "    'Catalan':  [\"Remot\", \"Oficina a casa\", \"Telecommute\", \"Oficina virtual\", \"Fora del lloc\", \"Treballa des de qualsevol lloc\", \"Equip distribuït\", \"Independent de la ubicació\", \"Força laboral mòbil\", \"Oficina a la núvol\", \"Espai de treball en línia\", \"Nòmada digital\", \"Ubicació flexible\", \"Oficina a qualsevol lloc\", \"Treball a distància\", \"Espai de treball virtual\", \"Oficina mòbil\", \"Feina itinerant\", \"Oficina sense fronteres\", \"Oficina satèl·lit\", \"Habilitat remotament\", \"Treball des de lluny\"],\n",
    "    'Czech': [\"Vzdálený\", \"Domácí kancelář\", \"Telekomunikace\", \"Virtuální kancelář\", \"Mimo provozovnu\", \"Práce odkudkoliv\", \"Rozptýlený tým\", \"Nezávislost na místě\", \"Mobilní pracovní síla\", \"Cloudová kancelář\", \"Online pracovní prostor\", \"Digitální nomád\", \"Flexibilní místo\", \"Kancelář kdekoli\", \"Práce na dálku\", \"Virtuální pracoviště\", \"Mobilní kancelář\", \"Pohyblivá práce\", \"Bezhraniční kancelář\", \"Satelitní kancelář\", \"Možnost práce na dálku\", \"Práce z dálky\"],\n",
    "    'German':  [\"Remote\", \"Home-Office\", \"Telearbeit\", \"Virtuelles Büro\", \"Externer Arbeitsplatz\", \"Arbeit von überall\", \"Verteiltes Team\", \"Ortsunabhängigkeit\", \"Mobile Belegschaft\", \"Cloud-Office\", \"Online-Arbeitsplatz\", \"Digitaler Nomade\", \"Flexible Arbeitsplätze\", \"Arbeitsplatz überall\", \"Fernarbeit\", \"Virtueller Arbeitsplatz\", \"Mobiles Büro\", \"Roaming-Job\", \"Grenzenloses Büro\", \"Satelliten-Büro\", \"Remote-fähig\", \"Arbeit von fern\", \"Dezentrale Arbeit\", \"Heimarbeit\", \"Telearbeit\"],\n",
    "    'Danish': [\"Fjern\", \"Hjemmekontor\", \"Telekommunikation\", \"Virtuelt kontor\", \"Uden for lokalerne\", \"Arbejd fra ethvert sted\", \"Fordelt hold\", \"Placering-uafhængig\", \"Mobil arbejdsstyrke\", \"Cloud-kontor\", \"Online arbejdsområde\", \"Digital nomade\", \"Fleksibel placering\", \"Kontor hvor som helst\", \"Distancearbejde\", \"Virtuelt arbejdssted\", \"Mobilkontor\", \"Roaming-job\", \"Grænseløst kontor\", \"Satellitkontor\", \"Fjern-kompatibel\", \"Arbejd fra lang afstand\", \"Eksternt arbejde\", \"Hjemmearbejde\", \"Telearbejde\"],\n",
    "    'Spanish':  [\"Remoto\", \"Oficina en casa\", \"Teletrabajo\", \"Oficina virtual\", \"Fuera del sitio\", \"Trabajar desde cualquier lugar\", \"Equipo distribuido\", \"Independencia del lugar\", \"Fuerza laboral móvil\", \"Oficina en la nube\", \"Espacio de trabajo en línea\", \"Nómada digital\", \"Ubicación flexible\", \"Oficina en cualquier lugar\", \"Trabajo a distancia\", \"Lugar de trabajo virtual\", \"Oficina móvil\", \"Trabajo itinerante\", \"Oficina sin fronteras\", \"Oficina satélite\", \"Habilitado para trabajar de forma remota\", \"Trabajo desde lejos\"],\n",
    "    'Finnish': [\"Etä-\", \"Kotitoimisto\", \"Etätyö\", \"Virtuaalitoimisto\", \"Poissa toimistolta\", \"Työskentely mistä tahansa\", \"Hajautettu tiimi\", \"Sijaintiriippumaton\", \"Mobiilityövoima\", \"Pilvitoimisto\", \"Verkkotyötila\", \"Digitaalinen kulkuri\", \"Joustava sijainti\", \"Missä tahansa toimisto\", \"Etätyöskentely\", \"Virtuaalityöpaikka\", \"Mobiilitoimisto\", \"Kiertävä työ\", \"Rajaton toimisto\", \"Satelliittoimisto\", \"Etätyö mahdollistettu\", \"Työskentely kaukaa\"],\n",
    "    'French': [\"À distance\", \"Télétravail\", \"Bureau virtuel\", \"Télétravailler\", \"Hors site\", \"Travailler de n'importe où\", \"Équipe distribuée\", \"Indépendant de l'emplacement\", \"Main-d'œuvre mobile\", \"Bureau de nuage\", \"Espace de travail en ligne\", \"Nomade digital\", \"Emplacement flexible\", \"Bureau n'importe où\", \"Travail à distance\", \"Espace de travail virtuel\", \"Bureau mobile\", \"Emploi itinérant\", \"Bureau sans frontières\", \"Bureau satellite\", \"Activé à distance\", \"Travailler à distance\"],\n",
    "    'Frisian': [\"Op ôfstân\", \"Thús kantoar\", \"Telekomme\", \"Firtueel kantoar\", \"Bûtenshûs\", \"Wurkje fan hokker plak dan ek mar\", \"Ferdield team\", \"Lokaasjefrij\", \"Mobile wurkforc\", \"Cloud kantoar\", \"Online wurkromte\", \"Digitale nomade\", \"Fleksibele lokaasje\", \"Dochs dêr kantoar\", \"Ofstân wurkje\", \"Virtuele wurkromte\", \"Mobile kantoar\", \"Roambanen\", \"Grenzelos kantoar\", \"Satellietkantoar\", \"Op ôfstân mooglik makke\", \"Wurkje fan fierren\"],\n",
    "    'Galician': [\"Remoto\", \"Oficina en casa\", \"Teletraballo\", \"Oficina virtual\", \"Fora do lugar de traballo\", \"Traballar desde calquera lugar\", \"Equipo distribuído\", \"Independente da localización\", \"Forza de traballo móbil\", \"Oficina en nube\", \"Espazo de traballo en liña\", \"Nómada dixital\", \"Localización flexible\", \"Oficina en calquera lugar\", \"Traballo a distancia\", \"Espazo de traballo virtual\", \"Oficina móbil\", \"Traballo itinerante\", \"Oficina sen fronteiras\", \"Oficina satélite\", \"Activado remotamente\", \"Traballar desde lonxe\"],\n",
    "    'Greek': [\"Απομακρυσμένο\", \"Γραφείο στο σπίτι\", \"Τηλεργασία\", \"Εικονικό γραφείο\", \"Εκτός έδρας\", \"Εργασία από οπουδήποτε\", \"Διανεμημένη ομάδα\", \"Ανεξάρτητος τόπος εργασίας\", \"Κινητό εργατικό δυναμικό\", \"Γραφείο στο cloud\", \"Διαδικτυακός χώρος εργασίας\", \"Ψηφιακός ταξιδιώτης\", \"Ευέλικτη τοποθεσία\", \"Γραφείο από οπουδήποτε\", \"Εργασία από απόσταση\", \"Εικονικός χώρος εργασίας\", \"Κινητό γραφείο\", \"Επαγγελματίας χωρίς σταθερή τοποθεσία\", \"Σατελλίτε γραφείο\", \"Επιτρεπόμενο απομακρυσμένο\", \"Εργασία από μακριά\"],\n",
    "    'Hebrew': [\"מרוחק\", \"משרד ביתי\", \"טלקום\", \"משרד וירטואלי\", \"מחוץ למשרד\", \"עבודה מכל מקום\", \"צוות מבוזר\", \"ללא תלות מיקום\", \"כוח עבודה נייד\", \"משרד ענן\", \"מרחב עבודה מקוון\", \"נומד דיגיטלי\", \"מיקום גמיש\", \"משרד מכל מקום\", \"עבודה מרחוק\", \"משרד וירטואלי\", \"משרד נייד\", \"עבודה רומנטית\", \"משרד לא מתפקד\", \"משרד סטליט\", \"אפשרי רחוק\", \"עבודה מרחוק\"],\n",
    "    'Hungarian': [\"Távoli\", \"Otthoni iroda\", \"Távmunka\", \"Virtuális iroda\", \"Távoli munkavégzés\", \"Elosztott csapat\", \"Helyfüggetlen\", \"Mobil munkaerő\", \"Felhő alapú iroda\", \"Online munkaterület\", \"Digitális nomád\", \"Rugalmas munkavégzés helye\", \"Bárholi iroda\", \"Távolléti munka\", \"Virtuális munkahely\", \"Mobil iroda\", \"Vándormunka\", \"Határok nélküli iroda\", \"Táviroda\", \"Távmunka engedélyezve\", \"Munka távolról\"],\n",
    "    'Italian': [\"Remoto\", \"Ufficio a casa\", \"Telelavoro\", \"Ufficio virtuale\", \"Fuori sede\", \"Lavorare ovunque\", \"Team distribuito\", \"Indipendenza dalla posizione\", \"Forza lavoro mobile\", \"Ufficio in cloud\", \"Spazio di lavoro online\", \"Nomade digitale\", \"Posizione flessibile\", \"Ufficio ovunque\", \"Lavoro a distanza\", \"Posto di lavoro virtuale\", \"Ufficio mobile\", \"Lavoro in itineranza\", \"Ufficio senza confini\", \"Ufficio satellite\", \"Abilitato al lavoro remoto\", \"Lavoro da lontano\"],\n",
    "    'Kurdish': [\"Dûrxistin\", \"Birca malê\", \"Telekomût\", \"Birca virtual\", \"Dîlber\", \"Karê ji her derê\", \"Tîma belavkirî\", \"Hînariya cîhêve\", \"Hêja kariyê\", \"Birca cloud\", \"Cîhê karê online\", \"Nomadê rûniştinê\", \"Cîhê karî kirinê kêfxweş\", \"Birca her derê\", \"Karê dûrve\", \"Cîhê karê rûniştinê\", \"Birca mobîl\", \"Kariya serderê\", \"Birca beşdarî nekirî\", \"Birca navendî\", \"Dabeşkirina karê dûrxistinê\", \"Karê ji dûrve\"],\n",
    "    'Dutch': [\"Afstandswerk\", \"Thuiswerkplek\", \"Telewerken\", \"Virtueel kantoor\", \"Buiten de deur\", \"Werk vanaf elke locatie\", \"Verspreid team\", \"Locatie-onafhankelijk\", \"Mobiele beroepsbevolking\", \"Cloud-kantoor\", \"Online werkruimte\", \"Digitale nomade\", \"Flexibele locatie\", \"Kantoor op elke locatie\", \"Werk op afstand\", \"Virtuele werkplek\", \"Mobiel kantoor\", \"Zwerfbaan\", \"Grenzeloos kantoor\", \"Satellietkantoor\", \"Op afstand mogelijk gemaakt\", \"Werken vanaf afstand\"],\n",
    "    'Norwegian': [\"Fjern\", \"Hjemmekontor\", \"Telekommunikasjon\", \"Virtuelt kontor\", \"Off-site\", \"Arbeid fra hvor som helst\", \"Distribuert team\", \"Stedsuavhengig\", \"Mobil arbeidsstyrke\", \"Skykontor\", \"Nettbasert arbeidsområde\", \"Digital nomade\", \"Fleksibelt sted\", \"Kontor hvor som helst\", \"Fjernarbeid\", \"Virtuelt arbeidsmiljø\", \"Mobilkontor\", \"Rundreisejobb\", \"Grenseløst kontor\", \"Satellittkontor\", \"Fjernaktivert\", \"Arbeid fra avstand\"],\n",
    "    'Polish':  [\"Zdalny\", \"Praca zdalna\", \"Telepraca\", \"Wirtualne biuro\", \"Praca poza siedzibą\", \"Praca z dowolnego miejsca\", \"Zespoły rozproszone\", \"Nieuzależniony od miejsca pracy\", \"Mobilna siła robocza\", \"Biuro w chmurze\", \"Przestrzeń robocza online\", \"Cyfrowy nomada\", \"Elastyczne miejsce pracy\", \"Biuro w dowolnym miejscu\", \"Praca na odległość\", \"Wirtualne miejsce pracy\", \"Mobilne biuro\", \"Praca mobilna\", \"Biuro bez granic\", \"Biuro satelitarne\", \"Zdalnie zarządzany\", \"Praca zdalna\"],\n",
    "    'Portuguese': [\"Remoto\", \"Escritório em casa\", \"Teletrabalho\", \"Escritório virtual\", \"Fora do local\", \"Trabalho em qualquer lugar\", \"Equipe distribuída\", \"Independente de localização\", \"Força de trabalho móvel\", \"Escritório em nuvem\", \"Espaço de trabalho online\", \"Nômade digital\", \"Localização flexível\", \"Escritório em qualquer lugar\", \"Trabalho à distância\", \"Local de trabalho virtual\", \"Escritório móvel\", \"Trabalho itinerante\", \"Escritório sem fronteiras\", \"Escritório satélite\", \"Habilitado para trabalho remoto\", \"Trabalho a distância\"],\n",
    "    'Romanian': [\"La distanță\", \"Lucru de acasă\", \"Telecomutare\", \"Birou virtual\", \"În afara locului de muncă\", \"Lucru de oriunde\", \"Echipa distribuită\", \"Independență față de locație\", \"Forță de muncă mobilă\", \"Birou în cloud\", \"Spațiu de lucru online\", \"Nomad digital\", \"Locație flexibilă\", \"Birou oriunde\", \"Lucru la distanță\", \"Loc de muncă virtual\", \"Birou mobil\", \"Muncă itinerantă\", \"Birou fără granițe\", \"Birou satelit\", \"Activat pentru lucru la distanță\", \"Lucru de departe\"],\n",
    "    'Slovakian': [\"Vzdialený\", \"Domáca kancelária\", \"Telekomutácia\", \"Virtuálna kancelária\", \"Mimo pracoviska\", \"Práca z akéhokoľvek miesta\", \"Distribuovaný tím\", \"Nezávislosť na mieste\", \"Mobilná pracovná sila\", \"Cloudová kancelária\", \"Online pracovný priestor\", \"Digitálny nomád\", \"Flexibilné miesto\", \"Kancelária kdekoľvek\", \"Práca na diaľku\", \"Virtuálna pracovná plocha\", \"Mobilná kancelária\", \"Roamingová práca\", \"Bezhraničná kancelária\", \"Satelitná kancelária\", \"Vzdialene zapojený\", \"Práca z diaľky\"],\n",
    "    'Slovenian': [\"Oddaljeno\", \"Domača pisarna\", \"Telekomutiranje\", \"Virtualna pisarna\", \"Oddaljeno delo\", \"Delo od koderkoli\", \"Distribuirana ekipa\", \"Lokacijsko neodvisno\", \"Mobilna delovna sila\", \"Oblak pisarna\", \"Spletni delovni prostor\", \"Digitalni nomad\", \"Fleksibilna lokacija\", \"Pisarna kjerkoli\", \"Delo na daljavo\", \"Virtualno delovno okolje\", \"Mobilna pisarna\", \"Potujoče delo\", \"Brezmejna pisarna\", \"Satelitska pisarna\", \"Oddaljeno omogočeno\", \"Delo od daleč\"],\n",
    "    'Swedish': [\"Distans\", \"Hemmakontor\", \"Telekommunikation\", \"Virtuellt kontor\", \"Utomhusarbete\", \"Arbeta från vilken plats som helst\", \"Distribuerat team\", \"Plats oberoende\", \"Mobil arbetsstyrka\", \"Molnkontor\", \"Online arbetsutrymme\", \"Digital nomad\", \"Flexibel plats\", \"Kontor var som helst\", \"Distansarbete\", \"Virtuellt arbetsområde\", \"Mobilt kontor\", \"Roaming-jobb\", \"Gränslöst kontor\", \"Satellitkontor\", \"Fjärrstyrt\", \"Arbeta från avlägsna platser\"],\n",
    "    'Turkish': [\"Uzaktan\", \"Ev ofisi\", \"Uzaktan çalışma\", \"Sanal ofis\", \"Ofis dışı\", \"Herhangi bir yerden çalışma\", \"Dağıtılmış ekip\", \"Konum bağımsız\", \"Mobil işgücü\", \"Bulut ofis\", \"Çevrimiçi çalışma alanı\", \"Dijital gezgin\", \"Esnek konum\", \"Her yerde ofis\", \"Uzaktan çalışma\", \"Sanal işyeri\", \"Mobil ofis\", \"Gezici iş\", \"Sınır tanımayan ofis\", \"Uydu ofisi\", \"Uzaktan çalışmaya uygun\", \"Uzaktan çalışma\"],\n",
    "    'Japanese': [\"在宅勤務\", \"テレワーク\", \"バーチャルオフィス\", \"オフサイト勤務\", \"どこでも勤務\", \"分散チーム\", \"場所に依存しない\", \"モバイルワークフォース\", \"クラウドオフィス\", \"オンラインワークスペース\", \"デジタルノマド\", \"柔軟な場所\", \"どこでもオフィス\", \"遠隔勤務\", \"仮想ワークプレイス\", \"モバイルオフィス\", \"ローミングジョブ\", \"境界のないオフィス\", \"サテライトオフィス\", \"リモート対応\", \"遠隔からの勤務\"],\n",
    "    'Korean': [\"재택 근무\", \"원격근무\", \"가상사무실\", \"사이트 외 근무\", \"어디서든 근무\", \"분산 팀\", \"위치 독립적\", \"모바일 근무\", \"클라우드 사무실\", \"온라인 작업 공간\", \"디지털 노마드\", \"유연한 위치\", \"어디서든 사무실\", \"원격 근무\", \"가상 작업장\", \"모바일 사무실\", \"로밍 직업\", \"경계없는 사무실\", \"위성 사무실\", \"원격 작동 가능\", \"멀리서 근무\"],\n",
    "    'Chinese_TR': [\"居家辦公\", \"遠端辦公\", \"虛擬辦公室\", \"外地辦公\", \"全球任務\", \"分散式團隊\", \"不受地理限制\", \"流動辦公\", \"雲端辦公室\", \"線上工作空間\", \"數位遊牧者\", \"靈活的工作地點\", \"無所不在的辦公室\", \"遠程工作\", \"虛擬工作場所\", \"行動辦公室\", \"漫遊工作\", \"無邊界辦公室\", \"衛星辦公室\", \"遠端啟用\", \"遠距工作\"],\n",
    "    'Chinese_SP': [\"远程工作\", \"家庭办公室\", \"远程办公\", \"虚拟办公室\", \"外场办公\", \"随处办公\", \"分散团队\", \"无固定办公地\", \"移动式劳动力\", \"云办公室\", \"在线工作空间\", \"数字游牧者\", \"灵活的位置\", \"无处不办公\", \"远程工作\", \"虚拟工作场所\", \"移动办公室\", \"漫游工作\", \"无边界的办公室\", \"卫星办公室\", \"远程启用\", \"远程工作\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\725771115.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Location'] = df['Location'].apply(lambda location: np.nan if location is np.nan or not isinstance(location, str) \\\n"
     ]
    }
   ],
   "source": [
    "def apply_locations(df, country, region):\n",
    "\n",
    "    def replace_underscore(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        for col in columns:\n",
    "            df[col] = df[col].str.replace('_', ' ')\n",
    "        return df\n",
    "\n",
    "    def remove_leading_trailing(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "        for col in columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean_columns_strings(df: pd.DataFrame, columns: list['str']):\n",
    "\n",
    "        df = replace_underscore(df, columns)\n",
    "\n",
    "        df = remove_leading_trailing(df, columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # All those fancy, pansy names for the remote\n",
    "    remote = [\n",
    "        \"Home office\",\n",
    "        \"Telecommute\",\n",
    "        \"Virtual office\",\n",
    "        \"Off-site\",\n",
    "        \"Work from anywhere\",\n",
    "        \"Distributed team\",\n",
    "        \"Location-independent\",\n",
    "        \"Mobile workforce\",\n",
    "        \"Cloud office\",\n",
    "        \"Online workspace\",\n",
    "        \"Digital nomad\",\n",
    "        \"Flexible location\",\n",
    "        \"Anywhere office\",\n",
    "        \"Distance work\",\n",
    "        \"Virtual workplace\",\n",
    "        \"Mobile office\",\n",
    "        \"Roaming job\",\n",
    "        \"Borderless office\",\n",
    "        \"Satellite office\",\n",
    "        \"Remote-enabled\",\n",
    "        \"Work from afar\"\n",
    "    ]\n",
    "\n",
    "    country_languages = countries_languages[country]\n",
    "\n",
    "    for language in country_languages:\n",
    "\n",
    "        not_eng_language = remote_language_versions[language]\n",
    "\n",
    "        remote.extend(not_eng_language)\n",
    "\n",
    "    country_languages =  [x.lower() for x in country_languages]\n",
    "\n",
    "\n",
    "    df['Location'] = df['Location'].apply(lambda location: np.nan if location is np.nan or not isinstance(location, str) \\\n",
    "                                          or location.strip() == \"\" else location)\n",
    "\n",
    "    df = df.apply(lambda row: \"Remote\" \\\n",
    "                  if (row['Location'] is not np.nan and isinstance(row['Location'], str) \\\n",
    "                    and row['Location'].lower() in remote) \\\n",
    "                    or \\\n",
    "                    (row['Job_title'] is not np.nan and isinstance(row['Job_title'], str) \\\n",
    "                    and row['Job_title'].lower() in remote) \\\n",
    "                    else row \n",
    "                    , axis=1)\n",
    "    \n",
    "\n",
    "    df['City'] = df['Location'].apply(lambda location: location.split(',')[0].strip() if location is not np.nan \\\n",
    "                                      and isinstance(location, str) and \",\" in location else location)\n",
    "    df['State'] = df['Location'].apply(lambda location: location.split(',')[1].strip() if location is not np.nan \\\n",
    "                                       and isinstance(location, str) and \",\" in location else np.nan)\n",
    "\n",
    "    df['Country'] = country\n",
    "    df['Region'] = region\n",
    "\n",
    "    df = clean_columns_strings(df, ['Region', 'Country', 'State', 'City'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_regions():\n",
    "\n",
    "    for country_name, country_df in dfs.items():\n",
    "        region = np.nan\n",
    "        if country_name in [\"United_States\", \"Canada\"]:\n",
    "            region = \"North America\"\n",
    "        elif country_name in [\"Japan\", \"Singapore\", \"Hong_Kong\", \"Taiwan\", \"South_Korea\"]:\n",
    "            region = \"Asia\"\n",
    "        elif country_name in [\"Australia\", \"New_Zealand\"]:\n",
    "            region = \"Oceania\"\n",
    "        elif country_name in [\"Austria\", \"Belgium\", \"Bulgaria\", \"Croatia\", \"Cyprus\", \"Canada\", \"Czech_Republic\", \"Denmark\", \"Estonia\", \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \"Ireland\", \"Israel\", \"Italy\", \"Luxembourg\", \"Netherlands\", \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Spain\", 'Sweden', \"Switzerland\", \"Turkey\", \"United_Kingdom\"]:\n",
    "            region = \"Europe\"\n",
    "        else:\n",
    "            raise KeyError(f\"\\rUnknown region/continent for:\\n{country_name}\")\n",
    "\n",
    "        dfs[country_name] = apply_locations(country_df, country_name, region)\n",
    "\n",
    "add_regions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Vienna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      City  State  Country  Region\n",
       "9   Vienna    NaN  Austria  Europe\n",
       "10  Vienna    NaN  Austria  Europe\n",
       "11  Vienna    NaN  Austria  Europe\n",
       "12  Vienna    NaN  Austria  Europe\n",
       "15  Vienna    NaN  Austria  Europe"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].iloc[: , -4:30].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Long Beach</td>\n",
       "      <td>CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trenton</td>\n",
       "      <td>NJ</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Irvine</td>\n",
       "      <td>CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City State        Country         Region\n",
       "0  Long Beach    CA  United States  North America\n",
       "1     Trenton    NJ  United States  North America\n",
       "2      Irvine    CA  United States  North America\n",
       "3      Remote   NaN  United States  North America\n",
       "4  Washington    DC  United States  North America"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States'].iloc[: , -4:30].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CA', 'NJ', nan, 'DC', 'NY', 'MA', 'MN', 'GA', 'TX', 'VA', 'CO',\n",
       "       'NC', 'DE', 'MI', 'NV', 'OH', 'FL', 'IL', 'UT', 'WI', 'WA', 'IA',\n",
       "       'AZ', 'KY', 'RI', 'TN', 'MD', 'OR', 'IN', 'PA', 'MT', 'CT', 'NH',\n",
       "       'HI', 'MO'], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States']['State'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   City  State      Country Region\n",
       "0   NaN    NaN  South Korea   Asia\n",
       "1   NaN    NaN  South Korea   Asia\n",
       "2   NaN    NaN  South Korea   Asia\n",
       "3   NaN    NaN  South Korea   Asia\n",
       "5   NaN    NaN  South Korea   Asia"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['South_Korea'].iloc[: , -4:30].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>North Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New South Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              City  State    Country   Region\n",
       "1        Melbourne    NaN  Australia  Oceania\n",
       "2     North Sydney    NaN  Australia  Oceania\n",
       "3           Sydney    NaN  Australia  Oceania\n",
       "4  New South Wales    NaN  Australia  Oceania\n",
       "5        Melbourne    NaN  Australia  Oceania"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Australia'].iloc[: , -4:30].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_remote_stats():\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    for country, df in dfs.items():\n",
    "        location_col = df['Location']\n",
    "\n",
    "        location_total = np.nan\n",
    "        location_remote = np.nan\n",
    "        location_onside = np.nan\n",
    "        location_remote_percent = np.nan\n",
    "\n",
    "        if location_col.dtype == 'object':\n",
    "            location_total = location_col.count()\n",
    "            location_remote = int(location_col.str.count('Remote').sum())\n",
    "            location_onside = int(location_total - location_remote)\n",
    "\n",
    "            if location_total != 0 and location_remote != 0:\n",
    "                location_remote_percent = int(round(location_remote/location_total, 2) * 100)\n",
    "            else:\n",
    "                location_remote_percent = 0\n",
    "\n",
    "        result[country] = (location_remote, location_onside, location_remote_percent)\n",
    "\n",
    "    result\n",
    "\n",
    "show_remote_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Remote         48\n",
       "New York       11\n",
       "Chicago        10\n",
       "Boston          8\n",
       "Dallas          8\n",
       "               ..\n",
       "Louisville      1\n",
       "Burbank         1\n",
       "Chandler        1\n",
       "Iowa City       1\n",
       "Saint Louis     1\n",
       "Name: City, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States']['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "London                   39\n",
       "Remote                   26\n",
       "Leeds                    10\n",
       "Bristol                   6\n",
       "United Kingdom            6\n",
       "Glasgow                   4\n",
       "Edinburgh                 4\n",
       "Halifax                   2\n",
       "Belfast                   2\n",
       "Reading                   2\n",
       "Manchester                1\n",
       "Birmingham                1\n",
       "Whitley Bay               1\n",
       "Scotland                  1\n",
       "Royal Tunbridge Wells     1\n",
       "Brentford                 1\n",
       "Cambridge                 1\n",
       "Derby                     1\n",
       "Stoke Poges               1\n",
       "Bracknell                 1\n",
       "Milton Keynes             1\n",
       "Sunbury                   1\n",
       "Newcastle upon Tyne       1\n",
       "Southampton               1\n",
       "Chester                   1\n",
       "Windsor                   1\n",
       "Name: City, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_Kingdom']['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "del add_regions, remote_language_versions, show_remote_stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Remove `Location` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in dfs.values():\n",
    "    del df['Location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company_name', 'Rating', 'Job_title', 'Description', 'Job_age',\n",
       "       'Easy_apply', 'Salary', 'Employees', 'Type_of_ownership', 'Sector',\n",
       "       'Founded', 'Industry', 'Revenue_USD', 'Friend_recommend',\n",
       "       'CEO_approval', 'Career_opportunities', 'Comp_&_benefits',\n",
       "       'Culture_&_values', 'Senior_management', 'Work/Life_balance', 'Pros',\n",
       "       'Cons', 'Benefits_rating', 'Benefits_reviews', 'City', 'State',\n",
       "       'Country', 'Region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company_name', 'Rating', 'Job_title', 'Description', 'Job_age',\n",
       "       'Easy_apply', 'Salary', 'Employees', 'Type_of_ownership', 'Sector',\n",
       "       'Founded', 'Industry', 'Revenue_USD', 'Friend_recommend',\n",
       "       'CEO_approval', 'Career_opportunities', 'Comp_&_benefits',\n",
       "       'Culture_&_values', 'Senior_management', 'Work/Life_balance', 'Pros',\n",
       "       'Cons', 'Benefits_rating', 'Benefits_reviews', 'City', 'State',\n",
       "       'Country', 'Region'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States'].columns == dfs['Austria'].columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Add job title seniority"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that some companies, especially FANG, have some unique seniority tiles, like: \"Level I', \"Level II\", etc. We didn't cover those examples, because they are too specific to the particular company. And each level means something different, depending on the company.\n",
    "\n",
    "Also it's worth to keep things simple and avoid seniority titles inflation, like \"Principle Senior Executive Manager\" etc.\n",
    "\n",
    "Of course, in some cases, it is disputable if management positions are more \"senior\" than senior positions. But it's good enough in our case. Senior we treat here more as \"Doer\", \"Management\" more as task coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "seniorities_translations= {\n",
    "    'Arabic': {\n",
    "        'Junior' : [\"مبتدئ\", \"مبتدئ\", \"تدريب\", \"متدرب\", \"متدرب\", \"شابّ حرفي\", \"مبتدئ\", \"مبتدئ\", \"تجريبي\"],\n",
    "        'Mid' : [\"وسط\", \"مساعد\", \"الإداري\"],\n",
    "        'Senior': [\"كبير\", \"مسن\", \"زعيم\", \"مبدأ\"], \n",
    "        'Management' : [\"مدير\", \"رئيس\", \"مدير عام\", \"رئيس تنفيذي\", \"مشرف\", \"منسق\", \"تنفيذي\"]\n",
    "    },\n",
    "    'Basque': {\n",
    "        'Junior' : [\"Praktikak\", \"Praktikante\", \"Lantaldean\", \"Lantokiari\", \"Hasiberri\", \"Hasi\", \"Proba\"],\n",
    "        'Mid' : [\"Erdi\", \" Kide Laguntzailea\"],\n",
    "        'Senior': [\"Zk.\", \"Seneur\", \"Lider\", \"Printzipio\"],\n",
    "        'Management' : [\"Kudeatzailea\", \"Burua\", \"Zuzendaria\", \"Buruzagia\", \"Buruzagi\", \"Koordinatzailea\", \"Gobernatzailea\", \"Ejecutivoa\"]\n",
    "    },\n",
    "    'Catalan': {\n",
    "        'Junior' : [\"Pràctiques\", \"Practicant\", \"Aprenent\", \"Aprendiz\", \"Novell\", \"Principiant\", \"Probatori\"],\n",
    "        'Mid' : [\"Middle\", \"Associat\", \"Associada\"],\n",
    "        'Senior': [\"Snr.\", \"Sènior\", \"Líder\", \"Principi\"],\n",
    "        'Management' : [\"Gerent\", \"Cap de\", \"Director\", \"Cap\", \"Supervisor\", \"Coordinador\", \"Executiu\"]\n",
    "    },\n",
    "    'Czech': {\n",
    "        'Junior' : [\"Stáž\", \"Stážista\", \"Staženík\", \"Učeň\", \"Nováček\", \"Začátečník\", \"Zkušební doba\"],\n",
    "        'Mid' : [\"Střední\", \"Asociát\"],\n",
    "        'Senior': [\"Sr.\", \"Starší\", \"Vedoucí\", \"Princip\"],\n",
    "        'Management' : [\"Manažer\", \"Šéf\", \"Ředitel\", \"Šéf\", \"Dozorce\", \"Koordinátor\", \"Výkonný\"]\n",
    "    },\n",
    "    'German': {\n",
    "        'Junior' : [\"Jr.\", \"Junior\", \"Praktikum\", \"Praktikant\", \"Auszubildender\", \"Lehrling\", \"Neuling\", \"Anfänger\", \"Probezeit\"],\n",
    "        'Mid' : [\"Mittel\", \"Assoziierter\", \"Assoziierte\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Leiter\", \"Prinzip\"],\n",
    "        'Management' : [\"Manager\", \"Leiter\", \"Direktor\", \"Chef\", \"Vorgesetzter\", \"Koordinator\", \"Executive\"]\n",
    "    },\n",
    "    'Danish': {\n",
    "        'Junior' : [\"Praktik\", \"Praktikant\", \"Trainee\", \"Lærling\", \"Nybegynder\", \"Begynder\", \"Prøvetid\"],\n",
    "        'Mid' : [\"Mellem\", \"Associeret\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Leder\", \"Princip\"],\n",
    "        'Management': [\"Manager\", \"Hoved af\", \"Direktør\", \"Chef\", \"Supervisor\", \"Koordinator\", \"Executive\"]\n",
    "    },\n",
    "    'Spanish': {\n",
    "        'Junior' : [\"Prácticas\", \"Practicante\", \"Practicante\", \"Aprendiz\", \"Novato\", \"Principiante\", \"De prueba\"],\n",
    "        'Mid' : [\"Medio\", \"Intermedio\", \"Asociado\", \"Asociada\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Líder\", \"Principio\"],\n",
    "        'Management' : [\"Gerente\", \"Jefe de\", \"Director\", \"Jefe\", \"Supervisor\", \"Coordinador\", \"Ejecutivo\"]\n",
    "    },\n",
    "    'Finnish': {\n",
    "        'Junior' : [\"Nuorempi\", \"Harjoittelu\", \"Harjoittelija\", \"Harjoittelija\", \"Oppipoika\", \"Uusi\", \"Aloittelija\", \"Koeaika\"],\n",
    "        'Mid' : [\"Keski-\", \"Keskivaiheen\", \"Yhdistynyt\"],\n",
    "        'Senior': [\"Sr.\", \"Seniori\", \"Johtaja\", \"Periaate\"],\n",
    "        'Management' : [\"Johtaja\", \"Pää\", \"Johtaja\", \"Päällikkö\", \"Valvoja\", \"Koordinaattori\", \"Toteuttava\"]\n",
    "    },\n",
    "    'French': {\n",
    "        'Junior' : [\"Stage\", \"Stagiaire\", \"Stagiaire\", \"Apprenti\", \"Novice\", \"Débutant\", \"Période d'essai\"],\n",
    "        'Mid' : [\"Milieu\", \"Associé\", \"Associée\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Principe\"],\n",
    "        'Management' : [\"Directeur\", \"Chef de\", \"Directeur\", \"Chef\", \"Superviseur\", \"Coordinateur\", \"Exécutif\"]\n",
    "    },\n",
    "    'Frisian': {\n",
    "        'Junior' : [\"Stazjê\", \"Stazjê\", \"Trainee\", \"Learling\", \"Nijkommer\", \"Nijbegjinne\", \"Probearperiode\"],\n",
    "        'Mid' : [\"Mids\", \"Midden\", \"Ferbûn\"],\n",
    "        'Senior': [\"Snr.\", \"Sinnior\", \"Lieder\", \"Prinsipe\"],\n",
    "        'Management' : [\"Manager\", \"Holwer\", \"Direkteur\", \"Chef\", \"Taufersjoch\", \"Koördinator\", \"Executive\"]\n",
    "    },\n",
    "    'Galician': {\n",
    "        'Junior' : [\"Novo\", \"Prácticas\", \"Estudante en prácticas\", \"Trainee\", \"Aprendiz\", \"Novato\", \"Principiante\", \"Probatorio\"],\n",
    "        'Mid' : [\"Medio\", \"Intermedio\", \"Asociado\", \"Asociada\"],\n",
    "        'Senior': [\"Snr.\", \"Sénior\", \"Líder\", \"Principio\"],\n",
    "        'Management' : [\"Xestor\", \"Xefe de\", \"Director\", \"Xefe\", \"Supervisor\", \"Coordinador\", \"Executivo\"]\n",
    "    },\n",
    "    'Greek': {\n",
    "        'Junior' : [\"Νεαρός\", \"Πρακτική άσκηση\", \"Πρακτικός\", \"Εκπαιδευόμενος\", \"Μαθητευόμενος\", \"Νέος\", \"Αρχάριος\", \"Δοκιμαστική περίοδος\"],\n",
    "        'Mid' : [\"Μεσαίος\", \"Μεσαίας\", \"Συνδεδεμένος \"],\n",
    "        'Senior': [\"Κος.\", \"Γερός\", \"Αρχηγός\", \"Αρχή\"],\n",
    "        'Management' : [\"Διευθυντής\", \"Αρχηγός\", \"Διευθυντής\", \"Αρχηγός\", \"Επόπτης\", \"Συντονιστής\", \"Εκτελεστικός\"]\n",
    "    },\n",
    "    'Hebrew': {\n",
    "        'Junior' : [\"תקופת הכשרה\", \"סטודנט לתקופת הכשרה\", \"מתמחה\", \"חניך\", \"חדש\", \"מתחיל\", \"תקופת ניסיון\"],\n",
    "        'Mid' : [\"אמצעי\", \"אמצעיים\", \"שותף\"],\n",
    "        'Senior': [\"סמסטר\", \"בכיר\", \"מנהיג\", \"עקרון\"],\n",
    "        'Management' : [\"מנהל\", \"ראש\", \"מנכ״ל\", \"ראשי\", \"מפקח\", \"מתאם\", \"מנהלי\"]\n",
    "    },\n",
    "    'Hungarian': {\n",
    "        'Junior' : [\"Jr.\", \"Junior\", \"Gyakornok\", \"Gyakornok\", \"Tanuló\", \"Apprentice\", \"Újonc\", \"Kezdő\", \"Próbaidős\"],\n",
    "        'Mid' : [\"Közép\", \"Középső\", \"Társult\"],\n",
    "        'Senior': [\"Sr.\", \"Idősebb\", \"Vezető\", \"Elv\"],\n",
    "        'Management' : [\"Menedzser\", \"Igazgató\", \"Fő\", \"Felügyelő\", \"Koordinátor\"]\n",
    "    },\n",
    "    'Italian': {\n",
    "        'Junior' : [\"Jr.\", \"Junior\", \"Stage\", \"Tirocinante\", \"Apprendista\", \"Apprendista\", \"Neofita\", \"Principiante\", \"In prova\"],\n",
    "        'Mid' : [\"Middle\", \"Associato\", \"Associata\"],\n",
    "        'Senior': [\"Sig.\", \"Senior\", \"Capo\", \"Principio\"],\n",
    "        'Management' : [\"Manager\", \"Capo di\", \"Direttore\", \"Capo\", \"Supervisore\", \"Coordinatore\", \"Esecutivo\"]\n",
    "    },\n",
    "    'Kurdish': {\n",
    "        'Junior' : [\"Stajyerî\", \"Stajyer\", \"Dîlmej\", \"Şagirt\", \"Nûjen\", \"Destpêkê\", \"Probasyon\"],\n",
    "        'Mid' : [\"Navend\", \"Pêdivî\", \"Pêdivîya\"],\n",
    "        'Senior': [\"Snr.\", \"Sînîor\", \"Serok\", \"Maf\"],\n",
    "        'Management' : [\"Bazirgani\", \"Serek\", \"Dirêjor\", \"Sereke\", \"Supervîzekar\", \"Koordinatêr\", \"Xebatkari\"]\n",
    "    },\n",
    "    'Dutch': {\n",
    "        'Junior' : [\"Stage\", \"Stagiair\", \"Leerling\", \"Leerling\", \"Beginner\", \"Beginnend\", \"Proef\"],\n",
    "        'Mid' : [\"Midden\", \"Associé\", \"Associée\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Leidinggevende\", \"Principe\"],\n",
    "        'Management' : [\"Manager\", \"Hoofd van\", \"Directeur\", \"Chef\", \"Toezichthouder\", \"Coördinator\", \"Uitvoerend\"]\n",
    "    },\n",
    "    'Norwegian': {\n",
    "        'Junior' : [\"Internship\", \"Intern\", \"Lærling\", \"Lærling\", \"Nybegynner\", \"Nybegynner\", \"Prøvetid\"],\n",
    "        'Mid' : [\"Midten\", \"Mellom\", \"Assosiert\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Leder\", \"Prinsipp\"],\n",
    "        'Management' : [\"Leder\", \"Hode av\", \"Direktør\", \"Sjef\", \"Veileder\", \"Koordinator\", \"Utførende\"]\n",
    "    },\n",
    "    'Polish': {\n",
    "        'Junior' : [\"Staż\", \"Stażysta\", \"Praktykant\", \"Praktykant\", \"Początkujący\", \"Nowicjusz\", \"Okres próbny\"],\n",
    "        'Mid' : [\"Średni\", \"Średniego\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Lider\"],\n",
    "        'Management' : [\"Manager\", \"Kierownik\", \"Dyrektor\", \"Szef\", \"Przełożony\", \"Koordynator\", \"Wykonawczy\"]\n",
    "    },\n",
    "    'Portuguese': {\n",
    "        'Junior' : [\"Jr.\", \"Júnior\", \"Estágio\", \"Estagiário\", \"Estagiário\", \"Aprendiz\", \"Novato\", \"Iniciante\", \"Probatório\"],\n",
    "        'Mid' : [\"Médio\", \"Média\", \"Associado\", \"Associada\"],\n",
    "        'Senior': [\"Sr.\", \"Sênior\", \"Líder\", \"Princípio\"],\n",
    "        'Management' : [\"Gerente\", \"Chefe de\", \"Diretor\", \"Chefe\", \"Supervisor\", \"Coordenador\", \"Executivo\"]\n",
    "    },\n",
    "    'Romanian': {\n",
    "        'Junior' : [\"Internship\", \"Intern\", \"Stagiar\", \"Stagiar\", \"Începător\", \"Novice\", \"Perioada de probă\"],\n",
    "        'Mid' : [\"Mijlociu\", \"Asociat\", \"Asociată\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Lider\", \"Principiu\"],\n",
    "        'Management' : [\"Manager\", \"Șef de\", \"Director\", \"Șef\", \"Supraveghetor\", \"Coordonator\", \"Executiv\"]\n",
    "    },\n",
    "    'Slovakian': {\n",
    "        'Junior' : [\"Stáž\", \"Stážista\", \"Učeň\", \"Učeň\", \"Nováčik\", \"Začiatočník\", \"Skúšobná doba\"],\n",
    "        'Mid' : [\"Stred\", \"Asociat\", \"Asociátka\"],\n",
    "        'Senior': [\"Srk.\", \"Starší\", \"Vedúci\", \"Princíp\"],\n",
    "        'Management' : [\"Manažér\", \"Vedúci\", \"Riaditeľ\", \"Šéf\", \"Dozorca\", \"Koordinátor\", \"Výkonný\"]\n",
    "    },\n",
    "    'Slovenian': {\n",
    "        'Junior' : [\"Staž\", \"Stažist\", \"Vajenec\", \"Vajenec\", \"Novinec\", \"Novinec\", \"Poskusno\"],\n",
    "        'Mid' : [\"Srednji\", \"Povezan\", \"Povezana\"],\n",
    "        'Senior': [\"G. g.\", \"Starejši\", \"Vodja\", \"Načelo\"],\n",
    "        'Management' : [\"Vodja\", \"Vodja\", \"Direktor\", \"Šef\", \"Nadzornik\", \"Koordinator\", \"Izvršni\"]\n",
    "    },\n",
    "    'Swedish': {\n",
    "        'Junior' : [\"Praktik\", \"Praktikant\", \"Lärling\", \"Lärling\", \"Nykomling\", \"Nybörjare\", \"Prövotid\"],\n",
    "        'Mid' : [\"Mellan\", \"Associerad\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Ledare\", \"Princip\"],\n",
    "        'Management' : [\"Chef\", \"Huvud av\", \"Direktör\", \"Chef\", \"Övervakare\", \"Koordinator\", \"Utförande\"]\n",
    "    },\n",
    "    'Turkish': {\n",
    "        'Junior' : [\"Stajyerlik\", \"Stajyer\", \"Stajyer\", \"Çırak\", \"Acemi\", \"Yeni başlayan\", \"Deneme süresi\"],\n",
    "        'Mid' : [\"Orta\", \"Ortak\", \"Orta düzeyli\"],\n",
    "        'Senior': [\"Sn.\", \"Kıdemli\", \"Lider\", \"Prensip\"],\n",
    "        'Management' : [\"Yönetici\", \"Başkanı\", \"Direktör\", \"Şef\", \"Denetçi\", \"Koordinatör\", \"Yürütücü\"]\n",
    "    },\n",
    "    'Japanese': {\n",
    "        'Junior' : [\"ジュニア\", \"インターンシップ\", \"インターン\", \"トレーニー\", \"見習い\", \"初心者\", \"ビギナー\", \"試用期間\"],\n",
    "        'Mid' : [\"ミッドレベル\", \"アソシエイト\", \"ミドル\"],\n",
    "        'Senior': [\"Sr.\", \"シニア\", \"リーダー\", \"原則\"],\n",
    "        'Management' : [\"マネージャー\", \"ヘッドオブ\", \"ディレクター\", \"チーフ\", \"スーパーバイザー\", \"コーディネーター\", \"エグゼクティブ\"]\n",
    "    },\n",
    "    'Korean': {\n",
    "        'Junior' : [\"주니어\", \"인턴십\", \"인턴\", \"연수생\", \"견습생\", \"초보자\", \"비전문가\", \"수습기간\"],\n",
    "        'Mid' : [\"중급\", \"어소시에이트\"],\n",
    "        'Senior': [\"선배\", \"시니어\", \"리더\", \"원칙\"],\n",
    "        'Management' : [\"매니저\", \"대표\", \"디렉터\", \"최고\", \"감독자\", \"코디네이터\", \"집행\"]\n",
    "    },\n",
    "    'Chinese_TR': {\n",
    "        'Junior' : [\"初級\", \"實習\", \"實習生\", \"見習生\", \"學徒\", \"新手\", \"初學者\", \"試用期\"],\n",
    "        'Mid' : [\"中級\", \"聯合\"],\n",
    "        'Senior': [\"高級\", \"資深\", \"領導\", \"原則\"],\n",
    "        'Management' : [\"經理\", \"負責人\", \"董事\", \"主管\", \"監督者\", \"協調人\", \"執行\"]\n",
    "    },\n",
    "    'Chinese_SP': {\n",
    "        'Junior' : [\"Jr.\", \"初级\", \"实习\", \"实习生\", \"见习生\", \"学徒\", \"新手\", \"初学者\", \"试用期\"],\n",
    "        'Mid' : [\"中级\", \"联合\"],\n",
    "        'Senior': [\"高级\", \"高级\", \"领导\", \"原则\"],\n",
    "        'Management' : [\"经理\", \"负责人\", \"董事\", \"主管\", \"监督者\", \"协调人\", \"执行\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Austria': ['German'],\n",
       " 'Belgium': ['French', 'Dutch', 'German'],\n",
       " 'Canada': ['French'],\n",
       " 'Czech_Republic': ['Czech', 'Slovakian', 'Hungarian'],\n",
       " 'Denmark': ['Danish'],\n",
       " 'Finland': ['Finnish', 'Swedish'],\n",
       " 'France': ['French', 'Catalan', 'Italian', 'Basque'],\n",
       " 'Germany': ['German'],\n",
       " 'Greece': ['Greek'],\n",
       " 'Hungary': ['Hungarian', 'Romanian'],\n",
       " 'Ireland': [],\n",
       " 'Israel': ['Hebrew', 'Arabic'],\n",
       " 'Italy': ['Italian', 'German', 'French', 'Catalan', 'Greek', 'Slovenian'],\n",
       " 'Luxembourg': ['German', 'French'],\n",
       " 'Netherlands': ['Dutch', 'Frisian'],\n",
       " 'Norway': ['Norwegian'],\n",
       " 'Poland': ['Polish'],\n",
       " 'Portugal': ['Portuguese'],\n",
       " 'Romania': ['Romanian'],\n",
       " 'Spain': ['Spanish', 'Basque', 'Catalan', 'Galician'],\n",
       " 'Sweden': ['Swedish', 'Finnish'],\n",
       " 'Switzerland': ['German', 'French', 'Italian'],\n",
       " 'Turkey': ['Turkish', 'Kurdish'],\n",
       " 'United_States': ['Spanish'],\n",
       " 'United_Kingdom': [],\n",
       " 'Japan': ['Japanese'],\n",
       " 'South_Korea': ['Korean'],\n",
       " 'Taiwan': ['Chinese_TR'],\n",
       " 'Singapore': ['Chinese_SP'],\n",
       " 'New_Zealand': [],\n",
       " 'Australia': [],\n",
       " 'Hong_Kong': ['Chinese_TR']}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Austria': Senior        28\n",
      "Junior        12\n",
      "Management     4\n",
      "Name: Seniority, dtype: int64, 'Belgium': Management    5\n",
      "Senior        2\n",
      "Junior        2\n",
      "Name: Seniority, dtype: int64, 'Canada': Series([], Name: Seniority, dtype: int64), 'Czech_Republic': Senior    13\n",
      "Junior     2\n",
      "Name: Seniority, dtype: int64, 'Denmark': Senior        14\n",
      "Management     5\n",
      "Mid            2\n",
      "Junior         1\n",
      "Name: Seniority, dtype: int64, 'Finland': Senior    13\n",
      "Mid        1\n",
      "Name: Seniority, dtype: int64, 'France': Senior        31\n",
      "Junior         8\n",
      "Management     2\n",
      "Name: Seniority, dtype: int64, 'Germany': Senior        36\n",
      "Junior         5\n",
      "Management     1\n",
      "Mid            1\n",
      "Name: Seniority, dtype: int64, 'Greece': Senior        9\n",
      "Junior        5\n",
      "Management    3\n",
      "Mid           3\n",
      "Name: Seniority, dtype: int64, 'Hungary': Senior    26\n",
      "Junior     3\n",
      "Mid        1\n",
      "Name: Seniority, dtype: int64, 'Ireland': Senior        13\n",
      "Junior         1\n",
      "Mid            1\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Israel': Senior        35\n",
      "Junior         4\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Italy': Junior        13\n",
      "Senior         7\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Luxembourg': Senior        11\n",
      "Junior         1\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Netherlands': Management    5\n",
      "Senior        4\n",
      "Junior        2\n",
      "Name: Seniority, dtype: int64, 'Norway': Senior    3\n",
      "Name: Seniority, dtype: int64, 'Poland': Senior        29\n",
      "Junior         7\n",
      "Management     1\n",
      "Mid            1\n",
      "Name: Seniority, dtype: int64, 'Portugal': Senior        54\n",
      "Junior         3\n",
      "Management     2\n",
      "Name: Seniority, dtype: int64, 'Romania': Senior        16\n",
      "Junior         3\n",
      "Management     1\n",
      "Mid            1\n",
      "Name: Seniority, dtype: int64, 'Spain': Senior        19\n",
      "Junior        12\n",
      "Mid            1\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Sweden': Senior        28\n",
      "Management     4\n",
      "Junior         1\n",
      "Name: Seniority, dtype: int64, 'Switzerland': Senior        7\n",
      "Junior        4\n",
      "Management    1\n",
      "Name: Seniority, dtype: int64, 'Turkey': Senior    1\n",
      "Name: Seniority, dtype: int64, 'United_States': Junior    6\n",
      "Senior    6\n",
      "Mid       3\n",
      "Name: Seniority, dtype: int64, 'United_Kingdom': Junior    8\n",
      "Senior    8\n",
      "Name: Seniority, dtype: int64, 'Japan': Senior        5\n",
      "Management    1\n",
      "Name: Seniority, dtype: int64, 'South_Korea': Mid           2\n",
      "Senior        2\n",
      "Management    2\n",
      "Name: Seniority, dtype: int64, 'Taiwan': Senior    5\n",
      "Junior    1\n",
      "Name: Seniority, dtype: int64, 'Singapore': Senior    27\n",
      "Mid        7\n",
      "Junior     2\n",
      "Name: Seniority, dtype: int64, 'New_Zealand': Senior        22\n",
      "Management     1\n",
      "Name: Seniority, dtype: int64, 'Australia': Senior        6\n",
      "Management    2\n",
      "Junior        1\n",
      "Name: Seniority, dtype: int64, 'Hong_Kong': Senior        15\n",
      "Junior         3\n",
      "Management     2\n",
      "Mid            2\n",
      "Name: Seniority, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def get_seniority(job_title:str, seniority_variants: list[dict] = []):\n",
    "    \n",
    "    def is_substring_in_string(substrings: list[str], string: str):\n",
    "        \"\"\"\n",
    "        Check if any substrings is present in the given string\n",
    "        \"\"\"\n",
    "        pattern = r\"\\b{}\\b\"\n",
    "        for substring in substrings:\n",
    "            if re.search(pattern.format(substring), string, re.IGNORECASE):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    seniority_foreign = {\n",
    "        'Junior' : [],\n",
    "        'Mid' : [],\n",
    "        'Senior': [],\n",
    "        'Management' : []\n",
    "    }\n",
    "\n",
    "    if seniority_variants:\n",
    "        for language in seniority_variants:\n",
    "            for seniority, value in language.items():\n",
    "                seniority_foreign[seniority] += value\n",
    "\n",
    "\n",
    "    seniorities_EN = {\n",
    "        # \"Internship\", \"Intern\", \"Trainee\", \"Apprentice\", etc. are basically the same as Junior but usually without pay\n",
    "        'Junior' : [\"Jr.\", \"Junior\", \"Internship\", \"Intern\", \"Trainee\", \"Apprentice\", \"Novice\", \"Beginner\", \"Probationary\"],\n",
    "        'Mid' : [\"Mid\", \"Associate\", \"Regular\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\", \"Lead\", \"Principle\"],\n",
    "        'Management' : [\"Manager\", \"Head of\", \"Director\", \"Chief\", \"Supervisor\", \"Coordinator\", \"Executive\"]\n",
    "    }\n",
    "\n",
    "    for seniority in seniorities_EN.keys():\n",
    "\n",
    "        seniorities_all = seniorities_EN[seniority] + seniority_foreign[seniority]\n",
    "\n",
    "        if is_substring_in_string(seniorities_all, job_title):\n",
    "            return seniority\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "        \n",
    "def get_all_countries_seniority(countries_languages, seniorities_langs):\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    for country, country_languages in countries_languages.items():\n",
    "\n",
    "        df = dfs[country]\n",
    "\n",
    "        languages = []\n",
    "\n",
    "        for language in country_languages:\n",
    "\n",
    "            not_eng_language = seniorities_langs[language]\n",
    "\n",
    "            languages.append(not_eng_language)\n",
    "\n",
    "\n",
    "        df['Seniority'] = df['Job_title'].apply(lambda job: get_seniority(job, languages))\n",
    "\n",
    "        summary[country] = df['Seniority'].value_counts()\n",
    "\n",
    "    print(summary)\n",
    "\n",
    "get_all_countries_seniority(countries_languages, seniorities_translations)\n",
    "\n",
    "del get_all_countries_seniority, get_seniority, seniorities_translations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add non-standard seniority"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Parse salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(selector_target: str, dfs: dict[str, pd.DataFrame], dropna: bool=True):\n",
    "    results = {}\n",
    "\n",
    "    for country, df in dfs.items():\n",
    "        results[country] = df[selector_target].value_counts(dropna=dropna)\n",
    "\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN                                        30\n",
       "$55.00 - $60.00 Per Hour(Employer est.)     6\n",
       "$50.00 - $55.00 Per Hour(Employer est.)     5\n",
       "$110K (Employer est.)                       4\n",
       "$65.00 - $75.00 Per Hour(Employer est.)     4\n",
       "                                           ..\n",
       "$95K - $133K (Glassdoor est.)               1\n",
       "$60.00 - $75.00 Per Hour(Employer est.)     1\n",
       "$143K - $178K (Employer est.)               1\n",
       "$57.63 - $65.00 Per Hour(Employer est.)     1\n",
       "$120K - $150K (Employer est.)               1\n",
       "Name: Salary, Length: 171, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States']['Salary'].value_counts(dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Employer provided salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    48\n",
      "True     19\n",
      "Name: Salary_employer_provided, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: Salary_employer_provided, dtype: int64, 'Belgium': False    58\n",
      "True      6\n",
      "Name: Salary_employer_provided, dtype: int64, 'Canada': False    4\n",
      "Name: Salary_employer_provided, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: Salary_employer_provided, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Salary_employer_provided, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: Salary_employer_provided, dtype: int64, 'France': False    126\n",
      "True     101\n",
      "Name: Salary_employer_provided, dtype: int64, 'Germany': False    150\n",
      "True      14\n",
      "Name: Salary_employer_provided, dtype: int64, 'Greece': False    56\n",
      "Name: Salary_employer_provided, dtype: int64, 'Hong_Kong': False    71\n",
      "True     34\n",
      "Name: Salary_employer_provided, dtype: int64, 'Hungary': False    86\n",
      "Name: Salary_employer_provided, dtype: int64, 'Ireland': False    57\n",
      "True     18\n",
      "Name: Salary_employer_provided, dtype: int64, 'Israel': False    252\n",
      "Name: Salary_employer_provided, dtype: int64, 'Italy': False    68\n",
      "True     12\n",
      "Name: Salary_employer_provided, dtype: int64, 'Japan': False    42\n",
      "Name: Salary_employer_provided, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Salary_employer_provided, dtype: int64, 'Netherlands': False    21\n",
      "True     19\n",
      "Name: Salary_employer_provided, dtype: int64, 'New_Zealand': False    49\n",
      "True      3\n",
      "Name: Salary_employer_provided, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Salary_employer_provided, dtype: int64, 'Poland': False    100\n",
      "True       9\n",
      "Name: Salary_employer_provided, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Salary_employer_provided, dtype: int64, 'Romania': False    120\n",
      "True       3\n",
      "Name: Salary_employer_provided, dtype: int64, 'Singapore': False    102\n",
      "True      32\n",
      "Name: Salary_employer_provided, dtype: int64, 'South_Korea': False    46\n",
      "Name: Salary_employer_provided, dtype: int64, 'Spain': False    114\n",
      "True      12\n",
      "Name: Salary_employer_provided, dtype: int64, 'Sweden': False    142\n",
      "True       3\n",
      "Name: Salary_employer_provided, dtype: int64, 'Switzerland': False    41\n",
      "True     19\n",
      "Name: Salary_employer_provided, dtype: int64, 'Taiwan': False    40\n",
      "Name: Salary_employer_provided, dtype: int64, 'Turkey': False    22\n",
      "Name: Salary_employer_provided, dtype: int64, 'United_Kingdom': False    66\n",
      "True     52\n",
      "Name: Salary_employer_provided, dtype: int64, 'United_States': True     141\n",
      "False    100\n",
      "Name: Salary_employer_provided, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "    \n",
    "    df['Salary_employer_provided'] = df['Salary'].apply(lambda salary : True if isinstance(salary, str) and \"(Employer est.)\" in salary else False)\n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_employer_provided', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Salary is hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    51\n",
      "NaN      12\n",
      "True      4\n",
      "Name: Salary_hourly, dtype: int64, 'Austria': NaN      137\n",
      "False      2\n",
      "Name: Salary_hourly, dtype: int64, 'Belgium': NaN      58\n",
      "False     5\n",
      "True      1\n",
      "Name: Salary_hourly, dtype: int64, 'Canada': False    3\n",
      "NaN      1\n",
      "Name: Salary_hourly, dtype: int64, 'Czech_Republic': NaN      66\n",
      "False     1\n",
      "True      1\n",
      "Name: Salary_hourly, dtype: int64, 'Denmark': NaN     90\n",
      "True     1\n",
      "Name: Salary_hourly, dtype: int64, 'Finland': NaN      57\n",
      "False     1\n",
      "Name: Salary_hourly, dtype: int64, 'France': NaN      126\n",
      "True      71\n",
      "False     30\n",
      "Name: Salary_hourly, dtype: int64, 'Germany': NaN      150\n",
      "False     12\n",
      "True       2\n",
      "Name: Salary_hourly, dtype: int64, 'Greece': NaN    56\n",
      "Name: Salary_hourly, dtype: int64, 'Hong_Kong': NaN      65\n",
      "False    40\n",
      "Name: Salary_hourly, dtype: int64, 'Hungary': NaN    86\n",
      "Name: Salary_hourly, dtype: int64, 'Ireland': False    63\n",
      "NaN       8\n",
      "True      4\n",
      "Name: Salary_hourly, dtype: int64, 'Israel': NaN    252\n",
      "Name: Salary_hourly, dtype: int64, 'Italy': NaN      68\n",
      "False    11\n",
      "True      1\n",
      "Name: Salary_hourly, dtype: int64, 'Japan': NaN    42\n",
      "Name: Salary_hourly, dtype: int64, 'Luxembourg': NaN    40\n",
      "Name: Salary_hourly, dtype: int64, 'Netherlands': NaN      21\n",
      "False    19\n",
      "Name: Salary_hourly, dtype: int64, 'New_Zealand': False    39\n",
      "NaN      13\n",
      "Name: Salary_hourly, dtype: int64, 'Norway': NaN      30\n",
      "False     1\n",
      "Name: Salary_hourly, dtype: int64, 'Poland': NaN      100\n",
      "False      6\n",
      "True       3\n",
      "Name: Salary_hourly, dtype: int64, 'Portugal': NaN      188\n",
      "False      1\n",
      "Name: Salary_hourly, dtype: int64, 'Romania': NaN      120\n",
      "False      3\n",
      "Name: Salary_hourly, dtype: int64, 'Singapore': NaN      74\n",
      "False    60\n",
      "Name: Salary_hourly, dtype: int64, 'South_Korea': NaN    46\n",
      "Name: Salary_hourly, dtype: int64, 'Spain': NaN      114\n",
      "False     11\n",
      "True       1\n",
      "Name: Salary_hourly, dtype: int64, 'Sweden': NaN      142\n",
      "False      2\n",
      "True       1\n",
      "Name: Salary_hourly, dtype: int64, 'Switzerland': NaN      41\n",
      "False    19\n",
      "Name: Salary_hourly, dtype: int64, 'Taiwan': NaN    40\n",
      "Name: Salary_hourly, dtype: int64, 'Turkey': NaN    22\n",
      "Name: Salary_hourly, dtype: int64, 'United_Kingdom': False    86\n",
      "NaN      23\n",
      "True      9\n",
      "Name: Salary_hourly, dtype: int64, 'United_States': False    137\n",
      "True      74\n",
      "NaN       30\n",
      "Name: Salary_hourly, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "def is_hourly(salary: Union[str, np.nan]):\n",
    "\n",
    "    if isinstance(salary, str) and len(salary.strip()) > 0:\n",
    "\n",
    "        return bool(\"Per Hour\" in salary)\n",
    "        \n",
    "    elif np.isnan(salary):\n",
    "\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "\n",
    "        raise ValueError(\"Salary must be a string or numpy.nan\")\n",
    "\n",
    "selector_target = 'Salary_hourly'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[selector_target] = df['Salary'].apply(is_hourly)\n",
    "    dfs[country] = df\n",
    "\n",
    "show_results(selector_target, dfs, dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Salary currency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': A$    55\n",
      "Name: Salary_currency, dtype: int64, 'Austria': €    2\n",
      "Name: Salary_currency, dtype: int64, 'Belgium': €    6\n",
      "Name: Salary_currency, dtype: int64, 'Canada': CA$    3\n",
      "Name: Salary_currency, dtype: int64, 'Czech_Republic': CZK    2\n",
      "Name: Salary_currency, dtype: int64, 'Denmark': DKK    1\n",
      "Name: Salary_currency, dtype: int64, 'Finland': €    1\n",
      "Name: Salary_currency, dtype: int64, 'France': €    101\n",
      "Name: Salary_currency, dtype: int64, 'Germany': €    14\n",
      "Name: Salary_currency, dtype: int64, 'Greece': Series([], Name: Salary_currency, dtype: int64), 'Hong_Kong': HK$    40\n",
      "Name: Salary_currency, dtype: int64, 'Hungary': Series([], Name: Salary_currency, dtype: int64), 'Ireland': €    67\n",
      "Name: Salary_currency, dtype: int64, 'Israel': Series([], Name: Salary_currency, dtype: int64), 'Italy': €    12\n",
      "Name: Salary_currency, dtype: int64, 'Japan': Series([], Name: Salary_currency, dtype: int64), 'Luxembourg': Series([], Name: Salary_currency, dtype: int64), 'Netherlands': €    19\n",
      "Name: Salary_currency, dtype: int64, 'New_Zealand': NZ$    39\n",
      "Name: Salary_currency, dtype: int64, 'Norway': NOK    1\n",
      "Name: Salary_currency, dtype: int64, 'Poland': PLN    9\n",
      "Name: Salary_currency, dtype: int64, 'Portugal': €    1\n",
      "Name: Salary_currency, dtype: int64, 'Romania': RON    3\n",
      "Name: Salary_currency, dtype: int64, 'Singapore': SGD    60\n",
      "Name: Salary_currency, dtype: int64, 'South_Korea': Series([], Name: Salary_currency, dtype: int64), 'Spain': €    12\n",
      "Name: Salary_currency, dtype: int64, 'Sweden': SEK    3\n",
      "Name: Salary_currency, dtype: int64, 'Switzerland': CHF    19\n",
      "Name: Salary_currency, dtype: int64, 'Taiwan': Series([], Name: Salary_currency, dtype: int64), 'Turkey': Series([], Name: Salary_currency, dtype: int64), 'United_Kingdom': £    95\n",
      "Name: Salary_currency, dtype: int64, 'United_States': $    211\n",
      "Name: Salary_currency, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def get_currency(salary: str):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_currency = r\"(.+?(?=\\d))\"\n",
    "\n",
    "        matched = re.search(pattern_currency, salary)\n",
    "\n",
    "        currency = matched.group(1).strip().replace(\":\", \"\")\n",
    "\n",
    "        return currency\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_currency'] = df['Salary'].apply(get_currency)\n",
    "    dfs[country] = df\n",
    " \n",
    "show_results('Salary_currency', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert currencies to ISO standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ISO(salary: str):\n",
    "    \n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        ISO_standard = {\n",
    "            '€': \"EUR\",\n",
    "            '$': \"USD\",\n",
    "            'CA$': \"CAD\",\n",
    "            'HK$': \"HKD\",\n",
    "            'NZ$': \"NZD\",\n",
    "            'A$': \"AUD\",\n",
    "            '£': \"GBP\",\n",
    "        }\n",
    "\n",
    "        for currency, ISO in ISO_standard.items():\n",
    "            if salary.strip() == currency:\n",
    "                return ISO\n",
    "            \n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': AUD    55\n",
      "Name: Salary_currency, dtype: int64, 'Austria': EUR    2\n",
      "Name: Salary_currency, dtype: int64, 'Belgium': EUR    6\n",
      "Name: Salary_currency, dtype: int64, 'Canada': CAD    3\n",
      "Name: Salary_currency, dtype: int64, 'Czech_Republic': CZK    2\n",
      "Name: Salary_currency, dtype: int64, 'Denmark': DKK    1\n",
      "Name: Salary_currency, dtype: int64, 'Finland': EUR    1\n",
      "Name: Salary_currency, dtype: int64, 'France': EUR    101\n",
      "Name: Salary_currency, dtype: int64, 'Germany': EUR    14\n",
      "Name: Salary_currency, dtype: int64, 'Greece': Series([], Name: Salary_currency, dtype: int64), 'Hong_Kong': HKD    40\n",
      "Name: Salary_currency, dtype: int64, 'Hungary': Series([], Name: Salary_currency, dtype: int64), 'Ireland': EUR    67\n",
      "Name: Salary_currency, dtype: int64, 'Israel': Series([], Name: Salary_currency, dtype: int64), 'Italy': EUR    12\n",
      "Name: Salary_currency, dtype: int64, 'Japan': Series([], Name: Salary_currency, dtype: int64), 'Luxembourg': Series([], Name: Salary_currency, dtype: int64), 'Netherlands': EUR    19\n",
      "Name: Salary_currency, dtype: int64, 'New_Zealand': NZD    39\n",
      "Name: Salary_currency, dtype: int64, 'Norway': NOK    1\n",
      "Name: Salary_currency, dtype: int64, 'Poland': PLN    9\n",
      "Name: Salary_currency, dtype: int64, 'Portugal': EUR    1\n",
      "Name: Salary_currency, dtype: int64, 'Romania': RON    3\n",
      "Name: Salary_currency, dtype: int64, 'Singapore': SGD    60\n",
      "Name: Salary_currency, dtype: int64, 'South_Korea': Series([], Name: Salary_currency, dtype: int64), 'Spain': EUR    12\n",
      "Name: Salary_currency, dtype: int64, 'Sweden': SEK    3\n",
      "Name: Salary_currency, dtype: int64, 'Switzerland': CHF    19\n",
      "Name: Salary_currency, dtype: int64, 'Taiwan': Series([], Name: Salary_currency, dtype: int64), 'Turkey': Series([], Name: Salary_currency, dtype: int64), 'United_Kingdom': GBP    95\n",
      "Name: Salary_currency, dtype: int64, 'United_States': USD    211\n",
      "Name: Salary_currency, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_currency'] = df['Salary_currency'].apply(convert_to_ISO)\n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_currency', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUD',\n",
       " 'CAD',\n",
       " 'CHF',\n",
       " 'CZK',\n",
       " 'DKK',\n",
       " 'EUR',\n",
       " 'GBP',\n",
       " 'HKD',\n",
       " 'NOK',\n",
       " 'NZD',\n",
       " 'PLN',\n",
       " 'RON',\n",
       " 'SEK',\n",
       " 'SGD',\n",
       " 'USD',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " nan}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currencies = set()\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    new_currency = df['Salary_currency'].unique().tolist()\n",
    "\n",
    "    currencies.update(new_currency)\n",
    "\n",
    "currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "del get_currency, convert_to_ISO, currencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Salary min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_metric_prefixes_numbers(match_max:str) -> float:\n",
    "\n",
    "    if \"K\" in match_max:\n",
    "        match_max = float(match_max.replace(\"K\", \"\"))\n",
    "        match_max *= 1.0e+3\n",
    "\n",
    "    elif \"M\" in match_max:\n",
    "        match_max = float(match_max.replace(\"M\", \"\"))\n",
    "        match_max *= 1.0e+6\n",
    "\n",
    "    elif \"G\" in match_max:\n",
    "        match_max = float(match_max.replace(\"G\", \"\"))\n",
    "        match_max *= 1.0e+9\n",
    "\n",
    "    return match_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_min(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?[KMG]?)\"\n",
    "        match_min: str = re.findall(pattern_salary, salary)[0][0]\n",
    "\n",
    "        match_min = change_metric_prefixes_numbers(match_min)\n",
    "\n",
    "        return float(match_min)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 109000.00    6\n",
      "90000.00     6\n",
      "100000.00    4\n",
      "120000.00    4\n",
      "80000.00     3\n",
      "112000.00    2\n",
      "96000.00     2\n",
      "91000.00     2\n",
      "99000.00     2\n",
      "110000.00    2\n",
      "75000.00     2\n",
      "92000.00     2\n",
      "75.00        2\n",
      "137.50       1\n",
      "113000.00    1\n",
      "116000.00    1\n",
      "101000.00    1\n",
      "138000.00    1\n",
      "104000.00    1\n",
      "200000.00    1\n",
      "85000.00     1\n",
      "93.75        1\n",
      "102000.00    1\n",
      "95000.00     1\n",
      "63000.00     1\n",
      "140000.00    1\n",
      "126000.00    1\n",
      "130000.00    1\n",
      "133000.00    1\n",
      "Name: Salary_min, dtype: int64, 'Austria': 3000.0    2\n",
      "Name: Salary_min, dtype: int64, 'Belgium': 45000.00    2\n",
      "42000.00    1\n",
      "40000.00    1\n",
      "68.75       1\n",
      "55000.00    1\n",
      "Name: Salary_min, dtype: int64, 'Canada': 65000.0    1\n",
      "88000.0    1\n",
      "57000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Czech_Republic': 8000.0    1\n",
      "400.0     1\n",
      "Name: Salary_min, dtype: int64, 'Denmark': 187.5    1\n",
      "Name: Salary_min, dtype: int64, 'Finland': 4000.0    1\n",
      "Name: Salary_min, dtype: int64, 'France': 50.00       28\n",
      "62.50        8\n",
      "56.25        7\n",
      "68.75        6\n",
      "60000.00     5\n",
      "40000.00     4\n",
      "25.00        3\n",
      "47.50        3\n",
      "55000.00     3\n",
      "50000.00     3\n",
      "34000.00     3\n",
      "37000.00     2\n",
      "87.50        2\n",
      "75.00        2\n",
      "81.25        2\n",
      "45000.00     2\n",
      "60.00        2\n",
      "700.00       1\n",
      "46000.00     1\n",
      "85000.00     1\n",
      "41.25        1\n",
      "30.00        1\n",
      "43.75        1\n",
      "35000.00     1\n",
      "48.75        1\n",
      "52.50        1\n",
      "45.00        1\n",
      "35.03        1\n",
      "65.00        1\n",
      "70000.00     1\n",
      "75000.00     1\n",
      "33000.00     1\n",
      "36000.00     1\n",
      "Name: Salary_min, dtype: int64, 'Germany': 60000.0    2\n",
      "45000.0    2\n",
      "80000.0    2\n",
      "65000.0    1\n",
      "4000.0     1\n",
      "50000.0    1\n",
      "14.5       1\n",
      "55000.0    1\n",
      "47000.0    1\n",
      "90000.0    1\n",
      "40.0       1\n",
      "Name: Salary_min, dtype: int64, 'Greece': Series([], Name: Salary_min, dtype: int64), 'Hong_Kong': 30000.0     4\n",
      "35000.0     3\n",
      "300000.0    3\n",
      "20000.0     2\n",
      "360000.0    2\n",
      "60000.0     2\n",
      "50000.0     2\n",
      "5000.0      1\n",
      "65000.0     1\n",
      "400000.0    1\n",
      "80000.0     1\n",
      "90000.0     1\n",
      "45000.0     1\n",
      "416000.0    1\n",
      "600000.0    1\n",
      "39000.0     1\n",
      "32000.0     1\n",
      "47000.0     1\n",
      "480000.0    1\n",
      "225000.0    1\n",
      "36000.0     1\n",
      "18000.0     1\n",
      "365000.0    1\n",
      "40000.0     1\n",
      "288000.0    1\n",
      "25000.0     1\n",
      "17000.0     1\n",
      "540000.0    1\n",
      "22000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Hungary': Series([], Name: Salary_min, dtype: int64), 'Ireland': 50000.00     5\n",
      "54000.00     4\n",
      "40000.00     3\n",
      "25000.00     3\n",
      "39000.00     3\n",
      "48000.00     2\n",
      "67000.00     2\n",
      "52000.00     2\n",
      "63000.00     2\n",
      "35000.00     2\n",
      "88000.00     2\n",
      "44000.00     2\n",
      "70000.00     2\n",
      "45000.00     2\n",
      "80000.00     2\n",
      "46.88        1\n",
      "56000.00     1\n",
      "86000.00     1\n",
      "34000.00     1\n",
      "47000.00     1\n",
      "95000.00     1\n",
      "66000.00     1\n",
      "65000.00     1\n",
      "32000.00     1\n",
      "75000.00     1\n",
      "87000.00     1\n",
      "23.75        1\n",
      "76000.00     1\n",
      "38000.00     1\n",
      "31000.00     1\n",
      "49000.00     1\n",
      "51000.00     1\n",
      "73000.00     1\n",
      "105000.00    1\n",
      "46000.00     1\n",
      "68.75        1\n",
      "57000.00     1\n",
      "84000.00     1\n",
      "50.00        1\n",
      "53000.00     1\n",
      "60000.00     1\n",
      "61000.00     1\n",
      "111000.00    1\n",
      "33000.00     1\n",
      "Name: Salary_min, dtype: int64, 'Israel': Series([], Name: Salary_min, dtype: int64), 'Italy': 30000.0     4\n",
      "25000.0     2\n",
      "155000.0    1\n",
      "800.0       1\n",
      "40000.0     1\n",
      "22000.0     1\n",
      "37.5        1\n",
      "32000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Japan': Series([], Name: Salary_min, dtype: int64), 'Luxembourg': Series([], Name: Salary_min, dtype: int64), 'Netherlands': 4000.0    12\n",
      "3000.0     5\n",
      "5000.0     2\n",
      "Name: Salary_min, dtype: int64, 'New_Zealand': 78000.0     9\n",
      "90000.0     4\n",
      "84000.0     2\n",
      "55000.0     2\n",
      "91000.0     2\n",
      "110000.0    2\n",
      "80000.0     1\n",
      "57000.0     1\n",
      "70000.0     1\n",
      "95000.0     1\n",
      "199000.0    1\n",
      "98000.0     1\n",
      "152000.0    1\n",
      "74000.0     1\n",
      "109000.0    1\n",
      "63000.0     1\n",
      "148000.0    1\n",
      "77000.0     1\n",
      "72000.0     1\n",
      "100000.0    1\n",
      "52000.0     1\n",
      "75000.0     1\n",
      "114000.0    1\n",
      "65000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Norway': 8000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Poland': 8000.0     1\n",
      "120.0      1\n",
      "85000.0    1\n",
      "20000.0    1\n",
      "18000.0    1\n",
      "15000.0    1\n",
      "157.0      1\n",
      "162.5      1\n",
      "27000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Portugal': 71000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Romania': 20000.0    1\n",
      "8000.0     1\n",
      "2000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Singapore': 6000.0      7\n",
      "7000.0      5\n",
      "4000.0      5\n",
      "3000.0      4\n",
      "5000.0      4\n",
      "72000.0     3\n",
      "71000.0     3\n",
      "68000.0     3\n",
      "48000.0     3\n",
      "70000.0     3\n",
      "36000.0     2\n",
      "120000.0    2\n",
      "66000.0     2\n",
      "10000.0     2\n",
      "180000.0    1\n",
      "8000.0      1\n",
      "42000.0     1\n",
      "45000.0     1\n",
      "300000.0    1\n",
      "82000.0     1\n",
      "50000.0     1\n",
      "44000.0     1\n",
      "77000.0     1\n",
      "65000.0     1\n",
      "79000.0     1\n",
      "9000.0      1\n",
      "Name: Salary_min, dtype: int64, 'South_Korea': Series([], Name: Salary_min, dtype: int64), 'Spain': 30000.0    2\n",
      "36000.0    2\n",
      "37.5       1\n",
      "24000.0    1\n",
      "39000.0    1\n",
      "40000.0    1\n",
      "45000.0    1\n",
      "38000.0    1\n",
      "50000.0    1\n",
      "2000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Sweden': 600000.0    1\n",
      "40.0        1\n",
      "40000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Switzerland': 100000.0    5\n",
      "145000.0    4\n",
      "135000.0    3\n",
      "140000.0    2\n",
      "120000.0    2\n",
      "90000.0     1\n",
      "80000.0     1\n",
      "115000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Taiwan': Series([], Name: Salary_min, dtype: int64), 'Turkey': Series([], Name: Salary_min, dtype: int64), 'United_Kingdom': 50000.00    9\n",
      "45000.00    8\n",
      "35000.00    6\n",
      "48000.00    5\n",
      "37000.00    5\n",
      "30000.00    4\n",
      "60000.00    4\n",
      "40000.00    4\n",
      "49000.00    4\n",
      "62000.00    3\n",
      "33000.00    3\n",
      "55000.00    3\n",
      "53000.00    2\n",
      "46000.00    2\n",
      "65000.00    2\n",
      "56000.00    2\n",
      "34000.00    2\n",
      "43000.00    2\n",
      "44000.00    2\n",
      "36000.00    1\n",
      "87.50       1\n",
      "47000.00    1\n",
      "1000.00     1\n",
      "69000.00    1\n",
      "62.50       1\n",
      "51000.00    1\n",
      "56.25       1\n",
      "50.00       1\n",
      "43.75       1\n",
      "42000.00    1\n",
      "90000.00    1\n",
      "70000.00    1\n",
      "59000.00    1\n",
      "29000.00    1\n",
      "31.25       1\n",
      "37.50       1\n",
      "28000.00    1\n",
      "20000.00    1\n",
      "41000.00    1\n",
      "68.75       1\n",
      "58000.00    1\n",
      "42.30       1\n",
      "Name: Salary_min, dtype: int64, 'United_States': 50.00        11\n",
      "60.00         9\n",
      "65.00         9\n",
      "110000.00     8\n",
      "140000.00     8\n",
      "             ..\n",
      "74000.00      1\n",
      "143000.00     1\n",
      "57.63         1\n",
      "125000.00     1\n",
      "66000.00      1\n",
      "Name: Salary_min, Length: 85, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_min'] = df['Salary'].apply(get_salary_min)\n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_min', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "del get_salary_min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Salary max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 120000.00    5\n",
      "150000.00    5\n",
      "128000.00    5\n",
      "105000.00    3\n",
      "122000.00    2\n",
      "100000.00    2\n",
      "90000.00     2\n",
      "180000.00    2\n",
      "177000.00    2\n",
      "130000.00    2\n",
      "155000.00    2\n",
      "137.50       1\n",
      "111000.00    1\n",
      "106000.00    1\n",
      "108000.00    1\n",
      "139000.00    1\n",
      "126000.00    1\n",
      "141000.00    1\n",
      "200000.00    1\n",
      "125000.00    1\n",
      "135000.00    1\n",
      "75000.00     1\n",
      "121000.00    1\n",
      "144000.00    1\n",
      "106.25       1\n",
      "76000.00     1\n",
      "164000.00    1\n",
      "118000.00    1\n",
      "90.00        1\n",
      "165000.00    1\n",
      "181000.00    1\n",
      "100.00       1\n",
      "174000.00    1\n",
      "158000.00    1\n",
      "Name: Salary_max, dtype: int64, 'Austria': 3000.0    1\n",
      "4000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Belgium': 65000.0    2\n",
      "70000.0    2\n",
      "42000.0    1\n",
      "87.5       1\n",
      "Name: Salary_max, dtype: int64, 'Canada': 75000.0    1\n",
      "94000.0    1\n",
      "71000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Czech_Republic': 8000.0    1\n",
      "800.0     1\n",
      "Name: Salary_max, dtype: int64, 'Denmark': 312.5    1\n",
      "Name: Salary_max, dtype: int64, 'Finland': 6000.0    1\n",
      "Name: Salary_max, dtype: int64, 'France': 68.75       16\n",
      "60000.00     9\n",
      "75.00        8\n",
      "56.25        5\n",
      "81.25        5\n",
      "70000.00     5\n",
      "62.50        5\n",
      "72.50        4\n",
      "52.50        4\n",
      "50.00        3\n",
      "65.00        3\n",
      "55000.00     3\n",
      "73.75        3\n",
      "80000.00     3\n",
      "87.50        2\n",
      "75000.00     2\n",
      "40000.00     2\n",
      "93.75        2\n",
      "1000.00      1\n",
      "95000.00     1\n",
      "100.00       1\n",
      "53.75        1\n",
      "51.25        1\n",
      "55.00        1\n",
      "98.75        1\n",
      "57.50        1\n",
      "42000.00     1\n",
      "71.25        1\n",
      "85.00        1\n",
      "45000.00     1\n",
      "60.00        1\n",
      "35.03        1\n",
      "50000.00     1\n",
      "66.25        1\n",
      "65000.00     1\n",
      "Name: Salary_max, dtype: int64, 'Germany': 80000.0     3\n",
      "75000.0     1\n",
      "85000.0     1\n",
      "6000.0      1\n",
      "70000.0     1\n",
      "14.5        1\n",
      "60000.0     1\n",
      "52000.0     1\n",
      "50000.0     1\n",
      "90000.0     1\n",
      "40.0        1\n",
      "100000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Greece': Series([], Name: Salary_max, dtype: int64), 'Hong_Kong': 50000.0     7\n",
      "40000.0     3\n",
      "480000.0    2\n",
      "30000.0     2\n",
      "60000.0     2\n",
      "32000.0     1\n",
      "65000.0     1\n",
      "800000.0    1\n",
      "95000.0     1\n",
      "75000.0     1\n",
      "90000.0     1\n",
      "416000.0    1\n",
      "720000.0    1\n",
      "66000.0     1\n",
      "55000.0     1\n",
      "47000.0     1\n",
      "360000.0    1\n",
      "576000.0    1\n",
      "600000.0    1\n",
      "38000.0     1\n",
      "22000.0     1\n",
      "512000.0    1\n",
      "468000.0    1\n",
      "446000.0    1\n",
      "474000.0    1\n",
      "25000.0     1\n",
      "17000.0     1\n",
      "540000.0    1\n",
      "5000.0      1\n",
      "Name: Salary_max, dtype: int64, 'Hungary': Series([], Name: Salary_max, dtype: int64), 'Ireland': 50000.00     5\n",
      "96000.00     4\n",
      "95000.00     4\n",
      "70000.00     3\n",
      "100000.00    3\n",
      "87000.00     2\n",
      "48000.00     2\n",
      "75000.00     2\n",
      "80000.00     2\n",
      "77000.00     2\n",
      "72000.00     2\n",
      "43000.00     2\n",
      "115000.00    2\n",
      "88000.00     2\n",
      "69000.00     2\n",
      "49000.00     2\n",
      "73000.00     2\n",
      "97000.00     2\n",
      "53.13        1\n",
      "117000.00    1\n",
      "99000.00     1\n",
      "94000.00     1\n",
      "65000.00     1\n",
      "86000.00     1\n",
      "130000.00    1\n",
      "55000.00     1\n",
      "44000.00     1\n",
      "85000.00     1\n",
      "98000.00     1\n",
      "71000.00     1\n",
      "128000.00    1\n",
      "39000.00     1\n",
      "66000.00     1\n",
      "32000.00     1\n",
      "67000.00     1\n",
      "65.00        1\n",
      "68.75        1\n",
      "40000.00     1\n",
      "47000.00     1\n",
      "27.50        1\n",
      "Name: Salary_max, dtype: int64, 'Israel': Series([], Name: Salary_max, dtype: int64), 'Italy': 60000.00     3\n",
      "45000.00     2\n",
      "33000.00     1\n",
      "155000.00    1\n",
      "800.00       1\n",
      "42000.00     1\n",
      "25000.00     1\n",
      "43.75        1\n",
      "38000.00     1\n",
      "Name: Salary_max, dtype: int64, 'Japan': Series([], Name: Salary_max, dtype: int64), 'Luxembourg': Series([], Name: Salary_max, dtype: int64), 'Netherlands': 5000.0    9\n",
      "6000.0    5\n",
      "7000.0    2\n",
      "4000.0    2\n",
      "3000.0    1\n",
      "Name: Salary_max, dtype: int64, 'New_Zealand': 120000.0    13\n",
      "140000.0     3\n",
      "100000.0     3\n",
      "127000.0     2\n",
      "130000.0     2\n",
      "80000.0      1\n",
      "180000.0     1\n",
      "123000.0     1\n",
      "102000.0     1\n",
      "200000.0     1\n",
      "184000.0     1\n",
      "71000.0      1\n",
      "135000.0     1\n",
      "121000.0     1\n",
      "111000.0     1\n",
      "145000.0     1\n",
      "116000.0     1\n",
      "101000.0     1\n",
      "97000.0      1\n",
      "131000.0     1\n",
      "99000.0      1\n",
      "Name: Salary_max, dtype: int64, 'Norway': 8000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Poland': 30000.0    2\n",
      "8000.0     1\n",
      "120.0      1\n",
      "85000.0    1\n",
      "15000.0    1\n",
      "157.0      1\n",
      "200.0      1\n",
      "31000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Portugal': 72000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Romania': 20000.0    1\n",
      "8000.0     1\n",
      "2000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Singapore': 8000.0      5\n",
      "5000.0      4\n",
      "7000.0      4\n",
      "6000.0      4\n",
      "96000.0     4\n",
      "12000.0     3\n",
      "10000.0     3\n",
      "9000.0      3\n",
      "113000.0    3\n",
      "108000.0    2\n",
      "71000.0     2\n",
      "100000.0    2\n",
      "120000.0    2\n",
      "111000.0    2\n",
      "90000.0     2\n",
      "60000.0     2\n",
      "45000.0     2\n",
      "4000.0      1\n",
      "144000.0    1\n",
      "66000.0     1\n",
      "360000.0    1\n",
      "101000.0    1\n",
      "300000.0    1\n",
      "52000.0     1\n",
      "80000.0     1\n",
      "84000.0     1\n",
      "20000.0     1\n",
      "15000.0     1\n",
      "Name: Salary_max, dtype: int64, 'South_Korea': Series([], Name: Salary_max, dtype: int64), 'Spain': 48000.0    2\n",
      "37.5       1\n",
      "33000.0    1\n",
      "42000.0    1\n",
      "40000.0    1\n",
      "24000.0    1\n",
      "45000.0    1\n",
      "50000.0    1\n",
      "38000.0    1\n",
      "55000.0    1\n",
      "2000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Sweden': 750000.0    1\n",
      "80.0        1\n",
      "60000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Switzerland': 145000.0    4\n",
      "100000.0    4\n",
      "135000.0    3\n",
      "120000.0    3\n",
      "140000.0    2\n",
      "110000.0    1\n",
      "95000.0     1\n",
      "115000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Taiwan': Series([], Name: Salary_max, dtype: int64), 'Turkey': Series([], Name: Salary_max, dtype: int64), 'United_Kingdom': 50000.00     7\n",
      "68000.00     6\n",
      "60000.00     5\n",
      "55000.00     5\n",
      "65000.00     4\n",
      "75000.00     4\n",
      "59000.00     4\n",
      "100000.00    3\n",
      "69000.00     3\n",
      "80000.00     3\n",
      "45000.00     3\n",
      "70000.00     3\n",
      "57000.00     2\n",
      "49000.00     2\n",
      "42000.00     2\n",
      "37.50        2\n",
      "56.25        2\n",
      "72000.00     2\n",
      "53000.00     2\n",
      "58000.00     2\n",
      "62000.00     2\n",
      "68.75        2\n",
      "38000.00     2\n",
      "30000.00     2\n",
      "63000.00     2\n",
      "67000.00     1\n",
      "62.50        1\n",
      "47000.00     1\n",
      "43000.00     1\n",
      "1000.00      1\n",
      "61000.00     1\n",
      "90000.00     1\n",
      "71000.00     1\n",
      "44000.00     1\n",
      "32000.00     1\n",
      "52000.00     1\n",
      "87.50        1\n",
      "35000.00     1\n",
      "86000.00     1\n",
      "88000.00     1\n",
      "22000.00     1\n",
      "121000.00    1\n",
      "85000.00     1\n",
      "90.18        1\n",
      "Name: Salary_max, dtype: int64, 'United_States': 60.0        12\n",
      "70.0        10\n",
      "110000.0     8\n",
      "65.0         8\n",
      "140000.0     7\n",
      "            ..\n",
      "178000.0     1\n",
      "120.0        1\n",
      "145000.0     1\n",
      "108000.0     1\n",
      "71.0         1\n",
      "Name: Salary_max, Length: 90, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def get_salary_max(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?[KMG]?)\"\n",
    "        match_max: str = re.findall(pattern_salary, salary)[-1][0]\n",
    "\n",
    "        match_max = change_metric_prefixes_numbers(match_max)\n",
    "\n",
    "        return float(match_max)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_max'] = df['Salary'].apply(get_salary_max)\n",
    "    \n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_max', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "del get_salary_max, change_metric_prefixes_numbers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Is salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': True     55\n",
      "False    12\n",
      "Name: Is_salary, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: Is_salary, dtype: int64, 'Belgium': False    58\n",
      "True      6\n",
      "Name: Is_salary, dtype: int64, 'Canada': True     3\n",
      "False    1\n",
      "Name: Is_salary, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: Is_salary, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Is_salary, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: Is_salary, dtype: int64, 'France': False    126\n",
      "True     101\n",
      "Name: Is_salary, dtype: int64, 'Germany': False    150\n",
      "True      14\n",
      "Name: Is_salary, dtype: int64, 'Greece': False    56\n",
      "Name: Is_salary, dtype: int64, 'Hong_Kong': False    65\n",
      "True     40\n",
      "Name: Is_salary, dtype: int64, 'Hungary': False    86\n",
      "Name: Is_salary, dtype: int64, 'Ireland': True     67\n",
      "False     8\n",
      "Name: Is_salary, dtype: int64, 'Israel': False    252\n",
      "Name: Is_salary, dtype: int64, 'Italy': False    68\n",
      "True     12\n",
      "Name: Is_salary, dtype: int64, 'Japan': False    42\n",
      "Name: Is_salary, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Is_salary, dtype: int64, 'Netherlands': False    21\n",
      "True     19\n",
      "Name: Is_salary, dtype: int64, 'New_Zealand': True     39\n",
      "False    13\n",
      "Name: Is_salary, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Is_salary, dtype: int64, 'Poland': False    100\n",
      "True       9\n",
      "Name: Is_salary, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Is_salary, dtype: int64, 'Romania': False    120\n",
      "True       3\n",
      "Name: Is_salary, dtype: int64, 'Singapore': False    74\n",
      "True     60\n",
      "Name: Is_salary, dtype: int64, 'South_Korea': False    46\n",
      "Name: Is_salary, dtype: int64, 'Spain': False    114\n",
      "True      12\n",
      "Name: Is_salary, dtype: int64, 'Sweden': False    142\n",
      "True       3\n",
      "Name: Is_salary, dtype: int64, 'Switzerland': False    41\n",
      "True     19\n",
      "Name: Is_salary, dtype: int64, 'Taiwan': False    40\n",
      "Name: Is_salary, dtype: int64, 'Turkey': False    22\n",
      "Name: Is_salary, dtype: int64, 'United_Kingdom': True     95\n",
      "False    23\n",
      "Name: Is_salary, dtype: int64, 'United_States': True     211\n",
      "False     30\n",
      "Name: Is_salary, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "selector_target = 'Is_salary'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[selector_target] = df['Salary_min'].notnull() | df['Salary_min'].notnull()\n",
    "    \n",
    "    dfs[country] = df\n",
    "\n",
    "show_results(selector_target, dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7 Convert hourly salaries to yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_yearly_income(hourly_rate):\n",
    "\n",
    "    hours_per_week = 40\n",
    "    WEEKS_PER_YEAR = 52\n",
    "    HOURS_PER_YEAR = WEEKS_PER_YEAR * hours_per_week\n",
    "    gross_income = hourly_rate * HOURS_PER_YEAR\n",
    "    return gross_income"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 109000.0    6\n",
      "90000.0     6\n",
      "100000.0    4\n",
      "120000.0    4\n",
      "80000.0     3\n",
      "112000.0    2\n",
      "96000.0     2\n",
      "91000.0     2\n",
      "99000.0     2\n",
      "110000.0    2\n",
      "75000.0     2\n",
      "92000.0     2\n",
      "156000.0    2\n",
      "286000.0    1\n",
      "113000.0    1\n",
      "116000.0    1\n",
      "101000.0    1\n",
      "138000.0    1\n",
      "104000.0    1\n",
      "200000.0    1\n",
      "85000.0     1\n",
      "195000.0    1\n",
      "102000.0    1\n",
      "95000.0     1\n",
      "63000.0     1\n",
      "140000.0    1\n",
      "126000.0    1\n",
      "130000.0    1\n",
      "133000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Austria': 3000.0    2\n",
      "Name: Salary_min, dtype: int64, 'Belgium': 45000.0     2\n",
      "42000.0     1\n",
      "40000.0     1\n",
      "143000.0    1\n",
      "55000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Canada': 65000.0    1\n",
      "88000.0    1\n",
      "57000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Czech_Republic': 8000.0      1\n",
      "832000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Denmark': 390000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Finland': 4000.0    1\n",
      "Name: Salary_min, dtype: int64, 'France': 104000.0    28\n",
      "130000.0     8\n",
      "117000.0     7\n",
      "143000.0     6\n",
      "60000.0      5\n",
      "40000.0      4\n",
      "52000.0      3\n",
      "98800.0      3\n",
      "55000.0      3\n",
      "50000.0      3\n",
      "34000.0      3\n",
      "37000.0      2\n",
      "182000.0     2\n",
      "156000.0     2\n",
      "169000.0     2\n",
      "45000.0      2\n",
      "124800.0     2\n",
      "700.0        1\n",
      "46000.0      1\n",
      "85000.0      1\n",
      "85800.0      1\n",
      "62400.0      1\n",
      "91000.0      1\n",
      "35000.0      1\n",
      "101400.0     1\n",
      "109200.0     1\n",
      "93600.0      1\n",
      "72862.4      1\n",
      "135200.0     1\n",
      "70000.0      1\n",
      "75000.0      1\n",
      "33000.0      1\n",
      "36000.0      1\n",
      "Name: Salary_min, dtype: int64, 'Germany': 60000.0    2\n",
      "45000.0    2\n",
      "80000.0    2\n",
      "65000.0    1\n",
      "4000.0     1\n",
      "50000.0    1\n",
      "30160.0    1\n",
      "55000.0    1\n",
      "47000.0    1\n",
      "90000.0    1\n",
      "83200.0    1\n",
      "Name: Salary_min, dtype: int64, 'Greece': Series([], Name: Salary_min, dtype: int64), 'Hong_Kong': 30000.0     4\n",
      "35000.0     3\n",
      "300000.0    3\n",
      "20000.0     2\n",
      "360000.0    2\n",
      "60000.0     2\n",
      "50000.0     2\n",
      "5000.0      1\n",
      "65000.0     1\n",
      "400000.0    1\n",
      "80000.0     1\n",
      "90000.0     1\n",
      "45000.0     1\n",
      "416000.0    1\n",
      "600000.0    1\n",
      "39000.0     1\n",
      "32000.0     1\n",
      "47000.0     1\n",
      "480000.0    1\n",
      "225000.0    1\n",
      "36000.0     1\n",
      "18000.0     1\n",
      "365000.0    1\n",
      "40000.0     1\n",
      "288000.0    1\n",
      "25000.0     1\n",
      "17000.0     1\n",
      "540000.0    1\n",
      "22000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Hungary': Series([], Name: Salary_min, dtype: int64), 'Ireland': 50000.0     5\n",
      "54000.0     4\n",
      "40000.0     3\n",
      "25000.0     3\n",
      "39000.0     3\n",
      "48000.0     2\n",
      "67000.0     2\n",
      "52000.0     2\n",
      "63000.0     2\n",
      "35000.0     2\n",
      "88000.0     2\n",
      "44000.0     2\n",
      "70000.0     2\n",
      "45000.0     2\n",
      "80000.0     2\n",
      "97510.4     1\n",
      "56000.0     1\n",
      "86000.0     1\n",
      "34000.0     1\n",
      "47000.0     1\n",
      "95000.0     1\n",
      "66000.0     1\n",
      "65000.0     1\n",
      "32000.0     1\n",
      "75000.0     1\n",
      "87000.0     1\n",
      "49400.0     1\n",
      "76000.0     1\n",
      "38000.0     1\n",
      "31000.0     1\n",
      "49000.0     1\n",
      "51000.0     1\n",
      "73000.0     1\n",
      "105000.0    1\n",
      "46000.0     1\n",
      "143000.0    1\n",
      "57000.0     1\n",
      "84000.0     1\n",
      "104000.0    1\n",
      "53000.0     1\n",
      "60000.0     1\n",
      "61000.0     1\n",
      "111000.0    1\n",
      "33000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Israel': Series([], Name: Salary_min, dtype: int64), 'Italy': 30000.0     4\n",
      "25000.0     2\n",
      "155000.0    1\n",
      "800.0       1\n",
      "40000.0     1\n",
      "22000.0     1\n",
      "78000.0     1\n",
      "32000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Japan': Series([], Name: Salary_min, dtype: int64), 'Luxembourg': Series([], Name: Salary_min, dtype: int64), 'Netherlands': 4000.0    12\n",
      "3000.0     5\n",
      "5000.0     2\n",
      "Name: Salary_min, dtype: int64, 'New_Zealand': 78000.0     9\n",
      "90000.0     4\n",
      "84000.0     2\n",
      "55000.0     2\n",
      "91000.0     2\n",
      "110000.0    2\n",
      "80000.0     1\n",
      "57000.0     1\n",
      "70000.0     1\n",
      "95000.0     1\n",
      "199000.0    1\n",
      "98000.0     1\n",
      "152000.0    1\n",
      "74000.0     1\n",
      "109000.0    1\n",
      "63000.0     1\n",
      "148000.0    1\n",
      "77000.0     1\n",
      "72000.0     1\n",
      "100000.0    1\n",
      "52000.0     1\n",
      "75000.0     1\n",
      "114000.0    1\n",
      "65000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Norway': 8000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Poland': 8000.0      1\n",
      "249600.0    1\n",
      "85000.0     1\n",
      "20000.0     1\n",
      "18000.0     1\n",
      "15000.0     1\n",
      "326560.0    1\n",
      "338000.0    1\n",
      "27000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Portugal': 71000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Romania': 20000.0    1\n",
      "8000.0     1\n",
      "2000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Singapore': 6000.0      7\n",
      "7000.0      5\n",
      "4000.0      5\n",
      "3000.0      4\n",
      "5000.0      4\n",
      "72000.0     3\n",
      "71000.0     3\n",
      "68000.0     3\n",
      "48000.0     3\n",
      "70000.0     3\n",
      "36000.0     2\n",
      "120000.0    2\n",
      "66000.0     2\n",
      "10000.0     2\n",
      "180000.0    1\n",
      "8000.0      1\n",
      "42000.0     1\n",
      "45000.0     1\n",
      "300000.0    1\n",
      "82000.0     1\n",
      "50000.0     1\n",
      "44000.0     1\n",
      "77000.0     1\n",
      "65000.0     1\n",
      "79000.0     1\n",
      "9000.0      1\n",
      "Name: Salary_min, dtype: int64, 'South_Korea': Series([], Name: Salary_min, dtype: int64), 'Spain': 30000.0    2\n",
      "36000.0    2\n",
      "78000.0    1\n",
      "24000.0    1\n",
      "39000.0    1\n",
      "40000.0    1\n",
      "45000.0    1\n",
      "38000.0    1\n",
      "50000.0    1\n",
      "2000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Sweden': 600000.0    1\n",
      "83200.0     1\n",
      "40000.0     1\n",
      "Name: Salary_min, dtype: int64, 'Switzerland': 100000.0    5\n",
      "145000.0    4\n",
      "135000.0    3\n",
      "140000.0    2\n",
      "120000.0    2\n",
      "90000.0     1\n",
      "80000.0     1\n",
      "115000.0    1\n",
      "Name: Salary_min, dtype: int64, 'Taiwan': Series([], Name: Salary_min, dtype: int64), 'Turkey': Series([], Name: Salary_min, dtype: int64), 'United_Kingdom': 50000.0     9\n",
      "45000.0     8\n",
      "35000.0     6\n",
      "48000.0     5\n",
      "37000.0     5\n",
      "30000.0     4\n",
      "60000.0     4\n",
      "40000.0     4\n",
      "49000.0     4\n",
      "65000.0     3\n",
      "55000.0     3\n",
      "33000.0     3\n",
      "62000.0     3\n",
      "34000.0     2\n",
      "44000.0     2\n",
      "43000.0     2\n",
      "46000.0     2\n",
      "56000.0     2\n",
      "53000.0     2\n",
      "117000.0    1\n",
      "182000.0    1\n",
      "36000.0     1\n",
      "104000.0    1\n",
      "1000.0      1\n",
      "69000.0     1\n",
      "130000.0    1\n",
      "51000.0     1\n",
      "91000.0     1\n",
      "47000.0     1\n",
      "42000.0     1\n",
      "90000.0     1\n",
      "70000.0     1\n",
      "59000.0     1\n",
      "29000.0     1\n",
      "78000.0     1\n",
      "28000.0     1\n",
      "20000.0     1\n",
      "41000.0     1\n",
      "143000.0    1\n",
      "58000.0     1\n",
      "87984.0     1\n",
      "Name: Salary_min, dtype: int64, 'United_States': 104000.0    11\n",
      "124800.0     9\n",
      "135200.0     9\n",
      "110000.0     8\n",
      "140000.0     8\n",
      "            ..\n",
      "74000.0      1\n",
      "143000.0     1\n",
      "119870.4     1\n",
      "125000.0     1\n",
      "66000.0      1\n",
      "Name: Salary_min, Length: 85, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_min'] = df.apply(\n",
    "            lambda row: calculate_yearly_income(row['Salary_min']) if row['Salary_hourly'] == True else row['Salary_min'],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_min', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 120000.0    5\n",
      "150000.0    5\n",
      "128000.0    5\n",
      "105000.0    3\n",
      "122000.0    2\n",
      "100000.0    2\n",
      "90000.0     2\n",
      "180000.0    2\n",
      "177000.0    2\n",
      "130000.0    2\n",
      "155000.0    2\n",
      "286000.0    1\n",
      "111000.0    1\n",
      "106000.0    1\n",
      "108000.0    1\n",
      "139000.0    1\n",
      "126000.0    1\n",
      "141000.0    1\n",
      "200000.0    1\n",
      "125000.0    1\n",
      "135000.0    1\n",
      "75000.0     1\n",
      "121000.0    1\n",
      "144000.0    1\n",
      "221000.0    1\n",
      "76000.0     1\n",
      "164000.0    1\n",
      "118000.0    1\n",
      "187200.0    1\n",
      "165000.0    1\n",
      "181000.0    1\n",
      "208000.0    1\n",
      "174000.0    1\n",
      "158000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Austria': 3000.0    1\n",
      "4000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Belgium': 65000.0     2\n",
      "70000.0     2\n",
      "42000.0     1\n",
      "182000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Canada': 75000.0    1\n",
      "94000.0    1\n",
      "71000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Czech_Republic': 8000.0       1\n",
      "1664000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Denmark': 650000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Finland': 6000.0    1\n",
      "Name: Salary_max, dtype: int64, 'France': 143000.0    16\n",
      "60000.0      9\n",
      "156000.0     8\n",
      "117000.0     5\n",
      "169000.0     5\n",
      "70000.0      5\n",
      "130000.0     5\n",
      "150800.0     4\n",
      "109200.0     4\n",
      "104000.0     3\n",
      "135200.0     3\n",
      "55000.0      3\n",
      "153400.0     3\n",
      "80000.0      3\n",
      "182000.0     2\n",
      "75000.0      2\n",
      "40000.0      2\n",
      "195000.0     2\n",
      "1000.0       1\n",
      "95000.0      1\n",
      "208000.0     1\n",
      "111800.0     1\n",
      "106600.0     1\n",
      "114400.0     1\n",
      "205400.0     1\n",
      "119600.0     1\n",
      "42000.0      1\n",
      "148200.0     1\n",
      "176800.0     1\n",
      "45000.0      1\n",
      "124800.0     1\n",
      "72862.4      1\n",
      "50000.0      1\n",
      "137800.0     1\n",
      "65000.0      1\n",
      "Name: Salary_max, dtype: int64, 'Germany': 80000.0     3\n",
      "75000.0     1\n",
      "85000.0     1\n",
      "6000.0      1\n",
      "70000.0     1\n",
      "30160.0     1\n",
      "60000.0     1\n",
      "52000.0     1\n",
      "50000.0     1\n",
      "90000.0     1\n",
      "83200.0     1\n",
      "100000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Greece': Series([], Name: Salary_max, dtype: int64), 'Hong_Kong': 50000.0     7\n",
      "40000.0     3\n",
      "480000.0    2\n",
      "30000.0     2\n",
      "60000.0     2\n",
      "32000.0     1\n",
      "65000.0     1\n",
      "800000.0    1\n",
      "95000.0     1\n",
      "75000.0     1\n",
      "90000.0     1\n",
      "416000.0    1\n",
      "720000.0    1\n",
      "66000.0     1\n",
      "55000.0     1\n",
      "47000.0     1\n",
      "360000.0    1\n",
      "576000.0    1\n",
      "600000.0    1\n",
      "38000.0     1\n",
      "22000.0     1\n",
      "512000.0    1\n",
      "468000.0    1\n",
      "446000.0    1\n",
      "474000.0    1\n",
      "25000.0     1\n",
      "17000.0     1\n",
      "540000.0    1\n",
      "5000.0      1\n",
      "Name: Salary_max, dtype: int64, 'Hungary': Series([], Name: Salary_max, dtype: int64), 'Ireland': 50000.0     5\n",
      "96000.0     4\n",
      "95000.0     4\n",
      "70000.0     3\n",
      "100000.0    3\n",
      "87000.0     2\n",
      "48000.0     2\n",
      "75000.0     2\n",
      "80000.0     2\n",
      "77000.0     2\n",
      "72000.0     2\n",
      "43000.0     2\n",
      "115000.0    2\n",
      "88000.0     2\n",
      "69000.0     2\n",
      "49000.0     2\n",
      "73000.0     2\n",
      "97000.0     2\n",
      "110510.4    1\n",
      "117000.0    1\n",
      "99000.0     1\n",
      "94000.0     1\n",
      "65000.0     1\n",
      "86000.0     1\n",
      "130000.0    1\n",
      "55000.0     1\n",
      "44000.0     1\n",
      "85000.0     1\n",
      "98000.0     1\n",
      "71000.0     1\n",
      "128000.0    1\n",
      "39000.0     1\n",
      "66000.0     1\n",
      "32000.0     1\n",
      "67000.0     1\n",
      "135200.0    1\n",
      "143000.0    1\n",
      "40000.0     1\n",
      "47000.0     1\n",
      "57200.0     1\n",
      "Name: Salary_max, dtype: int64, 'Israel': Series([], Name: Salary_max, dtype: int64), 'Italy': 60000.0     3\n",
      "45000.0     2\n",
      "33000.0     1\n",
      "155000.0    1\n",
      "800.0       1\n",
      "42000.0     1\n",
      "25000.0     1\n",
      "91000.0     1\n",
      "38000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Japan': Series([], Name: Salary_max, dtype: int64), 'Luxembourg': Series([], Name: Salary_max, dtype: int64), 'Netherlands': 5000.0    9\n",
      "6000.0    5\n",
      "7000.0    2\n",
      "4000.0    2\n",
      "3000.0    1\n",
      "Name: Salary_max, dtype: int64, 'New_Zealand': 120000.0    13\n",
      "140000.0     3\n",
      "100000.0     3\n",
      "127000.0     2\n",
      "130000.0     2\n",
      "80000.0      1\n",
      "180000.0     1\n",
      "123000.0     1\n",
      "102000.0     1\n",
      "200000.0     1\n",
      "184000.0     1\n",
      "71000.0      1\n",
      "135000.0     1\n",
      "121000.0     1\n",
      "111000.0     1\n",
      "145000.0     1\n",
      "116000.0     1\n",
      "101000.0     1\n",
      "97000.0      1\n",
      "131000.0     1\n",
      "99000.0      1\n",
      "Name: Salary_max, dtype: int64, 'Norway': 8000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Poland': 30000.0     2\n",
      "8000.0      1\n",
      "249600.0    1\n",
      "85000.0     1\n",
      "15000.0     1\n",
      "326560.0    1\n",
      "416000.0    1\n",
      "31000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Portugal': 72000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Romania': 20000.0    1\n",
      "8000.0     1\n",
      "2000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Singapore': 8000.0      5\n",
      "5000.0      4\n",
      "7000.0      4\n",
      "6000.0      4\n",
      "96000.0     4\n",
      "12000.0     3\n",
      "10000.0     3\n",
      "9000.0      3\n",
      "113000.0    3\n",
      "108000.0    2\n",
      "71000.0     2\n",
      "100000.0    2\n",
      "120000.0    2\n",
      "111000.0    2\n",
      "90000.0     2\n",
      "60000.0     2\n",
      "45000.0     2\n",
      "4000.0      1\n",
      "144000.0    1\n",
      "66000.0     1\n",
      "360000.0    1\n",
      "101000.0    1\n",
      "300000.0    1\n",
      "52000.0     1\n",
      "80000.0     1\n",
      "84000.0     1\n",
      "20000.0     1\n",
      "15000.0     1\n",
      "Name: Salary_max, dtype: int64, 'South_Korea': Series([], Name: Salary_max, dtype: int64), 'Spain': 48000.0    2\n",
      "78000.0    1\n",
      "33000.0    1\n",
      "42000.0    1\n",
      "40000.0    1\n",
      "24000.0    1\n",
      "45000.0    1\n",
      "50000.0    1\n",
      "38000.0    1\n",
      "55000.0    1\n",
      "2000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Sweden': 750000.0    1\n",
      "166400.0    1\n",
      "60000.0     1\n",
      "Name: Salary_max, dtype: int64, 'Switzerland': 145000.0    4\n",
      "100000.0    4\n",
      "135000.0    3\n",
      "120000.0    3\n",
      "140000.0    2\n",
      "110000.0    1\n",
      "95000.0     1\n",
      "115000.0    1\n",
      "Name: Salary_max, dtype: int64, 'Taiwan': Series([], Name: Salary_max, dtype: int64), 'Turkey': Series([], Name: Salary_max, dtype: int64), 'United_Kingdom': 50000.0     7\n",
      "68000.0     6\n",
      "60000.0     5\n",
      "55000.0     5\n",
      "65000.0     4\n",
      "75000.0     4\n",
      "59000.0     4\n",
      "100000.0    3\n",
      "69000.0     3\n",
      "80000.0     3\n",
      "45000.0     3\n",
      "70000.0     3\n",
      "57000.0     2\n",
      "49000.0     2\n",
      "42000.0     2\n",
      "78000.0     2\n",
      "117000.0    2\n",
      "72000.0     2\n",
      "53000.0     2\n",
      "58000.0     2\n",
      "62000.0     2\n",
      "143000.0    2\n",
      "38000.0     2\n",
      "30000.0     2\n",
      "63000.0     2\n",
      "67000.0     1\n",
      "130000.0    1\n",
      "47000.0     1\n",
      "43000.0     1\n",
      "1000.0      1\n",
      "61000.0     1\n",
      "90000.0     1\n",
      "71000.0     1\n",
      "44000.0     1\n",
      "32000.0     1\n",
      "52000.0     1\n",
      "182000.0    1\n",
      "35000.0     1\n",
      "86000.0     1\n",
      "88000.0     1\n",
      "22000.0     1\n",
      "121000.0    1\n",
      "85000.0     1\n",
      "187574.4    1\n",
      "Name: Salary_max, dtype: int64, 'United_States': 124800.0    12\n",
      "145600.0    10\n",
      "104000.0     9\n",
      "135200.0     8\n",
      "110000.0     8\n",
      "            ..\n",
      "178000.0     1\n",
      "249600.0     1\n",
      "145000.0     1\n",
      "108000.0     1\n",
      "147680.0     1\n",
      "Name: Salary_max, Length: 89, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_max'] = df.apply(\n",
    "            lambda row: calculate_yearly_income(row['Salary_max']) if row['Salary_hourly'] == True else row['Salary_max'],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_max', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calculate_yearly_income"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.8 Convert all salaries to USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Currency</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUD</th>\n",
       "      <td>1.474993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BGN</th>\n",
       "      <td>1.768834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRL</th>\n",
       "      <td>4.920865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAD</th>\n",
       "      <td>1.331736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHF</th>\n",
       "      <td>0.888758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CNY</th>\n",
       "      <td>6.851859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CZK</th>\n",
       "      <td>21.109704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DKK</th>\n",
       "      <td>6.738718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EUR</th>\n",
       "      <td>0.904404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBP</th>\n",
       "      <td>0.799855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HKD</th>\n",
       "      <td>7.849959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUF</th>\n",
       "      <td>337.957855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDR</th>\n",
       "      <td>14734.367369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ILS</th>\n",
       "      <td>3.656145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INR</th>\n",
       "      <td>81.721534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISK</th>\n",
       "      <td>135.389346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JPY</th>\n",
       "      <td>132.585692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KRW</th>\n",
       "      <td>1300.922493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MXN</th>\n",
       "      <td>18.051732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYR</th>\n",
       "      <td>4.402008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOK</th>\n",
       "      <td>10.312020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NZD</th>\n",
       "      <td>1.590667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PHP</th>\n",
       "      <td>55.279009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PLN</th>\n",
       "      <td>4.199602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RON</th>\n",
       "      <td>4.469838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEK</th>\n",
       "      <td>10.260921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD</th>\n",
       "      <td>1.326309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THB</th>\n",
       "      <td>34.059872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRY</th>\n",
       "      <td>19.373971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USD</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZAR</th>\n",
       "      <td>18.029484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Rate\n",
       "Currency              \n",
       "AUD           1.474993\n",
       "BGN           1.768834\n",
       "BRL           4.920865\n",
       "CAD           1.331736\n",
       "CHF           0.888758\n",
       "CNY           6.851859\n",
       "CZK          21.109704\n",
       "DKK           6.738718\n",
       "EUR           0.904404\n",
       "GBP           0.799855\n",
       "HKD           7.849959\n",
       "HUF         337.957855\n",
       "IDR       14734.367369\n",
       "ILS           3.656145\n",
       "INR          81.721534\n",
       "ISK         135.389346\n",
       "JPY         132.585692\n",
       "KRW        1300.922493\n",
       "MXN          18.051732\n",
       "MYR           4.402008\n",
       "NOK          10.312020\n",
       "NZD           1.590667\n",
       "PHP          55.279009\n",
       "PLN           4.199602\n",
       "RON           4.469838\n",
       "SEK          10.260921\n",
       "SGD           1.326309\n",
       "THB          34.059872\n",
       "TRY          19.373971\n",
       "USD           1.000000\n",
       "ZAR          18.029484"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dollar_rates = pd.read_csv(\"data\\clean\\_Socioeconomic data\\dollar_rates_04_14_2023.csv\", index_col='Currency')\n",
    "df_dollar_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Salary_max'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[salary_type] = df.apply(\n",
    "            lambda row: int(row[salary_type] / df_dollar_rates.loc[row['Salary_currency']]) if isinstance(row['Salary_currency'], str) else row[salary_type],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 81356.0     5\n",
      "101695.0    5\n",
      "86780.0     5\n",
      "71186.0     3\n",
      "82712.0     2\n",
      "67796.0     2\n",
      "61017.0     2\n",
      "122034.0    2\n",
      "120000.0    2\n",
      "88135.0     2\n",
      "105085.0    2\n",
      "193899.0    1\n",
      "75254.0     1\n",
      "71864.0     1\n",
      "73220.0     1\n",
      "94237.0     1\n",
      "85424.0     1\n",
      "95593.0     1\n",
      "135593.0    1\n",
      "84746.0     1\n",
      "91525.0     1\n",
      "50847.0     1\n",
      "82034.0     1\n",
      "97627.0     1\n",
      "149831.0    1\n",
      "51525.0     1\n",
      "111186.0    1\n",
      "80000.0     1\n",
      "126915.0    1\n",
      "111864.0    1\n",
      "122712.0    1\n",
      "141017.0    1\n",
      "117966.0    1\n",
      "107119.0    1\n",
      "Name: Salary_max, dtype: int64, 'Austria': 3317.0    1\n",
      "4422.0    1\n",
      "Name: Salary_max, dtype: int64, 'Belgium': 71870.0     2\n",
      "77399.0     2\n",
      "46439.0     1\n",
      "201237.0    1\n",
      "Name: Salary_max, dtype: int64, 'Canada': 56317.0    1\n",
      "70584.0    1\n",
      "53313.0    1\n",
      "Name: Salary_max, dtype: int64, 'Czech_Republic': 378.0      1\n",
      "78826.0    1\n",
      "Name: Salary_max, dtype: int64, 'Denmark': 96457.0    1\n",
      "Name: Salary_max, dtype: int64, 'Finland': 6634.0    1\n",
      "Name: Salary_max, dtype: int64, 'France': 158115.0    16\n",
      "66342.0      9\n",
      "172489.0     8\n",
      "129366.0     5\n",
      "186863.0     5\n",
      "77399.0      5\n",
      "143741.0     5\n",
      "166739.0     4\n",
      "120742.0     4\n",
      "114992.0     3\n",
      "149490.0     3\n",
      "60813.0      3\n",
      "169614.0     3\n",
      "88456.0      3\n",
      "201237.0     2\n",
      "82927.0      2\n",
      "44228.0      2\n",
      "215611.0     2\n",
      "1105.0       1\n",
      "105041.0     1\n",
      "229985.0     1\n",
      "123617.0     1\n",
      "117867.0     1\n",
      "126492.0     1\n",
      "227110.0     1\n",
      "132241.0     1\n",
      "46439.0      1\n",
      "163864.0     1\n",
      "195487.0     1\n",
      "49756.0      1\n",
      "137991.0     1\n",
      "80563.0      1\n",
      "55285.0      1\n",
      "152365.0     1\n",
      "71870.0      1\n",
      "Name: Salary_max, dtype: int64, 'Germany': 88456.0     3\n",
      "82927.0     1\n",
      "93984.0     1\n",
      "6634.0      1\n",
      "77399.0     1\n",
      "33347.0     1\n",
      "66342.0     1\n",
      "57496.0     1\n",
      "55285.0     1\n",
      "99513.0     1\n",
      "91994.0     1\n",
      "110570.0    1\n",
      "Name: Salary_max, dtype: int64, 'Greece': Series([], Name: Salary_max, dtype: int64), 'Hong_Kong': 6369.0      7\n",
      "5095.0      3\n",
      "61146.0     2\n",
      "3821.0      2\n",
      "7643.0      2\n",
      "4076.0      1\n",
      "8280.0      1\n",
      "101911.0    1\n",
      "12101.0     1\n",
      "9554.0      1\n",
      "11465.0     1\n",
      "52993.0     1\n",
      "91720.0     1\n",
      "8407.0      1\n",
      "7006.0      1\n",
      "5987.0      1\n",
      "45860.0     1\n",
      "73376.0     1\n",
      "76433.0     1\n",
      "4840.0      1\n",
      "2802.0      1\n",
      "65223.0     1\n",
      "59618.0     1\n",
      "56815.0     1\n",
      "60382.0     1\n",
      "3184.0      1\n",
      "2165.0      1\n",
      "68790.0     1\n",
      "636.0       1\n",
      "Name: Salary_max, dtype: int64, 'Hungary': Series([], Name: Salary_max, dtype: int64), 'Ireland': 55285.0     5\n",
      "106147.0    4\n",
      "105041.0    4\n",
      "77399.0     3\n",
      "110570.0    3\n",
      "96195.0     2\n",
      "53073.0     2\n",
      "82927.0     2\n",
      "88456.0     2\n",
      "85138.0     2\n",
      "79610.0     2\n",
      "47545.0     2\n",
      "127155.0    2\n",
      "97301.0     2\n",
      "76293.0     2\n",
      "54179.0     2\n",
      "80716.0     2\n",
      "107252.0    2\n",
      "122191.0    1\n",
      "129366.0    1\n",
      "109464.0    1\n",
      "103935.0    1\n",
      "71870.0     1\n",
      "95090.0     1\n",
      "143741.0    1\n",
      "60813.0     1\n",
      "48650.0     1\n",
      "93984.0     1\n",
      "108358.0    1\n",
      "78504.0     1\n",
      "141529.0    1\n",
      "43122.0     1\n",
      "72976.0     1\n",
      "35382.0     1\n",
      "74081.0     1\n",
      "149490.0    1\n",
      "158115.0    1\n",
      "44228.0     1\n",
      "51967.0     1\n",
      "63246.0     1\n",
      "Name: Salary_max, dtype: int64, 'Israel': Series([], Name: Salary_max, dtype: int64), 'Italy': 66342.0     3\n",
      "49756.0     2\n",
      "36488.0     1\n",
      "171383.0    1\n",
      "884.0       1\n",
      "46439.0     1\n",
      "27642.0     1\n",
      "100618.0    1\n",
      "42016.0     1\n",
      "Name: Salary_max, dtype: int64, 'Japan': Series([], Name: Salary_max, dtype: int64), 'Luxembourg': Series([], Name: Salary_max, dtype: int64), 'Netherlands': 5528.0    9\n",
      "6634.0    5\n",
      "7739.0    2\n",
      "4422.0    2\n",
      "3317.0    1\n",
      "Name: Salary_max, dtype: int64, 'New_Zealand': 75440.0     13\n",
      "88013.0      3\n",
      "62866.0      3\n",
      "79840.0      2\n",
      "81726.0      2\n",
      "50293.0      1\n",
      "113160.0     1\n",
      "77326.0      1\n",
      "64124.0      1\n",
      "125733.0     1\n",
      "115674.0     1\n",
      "44635.0      1\n",
      "84870.0      1\n",
      "76068.0      1\n",
      "69782.0      1\n",
      "91156.0      1\n",
      "72925.0      1\n",
      "63495.0      1\n",
      "60980.0      1\n",
      "82355.0      1\n",
      "62238.0      1\n",
      "Name: Salary_max, dtype: int64, 'Norway': 775.0    1\n",
      "Name: Salary_max, dtype: int64, 'Poland': 7143.0     2\n",
      "1904.0     1\n",
      "59434.0    1\n",
      "20240.0    1\n",
      "3571.0     1\n",
      "77759.0    1\n",
      "99057.0    1\n",
      "7381.0     1\n",
      "Name: Salary_max, dtype: int64, 'Portugal': 79610.0    1\n",
      "Name: Salary_max, dtype: int64, 'Romania': 4474.0    1\n",
      "1789.0    1\n",
      "447.0     1\n",
      "Name: Salary_max, dtype: int64, 'Singapore': 6031.0      5\n",
      "3769.0      4\n",
      "5277.0      4\n",
      "4523.0      4\n",
      "72381.0     4\n",
      "9047.0      3\n",
      "7539.0      3\n",
      "6785.0      3\n",
      "85198.0     3\n",
      "81428.0     2\n",
      "53532.0     2\n",
      "75397.0     2\n",
      "90476.0     2\n",
      "83690.0     2\n",
      "67857.0     2\n",
      "45238.0     2\n",
      "33928.0     2\n",
      "3015.0      1\n",
      "108571.0    1\n",
      "49762.0     1\n",
      "271429.0    1\n",
      "76151.0     1\n",
      "226191.0    1\n",
      "39206.0     1\n",
      "60317.0     1\n",
      "63333.0     1\n",
      "15079.0     1\n",
      "11309.0     1\n",
      "Name: Salary_max, dtype: int64, 'South_Korea': Series([], Name: Salary_max, dtype: int64), 'Spain': 53073.0    2\n",
      "86244.0    1\n",
      "36488.0    1\n",
      "46439.0    1\n",
      "44228.0    1\n",
      "26536.0    1\n",
      "49756.0    1\n",
      "55285.0    1\n",
      "42016.0    1\n",
      "60813.0    1\n",
      "2211.0     1\n",
      "Name: Salary_max, dtype: int64, 'Sweden': 73092.0    1\n",
      "16216.0    1\n",
      "5847.0     1\n",
      "Name: Salary_max, dtype: int64, 'Switzerland': 163148.0    4\n",
      "112516.0    4\n",
      "151897.0    3\n",
      "135019.0    3\n",
      "157523.0    2\n",
      "123768.0    1\n",
      "106890.0    1\n",
      "129394.0    1\n",
      "Name: Salary_max, dtype: int64, 'Taiwan': Series([], Name: Salary_max, dtype: int64), 'Turkey': Series([], Name: Salary_max, dtype: int64), 'United_Kingdom': 62511.0     7\n",
      "85015.0     6\n",
      "75013.0     5\n",
      "68762.0     5\n",
      "81264.0     4\n",
      "93766.0     4\n",
      "73763.0     4\n",
      "125022.0    3\n",
      "86265.0     3\n",
      "100018.0    3\n",
      "56260.0     3\n",
      "87515.0     3\n",
      "71262.0     2\n",
      "61261.0     2\n",
      "52509.0     2\n",
      "97517.0     2\n",
      "146276.0    2\n",
      "90016.0     2\n",
      "66261.0     2\n",
      "72513.0     2\n",
      "77514.0     2\n",
      "178782.0    2\n",
      "47508.0     2\n",
      "37506.0     2\n",
      "78764.0     2\n",
      "83765.0     1\n",
      "162529.0    1\n",
      "58760.0     1\n",
      "53759.0     1\n",
      "1250.0      1\n",
      "76263.0     1\n",
      "112520.0    1\n",
      "88766.0     1\n",
      "55009.0     1\n",
      "40007.0     1\n",
      "65011.0     1\n",
      "227541.0    1\n",
      "43757.0     1\n",
      "107519.0    1\n",
      "110019.0    1\n",
      "27504.0     1\n",
      "151277.0    1\n",
      "106269.0    1\n",
      "234510.0    1\n",
      "Name: Salary_max, dtype: int64, 'United_States': 124800.0    12\n",
      "145600.0    10\n",
      "104000.0     9\n",
      "135200.0     8\n",
      "110000.0     8\n",
      "            ..\n",
      "178000.0     1\n",
      "249600.0     1\n",
      "145000.0     1\n",
      "108000.0     1\n",
      "147680.0     1\n",
      "Name: Salary_max, Length: 89, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "show_results(salary_type, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_type = 'Salary_min'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[salary_type] = df.apply(\n",
    "            lambda row: int(row[salary_type] / df_dollar_rates.loc[row['Salary_currency']]) if isinstance(row['Salary_currency'], str) else row[salary_type],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Australia': 42712.0     1\n",
       " 50847.0     2\n",
       " 54237.0     3\n",
       " 57627.0     1\n",
       " 61017.0     6\n",
       " 61695.0     2\n",
       " 62373.0     2\n",
       " 64407.0     1\n",
       " 65085.0     2\n",
       " 67118.0     2\n",
       " 67796.0     4\n",
       " 68474.0     1\n",
       " 69152.0     1\n",
       " 70508.0     1\n",
       " 73898.0     6\n",
       " 74576.0     2\n",
       " 75932.0     2\n",
       " 76610.0     1\n",
       " 78644.0     1\n",
       " 81356.0     4\n",
       " 85424.0     1\n",
       " 88135.0     1\n",
       " 90169.0     1\n",
       " 93559.0     1\n",
       " 94915.0     1\n",
       " 105763.0    2\n",
       " 132203.0    1\n",
       " 135593.0    1\n",
       " 193899.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Austria': 3317.0    2\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Belgium': 44228.0     1\n",
       " 46439.0     1\n",
       " 49756.0     2\n",
       " 60813.0     1\n",
       " 158115.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Canada': 42801.0    1\n",
       " 48808.0    1\n",
       " 66079.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Czech_Republic': 378.0      1\n",
       " 39413.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Denmark': 57874.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Finland': 4422.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'France': 773.0        1\n",
       " 36488.0      1\n",
       " 37593.0      3\n",
       " 38699.0      1\n",
       " 39805.0      1\n",
       " 40910.0      2\n",
       " 44228.0      4\n",
       " 49756.0      2\n",
       " 50862.0      1\n",
       " 55285.0      3\n",
       " 57496.0      3\n",
       " 60813.0      3\n",
       " 66342.0      5\n",
       " 68995.0      1\n",
       " 77399.0      1\n",
       " 80563.0      1\n",
       " 82927.0      1\n",
       " 93984.0      1\n",
       " 94869.0      1\n",
       " 100618.0     1\n",
       " 103493.0     1\n",
       " 109243.0     3\n",
       " 112117.0     1\n",
       " 114992.0    28\n",
       " 120742.0     1\n",
       " 129366.0     7\n",
       " 137991.0     2\n",
       " 143741.0     8\n",
       " 149490.0     1\n",
       " 158115.0     6\n",
       " 172489.0     2\n",
       " 186863.0     2\n",
       " 201237.0     2\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Germany': 4422.0     1\n",
       " 33347.0    1\n",
       " 49756.0    2\n",
       " 51967.0    1\n",
       " 55285.0    1\n",
       " 60813.0    1\n",
       " 66342.0    2\n",
       " 71870.0    1\n",
       " 88456.0    2\n",
       " 91994.0    1\n",
       " 99513.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Greece': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Hong_Kong': 636.0      1\n",
       " 2165.0     1\n",
       " 2293.0     1\n",
       " 2547.0     2\n",
       " 2802.0     1\n",
       " 3184.0     1\n",
       " 3821.0     4\n",
       " 4076.0     1\n",
       " 4458.0     3\n",
       " 4586.0     1\n",
       " 4968.0     1\n",
       " 5095.0     1\n",
       " 5732.0     1\n",
       " 5987.0     1\n",
       " 6369.0     2\n",
       " 7643.0     2\n",
       " 8280.0     1\n",
       " 10191.0    1\n",
       " 11465.0    1\n",
       " 28662.0    1\n",
       " 36688.0    1\n",
       " 38216.0    3\n",
       " 45860.0    2\n",
       " 46497.0    1\n",
       " 50955.0    1\n",
       " 52993.0    1\n",
       " 61146.0    1\n",
       " 68790.0    1\n",
       " 76433.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Hungary': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Ireland': 27642.0     3\n",
       " 34276.0     1\n",
       " 35382.0     1\n",
       " 36488.0     1\n",
       " 37593.0     1\n",
       " 38699.0     2\n",
       " 42016.0     1\n",
       " 43122.0     3\n",
       " 44228.0     3\n",
       " 48650.0     2\n",
       " 49756.0     2\n",
       " 50862.0     1\n",
       " 51967.0     1\n",
       " 53073.0     2\n",
       " 54179.0     1\n",
       " 54621.0     1\n",
       " 55285.0     5\n",
       " 56390.0     1\n",
       " 57496.0     2\n",
       " 58602.0     1\n",
       " 59707.0     4\n",
       " 61919.0     1\n",
       " 63024.0     1\n",
       " 66342.0     1\n",
       " 67447.0     1\n",
       " 69659.0     2\n",
       " 71870.0     1\n",
       " 72976.0     1\n",
       " 74081.0     2\n",
       " 77399.0     2\n",
       " 80716.0     1\n",
       " 82927.0     1\n",
       " 84033.0     1\n",
       " 88456.0     2\n",
       " 92878.0     1\n",
       " 95090.0     1\n",
       " 96195.0     1\n",
       " 97301.0     2\n",
       " 105041.0    1\n",
       " 107817.0    1\n",
       " 114992.0    1\n",
       " 116098.0    1\n",
       " 122732.0    1\n",
       " 158115.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Israel': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Italy': 884.0       1\n",
       " 24325.0     1\n",
       " 27642.0     2\n",
       " 33171.0     4\n",
       " 35382.0     1\n",
       " 44228.0     1\n",
       " 86244.0     1\n",
       " 171383.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Japan': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Luxembourg': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Netherlands': 3317.0     5\n",
       " 4422.0    12\n",
       " 5528.0     2\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'New_Zealand': 32690.0     1\n",
       " 34576.0     2\n",
       " 35834.0     1\n",
       " 39606.0     1\n",
       " 40863.0     1\n",
       " 44006.0     1\n",
       " 45264.0     1\n",
       " 46521.0     1\n",
       " 47150.0     1\n",
       " 48407.0     1\n",
       " 49036.0     9\n",
       " 50293.0     1\n",
       " 52808.0     2\n",
       " 56580.0     4\n",
       " 57208.0     2\n",
       " 59723.0     1\n",
       " 61609.0     1\n",
       " 62866.0     1\n",
       " 68524.0     1\n",
       " 69153.0     2\n",
       " 71668.0     1\n",
       " 93042.0     1\n",
       " 95557.0     1\n",
       " 125104.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Norway': 775.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Poland': 1904.0     1\n",
       " 3571.0     1\n",
       " 4286.0     1\n",
       " 4762.0     1\n",
       " 6429.0     1\n",
       " 20240.0    1\n",
       " 59434.0    1\n",
       " 77759.0    1\n",
       " 80483.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Portugal': 78504.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Romania': 447.0     1\n",
       " 1789.0    1\n",
       " 4474.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Singapore': 2261.0      4\n",
       " 3015.0      5\n",
       " 3769.0      4\n",
       " 4523.0      7\n",
       " 5277.0      5\n",
       " 6031.0      1\n",
       " 6785.0      1\n",
       " 7539.0      2\n",
       " 27142.0     2\n",
       " 31666.0     1\n",
       " 33174.0     1\n",
       " 33928.0     1\n",
       " 36190.0     3\n",
       " 37698.0     1\n",
       " 49008.0     1\n",
       " 49762.0     2\n",
       " 51270.0     3\n",
       " 52778.0     3\n",
       " 53532.0     3\n",
       " 54285.0     3\n",
       " 58055.0     1\n",
       " 59563.0     1\n",
       " 61825.0     1\n",
       " 90476.0     2\n",
       " 135714.0    1\n",
       " 226191.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'South_Korea': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Spain': 2211.0     1\n",
       " 26536.0    1\n",
       " 33171.0    2\n",
       " 39805.0    2\n",
       " 42016.0    1\n",
       " 43122.0    1\n",
       " 44228.0    1\n",
       " 49756.0    1\n",
       " 55285.0    1\n",
       " 86244.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Sweden': 3898.0     1\n",
       " 8108.0     1\n",
       " 58474.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Switzerland': 90013.0     1\n",
       " 101264.0    1\n",
       " 112516.0    5\n",
       " 129394.0    1\n",
       " 135019.0    2\n",
       " 151897.0    3\n",
       " 157523.0    2\n",
       " 163148.0    4\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'Taiwan': Series([], Name: Salary_min, dtype: int64),\n",
       " 'Turkey': Series([], Name: Salary_min, dtype: int64),\n",
       " 'United_Kingdom': 1250.0      1\n",
       " 25004.0     1\n",
       " 35006.0     1\n",
       " 36256.0     1\n",
       " 37506.0     4\n",
       " 41257.0     3\n",
       " 42507.0     2\n",
       " 43757.0     6\n",
       " 45008.0     1\n",
       " 46258.0     5\n",
       " 50009.0     4\n",
       " 51259.0     1\n",
       " 52509.0     1\n",
       " 53759.0     2\n",
       " 55009.0     2\n",
       " 56260.0     8\n",
       " 57510.0     2\n",
       " 58760.0     1\n",
       " 60010.0     5\n",
       " 61261.0     4\n",
       " 62511.0     9\n",
       " 63761.0     1\n",
       " 66261.0     2\n",
       " 68762.0     3\n",
       " 70012.0     2\n",
       " 72513.0     1\n",
       " 73763.0     1\n",
       " 75013.0     4\n",
       " 77514.0     3\n",
       " 81264.0     3\n",
       " 86265.0     1\n",
       " 87515.0     1\n",
       " 97517.0     1\n",
       " 109999.0    1\n",
       " 112520.0    1\n",
       " 113770.0    1\n",
       " 130023.0    1\n",
       " 146276.0    1\n",
       " 162529.0    1\n",
       " 178782.0    1\n",
       " 227541.0    1\n",
       " Name: Salary_min, dtype: int64,\n",
       " 'United_States': 39000.0     1\n",
       " 39644.0     1\n",
       " 45000.0     1\n",
       " 55000.0     1\n",
       " 56160.0     1\n",
       "            ..\n",
       " 165000.0    1\n",
       " 166400.0    1\n",
       " 176800.0    2\n",
       " 187200.0    2\n",
       " 208000.0    1\n",
       " Name: Salary_min, Length: 85, dtype: int64}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for country, df in dfs.items():\n",
    "    results[country] = df[salary_type].value_counts().sort_index()\n",
    "\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9 Convert monthly salaries to yearly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job offers outside of the US, particularly in Europe, often provide the salary as a monthly figure rather than yearly. To determine the monthly salary, it should generally not be lower than the monthly poverty line or higher than the yearly poverty line. Typically, the average salary for the country is used, and sometimes the median is used instead. However, in some cases, the median salary may not be officially available due to political reasons, such as in Poland. In countries with significant income inequality, the average salary can differ greatly from the median.\n",
    "\n",
    "To address this issue, we use a rule of thumb where the poverty line is half of the median salary in the selected region. We then take the average or median salary and divide it by a factor of 2 to avoid confusion between daily, weekly, and monthly salaries. Any undefined fields are left empty to ensure accurate results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download poverty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australia</th>\n",
       "      <td>2358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austria</th>\n",
       "      <td>2424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belgium</th>\n",
       "      <td>2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Canada</th>\n",
       "      <td>2333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Czech_Republic</th>\n",
       "      <td>1321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 USD\n",
       "LOCATION            \n",
       "Australia       2358\n",
       "Austria         2424\n",
       "Belgium         2462\n",
       "Canada          2333\n",
       "Czech_Republic  1321"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"OECD_poverty_line_monthly.csv\"\n",
    "file_path = f\"data\\clean\\_Socioeconomic data\\{file_name}\"\n",
    "df_poverty_line = pd.read_csv(file_path, index_col='LOCATION')\n",
    "df_poverty_line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def monthly_to_yearly_salary(\n",
    "        row: pd.Series, \n",
    "        salary_type: Literal['Salary_min', 'Salary_max'],\n",
    "        df_poverty_line: pd.DataFrame,\n",
    "        country: str,\n",
    "    ) -> float:\n",
    "\n",
    "    salary_value = row[salary_type]\n",
    "\n",
    "    if not np.isnan(salary_value):\n",
    "\n",
    "        monthly_poverty_line = df_poverty_line.loc[country]\n",
    "        yearly_poverty_line = monthly_poverty_line * 12\n",
    "\n",
    "\n",
    "        if (salary_value <= 0 or salary_value < monthly_poverty_line).bool():\n",
    "            return np.nan\n",
    "        \n",
    "        elif (salary_value >= monthly_poverty_line).bool() and (salary_value < yearly_poverty_line).bool():\n",
    "            return salary_value * 12\n",
    "\n",
    "    return salary_value\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_target = 'Salary_min'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[selector_target] = df.apply(\n",
    "            lambda row: monthly_to_yearly_salary(row, selector_target, df_poverty_line, country),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 193899.0    1\n",
      "135593.0    1\n",
      "132203.0    1\n",
      "105763.0    2\n",
      "94915.0     1\n",
      "93559.0     1\n",
      "90169.0     1\n",
      "88135.0     1\n",
      "85424.0     1\n",
      "81356.0     4\n",
      "78644.0     1\n",
      "76610.0     1\n",
      "75932.0     2\n",
      "74576.0     2\n",
      "73898.0     6\n",
      "70508.0     1\n",
      "69152.0     1\n",
      "68474.0     1\n",
      "67796.0     4\n",
      "67118.0     2\n",
      "65085.0     2\n",
      "64407.0     1\n",
      "62373.0     2\n",
      "61695.0     2\n",
      "61017.0     6\n",
      "57627.0     1\n",
      "54237.0     3\n",
      "50847.0     2\n",
      "42712.0     1\n",
      "Name: Salary_min, dtype: int64, 'Austria': 39804.0    2\n",
      "Name: Salary_min, dtype: int64, 'Belgium': 158115.0    1\n",
      "60813.0     1\n",
      "49756.0     2\n",
      "46439.0     1\n",
      "44228.0     1\n",
      "Name: Salary_min, dtype: int64, 'Canada': 66079.0    1\n",
      "48808.0    1\n",
      "42801.0    1\n",
      "Name: Salary_min, dtype: int64, 'Czech_Republic': 39413.0    1\n",
      "Name: Salary_min, dtype: int64, 'Denmark': 57874.0    1\n",
      "Name: Salary_min, dtype: int64, 'Finland': 53064.0    1\n",
      "Name: Salary_min, dtype: int64, 'France': 201237.0     2\n",
      "186863.0     2\n",
      "172489.0     2\n",
      "158115.0     6\n",
      "149490.0     1\n",
      "143741.0     8\n",
      "137991.0     2\n",
      "129366.0     7\n",
      "120742.0     1\n",
      "114992.0    28\n",
      "112117.0     1\n",
      "109243.0     3\n",
      "103493.0     1\n",
      "100618.0     1\n",
      "94869.0      1\n",
      "93984.0      1\n",
      "82927.0      1\n",
      "80563.0      1\n",
      "77399.0      1\n",
      "68995.0      1\n",
      "66342.0      5\n",
      "60813.0      3\n",
      "57496.0      3\n",
      "55285.0      3\n",
      "50862.0      1\n",
      "49756.0      2\n",
      "44228.0      4\n",
      "40910.0      2\n",
      "39805.0      1\n",
      "38699.0      1\n",
      "37593.0      3\n",
      "36488.0      1\n",
      "Name: Salary_min, dtype: int64, 'Germany': 99513.0    1\n",
      "91994.0    1\n",
      "88456.0    2\n",
      "71870.0    1\n",
      "66342.0    2\n",
      "60813.0    1\n",
      "55285.0    1\n",
      "53064.0    1\n",
      "51967.0    1\n",
      "49756.0    2\n",
      "33347.0    1\n",
      "Name: Salary_min, dtype: int64, 'Greece': Series([], Name: Salary_min, dtype: int64), 'Hong_Kong': 137580.0    1\n",
      "122292.0    1\n",
      "99360.0     1\n",
      "91716.0     2\n",
      "76433.0     1\n",
      "76428.0     2\n",
      "71844.0     1\n",
      "68790.0     1\n",
      "68784.0     1\n",
      "61146.0     1\n",
      "61140.0     1\n",
      "59616.0     1\n",
      "55032.0     1\n",
      "53496.0     3\n",
      "52993.0     1\n",
      "50955.0     1\n",
      "48912.0     1\n",
      "46497.0     1\n",
      "45860.0     2\n",
      "45852.0     4\n",
      "38216.0     3\n",
      "38208.0     1\n",
      "36688.0     1\n",
      "33624.0     1\n",
      "30564.0     2\n",
      "28662.0     1\n",
      "27516.0     1\n",
      "25980.0     1\n",
      "Name: Salary_min, dtype: int64, 'Hungary': Series([], Name: Salary_min, dtype: int64), 'Ireland': 158115.0    1\n",
      "122732.0    1\n",
      "116098.0    1\n",
      "114992.0    1\n",
      "107817.0    1\n",
      "105041.0    1\n",
      "97301.0     2\n",
      "96195.0     1\n",
      "95090.0     1\n",
      "92878.0     1\n",
      "88456.0     2\n",
      "84033.0     1\n",
      "82927.0     1\n",
      "80716.0     1\n",
      "77399.0     2\n",
      "74081.0     2\n",
      "72976.0     1\n",
      "71870.0     1\n",
      "69659.0     2\n",
      "67447.0     1\n",
      "66342.0     1\n",
      "63024.0     1\n",
      "61919.0     1\n",
      "59707.0     4\n",
      "58602.0     1\n",
      "57496.0     2\n",
      "56390.0     1\n",
      "55285.0     5\n",
      "54621.0     1\n",
      "54179.0     1\n",
      "53073.0     2\n",
      "51967.0     1\n",
      "50862.0     1\n",
      "49756.0     2\n",
      "48650.0     2\n",
      "44228.0     3\n",
      "43122.0     3\n",
      "42016.0     1\n",
      "38699.0     2\n",
      "37593.0     1\n",
      "36488.0     1\n",
      "35382.0     1\n",
      "34276.0     1\n",
      "27642.0     3\n",
      "Name: Salary_min, dtype: int64, 'Israel': Series([], Name: Salary_min, dtype: int64), 'Italy': 171383.0    1\n",
      "86244.0     1\n",
      "44228.0     1\n",
      "35382.0     1\n",
      "33171.0     4\n",
      "27642.0     2\n",
      "24325.0     1\n",
      "Name: Salary_min, dtype: int64, 'Japan': Series([], Name: Salary_min, dtype: int64), 'Luxembourg': Series([], Name: Salary_min, dtype: int64), 'Netherlands': 66336.0     2\n",
      "53064.0    12\n",
      "39804.0     5\n",
      "Name: Salary_min, dtype: int64, 'New_Zealand': 125104.0    1\n",
      "95557.0     1\n",
      "93042.0     1\n",
      "71668.0     1\n",
      "69153.0     2\n",
      "68524.0     1\n",
      "62866.0     1\n",
      "61609.0     1\n",
      "59723.0     1\n",
      "57208.0     2\n",
      "56580.0     4\n",
      "52808.0     2\n",
      "50293.0     1\n",
      "49036.0     9\n",
      "48407.0     1\n",
      "47150.0     1\n",
      "46521.0     1\n",
      "45264.0     1\n",
      "44006.0     1\n",
      "40863.0     1\n",
      "39606.0     1\n",
      "35834.0     1\n",
      "34576.0     2\n",
      "32690.0     1\n",
      "Name: Salary_min, dtype: int64, 'Norway': Series([], Name: Salary_min, dtype: int64), 'Poland': 80483.0    1\n",
      "77759.0    1\n",
      "77148.0    1\n",
      "59434.0    1\n",
      "57144.0    1\n",
      "51432.0    1\n",
      "42852.0    1\n",
      "22848.0    1\n",
      "20240.0    1\n",
      "Name: Salary_min, dtype: int64, 'Portugal': 78504.0    1\n",
      "Name: Salary_min, dtype: int64, 'Romania': 53688.0    1\n",
      "21468.0    1\n",
      "Name: Salary_min, dtype: int64, 'Singapore': 325704.0    2\n",
      "226191.0    1\n",
      "135714.0    1\n",
      "90476.0     2\n",
      "90468.0     2\n",
      "81420.0     1\n",
      "72372.0     1\n",
      "63324.0     5\n",
      "61825.0     1\n",
      "59563.0     1\n",
      "58055.0     1\n",
      "54285.0     3\n",
      "54276.0     7\n",
      "53532.0     3\n",
      "52778.0     3\n",
      "51270.0     3\n",
      "49762.0     2\n",
      "49008.0     1\n",
      "45228.0     4\n",
      "37698.0     1\n",
      "36190.0     3\n",
      "36180.0     5\n",
      "33928.0     1\n",
      "33174.0     1\n",
      "31666.0     1\n",
      "Name: Salary_min, dtype: int64, 'South_Korea': Series([], Name: Salary_min, dtype: int64), 'Spain': 86244.0    1\n",
      "55285.0    1\n",
      "49756.0    1\n",
      "44228.0    1\n",
      "43122.0    1\n",
      "42016.0    1\n",
      "39805.0    2\n",
      "33171.0    2\n",
      "26536.0    1\n",
      "26532.0    1\n",
      "Name: Salary_min, dtype: int64, 'Sweden': 97296.0    1\n",
      "58474.0    1\n",
      "46776.0    1\n",
      "Name: Salary_min, dtype: int64, 'Switzerland': 163148.0    4\n",
      "157523.0    2\n",
      "151897.0    3\n",
      "135019.0    2\n",
      "129394.0    1\n",
      "112516.0    5\n",
      "101264.0    1\n",
      "90013.0     1\n",
      "Name: Salary_min, dtype: int64, 'Taiwan': Series([], Name: Salary_min, dtype: int64), 'Turkey': Series([], Name: Salary_min, dtype: int64), 'United_Kingdom': 227541.0    1\n",
      "178782.0    1\n",
      "162529.0    1\n",
      "146276.0    1\n",
      "130023.0    1\n",
      "113770.0    1\n",
      "112520.0    1\n",
      "109999.0    1\n",
      "97517.0     1\n",
      "87515.0     1\n",
      "86265.0     1\n",
      "81264.0     3\n",
      "77514.0     3\n",
      "75013.0     4\n",
      "73763.0     1\n",
      "72513.0     1\n",
      "70012.0     2\n",
      "68762.0     3\n",
      "66261.0     2\n",
      "63761.0     1\n",
      "62511.0     9\n",
      "61261.0     4\n",
      "60010.0     5\n",
      "58760.0     1\n",
      "57510.0     2\n",
      "56260.0     8\n",
      "55009.0     2\n",
      "53759.0     2\n",
      "52509.0     1\n",
      "51259.0     1\n",
      "50009.0     4\n",
      "46258.0     5\n",
      "45008.0     1\n",
      "43757.0     6\n",
      "42507.0     2\n",
      "41257.0     3\n",
      "37506.0     4\n",
      "36256.0     1\n",
      "35006.0     1\n",
      "25004.0     1\n",
      "Name: Salary_min, dtype: int64, 'United_States': 208000.0    1\n",
      "187200.0    2\n",
      "176800.0    2\n",
      "166400.0    1\n",
      "165000.0    1\n",
      "           ..\n",
      "56160.0     1\n",
      "55000.0     1\n",
      "45000.0     1\n",
      "39644.0     1\n",
      "39000.0     1\n",
      "Name: Salary_min, Length: 85, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def show_results_salary(selector_target: str, dfs:dict[pd.DataFrame]):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for country, df in dfs.items():\n",
    "        results[country] = df[selector_target].value_counts().sort_index(ascending=False)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "show_results_salary(selector_target, dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salary max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 193899.0    1\n",
      "149831.0    1\n",
      "141017.0    1\n",
      "135593.0    1\n",
      "126915.0    1\n",
      "122712.0    1\n",
      "122034.0    2\n",
      "120000.0    2\n",
      "117966.0    1\n",
      "111864.0    1\n",
      "111186.0    1\n",
      "107119.0    1\n",
      "105085.0    2\n",
      "101695.0    5\n",
      "97627.0     1\n",
      "95593.0     1\n",
      "94237.0     1\n",
      "91525.0     1\n",
      "88135.0     2\n",
      "86780.0     5\n",
      "85424.0     1\n",
      "84746.0     1\n",
      "82712.0     2\n",
      "82034.0     1\n",
      "81356.0     5\n",
      "80000.0     1\n",
      "75254.0     1\n",
      "73220.0     1\n",
      "71864.0     1\n",
      "71186.0     3\n",
      "67796.0     2\n",
      "61017.0     2\n",
      "51525.0     1\n",
      "50847.0     1\n",
      "Name: Salary_max, dtype: int64, 'Austria': 53064.0    1\n",
      "39804.0    1\n",
      "Name: Salary_max, dtype: int64, 'Belgium': 201237.0    1\n",
      "77399.0     2\n",
      "71870.0     2\n",
      "46439.0     1\n",
      "Name: Salary_max, dtype: int64, 'Canada': 70584.0    1\n",
      "56317.0    1\n",
      "53313.0    1\n",
      "Name: Salary_max, dtype: int64, 'Czech_Republic': 78826.0    1\n",
      "Name: Salary_max, dtype: int64, 'Denmark': 96457.0    1\n",
      "Name: Salary_max, dtype: int64, 'Finland': 79608.0    1\n",
      "Name: Salary_max, dtype: int64, 'France': 229985.0     1\n",
      "227110.0     1\n",
      "215611.0     2\n",
      "201237.0     2\n",
      "195487.0     1\n",
      "186863.0     5\n",
      "172489.0     8\n",
      "169614.0     3\n",
      "166739.0     4\n",
      "163864.0     1\n",
      "158115.0    16\n",
      "152365.0     1\n",
      "149490.0     3\n",
      "143741.0     5\n",
      "137991.0     1\n",
      "132241.0     1\n",
      "129366.0     5\n",
      "126492.0     1\n",
      "123617.0     1\n",
      "120742.0     4\n",
      "117867.0     1\n",
      "114992.0     3\n",
      "105041.0     1\n",
      "88456.0      3\n",
      "82927.0      2\n",
      "80563.0      1\n",
      "77399.0      5\n",
      "71870.0      1\n",
      "66342.0      9\n",
      "60813.0      3\n",
      "55285.0      1\n",
      "49756.0      1\n",
      "46439.0      1\n",
      "44228.0      2\n",
      "Name: Salary_max, dtype: int64, 'Germany': 110570.0    1\n",
      "99513.0     1\n",
      "93984.0     1\n",
      "91994.0     1\n",
      "88456.0     3\n",
      "82927.0     1\n",
      "79608.0     1\n",
      "77399.0     1\n",
      "66342.0     1\n",
      "57496.0     1\n",
      "55285.0     1\n",
      "33347.0     1\n",
      "Name: Salary_max, dtype: int64, 'Greece': Series([], Name: Salary_max, dtype: int64), 'Hong_Kong': 145212.0    1\n",
      "137580.0    1\n",
      "114648.0    1\n",
      "101911.0    1\n",
      "100884.0    1\n",
      "99360.0     1\n",
      "91720.0     1\n",
      "91716.0     2\n",
      "84072.0     1\n",
      "76433.0     1\n",
      "76428.0     7\n",
      "73376.0     1\n",
      "71844.0     1\n",
      "68790.0     1\n",
      "65223.0     1\n",
      "61146.0     2\n",
      "61140.0     3\n",
      "60382.0     1\n",
      "59618.0     1\n",
      "58080.0     1\n",
      "56815.0     1\n",
      "52993.0     1\n",
      "48912.0     1\n",
      "45860.0     1\n",
      "45852.0     2\n",
      "38208.0     1\n",
      "33624.0     1\n",
      "25980.0     1\n",
      "Name: Salary_max, dtype: int64, 'Hungary': Series([], Name: Salary_max, dtype: int64), 'Ireland': 158115.0    1\n",
      "149490.0    1\n",
      "143741.0    1\n",
      "141529.0    1\n",
      "129366.0    1\n",
      "127155.0    2\n",
      "122191.0    1\n",
      "110570.0    3\n",
      "109464.0    1\n",
      "108358.0    1\n",
      "107252.0    2\n",
      "106147.0    4\n",
      "105041.0    4\n",
      "103935.0    1\n",
      "97301.0     2\n",
      "96195.0     2\n",
      "95090.0     1\n",
      "93984.0     1\n",
      "88456.0     2\n",
      "85138.0     2\n",
      "82927.0     2\n",
      "80716.0     2\n",
      "79610.0     2\n",
      "78504.0     1\n",
      "77399.0     3\n",
      "76293.0     2\n",
      "74081.0     1\n",
      "72976.0     1\n",
      "71870.0     1\n",
      "63246.0     1\n",
      "60813.0     1\n",
      "55285.0     5\n",
      "54179.0     2\n",
      "53073.0     2\n",
      "51967.0     1\n",
      "48650.0     1\n",
      "47545.0     2\n",
      "44228.0     1\n",
      "43122.0     1\n",
      "35382.0     1\n",
      "Name: Salary_max, dtype: int64, 'Israel': Series([], Name: Salary_max, dtype: int64), 'Italy': 171383.0    1\n",
      "100618.0    1\n",
      "66342.0     3\n",
      "49756.0     2\n",
      "46439.0     1\n",
      "42016.0     1\n",
      "36488.0     1\n",
      "27642.0     1\n",
      "Name: Salary_max, dtype: int64, 'Japan': Series([], Name: Salary_max, dtype: int64), 'Luxembourg': Series([], Name: Salary_max, dtype: int64), 'Netherlands': 92868.0    2\n",
      "79608.0    5\n",
      "66336.0    9\n",
      "53064.0    2\n",
      "39804.0    1\n",
      "Name: Salary_max, dtype: int64, 'New_Zealand': 125733.0     1\n",
      "115674.0     1\n",
      "113160.0     1\n",
      "91156.0      1\n",
      "88013.0      3\n",
      "84870.0      1\n",
      "82355.0      1\n",
      "81726.0      2\n",
      "79840.0      2\n",
      "77326.0      1\n",
      "76068.0      1\n",
      "75440.0     13\n",
      "72925.0      1\n",
      "69782.0      1\n",
      "64124.0      1\n",
      "63495.0      1\n",
      "62866.0      3\n",
      "62238.0      1\n",
      "60980.0      1\n",
      "50293.0      1\n",
      "44635.0      1\n",
      "Name: Salary_max, dtype: int64, 'Norway': Series([], Name: Salary_max, dtype: int64), 'Poland': 99057.0    1\n",
      "88572.0    1\n",
      "85716.0    2\n",
      "77759.0    1\n",
      "59434.0    1\n",
      "42852.0    1\n",
      "22848.0    1\n",
      "20240.0    1\n",
      "Name: Salary_max, dtype: int64, 'Portugal': 79610.0    1\n",
      "Name: Salary_max, dtype: int64, 'Romania': 53688.0    1\n",
      "21468.0    1\n",
      "Name: Salary_max, dtype: int64, 'Singapore': 271429.0    1\n",
      "226191.0    1\n",
      "180948.0    1\n",
      "135708.0    1\n",
      "108571.0    1\n",
      "108564.0    3\n",
      "90476.0     2\n",
      "90468.0     3\n",
      "85198.0     3\n",
      "83690.0     2\n",
      "81428.0     2\n",
      "81420.0     3\n",
      "76151.0     1\n",
      "75397.0     2\n",
      "72381.0     4\n",
      "72372.0     5\n",
      "67857.0     2\n",
      "63333.0     1\n",
      "63324.0     4\n",
      "60317.0     1\n",
      "54276.0     4\n",
      "53532.0     2\n",
      "49762.0     1\n",
      "45238.0     2\n",
      "45228.0     4\n",
      "39206.0     1\n",
      "36180.0     1\n",
      "33928.0     2\n",
      "Name: Salary_max, dtype: int64, 'South_Korea': Series([], Name: Salary_max, dtype: int64), 'Spain': 86244.0    1\n",
      "60813.0    1\n",
      "55285.0    1\n",
      "53073.0    2\n",
      "49756.0    1\n",
      "46439.0    1\n",
      "44228.0    1\n",
      "42016.0    1\n",
      "36488.0    1\n",
      "26536.0    1\n",
      "26532.0    1\n",
      "Name: Salary_max, dtype: int64, 'Sweden': 194592.0    1\n",
      "73092.0     1\n",
      "70164.0     1\n",
      "Name: Salary_max, dtype: int64, 'Switzerland': 163148.0    4\n",
      "157523.0    2\n",
      "151897.0    3\n",
      "135019.0    3\n",
      "129394.0    1\n",
      "123768.0    1\n",
      "112516.0    4\n",
      "106890.0    1\n",
      "Name: Salary_max, dtype: int64, 'Taiwan': Series([], Name: Salary_max, dtype: int64), 'Turkey': Series([], Name: Salary_max, dtype: int64), 'United_Kingdom': 234510.0    1\n",
      "227541.0    1\n",
      "178782.0    2\n",
      "162529.0    1\n",
      "151277.0    1\n",
      "146276.0    2\n",
      "125022.0    3\n",
      "112520.0    1\n",
      "110019.0    1\n",
      "107519.0    1\n",
      "106269.0    1\n",
      "100018.0    3\n",
      "97517.0     2\n",
      "93766.0     4\n",
      "90016.0     2\n",
      "88766.0     1\n",
      "87515.0     3\n",
      "86265.0     3\n",
      "85015.0     6\n",
      "83765.0     1\n",
      "81264.0     4\n",
      "78764.0     2\n",
      "77514.0     2\n",
      "76263.0     1\n",
      "75013.0     5\n",
      "73763.0     4\n",
      "72513.0     2\n",
      "71262.0     2\n",
      "68762.0     5\n",
      "66261.0     2\n",
      "65011.0     1\n",
      "62511.0     7\n",
      "61261.0     2\n",
      "58760.0     1\n",
      "56260.0     3\n",
      "55009.0     1\n",
      "53759.0     1\n",
      "52509.0     2\n",
      "47508.0     2\n",
      "43757.0     1\n",
      "40007.0     1\n",
      "37506.0     2\n",
      "27504.0     1\n",
      "Name: Salary_max, dtype: int64, 'United_States': 249600.0    1\n",
      "214240.0    1\n",
      "208000.0    2\n",
      "200000.0    1\n",
      "197600.0    1\n",
      "           ..\n",
      "75000.0     1\n",
      "70000.0     1\n",
      "64480.0     1\n",
      "58000.0     1\n",
      "45000.0     1\n",
      "Name: Salary_max, Length: 89, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "selector_target = 'Salary_max'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df[selector_target] = df.apply(\n",
    "            lambda row: monthly_to_yearly_salary(row, selector_target, df_poverty_line, country),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    dfs[country] = df\n",
    "\n",
    "show_results_salary(selector_target, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "del show_results_salary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.10 Salary average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 80339.0     5\n",
      "91525.5     3\n",
      "81356.0     3\n",
      "93559.0     2\n",
      "61017.0     2\n",
      "66779.5     2\n",
      "73220.5     2\n",
      "86440.5     2\n",
      "76949.0     1\n",
      "62711.5     1\n",
      "78644.0     1\n",
      "72203.5     1\n",
      "72203.0     1\n",
      "78983.0     1\n",
      "50847.0     1\n",
      "76271.0     1\n",
      "107796.5    1\n",
      "71186.5     1\n",
      "135593.0    1\n",
      "141017.0    1\n",
      "98644.0     1\n",
      "83050.5     1\n",
      "63728.5     1\n",
      "82033.5     1\n",
      "61016.5     1\n",
      "47118.5     1\n",
      "123390.0    1\n",
      "73898.0     1\n",
      "116339.0    1\n",
      "92542.5     1\n",
      "101695.0    1\n",
      "86779.5     1\n",
      "193899.0    1\n",
      "67796.5     1\n",
      "103050.5    1\n",
      "96610.0     1\n",
      "89830.0     1\n",
      "97966.0     1\n",
      "66440.5     1\n",
      "77966.0     1\n",
      "67796.0     1\n",
      "74576.0     1\n",
      "Name: Salary_avg, dtype: int64, 'Austria': 39804.0    1\n",
      "46434.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Belgium': 60813.0     2\n",
      "46439.0     1\n",
      "60813.5     1\n",
      "179676.0    1\n",
      "69106.0     1\n",
      "Name: Salary_avg, dtype: int64, 'Canada': 52562.5    1\n",
      "68331.5    1\n",
      "48057.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Czech_Republic': 59119.5    1\n",
      "Name: Salary_avg, dtype: int64, 'Denmark': 77165.5    1\n",
      "Name: Salary_avg, dtype: int64, 'Finland': 66336.0    1\n",
      "Name: Salary_avg, dtype: int64, 'France': 136553.5    13\n",
      "158115.0     5\n",
      "143740.5     4\n",
      "51967.5      3\n",
      "77399.0      3\n",
      "            ..\n",
      "155239.5     1\n",
      "80563.0      1\n",
      "122179.0     1\n",
      "146615.0     1\n",
      "44228.0      1\n",
      "Name: Salary_avg, Length: 62, dtype: int64, 'Germany': 88456.0    3\n",
      "77399.0    1\n",
      "66341.5    1\n",
      "82927.0    1\n",
      "66336.0    1\n",
      "66342.0    1\n",
      "33347.0    1\n",
      "63577.5    1\n",
      "54731.5    1\n",
      "52520.5    1\n",
      "99513.0    1\n",
      "91994.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Greece': Series([], Name: Salary_avg, dtype: int64), 'Hong_Kong': 53496.0     3\n",
      "76428.0     3\n",
      "45852.0     2\n",
      "64962.0     2\n",
      "25980.0     1\n",
      "80250.0     1\n",
      "39738.0     1\n",
      "99360.0     1\n",
      "56050.5     1\n",
      "70063.5     1\n",
      "61140.0     1\n",
      "133752.0    1\n",
      "103182.0    1\n",
      "91716.0     1\n",
      "137580.0    1\n",
      "72606.0     1\n",
      "52993.0     1\n",
      "84076.5     1\n",
      "68784.0     1\n",
      "38208.0     1\n",
      "71844.0     1\n",
      "68789.5     1\n",
      "59618.0     1\n",
      "37261.0     1\n",
      "56556.0     1\n",
      "30570.0     1\n",
      "55860.0     1\n",
      "53503.0     1\n",
      "48917.0     1\n",
      "68790.0     1\n",
      "46751.5     1\n",
      "49299.0     1\n",
      "48912.0     1\n",
      "Name: Salary_avg, dtype: int64, 'Hungary': Series([], Name: Salary_avg, dtype: int64), 'Ireland': 82927.0     5\n",
      "70211.5     3\n",
      "46992.0     2\n",
      "45333.5     2\n",
      "66341.5     2\n",
      "90114.5     2\n",
      "92325.5     2\n",
      "78504.0     2\n",
      "64683.0     2\n",
      "49756.5     2\n",
      "63577.5     2\n",
      "89561.0     1\n",
      "82927.5     1\n",
      "58933.5     1\n",
      "38699.0     1\n",
      "112780.5    1\n",
      "94537.0     1\n",
      "49203.5     1\n",
      "103382.5    1\n",
      "69106.0     1\n",
      "115004.0    1\n",
      "80163.0     1\n",
      "43121.5     1\n",
      "50861.5     1\n",
      "54179.0     1\n",
      "95090.0     1\n",
      "105041.0    1\n",
      "89008.5     1\n",
      "96195.5     1\n",
      "75740.0     1\n",
      "80716.0     1\n",
      "31512.0     1\n",
      "100065.0    1\n",
      "133236.5    1\n",
      "55284.5     1\n",
      "85691.5     1\n",
      "72975.5     1\n",
      "40910.5     1\n",
      "99513.0     1\n",
      "71870.5     1\n",
      "132241.0    1\n",
      "71870.0     1\n",
      "61365.5     1\n",
      "158115.0    1\n",
      "51414.5     1\n",
      "102829.5    1\n",
      "128813.5    1\n",
      "76293.0     1\n",
      "35935.0     1\n",
      "70764.5     1\n",
      "59154.5     1\n",
      "44780.5     1\n",
      "Name: Salary_avg, dtype: int64, 'Israel': Series([], Name: Salary_avg, dtype: int64), 'Italy': 38699.0     2\n",
      "49756.5     2\n",
      "34829.5     1\n",
      "171383.0    1\n",
      "37040.5     1\n",
      "55285.0     1\n",
      "25983.5     1\n",
      "41463.5     1\n",
      "93431.0     1\n",
      "Name: Salary_avg, dtype: int64, 'Japan': Series([], Name: Salary_avg, dtype: int64), 'Luxembourg': Series([], Name: Salary_avg, dtype: int64), 'Netherlands': 66336.0    7\n",
      "59700.0    6\n",
      "53070.0    2\n",
      "39804.0    1\n",
      "79602.0    1\n",
      "53064.0    1\n",
      "46434.0    1\n",
      "Name: Salary_avg, dtype: int64, 'New_Zealand': 62238.0     9\n",
      "68210.0     2\n",
      "48721.0     2\n",
      "60666.0     2\n",
      "50293.0     1\n",
      "103101.0    1\n",
      "58466.0     1\n",
      "73868.0     1\n",
      "125418.5    1\n",
      "66324.0     1\n",
      "75439.5     1\n",
      "74811.0     1\n",
      "105615.5    1\n",
      "60980.5     1\n",
      "40234.5     1\n",
      "72296.0     1\n",
      "80154.5     1\n",
      "66010.0     1\n",
      "59094.5     1\n",
      "70725.0     1\n",
      "65066.5     1\n",
      "54379.5     1\n",
      "69153.0     1\n",
      "47778.0     1\n",
      "54065.0     1\n",
      "67581.5     1\n",
      "79840.5     1\n",
      "51550.5     1\n",
      "Name: Salary_avg, dtype: int64, 'Norway': Series([], Name: Salary_avg, dtype: int64), 'Poland': 22848.0    1\n",
      "59434.0    1\n",
      "20240.0    1\n",
      "71430.0    1\n",
      "68574.0    1\n",
      "42852.0    1\n",
      "77759.0    1\n",
      "89770.0    1\n",
      "82860.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Portugal': 79057.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Romania': 53688.0    1\n",
      "21468.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Singapore': 67848.0     4\n",
      "81420.0     4\n",
      "68988.0     3\n",
      "63324.0     3\n",
      "49752.0     3\n",
      "40704.0     3\n",
      "90476.0     2\n",
      "61825.5     2\n",
      "61071.5     2\n",
      "185471.0    2\n",
      "64841.5     1\n",
      "67480.0     1\n",
      "68987.5     1\n",
      "45615.0     1\n",
      "44861.0     1\n",
      "63333.5     1\n",
      "54276.0     1\n",
      "52023.5     1\n",
      "50515.5     1\n",
      "66726.0     1\n",
      "49385.0     1\n",
      "84067.0     1\n",
      "72757.5     1\n",
      "67856.5     1\n",
      "226191.0    1\n",
      "32797.0     1\n",
      "85944.0     1\n",
      "203571.5    1\n",
      "37698.0     1\n",
      "57301.0     1\n",
      "45228.0     1\n",
      "76896.0     1\n",
      "33928.0     1\n",
      "58432.5     1\n",
      "135708.0    1\n",
      "58800.0     1\n",
      "72372.0     1\n",
      "113088.0    1\n",
      "Name: Salary_avg, dtype: int64, 'South_Korea': Series([], Name: Salary_avg, dtype: int64), 'Spain': 46439.0    2\n",
      "86244.0    1\n",
      "34829.5    1\n",
      "43122.0    1\n",
      "38699.5    1\n",
      "26536.0    1\n",
      "49756.5    1\n",
      "51414.5    1\n",
      "42016.0    1\n",
      "58049.0    1\n",
      "26532.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Sweden': 65783.0     1\n",
      "145944.0    1\n",
      "58470.0     1\n",
      "Name: Salary_avg, dtype: int64, 'Switzerland': 112516.0    5\n",
      "163148.0    4\n",
      "151897.0    3\n",
      "157523.0    2\n",
      "135019.0    2\n",
      "123767.5    1\n",
      "98451.5     1\n",
      "129394.0    1\n",
      "Name: Salary_avg, dtype: int64, 'Taiwan': Series([], Name: Salary_avg, dtype: int64), 'Turkey': Series([], Name: Salary_avg, dtype: int64), 'United_Kingdom': 62511.0    5\n",
      "71887.5    3\n",
      "68762.0    3\n",
      "60010.5    3\n",
      "56259.5    3\n",
      "          ..\n",
      "41882.0    1\n",
      "49383.0    1\n",
      "53759.5    1\n",
      "86890.0    1\n",
      "65636.5    1\n",
      "Name: Salary_avg, Length: 68, dtype: int64, 'United_States': 145600.0    7\n",
      "110000.0    7\n",
      "119600.0    7\n",
      "120000.0    6\n",
      "115000.0    5\n",
      "           ..\n",
      "211120.0    1\n",
      "151840.0    1\n",
      "48500.0     1\n",
      "88000.0     1\n",
      "81500.0     1\n",
      "Name: Salary_avg, Length: 118, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Salary_avg'] = (df['Salary_max']+df['Salary_min'])/2\n",
    "    dfs[country] = df\n",
    "\n",
    "show_results('Salary_avg', dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    del df['Salary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 10000+           27\n",
      "51 to 200         7\n",
      "1 to 50           7\n",
      "501 to 1000       5\n",
      "1001 to 5000      4\n",
      "201 to 500        3\n",
      "5001 to 10000     1\n",
      "Name: Employees, dtype: int64, 'Austria': 10000+           44\n",
      "1001 to 5000     21\n",
      "1 to 50          14\n",
      "51 to 200        12\n",
      "501 to 1000      11\n",
      "5001 to 10000     9\n",
      "201 to 500        1\n",
      "Name: Employees, dtype: int64, 'Belgium': 1001 to 5000     17\n",
      "1 to 50          10\n",
      "201 to 500        8\n",
      "501 to 1000       6\n",
      "5001 to 10000     5\n",
      "51 to 200         5\n",
      "10000+            3\n",
      "Name: Employees, dtype: int64, 'Canada': 51 to 200      1\n",
      "501 to 1000    1\n",
      "10000+         1\n",
      "1 to 50        1\n",
      "Name: Employees, dtype: int64, 'Czech_Republic': 10000+           25\n",
      "1001 to 5000     11\n",
      "501 to 1000       7\n",
      "201 to 500        6\n",
      "1 to 50           5\n",
      "5001 to 10000     5\n",
      "51 to 200         4\n",
      "Name: Employees, dtype: int64, 'Denmark': 10000+           28\n",
      "1001 to 5000     13\n",
      "1 to 50           8\n",
      "201 to 500        8\n",
      "501 to 1000       8\n",
      "51 to 200         7\n",
      "5001 to 10000     3\n",
      "Name: Employees, dtype: int64, 'Finland': 1001 to 5000     19\n",
      "10000+            8\n",
      "51 to 200         7\n",
      "501 to 1000       7\n",
      "201 to 500        5\n",
      "1 to 50           3\n",
      "5001 to 10000     2\n",
      "Name: Employees, dtype: int64, 'France': 1 to 50          49\n",
      "10000+           43\n",
      "1001 to 5000     21\n",
      "201 to 500       15\n",
      "51 to 200        15\n",
      "501 to 1000      13\n",
      "5001 to 10000     4\n",
      "Name: Employees, dtype: int64, 'Germany': 10000+           43\n",
      "51 to 200        39\n",
      "1001 to 5000     34\n",
      "501 to 1000      12\n",
      "201 to 500       11\n",
      "1 to 50           5\n",
      "5001 to 10000     4\n",
      "Name: Employees, dtype: int64, 'Greece': 10000+           20\n",
      "501 to 1000      10\n",
      "51 to 200         5\n",
      "1 to 50           5\n",
      "1001 to 5000      3\n",
      "201 to 500        2\n",
      "5001 to 10000     2\n",
      "Name: Employees, dtype: int64, 'Hong_Kong': 10000+           23\n",
      "1 to 50          23\n",
      "51 to 200        15\n",
      "5001 to 10000     8\n",
      "201 to 500        7\n",
      "1001 to 5000      7\n",
      "501 to 1000       5\n",
      "Name: Employees, dtype: int64, 'Hungary': 10000+           40\n",
      "1 to 50           9\n",
      "1001 to 5000      7\n",
      "51 to 200         5\n",
      "5001 to 10000     4\n",
      "501 to 1000       4\n",
      "201 to 500        3\n",
      "Name: Employees, dtype: int64, 'Ireland': 10000+           26\n",
      "1 to 50          14\n",
      "1001 to 5000      9\n",
      "5001 to 10000     7\n",
      "201 to 500        5\n",
      "501 to 1000       4\n",
      "51 to 200         1\n",
      "Name: Employees, dtype: int64, 'Israel': 51 to 200       120\n",
      "1001 to 5000     92\n",
      "1 to 50          17\n",
      "201 to 500        8\n",
      "501 to 1000       5\n",
      "10000+            3\n",
      "Name: Employees, dtype: int64, 'Italy': 10000+           19\n",
      "1001 to 5000     10\n",
      "51 to 200         9\n",
      "1 to 50           7\n",
      "201 to 500        5\n",
      "5001 to 10000     2\n",
      "501 to 1000       2\n",
      "Name: Employees, dtype: int64, 'Japan': Series([], Name: Employees, dtype: int64), 'Luxembourg': 1001 to 5000     9\n",
      "1 to 50          7\n",
      "10000+           7\n",
      "501 to 1000      5\n",
      "51 to 200        3\n",
      "201 to 500       2\n",
      "5001 to 10000    1\n",
      "Name: Employees, dtype: int64, 'Netherlands': 1001 to 5000     13\n",
      "10000+            9\n",
      "5001 to 10000     5\n",
      "51 to 200         4\n",
      "201 to 500        2\n",
      "501 to 1000       1\n",
      "1 to 50           1\n",
      "Name: Employees, dtype: int64, 'New_Zealand': 1 to 50          18\n",
      "10000+            8\n",
      "201 to 500        7\n",
      "501 to 1000       3\n",
      "51 to 200         2\n",
      "5001 to 10000     2\n",
      "1001 to 5000      1\n",
      "Name: Employees, dtype: int64, 'Norway': 201 to 500       8\n",
      "10000+           6\n",
      "51 to 200        5\n",
      "5001 to 10000    4\n",
      "1001 to 5000     3\n",
      "501 to 1000      1\n",
      "Name: Employees, dtype: int64, 'Poland': 10000+           39\n",
      "201 to 500       14\n",
      "1 to 50          13\n",
      "1001 to 5000     12\n",
      "51 to 200         9\n",
      "501 to 1000       7\n",
      "5001 to 10000     5\n",
      "Name: Employees, dtype: int64, 'Portugal': 10000+           45\n",
      "51 to 200        30\n",
      "501 to 1000      27\n",
      "201 to 500       21\n",
      "1 to 50          20\n",
      "5001 to 10000    14\n",
      "1001 to 5000     14\n",
      "Name: Employees, dtype: int64, 'Romania': 10000+           66\n",
      "1001 to 5000     16\n",
      "201 to 500       11\n",
      "51 to 200         7\n",
      "1 to 50           6\n",
      "501 to 1000       6\n",
      "5001 to 10000     5\n",
      "Name: Employees, dtype: int64, 'Singapore': 10000+           38\n",
      "1001 to 5000     26\n",
      "201 to 500       18\n",
      "1 to 50          14\n",
      "51 to 200         9\n",
      "501 to 1000       7\n",
      "5001 to 10000     4\n",
      "Name: Employees, dtype: int64, 'South_Korea': Series([], Name: Employees, dtype: int64), 'Spain': 10000+           32\n",
      "201 to 500       17\n",
      "51 to 200        17\n",
      "5001 to 10000    12\n",
      "1001 to 5000     10\n",
      "1 to 50           9\n",
      "501 to 1000       5\n",
      "Name: Employees, dtype: int64, 'Sweden': 51 to 200        21\n",
      "1 to 50          18\n",
      "201 to 500       18\n",
      "501 to 1000      17\n",
      "10000+           14\n",
      "1001 to 5000     10\n",
      "5001 to 10000     3\n",
      "Name: Employees, dtype: int64, 'Switzerland': 1 to 50          18\n",
      "51 to 200         4\n",
      "10000+            4\n",
      "5001 to 10000     3\n",
      "1001 to 5000      3\n",
      "501 to 1000       2\n",
      "201 to 500        1\n",
      "Name: Employees, dtype: int64, 'Taiwan': Series([], Name: Employees, dtype: int64), 'Turkey': 10000+          6\n",
      "201 to 500      4\n",
      "501 to 1000     4\n",
      "1001 to 5000    3\n",
      "1 to 50         1\n",
      "51 to 200       1\n",
      "Name: Employees, dtype: int64, 'United_Kingdom': 10000+           29\n",
      "1 to 50          23\n",
      "51 to 200        19\n",
      "501 to 1000      11\n",
      "5001 to 10000     8\n",
      "1001 to 5000      8\n",
      "201 to 500        7\n",
      "Name: Employees, dtype: int64, 'United_States': 1 to 50          58\n",
      "51 to 200        48\n",
      "201 to 500       30\n",
      "501 to 1000      25\n",
      "10000+           23\n",
      "1001 to 5000     11\n",
      "5001 to 10000     3\n",
      "Name: Employees, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "show_results('Employees', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Type of ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': Company - Public     30\n",
      "Company - Private    27\n",
      "Self-employed         1\n",
      "Government            1\n",
      "Name: Type_of_ownership, dtype: int64, 'Austria': Company - Private                 65\n",
      "Company - Public                  41\n",
      "Subsidiary or Business Segment     3\n",
      "Government                         2\n",
      "Self-employed                      2\n",
      "College / University               1\n",
      "Nonprofit Organization             1\n",
      "Name: Type_of_ownership, dtype: int64, 'Belgium': Company - Private                 27\n",
      "Company - Public                  16\n",
      "Self-employed                      5\n",
      "Nonprofit Organization             3\n",
      "Subsidiary or Business Segment     2\n",
      "Private Practice / Firm            1\n",
      "Name: Type_of_ownership, dtype: int64, 'Canada': Company - Private    3\n",
      "Company - Public     1\n",
      "Name: Type_of_ownership, dtype: int64, 'Czech_Republic': Company - Private    36\n",
      "Company - Public     26\n",
      "Self-employed         1\n",
      "Name: Type_of_ownership, dtype: int64, 'Denmark': Company - Private                 47\n",
      "Company - Public                  31\n",
      "Subsidiary or Business Segment     3\n",
      "Government                         1\n",
      "Name: Type_of_ownership, dtype: int64, 'Finland': Company - Private                 22\n",
      "Company - Public                  16\n",
      "Subsidiary or Business Segment     2\n",
      "Nonprofit Organization             1\n",
      "Name: Type_of_ownership, dtype: int64, 'France': Company - Private                 111\n",
      "Company - Public                   83\n",
      "Private Practice / Firm             8\n",
      "Self-employed                       4\n",
      "Subsidiary or Business Segment      3\n",
      "Government                          1\n",
      "Name: Type_of_ownership, dtype: int64, 'Germany': Company - Private                 85\n",
      "Company - Public                  44\n",
      "Subsidiary or Business Segment    16\n",
      "Nonprofit Organization             3\n",
      "Self-employed                      1\n",
      "Government                         1\n",
      "College / University               1\n",
      "Name: Type_of_ownership, dtype: int64, 'Greece': Company - Private    28\n",
      "Company - Public     24\n",
      "Name: Type_of_ownership, dtype: int64, 'Hong_Kong': Company - Private                 66\n",
      "Company - Public                  30\n",
      "Subsidiary or Business Segment     2\n",
      "Private Practice / Firm            2\n",
      "Nonprofit Organization             1\n",
      "Self-employed                      1\n",
      "Name: Type_of_ownership, dtype: int64, 'Hungary': Company - Public                  51\n",
      "Company - Private                 30\n",
      "Subsidiary or Business Segment     1\n",
      "Self-employed                      1\n",
      "Name: Type_of_ownership, dtype: int64, 'Ireland': Company - Private                 38\n",
      "Company - Public                  31\n",
      "Government                         1\n",
      "Subsidiary or Business Segment     1\n",
      "Name: Type_of_ownership, dtype: int64, 'Israel': Company - Private    139\n",
      "Company - Public      10\n",
      "Franchise              1\n",
      "Name: Type_of_ownership, dtype: int64, 'Italy': Company - Private                 39\n",
      "Company - Public                  26\n",
      "Subsidiary or Business Segment     2\n",
      "Nonprofit Organization             1\n",
      "Name: Type_of_ownership, dtype: int64, 'Japan': Series([], Name: Type_of_ownership, dtype: int64), 'Luxembourg': Company - Private                 25\n",
      "Company - Public                  10\n",
      "Self-employed                      2\n",
      "Subsidiary or Business Segment     1\n",
      "Name: Type_of_ownership, dtype: int64, 'Netherlands': Company - Public                  13\n",
      "Company - Private                 10\n",
      "Subsidiary or Business Segment     9\n",
      "Government                         4\n",
      "Nonprofit Organization             1\n",
      "Self-employed                      1\n",
      "Name: Type_of_ownership, dtype: int64, 'New_Zealand': Company - Private    30\n",
      "Company - Public     10\n",
      "Government            4\n",
      "Self-employed         2\n",
      "Contract              1\n",
      "Hospital              1\n",
      "Name: Type_of_ownership, dtype: int64, 'Norway': Company - Private         17\n",
      "Company - Public           8\n",
      "College / University       2\n",
      "Self-employed              1\n",
      "Nonprofit Organization     1\n",
      "Name: Type_of_ownership, dtype: int64, 'Poland': Company - Private                 51\n",
      "Company - Public                  44\n",
      "Subsidiary or Business Segment     4\n",
      "Government                         2\n",
      "Contract                           2\n",
      "Self-employed                      1\n",
      "Name: Type_of_ownership, dtype: int64, 'Portugal': Company - Private                 113\n",
      "Company - Public                   53\n",
      "Subsidiary or Business Segment      6\n",
      "Contract                            2\n",
      "Private Practice / Firm             2\n",
      "Name: Type_of_ownership, dtype: int64, 'Romania': Company - Public                  67\n",
      "Company - Private                 44\n",
      "Subsidiary or Business Segment     5\n",
      "Self-employed                      2\n",
      "Private Practice / Firm            1\n",
      "Name: Type_of_ownership, dtype: int64, 'Singapore': Company - Private                 61\n",
      "Company - Public                  44\n",
      "Government                        10\n",
      "Contract                           3\n",
      "Subsidiary or Business Segment     1\n",
      "Self-employed                      1\n",
      "Name: Type_of_ownership, dtype: int64, 'South_Korea': Series([], Name: Type_of_ownership, dtype: int64), 'Spain': Company - Private                 72\n",
      "Company - Public                  42\n",
      "Subsidiary or Business Segment     3\n",
      "Nonprofit Organization             1\n",
      "College / University               1\n",
      "Name: Type_of_ownership, dtype: int64, 'Sweden': Company - Private                 84\n",
      "Company - Public                  30\n",
      "Subsidiary or Business Segment     4\n",
      "College / University               2\n",
      "Nonprofit Organization             2\n",
      "Self-employed                      1\n",
      "Private Practice / Firm            1\n",
      "Government                         1\n",
      "Name: Type_of_ownership, dtype: int64, 'Switzerland': Company - Private                 36\n",
      "Company - Public                  17\n",
      "Subsidiary or Business Segment     4\n",
      "Self-employed                      2\n",
      "College / University               1\n",
      "Name: Type_of_ownership, dtype: int64, 'Taiwan': Series([], Name: Type_of_ownership, dtype: int64), 'Turkey': Company - Private    12\n",
      "Company - Public      9\n",
      "Name: Type_of_ownership, dtype: int64, 'United_Kingdom': Company - Private                 73\n",
      "Company - Public                  33\n",
      "Nonprofit Organization             3\n",
      "Subsidiary or Business Segment     3\n",
      "Government                         2\n",
      "Name: Type_of_ownership, dtype: int64, 'United_States': Company - Private                 149\n",
      "Company - Public                   49\n",
      "Subsidiary or Business Segment      9\n",
      "Contract                            5\n",
      "Private Practice / Firm             3\n",
      "Nonprofit Organization              3\n",
      "College / University                2\n",
      "Self-employed                       1\n",
      "Name: Type_of_ownership, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "show_results('Type_of_ownership', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': Information Technology        15\n",
      "Financial Services            12\n",
      "Human Resources & Staffing     4\n",
      "Media & Communication          4\n",
      "Energy, Mining & Utilities     3\n",
      "Retail & Wholesale             2\n",
      "Telecommunications             1\n",
      "Real Estate                    1\n",
      "Aerospace & Defense            1\n",
      "Transportation & Logistics     1\n",
      "Management & Consulting        1\n",
      "Name: Sector, dtype: int64, 'Austria': Information Technology                         26\n",
      "Manufacturing                                  20\n",
      "Human Resources & Staffing                      9\n",
      "Financial Services                              7\n",
      "Management & Consulting                         5\n",
      "Energy, Mining & Utilities                      4\n",
      "Transportation & Logistics                      3\n",
      "Construction, Repair & Maintenance Services     3\n",
      "Education                                       3\n",
      "Retail & Wholesale                              2\n",
      "Insurance                                       2\n",
      "Telecommunications                              2\n",
      "Arts, Entertainment & Recreation                1\n",
      "Nonprofit & NGO                                 1\n",
      "Name: Sector, dtype: int64, 'Belgium': Management & Consulting             12\n",
      "Information Technology              11\n",
      "Human Resources & Staffing          10\n",
      "Financial Services                   4\n",
      "Telecommunications                   3\n",
      "Insurance                            2\n",
      "Arts, Entertainment & Recreation     1\n",
      "Healthcare                           1\n",
      "Retail & Wholesale                   1\n",
      "Transportation & Logistics           1\n",
      "Name: Sector, dtype: int64, 'Canada': Financial Services        1\n",
      "Information Technology    1\n",
      "Retail & Wholesale        1\n",
      "Media & Communication     1\n",
      "Name: Sector, dtype: int64, 'Czech_Republic': Information Technology                22\n",
      "Financial Services                     9\n",
      "Media & Communication                  7\n",
      "Manufacturing                          6\n",
      "Management & Consulting                4\n",
      "Pharmaceutical & Biotechnology         2\n",
      "Transportation & Logistics             2\n",
      "Telecommunications                     2\n",
      "Government & Public Administration     1\n",
      "Energy, Mining & Utilities             1\n",
      "Human Resources & Staffing             1\n",
      "Insurance                              1\n",
      "Name: Sector, dtype: int64, 'Denmark': Information Technology            12\n",
      "Manufacturing                     10\n",
      "Pharmaceutical & Biotechnology     6\n",
      "Financial Services                 5\n",
      "Transportation & Logistics         4\n",
      "Management & Consulting            2\n",
      "Retail & Wholesale                 2\n",
      "Human Resources & Staffing         1\n",
      "Media & Communication              1\n",
      "Energy, Mining & Utilities         1\n",
      "Personal Consumer Services         1\n",
      "Name: Sector, dtype: int64, 'Finland': Information Technology                         19\n",
      "Manufacturing                                   4\n",
      "Management & Consulting                         4\n",
      "Telecommunications                              1\n",
      "Financial Services                              1\n",
      "Construction, Repair & Maintenance Services     1\n",
      "Media & Communication                           1\n",
      "Name: Sector, dtype: int64, 'France': Information Technology                         48\n",
      "Management & Consulting                        18\n",
      "Human Resources & Staffing                     17\n",
      "Financial Services                             13\n",
      "Manufacturing                                   7\n",
      "Energy, Mining & Utilities                      5\n",
      "Aerospace & Defense                             4\n",
      "Retail & Wholesale                              3\n",
      "Pharmaceutical & Biotechnology                  3\n",
      "Construction, Repair & Maintenance Services     3\n",
      "Insurance                                       2\n",
      "Media & Communication                           1\n",
      "Transportation & Logistics                      1\n",
      "Hotels & Travel Accommodation                   1\n",
      "Education                                       1\n",
      "Name: Sector, dtype: int64, 'Germany': Information Technology                         43\n",
      "Human Resources & Staffing                     28\n",
      "Management & Consulting                        15\n",
      "Manufacturing                                  13\n",
      "Retail & Wholesale                              9\n",
      "Financial Services                              8\n",
      "Transportation & Logistics                      7\n",
      "Energy, Mining & Utilities                      5\n",
      "Media & Communication                           4\n",
      "Construction, Repair & Maintenance Services     2\n",
      "Healthcare                                      1\n",
      "Insurance                                       1\n",
      "Education                                       1\n",
      "Aerospace & Defense                             1\n",
      "Hotels & Travel Accommodation                   1\n",
      "Name: Sector, dtype: int64, 'Greece': Information Technology              12\n",
      "Financial Services                   8\n",
      "Management & Consulting              6\n",
      "Pharmaceutical & Biotechnology       4\n",
      "Telecommunications                   3\n",
      "Insurance                            3\n",
      "Manufacturing                        2\n",
      "Human Resources & Staffing           2\n",
      "Arts, Entertainment & Recreation     1\n",
      "Media & Communication                1\n",
      "Name: Sector, dtype: int64, 'Hong_Kong': Human Resources & Staffing                     24\n",
      "Information Technology                         10\n",
      "Management & Consulting                         7\n",
      "Financial Services                              6\n",
      "Retail & Wholesale                              3\n",
      "Media & Communication                           3\n",
      "Construction, Repair & Maintenance Services     3\n",
      "Transportation & Logistics                      2\n",
      "Aerospace & Defense                             2\n",
      "Real Estate                                     2\n",
      "Insurance                                       2\n",
      "Arts, Entertainment & Recreation                1\n",
      "Name: Sector, dtype: int64, 'Hungary': Information Technology                         25\n",
      "Manufacturing                                  11\n",
      "Telecommunications                              9\n",
      "Management & Consulting                         6\n",
      "Human Resources & Staffing                      5\n",
      "Financial Services                              3\n",
      "Pharmaceutical & Biotechnology                  2\n",
      "Healthcare                                      1\n",
      "Construction, Repair & Maintenance Services     1\n",
      "Transportation & Logistics                      1\n",
      "Energy, Mining & Utilities                      1\n",
      "Name: Sector, dtype: int64, 'Ireland': Information Technology                22\n",
      "Human Resources & Staffing            12\n",
      "Management & Consulting                7\n",
      "Financial Services                     6\n",
      "Manufacturing                          3\n",
      "Media & Communication                  3\n",
      "Healthcare                             3\n",
      "Insurance                              2\n",
      "Government & Public Administration     1\n",
      "Agriculture                            1\n",
      "Telecommunications                     1\n",
      "Pharmaceutical & Biotechnology         1\n",
      "Transportation & Logistics             1\n",
      "Name: Sector, dtype: int64, 'Israel': Information Technology                         114\n",
      "Human Resources & Staffing                     101\n",
      "Insurance                                        4\n",
      "Management & Consulting                          2\n",
      "Manufacturing                                    2\n",
      "Financial Services                               2\n",
      "Energy, Mining & Utilities                       2\n",
      "Construction, Repair & Maintenance Services      2\n",
      "Real Estate                                      1\n",
      "Name: Sector, dtype: int64, 'Italy': Information Technology        11\n",
      "Management & Consulting        8\n",
      "Media & Communication          7\n",
      "Manufacturing                  5\n",
      "Telecommunications             3\n",
      "Financial Services             3\n",
      "Energy, Mining & Utilities     3\n",
      "Aerospace & Defense            2\n",
      "Retail & Wholesale             2\n",
      "Human Resources & Staffing     2\n",
      "Transportation & Logistics     1\n",
      "Real Estate                    1\n",
      "Name: Sector, dtype: int64, 'Japan': Series([], Name: Sector, dtype: int64), 'Luxembourg': Information Technology                         8\n",
      "Management & Consulting                        4\n",
      "Financial Services                             3\n",
      "Manufacturing                                  2\n",
      "Legal                                          2\n",
      "Construction, Repair & Maintenance Services    2\n",
      "Human Resources & Staffing                     2\n",
      "Transportation & Logistics                     2\n",
      "Media & Communication                          1\n",
      "Energy, Mining & Utilities                     1\n",
      "Retail & Wholesale                             1\n",
      "Aerospace & Defense                            1\n",
      "Telecommunications                             1\n",
      "Name: Sector, dtype: int64, 'Netherlands': Information Technology                         8\n",
      "Human Resources & Staffing                     6\n",
      "Insurance                                      3\n",
      "Transportation & Logistics                     3\n",
      "Government & Public Administration             3\n",
      "Manufacturing                                  2\n",
      "Media & Communication                          2\n",
      "Financial Services                             1\n",
      "Construction, Repair & Maintenance Services    1\n",
      "Energy, Mining & Utilities                     1\n",
      "Retail & Wholesale                             1\n",
      "Management & Consulting                        1\n",
      "Name: Sector, dtype: int64, 'New_Zealand': Information Technology                6\n",
      "Human Resources & Staffing            4\n",
      "Financial Services                    3\n",
      "Media & Communication                 3\n",
      "Real Estate                           2\n",
      "Retail & Wholesale                    2\n",
      "Management & Consulting               2\n",
      "Government & Public Administration    2\n",
      "Education                             1\n",
      "Healthcare                            1\n",
      "Name: Sector, dtype: int64, 'Norway': Information Technology                         10\n",
      "Construction, Repair & Maintenance Services     3\n",
      "Financial Services                              2\n",
      "Education                                       2\n",
      "Government & Public Administration              1\n",
      "Energy, Mining & Utilities                      1\n",
      "Real Estate                                     1\n",
      "Pharmaceutical & Biotechnology                  1\n",
      "Name: Sector, dtype: int64, 'Poland': Information Technology                31\n",
      "Financial Services                    12\n",
      "Management & Consulting               11\n",
      "Manufacturing                          7\n",
      "Energy, Mining & Utilities             4\n",
      "Human Resources & Staffing             4\n",
      "Pharmaceutical & Biotechnology         4\n",
      "Media & Communication                  2\n",
      "Aerospace & Defense                    2\n",
      "Government & Public Administration     1\n",
      "Telecommunications                     1\n",
      "Name: Sector, dtype: int64, 'Portugal': Information Technology                         70\n",
      "Human Resources & Staffing                     29\n",
      "Management & Consulting                        12\n",
      "Financial Services                              8\n",
      "Manufacturing                                   7\n",
      "Transportation & Logistics                      7\n",
      "Media & Communication                           5\n",
      "Retail & Wholesale                              4\n",
      "Telecommunications                              3\n",
      "Pharmaceutical & Biotechnology                  2\n",
      "Energy, Mining & Utilities                      2\n",
      "Construction, Repair & Maintenance Services     2\n",
      "Real Estate                                     1\n",
      "Name: Sector, dtype: int64, 'Romania': Information Technology                43\n",
      "Financial Services                    23\n",
      "Management & Consulting               10\n",
      "Telecommunications                     7\n",
      "Manufacturing                          6\n",
      "Human Resources & Staffing             4\n",
      "Energy, Mining & Utilities             3\n",
      "Healthcare                             2\n",
      "Retail & Wholesale                     2\n",
      "Government & Public Administration     1\n",
      "Pharmaceutical & Biotechnology         1\n",
      "Transportation & Logistics             1\n",
      "Name: Sector, dtype: int64, 'Singapore': Information Technology                35\n",
      "Financial Services                    19\n",
      "Government & Public Administration     9\n",
      "Management & Consulting                9\n",
      "Telecommunications                     4\n",
      "Healthcare                             3\n",
      "Manufacturing                          3\n",
      "Media & Communication                  3\n",
      "Education                              3\n",
      "Insurance                              2\n",
      "Transportation & Logistics             2\n",
      "Human Resources & Staffing             2\n",
      "Hotels & Travel Accommodation          1\n",
      "Real Estate                            1\n",
      "Retail & Wholesale                     1\n",
      "Name: Sector, dtype: int64, 'South_Korea': Series([], Name: Sector, dtype: int64), 'Spain': Information Technology                         39\n",
      "Management & Consulting                        15\n",
      "Financial Services                              7\n",
      "Media & Communication                           6\n",
      "Manufacturing                                   4\n",
      "Pharmaceutical & Biotechnology                  4\n",
      "Aerospace & Defense                             3\n",
      "Energy, Mining & Utilities                      3\n",
      "Insurance                                       1\n",
      "Hotels & Travel Accommodation                   1\n",
      "Education                                       1\n",
      "Telecommunications                              1\n",
      "Human Resources & Staffing                      1\n",
      "Construction, Repair & Maintenance Services     1\n",
      "Name: Sector, dtype: int64, 'Sweden': Information Technology                         30\n",
      "Manufacturing                                   7\n",
      "Media & Communication                           6\n",
      "Financial Services                              5\n",
      "Education                                       3\n",
      "Human Resources & Staffing                      3\n",
      "Management & Consulting                         3\n",
      "Energy, Mining & Utilities                      2\n",
      "Arts, Entertainment & Recreation                2\n",
      "Real Estate                                     1\n",
      "Telecommunications                              1\n",
      "Legal                                           1\n",
      "Pharmaceutical & Biotechnology                  1\n",
      "Aerospace & Defense                             1\n",
      "Transportation & Logistics                      1\n",
      "Hotels & Travel Accommodation                   1\n",
      "Insurance                                       1\n",
      "Construction, Repair & Maintenance Services     1\n",
      "Name: Sector, dtype: int64, 'Switzerland': Human Resources & Staffing        15\n",
      "Insurance                          1\n",
      "Retail & Wholesale                 1\n",
      "Pharmaceutical & Biotechnology     1\n",
      "Transportation & Logistics         1\n",
      "Education                          1\n",
      "Management & Consulting            1\n",
      "Name: Sector, dtype: int64, 'Taiwan': Series([], Name: Sector, dtype: int64), 'Turkey': Information Technology        9\n",
      "Telecommunications            4\n",
      "Human Resources & Staffing    2\n",
      "Manufacturing                 2\n",
      "Transportation & Logistics    1\n",
      "Name: Sector, dtype: int64, 'United_Kingdom': Information Technology                22\n",
      "Human Resources & Staffing            15\n",
      "Financial Services                    11\n",
      "Management & Consulting               11\n",
      "Energy, Mining & Utilities             6\n",
      "Manufacturing                          4\n",
      "Arts, Entertainment & Recreation       4\n",
      "Insurance                              4\n",
      "Education                              3\n",
      "Media & Communication                  3\n",
      "Transportation & Logistics             2\n",
      "Aerospace & Defense                    2\n",
      "Government & Public Administration     2\n",
      "Retail & Wholesale                     2\n",
      "Pharmaceutical & Biotechnology         2\n",
      "Hotels & Travel Accommodation          1\n",
      "Real Estate                            1\n",
      "Name: Sector, dtype: int64, 'United_States': Information Technology                         83\n",
      "Financial Services                             13\n",
      "Healthcare                                     12\n",
      "Manufacturing                                   9\n",
      "Retail & Wholesale                              8\n",
      "Media & Communication                           8\n",
      "Human Resources & Staffing                      6\n",
      "Insurance                                       5\n",
      "Management & Consulting                         5\n",
      "Energy, Mining & Utilities                      4\n",
      "Telecommunications                              2\n",
      "Pharmaceutical & Biotechnology                  2\n",
      "Aerospace & Defense                             2\n",
      "Education                                       2\n",
      "Real Estate                                     1\n",
      "Government & Public Administration              1\n",
      "Personal Consumer Services                      1\n",
      "Construction, Repair & Maintenance Services     1\n",
      "Nonprofit & NGO                                 1\n",
      "Transportation & Logistics                      1\n",
      "Name: Sector, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "show_results('Sector', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': Investment & Asset Management              8\n",
      "Enterprise Software & Network Solutions    4\n",
      "Information Technology Support Services    4\n",
      "Internet & Web Services                    3\n",
      "Banking & Lending                          3\n",
      "HR Consulting                              3\n",
      "Advertising & Public Relations             3\n",
      "Computer Hardware Development              2\n",
      "Software Development                       2\n",
      "Energy & Utilities                         2\n",
      "Cable, Internet & Telephone Providers      1\n",
      "Real Estate                                1\n",
      "Aerospace & Defense                        1\n",
      "Staffing & Subcontracting                  1\n",
      "Accounting & Tax                           1\n",
      "Video Game Publishing                      1\n",
      "Food & Beverage Stores                     1\n",
      "Taxi & Car Services                        1\n",
      "Business Consulting                        1\n",
      "Drug & Health Stores                       1\n",
      "Mining & Metals                            1\n",
      "Name: Industry, dtype: int64, 'Austria': Information Technology Support Services    12\n",
      "Enterprise Software & Network Solutions     9\n",
      "HR Consulting                               8\n",
      "Machinery Manufacturing                     5\n",
      "Investment & Asset Management               5\n",
      "Transportation Equipment Manufacturing      4\n",
      "Business Consulting                         4\n",
      "Energy & Utilities                          4\n",
      "Computer Hardware Development               4\n",
      "Electronics Manufacturing                   3\n",
      "Consumer Product Manufacturing              3\n",
      "Telecommunications Services                 2\n",
      "Rail Transportation                         2\n",
      "Wood & Paper Manufacturing                  2\n",
      "Insurance Carriers                          2\n",
      "Architectural & Engineering Services        2\n",
      "Education & Training Services               2\n",
      "Banking & Lending                           2\n",
      "Civic & Social Services                     1\n",
      "Internet & Web Services                     1\n",
      "Security & Protective                       1\n",
      "Colleges & Universities                     1\n",
      "Taxi & Car Services                         1\n",
      "Food & Beverage Manufacturing               1\n",
      "Chemical Manufacturing                      1\n",
      "Food & Beverage Stores                      1\n",
      "Metal & Mineral Manufacturing               1\n",
      "Construction                                1\n",
      "Grocery Stores                              1\n",
      "Gambling                                    1\n",
      "Staffing & Subcontracting                   1\n",
      "Name: Industry, dtype: int64, 'Belgium': Information Technology Support Services    11\n",
      "HR Consulting                              10\n",
      "Business Consulting                         7\n",
      "Research & Development                      3\n",
      "Cable, Internet & Telephone Providers       3\n",
      "Banking & Lending                           2\n",
      "Building & Personnel Services               2\n",
      "Insurance Agencies & Brokerages             1\n",
      "Culture & Entertainment                     1\n",
      "Accounting & Tax                            1\n",
      "Health Care Services & Hospitals            1\n",
      "Department, Clothing & Shoe Stores          1\n",
      "Financial Transaction Processing            1\n",
      "Insurance Carriers                          1\n",
      "Taxi & Car Services                         1\n",
      "Name: Industry, dtype: int64, 'Canada': Financial Transaction Processing            1\n",
      "Computer Hardware Development               1\n",
      "Consumer Electronics & Appliances Stores    1\n",
      "Advertising & Public Relations              1\n",
      "Name: Industry, dtype: int64, 'Czech_Republic': Information Technology Support Services    10\n",
      "Enterprise Software & Network Solutions     6\n",
      "Financial Transaction Processing            5\n",
      "Advertising & Public Relations              5\n",
      "Machinery Manufacturing                     4\n",
      "Accounting & Tax                            4\n",
      "Computer Hardware Development               4\n",
      "Research & Development                      2\n",
      "Business Consulting                         2\n",
      "Publishing                                  2\n",
      "Taxi & Car Services                         2\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Cable, Internet & Telephone Providers       2\n",
      "HR Consulting                               1\n",
      "Insurance Carriers                          1\n",
      "National Agencies                           1\n",
      "Health Care Products Manufacturing          1\n",
      "Energy & Utilities                          1\n",
      "Software Development                        1\n",
      "Internet & Web Services                     1\n",
      "Electronics Manufacturing                   1\n",
      "Name: Industry, dtype: int64, 'Denmark': Biotech & Pharmaceuticals                  6\n",
      "Internet & Web Services                    5\n",
      "Consumer Product Manufacturing             4\n",
      "Food & Beverage Manufacturing              4\n",
      "Computer Hardware Development              4\n",
      "Marine Transportation                      3\n",
      "Information Technology Support Services    2\n",
      "Electronics Manufacturing                  2\n",
      "Business Consulting                        2\n",
      "Financial Transaction Processing           2\n",
      "Beauty & Personal Accessories Stores       2\n",
      "Enterprise Software & Network Solutions    1\n",
      "Music & Sound Production                   1\n",
      "Banking & Lending                          1\n",
      "Staffing & Subcontracting                  1\n",
      "Investment & Asset Management              1\n",
      "Energy & Utilities                         1\n",
      "Beauty & Wellness                          1\n",
      "Stock Exchanges                            1\n",
      "Rail Transportation                        1\n",
      "Name: Industry, dtype: int64, 'Finland': Information Technology Support Services    7\n",
      "Enterprise Software & Network Solutions    6\n",
      "Business Consulting                        4\n",
      "Computer Hardware Development              3\n",
      "Software Development                       3\n",
      "Electronics Manufacturing                  2\n",
      "Telecommunications Services                1\n",
      "Financial Transaction Processing           1\n",
      "Architectural & Engineering Services       1\n",
      "Wood & Paper Manufacturing                 1\n",
      "Video Game Publishing                      1\n",
      "Machinery Manufacturing                    1\n",
      "Name: Industry, dtype: int64, 'France': Information Technology Support Services    40\n",
      "HR Consulting                              16\n",
      "Business Consulting                        13\n",
      "Banking & Lending                           7\n",
      "Enterprise Software & Network Solutions     5\n",
      "Energy & Utilities                          5\n",
      "Investment & Asset Management               5\n",
      "Aerospace & Defense                         4\n",
      "General Merchandise & Superstores           3\n",
      "Food & Beverage Manufacturing               3\n",
      "Biotech & Pharmaceuticals                   3\n",
      "Insurance Agencies & Brokerages             2\n",
      "Building & Personnel Services               2\n",
      "Architectural & Engineering Services        2\n",
      "Advertising & Public Relations              1\n",
      "Hotels & Resorts                            1\n",
      "Membership Organizations                    1\n",
      "Chemical Manufacturing                      1\n",
      "Internet & Web Services                     1\n",
      "Education & Training Services               1\n",
      "Airlines, Airports & Air Transportation     1\n",
      "Staffing & Subcontracting                   1\n",
      "Computer Hardware Development               1\n",
      "Electronics Manufacturing                   1\n",
      "Construction                                1\n",
      "Transportation Equipment Manufacturing      1\n",
      "Software Development                        1\n",
      "Security & Protective                       1\n",
      "Machinery Manufacturing                     1\n",
      "Research & Development                      1\n",
      "Accounting & Tax                            1\n",
      "Name: Industry, dtype: int64, 'Germany': Information Technology Support Services     28\n",
      "HR Consulting                               24\n",
      "Business Consulting                         13\n",
      "Internet & Web Services                      9\n",
      "Machinery Manufacturing                      7\n",
      "Rail Transportation                          7\n",
      "Accounting & Tax                             7\n",
      "Energy & Utilities                           5\n",
      "Staffing & Subcontracting                    4\n",
      "Advertising & Public Relations               3\n",
      "Consumer Electronics & Appliances Stores     3\n",
      "Software Development                         2\n",
      "Security & Protective                        2\n",
      "Enterprise Software & Network Solutions      2\n",
      "Computer Hardware Development                2\n",
      "Chemical Manufacturing                       2\n",
      "Grocery Stores                               2\n",
      "General Repair & Maintenance                 2\n",
      "General Merchandise & Superstores            2\n",
      "Insurance Carriers                           1\n",
      "Commercial Printing                          1\n",
      "Sporting Goods Stores                        1\n",
      "Broadcast Media                              1\n",
      "Transportation Equipment Manufacturing       1\n",
      "Financial Transaction Processing             1\n",
      "Colleges & Universities                      1\n",
      "Electronics Manufacturing                    1\n",
      "Consumer Product Manufacturing               1\n",
      "Office Supply & Copy Stores                  1\n",
      "Aerospace & Defense                          1\n",
      "Hotels & Resorts                             1\n",
      "Health Care Services & Hospitals             1\n",
      "Name: Industry, dtype: int64, 'Greece': Business Consulting                        6\n",
      "Accounting & Tax                           5\n",
      "Information Technology Support Services    4\n",
      "Biotech & Pharmaceuticals                  4\n",
      "Enterprise Software & Network Solutions    3\n",
      "Banking & Lending                          3\n",
      "Computer Hardware Development              3\n",
      "Insurance Carriers                         3\n",
      "Telecommunications Services                2\n",
      "Internet & Web Services                    2\n",
      "HR Consulting                              2\n",
      "Machinery Manufacturing                    1\n",
      "Cable, Internet & Telephone Providers      1\n",
      "Gambling                                   1\n",
      "Advertising & Public Relations             1\n",
      "Electronics Manufacturing                  1\n",
      "Name: Industry, dtype: int64, 'Hong_Kong': HR Consulting                              21\n",
      "Security & Protective                       5\n",
      "Enterprise Software & Network Solutions     4\n",
      "Staffing & Subcontracting                   3\n",
      "Architectural & Engineering Services        3\n",
      "Investment & Asset Management               3\n",
      "Information Technology Support Services     3\n",
      "Business Consulting                         2\n",
      "Advertising & Public Relations              2\n",
      "Accounting & Tax                            2\n",
      "Real Estate                                 2\n",
      "Drug & Health Stores                        2\n",
      "Internet & Web Services                     2\n",
      "Insurance Carriers                          2\n",
      "Aerospace & Defense                         2\n",
      "Beauty & Personal Accessories Stores        1\n",
      "Taxi & Car Services                         1\n",
      "Culture & Entertainment                     1\n",
      "Computer Hardware Development               1\n",
      "Broadcast Media                             1\n",
      "Shipping & Trucking                         1\n",
      "Stock Exchanges                             1\n",
      "Name: Industry, dtype: int64, 'Hungary': Information Technology Support Services    15\n",
      "Telecommunications Services                 9\n",
      "Business Consulting                         5\n",
      "Electronics Manufacturing                   5\n",
      "HR Consulting                               5\n",
      "Enterprise Software & Network Solutions     5\n",
      "Consumer Product Manufacturing              4\n",
      "Computer Hardware Development               4\n",
      "Investment & Asset Management               3\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Transportation Equipment Manufacturing      2\n",
      "Car & Truck Rental                          1\n",
      "Software Development                        1\n",
      "Research & Development                      1\n",
      "Construction                                1\n",
      "Health Care Services & Hospitals            1\n",
      "Energy & Utilities                          1\n",
      "Name: Industry, dtype: int64, 'Ireland': HR Consulting                              11\n",
      "Business Consulting                         7\n",
      "Enterprise Software & Network Solutions     6\n",
      "Information Technology Support Services     6\n",
      "Internet & Web Services                     5\n",
      "Financial Transaction Processing            4\n",
      "Software Development                        3\n",
      "Health Care Services & Hospitals            3\n",
      "Stock Exchanges                             2\n",
      "Computer Hardware Development               2\n",
      "Insurance Carriers                          2\n",
      "Consumer Product Manufacturing              1\n",
      "Advertising & Public Relations              1\n",
      "Crop Production                             1\n",
      "National Agencies                           1\n",
      "Cable, Internet & Telephone Providers       1\n",
      "Broadcast Media                             1\n",
      "Biotech & Pharmaceuticals                   1\n",
      "Machinery Manufacturing                     1\n",
      "Staffing & Subcontracting                   1\n",
      "Video Game Publishing                       1\n",
      "Electronics Manufacturing                   1\n",
      "Shipping & Trucking                         1\n",
      "Name: Industry, dtype: int64, 'Israel': Staffing & Subcontracting                  100\n",
      "Computer Hardware Development               91\n",
      "Enterprise Software & Network Solutions      8\n",
      "Information Technology Support Services      7\n",
      "Internet & Web Services                      5\n",
      "Insurance Carriers                           4\n",
      "Software Development                         3\n",
      "Consumer Product Manufacturing               2\n",
      "Energy & Utilities                           2\n",
      "Construction                                 2\n",
      "Business Consulting                          1\n",
      "Research & Development                       1\n",
      "Real Estate                                  1\n",
      "Accounting & Tax                             1\n",
      "HR Consulting                                1\n",
      "Banking & Lending                            1\n",
      "Name: Industry, dtype: int64, 'Italy': Information Technology Support Services    8\n",
      "Advertising & Public Relations             4\n",
      "Energy & Utilities                         3\n",
      "Electronics Manufacturing                  3\n",
      "Business Consulting                        3\n",
      "Cable, Internet & Telephone Providers      3\n",
      "Research & Development                     3\n",
      "Aerospace & Defense                        2\n",
      "Security & Protective                      2\n",
      "Broadcast Media                            2\n",
      "Banking & Lending                          2\n",
      "HR Consulting                              2\n",
      "Computer Hardware Development              2\n",
      "General Merchandise & Superstores          1\n",
      "Publishing                                 1\n",
      "Enterprise Software & Network Solutions    1\n",
      "Consumer Product Manufacturing             1\n",
      "Taxi & Car Services                        1\n",
      "Financial Transaction Processing           1\n",
      "Home Furniture & Housewares Stores         1\n",
      "Transportation Equipment Manufacturing     1\n",
      "Real Estate                                1\n",
      "Name: Industry, dtype: int64, 'Japan': Series([], Name: Industry, dtype: int64), 'Luxembourg': Information Technology Support Services    7\n",
      "Business Consulting                        4\n",
      "Legal                                      2\n",
      "Banking & Lending                          2\n",
      "Architectural & Engineering Services       2\n",
      "HR Consulting                              2\n",
      "Airlines, Airports & Air Transportation    2\n",
      "Metal & Mineral Manufacturing              1\n",
      "Advertising & Public Relations             1\n",
      "Energy & Utilities                         1\n",
      "Internet & Web Services                    1\n",
      "Machinery Manufacturing                    1\n",
      "General Merchandise & Superstores          1\n",
      "Aerospace & Defense                        1\n",
      "Telecommunications Services                1\n",
      "Investment & Asset Management              1\n",
      "Name: Industry, dtype: int64, 'Netherlands': HR Consulting                              6\n",
      "Information Technology Support Services    4\n",
      "Internet & Web Services                    4\n",
      "Insurance Carriers                         3\n",
      "Taxi & Car Services                        3\n",
      "National Agencies                          3\n",
      "Electronics Manufacturing                  2\n",
      "Advertising & Public Relations             2\n",
      "Financial Transaction Processing           1\n",
      "Architectural & Engineering Services       1\n",
      "Energy & Utilities                         1\n",
      "Vehicle Dealers                            1\n",
      "Business Consulting                        1\n",
      "Name: Industry, dtype: int64, 'New_Zealand': HR Consulting                              4\n",
      "Computer Hardware Development              3\n",
      "Real Estate                                2\n",
      "Banking & Lending                          2\n",
      "Business Consulting                        2\n",
      "Advertising & Public Relations             2\n",
      "National Agencies                          2\n",
      "Department, Clothing & Shoe Stores         1\n",
      "Video Game Publishing                      1\n",
      "Software Development                       1\n",
      "Grocery Stores                             1\n",
      "Primary & Secondary Schools                1\n",
      "Information Technology Support Services    1\n",
      "Enterprise Software & Network Solutions    1\n",
      "Accounting & Tax                           1\n",
      "Health Care Services & Hospitals           1\n",
      "Name: Industry, dtype: int64, 'Norway': Enterprise Software & Network Solutions    5\n",
      "Information Technology Support Services    4\n",
      "Construction                               3\n",
      "Financial Transaction Processing           2\n",
      "Colleges & Universities                    2\n",
      "National Agencies                          1\n",
      "Internet & Web Services                    1\n",
      "Energy & Utilities                         1\n",
      "Real Estate                                1\n",
      "Biotech & Pharmaceuticals                  1\n",
      "Name: Industry, dtype: int64, 'Poland': Information Technology Support Services    14\n",
      "Enterprise Software & Network Solutions    11\n",
      "Business Consulting                         8\n",
      "Investment & Asset Management               7\n",
      "HR Consulting                               4\n",
      "Consumer Product Manufacturing              4\n",
      "Energy & Utilities                          4\n",
      "Biotech & Pharmaceuticals                   3\n",
      "Accounting & Tax                            3\n",
      "Research & Development                      3\n",
      "Software Development                        3\n",
      "Advertising & Public Relations              2\n",
      "Transportation Equipment Manufacturing      2\n",
      "Aerospace & Defense                         2\n",
      "Banking & Lending                           2\n",
      "Computer Hardware Development               2\n",
      "Telecommunications Services                 1\n",
      "Biotechnology                               1\n",
      "Health Care Products Manufacturing          1\n",
      "Internet & Web Services                     1\n",
      "National Agencies                           1\n",
      "Name: Industry, dtype: int64, 'Portugal': Information Technology Support Services    31\n",
      "HR Consulting                              29\n",
      "Computer Hardware Development              16\n",
      "Enterprise Software & Network Solutions    12\n",
      "Internet & Web Services                     9\n",
      "Business Consulting                         7\n",
      "Financial Transaction Processing            4\n",
      "Taxi & Car Services                         4\n",
      "Advertising & Public Relations              4\n",
      "Research & Development                      4\n",
      "Consumer Product Manufacturing              4\n",
      "Home Furniture & Housewares Stores          3\n",
      "Software Development                        2\n",
      "Energy & Utilities                          2\n",
      "Stock Exchanges                             2\n",
      "Marine Transportation                       2\n",
      "Architectural & Engineering Services        2\n",
      "Electronics Manufacturing                   2\n",
      "Cable, Internet & Telephone Providers       2\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Investment & Asset Management               1\n",
      "Department, Clothing & Shoe Stores          1\n",
      "Security & Protective                       1\n",
      "Telecommunications Services                 1\n",
      "Real Estate                                 1\n",
      "Publishing                                  1\n",
      "Car & Truck Rental                          1\n",
      "Banking & Lending                           1\n",
      "Transportation Equipment Manufacturing      1\n",
      "Name: Industry, dtype: int64, 'Romania': Information Technology Support Services    18\n",
      "Investment & Asset Management              10\n",
      "Research & Development                      9\n",
      "Enterprise Software & Network Solutions     9\n",
      "Cable, Internet & Telephone Providers       7\n",
      "Stock Exchanges                             7\n",
      "Internet & Web Services                     7\n",
      "Computer Hardware Development               6\n",
      "Financial Transaction Processing            5\n",
      "Energy & Utilities                          3\n",
      "Software Development                        3\n",
      "HR Consulting                               2\n",
      "Machinery Manufacturing                     2\n",
      "Staffing & Subcontracting                   2\n",
      "Consumer Product Manufacturing              2\n",
      "Health Care Services & Hospitals            2\n",
      "Biotech & Pharmaceuticals                   1\n",
      "Taxi & Car Services                         1\n",
      "Vehicle Dealers                             1\n",
      "Business Consulting                         1\n",
      "Food & Beverage Manufacturing               1\n",
      "Transportation Equipment Manufacturing      1\n",
      "Banking & Lending                           1\n",
      "Department, Clothing & Shoe Stores          1\n",
      "National Agencies                           1\n",
      "Name: Industry, dtype: int64, 'Singapore': Information Technology Support Services    12\n",
      "Internet & Web Services                    11\n",
      "Banking & Lending                          11\n",
      "National Agencies                           9\n",
      "Business Consulting                         7\n",
      "Enterprise Software & Network Solutions     7\n",
      "Investment & Asset Management               6\n",
      "Computer Hardware Development               5\n",
      "Telecommunications Services                 4\n",
      "Colleges & Universities                     3\n",
      "Health Care Services & Hospitals            3\n",
      "Security & Protective                       2\n",
      "Advertising & Public Relations              2\n",
      "HR Consulting                               2\n",
      "Insurance Carriers                          2\n",
      "Machinery Manufacturing                     1\n",
      "Financial Transaction Processing            1\n",
      "Shipping & Trucking                         1\n",
      "Travel Agencies                             1\n",
      "Home Furniture & Housewares Stores          1\n",
      "Consumer Product Manufacturing              1\n",
      "Real Estate                                 1\n",
      "Broadcast Media                             1\n",
      "Electronics Manufacturing                   1\n",
      "Accounting & Tax                            1\n",
      "Airlines, Airports & Air Transportation     1\n",
      "Name: Industry, dtype: int64, 'South_Korea': Series([], Name: Industry, dtype: int64), 'Spain': Information Technology Support Services    15\n",
      "Business Consulting                        14\n",
      "Enterprise Software & Network Solutions     9\n",
      "Computer Hardware Development               6\n",
      "Internet & Web Services                     5\n",
      "Software Development                        4\n",
      "Banking & Lending                           3\n",
      "Energy & Utilities                          3\n",
      "Aerospace & Defense                         3\n",
      "Video Game Publishing                       3\n",
      "Advertising & Public Relations              2\n",
      "Accounting & Tax                            2\n",
      "Investment & Asset Management               2\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Consumer Product Manufacturing              1\n",
      "Colleges & Universities                     1\n",
      "Research & Development                      1\n",
      "HR Consulting                               1\n",
      "Cable, Internet & Telephone Providers       1\n",
      "Biotechnology                               1\n",
      "Insurance Carriers                          1\n",
      "Electronics Manufacturing                   1\n",
      "Pharmaceutical                              1\n",
      "Travel Agencies                             1\n",
      "Broadcast Media                             1\n",
      "Food & Beverage Manufacturing               1\n",
      "Machinery Manufacturing                     1\n",
      "Architectural & Engineering Services        1\n",
      "Name: Industry, dtype: int64, 'Sweden': Information Technology Support Services    18\n",
      "Enterprise Software & Network Solutions     8\n",
      "Consumer Product Manufacturing              4\n",
      "HR Consulting                               3\n",
      "Business Consulting                         3\n",
      "Computer Hardware Development               3\n",
      "Broadcast Media                             2\n",
      "Publishing                                  2\n",
      "Video Game Publishing                       2\n",
      "Gambling                                    2\n",
      "Energy & Utilities                          2\n",
      "Financial Transaction Processing            2\n",
      "Banking & Lending                           2\n",
      "Machinery Manufacturing                     2\n",
      "Colleges & Universities                     2\n",
      "Law Firms                                   1\n",
      "Education & Training Services               1\n",
      "Software Development                        1\n",
      "Biotech & Pharmaceuticals                   1\n",
      "Aerospace & Defense                         1\n",
      "Taxi & Car Services                         1\n",
      "Investment & Asset Management               1\n",
      "Hotels & Resorts                            1\n",
      "Transportation Equipment Manufacturing      1\n",
      "Real Estate                                 1\n",
      "Telecommunications Services                 1\n",
      "Insurance Carriers                          1\n",
      "Architectural & Engineering Services        1\n",
      "Name: Industry, dtype: int64, 'Switzerland': HR Consulting                14\n",
      "Insurance Carriers            1\n",
      "Vehicle Dealers               1\n",
      "Biotech & Pharmaceuticals     1\n",
      "Taxi & Car Services           1\n",
      "Colleges & Universities       1\n",
      "Staffing & Subcontracting     1\n",
      "Business Consulting           1\n",
      "Name: Industry, dtype: int64, 'Taiwan': Series([], Name: Industry, dtype: int64), 'Turkey': Internet & Web Services                    5\n",
      "Enterprise Software & Network Solutions    3\n",
      "Cable, Internet & Telephone Providers      2\n",
      "Telecommunications Services                2\n",
      "HR Consulting                              2\n",
      "Electronics Manufacturing                  2\n",
      "Shipping & Trucking                        1\n",
      "Computer Hardware Development              1\n",
      "Name: Industry, dtype: int64, 'United_Kingdom': Information Technology Support Services    17\n",
      "HR Consulting                              13\n",
      "Business Consulting                        10\n",
      "Banking & Lending                           8\n",
      "Energy & Utilities                          6\n",
      "Insurance Agencies & Brokerages             3\n",
      "National Agencies                           2\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Electronics Manufacturing                   2\n",
      "Aerospace & Defense                         2\n",
      "Software Development                        2\n",
      "Education & Training Services               2\n",
      "Staffing & Subcontracting                   2\n",
      "Gambling                                    2\n",
      "Publishing                                  2\n",
      "Enterprise Software & Network Solutions     2\n",
      "Sports & Recreation                         2\n",
      "Security & Protective                       1\n",
      "Investment & Asset Management               1\n",
      "Advertising & Public Relations              1\n",
      "Financial Transaction Processing            1\n",
      "Consumer Product Manufacturing              1\n",
      "Insurance Carriers                          1\n",
      "Real Estate                                 1\n",
      "Accounting & Tax                            1\n",
      "Grocery Stores                              1\n",
      "Shipping & Trucking                         1\n",
      "Internet & Web Services                     1\n",
      "Primary & Secondary Schools                 1\n",
      "Taxi & Car Services                         1\n",
      "Drug & Health Stores                        1\n",
      "Travel Agencies                             1\n",
      "Food & Beverage Manufacturing               1\n",
      "Name: Industry, dtype: int64, 'United_States': Information Technology Support Services    48\n",
      "Computer Hardware Development              13\n",
      "Health Care Services & Hospitals           12\n",
      "Enterprise Software & Network Solutions    11\n",
      "Software Development                        8\n",
      "Investment & Asset Management               7\n",
      "Advertising & Public Relations              5\n",
      "Business Consulting                         5\n",
      "Transportation Equipment Manufacturing      4\n",
      "Other Retail Stores                         3\n",
      "HR Consulting                               3\n",
      "Energy & Utilities                          3\n",
      "Insurance Carriers                          3\n",
      "Staffing & Subcontracting                   3\n",
      "Internet & Web Services                     3\n",
      "Biotech & Pharmaceuticals                   2\n",
      "Aerospace & Defense                         2\n",
      "Accounting & Tax                            2\n",
      "Financial Transaction Processing            2\n",
      "Banking & Lending                           2\n",
      "Colleges & Universities                     2\n",
      "Telecommunications Services                 2\n",
      "Publishing                                  2\n",
      "Consumer Product Manufacturing              2\n",
      "Insurance Agencies & Brokerages             2\n",
      "Airlines, Airports & Air Transportation     1\n",
      "Civic & Social Services                     1\n",
      "Food & Beverage Manufacturing               1\n",
      "General Merchandise & Superstores           1\n",
      "Architectural & Engineering Services        1\n",
      "Film Production                             1\n",
      "Home Furniture & Housewares Stores          1\n",
      "Sporting Goods Stores                       1\n",
      "Beauty & Wellness                           1\n",
      "Electronics Manufacturing                   1\n",
      "Food & Beverage Stores                      1\n",
      "National Agencies                           1\n",
      "Mining & Metals                             1\n",
      "Real Estate                                 1\n",
      "Vehicle Dealers                             1\n",
      "Health Care Products Manufacturing          1\n",
      "Name: Industry, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "show_results('Industry', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 54.0     8\n",
      "188.0    3\n",
      "29.0     3\n",
      "21.0     2\n",
      "10.0     2\n",
      "20.0     2\n",
      "56.0     2\n",
      "97.0     2\n",
      "25.0     2\n",
      "7.0      1\n",
      "6.0      1\n",
      "150.0    1\n",
      "39.0     1\n",
      "13.0     1\n",
      "22.0     1\n",
      "16.0     1\n",
      "117.0    1\n",
      "3.0      1\n",
      "9.0      1\n",
      "5.0      1\n",
      "38.0     1\n",
      "18.0     1\n",
      "96.0     1\n",
      "19.0     1\n",
      "Name: Company_age, dtype: int64, 'Austria': 57.0     5\n",
      "32.0     5\n",
      "17.0     4\n",
      "23.0     4\n",
      "75.0     4\n",
      "55.0     4\n",
      "18.0     4\n",
      "19.0     3\n",
      "27.0     3\n",
      "11.0     2\n",
      "7.0      2\n",
      "22.0     2\n",
      "67.0     2\n",
      "76.0     2\n",
      "68.0     2\n",
      "10.0     2\n",
      "71.0     2\n",
      "91.0     2\n",
      "74.0     2\n",
      "137.0    2\n",
      "66.0     2\n",
      "131.0    1\n",
      "53.0     1\n",
      "142.0    1\n",
      "56.0     1\n",
      "29.0     1\n",
      "60.0     1\n",
      "88.0     1\n",
      "149.0    1\n",
      "40.0     1\n",
      "39.0     1\n",
      "30.0     1\n",
      "204.0    1\n",
      "9.0      1\n",
      "54.0     1\n",
      "34.0     1\n",
      "28.0     1\n",
      "24.0     1\n",
      "109.0    1\n",
      "36.0     1\n",
      "112.0    1\n",
      "144.0    1\n",
      "140.0    1\n",
      "96.0     1\n",
      "15.0     1\n",
      "78.0     1\n",
      "13.0     1\n",
      "124.0    1\n",
      "64.0     1\n",
      "41.0     1\n",
      "Name: Company_age, dtype: int64, 'Belgium': 50.0     7\n",
      "55.0     5\n",
      "21.0     3\n",
      "84.0     3\n",
      "32.0     3\n",
      "104.0    2\n",
      "35.0     2\n",
      "26.0     2\n",
      "60.0     2\n",
      "75.0     1\n",
      "16.0     1\n",
      "38.0     1\n",
      "6.0      1\n",
      "28.0     1\n",
      "34.0     1\n",
      "4.0      1\n",
      "56.0     1\n",
      "9.0      1\n",
      "173.0    1\n",
      "137.0    1\n",
      "68.0     1\n",
      "Name: Company_age, dtype: int64, 'Canada': 8     1\n",
      "38    1\n",
      "57    1\n",
      "13    1\n",
      "Name: Company_age, dtype: int64, 'Czech_Republic': 2.0      5\n",
      "150.0    4\n",
      "19.0     3\n",
      "25.0     3\n",
      "23.0     3\n",
      "9.0      3\n",
      "10.0     3\n",
      "132.0    2\n",
      "20.0     2\n",
      "55.0     2\n",
      "16.0     2\n",
      "59.0     2\n",
      "57.0     2\n",
      "92.0     2\n",
      "17.0     1\n",
      "21.0     1\n",
      "89.0     1\n",
      "30.0     1\n",
      "173.0    1\n",
      "39.0     1\n",
      "27.0     1\n",
      "14.0     1\n",
      "86.0     1\n",
      "31.0     1\n",
      "29.0     1\n",
      "41.0     1\n",
      "6.0      1\n",
      "34.0     1\n",
      "15.0     1\n",
      "8.0      1\n",
      "32.0     1\n",
      "Name: Company_age, dtype: int64, 'Denmark': 91.0     4\n",
      "100.0    4\n",
      "14.0     4\n",
      "95.0     4\n",
      "12.0     2\n",
      "32.0     2\n",
      "19.0     2\n",
      "41.0     2\n",
      "60.0     1\n",
      "56.0     1\n",
      "4.0      1\n",
      "31.0     1\n",
      "23.0     1\n",
      "51.0     1\n",
      "325.0    1\n",
      "27.0     1\n",
      "152.0    1\n",
      "Name: Company_age, dtype: int64, 'Finland': 23.0     4\n",
      "34.0     3\n",
      "27.0     2\n",
      "141.0    1\n",
      "60.0     1\n",
      "18.0     1\n",
      "19.0     1\n",
      "140.0    1\n",
      "2.0      1\n",
      "59.0     1\n",
      "10.0     1\n",
      "8.0      1\n",
      "182.0    1\n",
      "12.0     1\n",
      "13.0     1\n",
      "30.0     1\n",
      "50.0     1\n",
      "25.0     1\n",
      "6.0      1\n",
      "128.0    1\n",
      "22.0     1\n",
      "14.0     1\n",
      "Name: Company_age, dtype: int64, 'France': 9.0      9\n",
      "33.0     8\n",
      "15.0     6\n",
      "26.0     5\n",
      "16.0     4\n",
      "23.0     4\n",
      "7.0      4\n",
      "37.0     4\n",
      "13.0     4\n",
      "4.0      4\n",
      "141.0    4\n",
      "55.0     4\n",
      "53.0     4\n",
      "56.0     4\n",
      "40.0     3\n",
      "54.0     3\n",
      "17.0     3\n",
      "99.0     3\n",
      "10.0     3\n",
      "160.0    2\n",
      "14.0     2\n",
      "3.0      2\n",
      "2.0      2\n",
      "90.0     2\n",
      "24.0     2\n",
      "159.0    2\n",
      "77.0     2\n",
      "35.0     2\n",
      "32.0     2\n",
      "112.0    1\n",
      "34.0     1\n",
      "60.0     1\n",
      "6.0      1\n",
      "47.0     1\n",
      "104.0    1\n",
      "12.0     1\n",
      "30.0     1\n",
      "5.0      1\n",
      "73.0     1\n",
      "130.0    1\n",
      "11.0     1\n",
      "21.0     1\n",
      "27.0     1\n",
      "100.0    1\n",
      "69.0     1\n",
      "134.0    1\n",
      "78.0     1\n",
      "Name: Company_age, dtype: int64, 'Germany': 10.0     14\n",
      "172.0     8\n",
      "25.0      7\n",
      "29.0      7\n",
      "23.0      5\n",
      "13.0      5\n",
      "56.0      4\n",
      "60.0      4\n",
      "48.0      4\n",
      "24.0      4\n",
      "27.0      3\n",
      "5.0       3\n",
      "37.0      3\n",
      "173.0     3\n",
      "44.0      3\n",
      "19.0      3\n",
      "38.0      3\n",
      "62.0      2\n",
      "17.0      2\n",
      "57.0      2\n",
      "8.0       2\n",
      "125.0     2\n",
      "7.0       2\n",
      "14.0      2\n",
      "74.0      2\n",
      "97.0      2\n",
      "42.0      2\n",
      "26.0      2\n",
      "16.0      2\n",
      "104.0     2\n",
      "22.0      2\n",
      "33.0      2\n",
      "28.0      2\n",
      "82.0      1\n",
      "40.0      1\n",
      "46.0      1\n",
      "3.0       1\n",
      "78.0      1\n",
      "55.0      1\n",
      "176.0     1\n",
      "96.0      1\n",
      "20.0      1\n",
      "9.0       1\n",
      "45.0      1\n",
      "188.0     1\n",
      "70.0      1\n",
      "6.0       1\n",
      "122.0     1\n",
      "Name: Company_age, dtype: int64, 'Greece': 173.0    5\n",
      "174.0    4\n",
      "34.0     4\n",
      "13.0     3\n",
      "19.0     3\n",
      "231.0    3\n",
      "9.0      2\n",
      "4.0      1\n",
      "56.0     1\n",
      "63.0     1\n",
      "28.0     1\n",
      "127.0    1\n",
      "59.0     1\n",
      "66.0     1\n",
      "24.0     1\n",
      "6.0      1\n",
      "20.0     1\n",
      "41.0     1\n",
      "15.0     1\n",
      "10.0     1\n",
      "30.0     1\n",
      "Name: Company_age, dtype: int64, 'Hong_Kong': 13.0     6\n",
      "25.0     4\n",
      "38.0     4\n",
      "77.0     3\n",
      "9.0      3\n",
      "19.0     2\n",
      "63.0     2\n",
      "73.0     2\n",
      "182.0    2\n",
      "24.0     2\n",
      "4.0      2\n",
      "34.0     2\n",
      "51.0     2\n",
      "48.0     2\n",
      "325.0    1\n",
      "26.0     1\n",
      "16.0     1\n",
      "167.0    1\n",
      "46.0     1\n",
      "3.0      1\n",
      "175.0    1\n",
      "32.0     1\n",
      "8.0      1\n",
      "47.0     1\n",
      "75.0     1\n",
      "29.0     1\n",
      "139.0    1\n",
      "21.0     1\n",
      "69.0     1\n",
      "54.0     1\n",
      "5.0      1\n",
      "12.0     1\n",
      "Name: Company_age, dtype: int64, 'Hungary': 30.0     8\n",
      "28.0     6\n",
      "137.0    4\n",
      "34.0     4\n",
      "177.0    4\n",
      "12.0     3\n",
      "47.0     2\n",
      "112.0    2\n",
      "147.0    2\n",
      "60.0     1\n",
      "54.0     1\n",
      "19.0     1\n",
      "15.0     1\n",
      "63.0     1\n",
      "115.0    1\n",
      "51.0     1\n",
      "108.0    1\n",
      "75.0     1\n",
      "7.0      1\n",
      "77.0     1\n",
      "33.0     1\n",
      "21.0     1\n",
      "14.0     1\n",
      "50.0     1\n",
      "6.0      1\n",
      "39.0     1\n",
      "11.0     1\n",
      "41.0     1\n",
      "17.0     1\n",
      "131.0    1\n",
      "22.0     1\n",
      "158.0    1\n",
      "121.0    1\n",
      "38.0     1\n",
      "Name: Company_age, dtype: int64, 'Ireland': 22.0     6\n",
      "55.0     4\n",
      "36.0     4\n",
      "57.0     4\n",
      "25.0     4\n",
      "12.0     3\n",
      "18.0     3\n",
      "4.0      2\n",
      "26.0     2\n",
      "29.0     2\n",
      "38.0     2\n",
      "24.0     2\n",
      "20.0     2\n",
      "46.0     2\n",
      "15.0     2\n",
      "151.0    1\n",
      "23.0     1\n",
      "58.0     1\n",
      "44.0     1\n",
      "35.0     1\n",
      "31.0     1\n",
      "10.0     1\n",
      "136.0    1\n",
      "5.0      1\n",
      "8.0      1\n",
      "39.0     1\n",
      "3.0      1\n",
      "41.0     1\n",
      "97.0     1\n",
      "40.0     1\n",
      "14.0     1\n",
      "217.0    1\n",
      "143.0    1\n",
      "71.0     1\n",
      "Name: Company_age, dtype: int64, 'Israel': 7.0      8\n",
      "5.0      6\n",
      "13.0     3\n",
      "9.0      3\n",
      "8.0      3\n",
      "17.0     2\n",
      "12.0     2\n",
      "6.0      1\n",
      "10.0     1\n",
      "47.0     1\n",
      "2.0      1\n",
      "173.0    1\n",
      "19.0     1\n",
      "3.0      1\n",
      "15.0     1\n",
      "26.0     1\n",
      "Name: Company_age, dtype: int64, 'Italy': 56.0     4\n",
      "35.0     3\n",
      "23.0     3\n",
      "10.0     3\n",
      "141.0    2\n",
      "17.0     2\n",
      "15.0     1\n",
      "119.0    1\n",
      "112.0    1\n",
      "36.0     1\n",
      "3.0      1\n",
      "71.0     1\n",
      "153.0    1\n",
      "30.0     1\n",
      "24.0     1\n",
      "26.0     1\n",
      "44.0     1\n",
      "55.0     1\n",
      "100.0    1\n",
      "11.0     1\n",
      "9.0      1\n",
      "Name: Company_age, dtype: int64, 'Japan': Series([], Name: Company_age, dtype: int64), 'Luxembourg': 29.0     2\n",
      "61.0     2\n",
      "23.0     2\n",
      "4.0      2\n",
      "44.0     1\n",
      "102.0    1\n",
      "36.0     1\n",
      "14.0     1\n",
      "57.0     1\n",
      "31.0     1\n",
      "75.0     1\n",
      "11.0     1\n",
      "63.0     1\n",
      "42.0     1\n",
      "103.0    1\n",
      "30.0     1\n",
      "Name: Company_age, dtype: int64, 'Netherlands': 27.0     4\n",
      "70.0     3\n",
      "88.0     2\n",
      "52.0     2\n",
      "25.0     2\n",
      "26.0     1\n",
      "56.0     1\n",
      "37.0     1\n",
      "14.0     1\n",
      "39.0     1\n",
      "29.0     1\n",
      "9.0      1\n",
      "21.0     1\n",
      "54.0     1\n",
      "100.0    1\n",
      "15.0     1\n",
      "45.0     1\n",
      "13.0     1\n",
      "Name: Company_age, dtype: int64, 'New_Zealand': 19.0     5\n",
      "171.0    2\n",
      "14.0     2\n",
      "16.0     2\n",
      "13.0     1\n",
      "23.0     1\n",
      "173.0    1\n",
      "17.0     1\n",
      "27.0     1\n",
      "21.0     1\n",
      "55.0     1\n",
      "162.0    1\n",
      "42.0     1\n",
      "6.0      1\n",
      "22.0     1\n",
      "26.0     1\n",
      "41.0     1\n",
      "75.0     1\n",
      "Name: Company_age, dtype: int64, 'Norway': 17.0     3\n",
      "29.0     2\n",
      "62.0     1\n",
      "14.0     1\n",
      "128.0    1\n",
      "23.0     1\n",
      "28.0     1\n",
      "117.0    1\n",
      "16.0     1\n",
      "56.0     1\n",
      "57.0     1\n",
      "39.0     1\n",
      "7.0      1\n",
      "Name: Company_age, dtype: int64, 'Poland': 34.0     8\n",
      "123.0    5\n",
      "24.0     4\n",
      "25.0     4\n",
      "52.0     3\n",
      "17.0     3\n",
      "135.0    3\n",
      "137.0    3\n",
      "2.0      2\n",
      "8.0      2\n",
      "51.0     2\n",
      "9.0      2\n",
      "4.0      2\n",
      "116.0    2\n",
      "37.0     2\n",
      "5.0      2\n",
      "32.0     1\n",
      "15.0     1\n",
      "39.0     1\n",
      "11.0     1\n",
      "153.0    1\n",
      "16.0     1\n",
      "13.0     1\n",
      "23.0     1\n",
      "114.0    1\n",
      "186.0    1\n",
      "224.0    1\n",
      "193.0    1\n",
      "160.0    1\n",
      "36.0     1\n",
      "6.0      1\n",
      "3.0      1\n",
      "211.0    1\n",
      "182.0    1\n",
      "20.0     1\n",
      "65.0     1\n",
      "147.0    1\n",
      "31.0     1\n",
      "26.0     1\n",
      "Name: Company_age, dtype: int64, 'Portugal': 10.0     13\n",
      "23.0     11\n",
      "56.0      9\n",
      "29.0      8\n",
      "8.0       8\n",
      "55.0      8\n",
      "28.0      7\n",
      "34.0      5\n",
      "19.0      5\n",
      "17.0      4\n",
      "6.0       4\n",
      "2.0       4\n",
      "25.0      3\n",
      "137.0     3\n",
      "22.0      3\n",
      "5.0       2\n",
      "41.0      2\n",
      "13.0      2\n",
      "15.0      2\n",
      "9.0       2\n",
      "57.0      2\n",
      "14.0      2\n",
      "30.0      2\n",
      "20.0      2\n",
      "133.0     2\n",
      "21.0      1\n",
      "78.0      1\n",
      "16.0      1\n",
      "74.0      1\n",
      "52.0      1\n",
      "3.0       1\n",
      "82.0      1\n",
      "18.0      1\n",
      "94.0      1\n",
      "24.0      1\n",
      "38.0      1\n",
      "39.0      1\n",
      "11.0      1\n",
      "7.0       1\n",
      "111.0     1\n",
      "45.0      1\n",
      "43.0      1\n",
      "77.0      1\n",
      "176.0     1\n",
      "121.0     1\n",
      "Name: Company_age, dtype: int64, 'Romania': 123.0    9\n",
      "56.0     7\n",
      "41.0     7\n",
      "325.0    7\n",
      "25.0     6\n",
      "35.0     5\n",
      "11.0     4\n",
      "30.0     4\n",
      "13.0     4\n",
      "23.0     3\n",
      "92.0     3\n",
      "112.0    3\n",
      "12.0     3\n",
      "84.0     2\n",
      "29.0     2\n",
      "27.0     2\n",
      "19.0     2\n",
      "125.0    2\n",
      "20.0     2\n",
      "10.0     2\n",
      "3.0      2\n",
      "24.0     2\n",
      "15.0     1\n",
      "33.0     1\n",
      "17.0     1\n",
      "135.0    1\n",
      "159.0    1\n",
      "116.0    1\n",
      "46.0     1\n",
      "153.0    1\n",
      "32.0     1\n",
      "8.0      1\n",
      "83.0     1\n",
      "36.0     1\n",
      "18.0     1\n",
      "9.0      1\n",
      "137.0    1\n",
      "22.0     1\n",
      "Name: Company_age, dtype: int64, 'Singapore': 7.0      7\n",
      "34.0     6\n",
      "91.0     5\n",
      "55.0     5\n",
      "9.0      5\n",
      "28.0     4\n",
      "19.0     4\n",
      "25.0     4\n",
      "6.0      3\n",
      "23.0     3\n",
      "24.0     3\n",
      "20.0     3\n",
      "11.0     3\n",
      "3.0      2\n",
      "42.0     2\n",
      "59.0     2\n",
      "10.0     2\n",
      "18.0     2\n",
      "31.0     2\n",
      "8.0      2\n",
      "13.0     2\n",
      "144.0    2\n",
      "14.0     2\n",
      "16.0     2\n",
      "29.0     1\n",
      "82.0     1\n",
      "51.0     1\n",
      "22.0     1\n",
      "41.0     1\n",
      "56.0     1\n",
      "97.0     1\n",
      "12.0     1\n",
      "44.0     1\n",
      "143.0    1\n",
      "175.0    1\n",
      "87.0     1\n",
      "30.0     1\n",
      "135.0    1\n",
      "224.0    1\n",
      "54.0     1\n",
      "Name: Company_age, dtype: int64, 'South_Korea': Series([], Name: Company_age, dtype: int64), 'Spain': 16.0     5\n",
      "20.0     5\n",
      "53.0     4\n",
      "8.0      4\n",
      "34.0     4\n",
      "15.0     3\n",
      "6.0      3\n",
      "19.0     3\n",
      "56.0     3\n",
      "167.0    3\n",
      "35.0     3\n",
      "36.0     3\n",
      "29.0     2\n",
      "2.0      2\n",
      "176.0    2\n",
      "10.0     2\n",
      "30.0     2\n",
      "109.0    2\n",
      "60.0     2\n",
      "173.0    2\n",
      "41.0     2\n",
      "9.0      2\n",
      "11.0     2\n",
      "23.0     1\n",
      "39.0     1\n",
      "7.0      1\n",
      "84.0     1\n",
      "12.0     1\n",
      "151.0    1\n",
      "100.0    1\n",
      "141.0    1\n",
      "24.0     1\n",
      "3.0      1\n",
      "4.0      1\n",
      "5.0      1\n",
      "58.0     1\n",
      "85.0     1\n",
      "18.0     1\n",
      "187.0    1\n",
      "22.0     1\n",
      "91.0     1\n",
      "123.0    1\n",
      "127.0    1\n",
      "55.0     1\n",
      "Name: Company_age, dtype: int64, 'Sweden': 14.0     5\n",
      "56.0     4\n",
      "23.0     4\n",
      "3.0      4\n",
      "11.0     3\n",
      "25.0     3\n",
      "94.0     2\n",
      "24.0     2\n",
      "55.0     2\n",
      "9.0      2\n",
      "62.0     2\n",
      "8.0      2\n",
      "15.0     2\n",
      "96.0     2\n",
      "16.0     2\n",
      "30.0     2\n",
      "60.0     1\n",
      "18.0     1\n",
      "128.0    1\n",
      "132.0    1\n",
      "29.0     1\n",
      "38.0     1\n",
      "86.0     1\n",
      "59.0     1\n",
      "12.0     1\n",
      "110.0    1\n",
      "168.0    1\n",
      "22.0     1\n",
      "26.0     1\n",
      "21.0     1\n",
      "19.0     1\n",
      "104.0    1\n",
      "39.0     1\n",
      "4.0      1\n",
      "33.0     1\n",
      "Name: Company_age, dtype: int64, 'Switzerland': 18.0     15\n",
      "75.0      3\n",
      "25.0      2\n",
      "160.0     1\n",
      "78.0      1\n",
      "42.0      1\n",
      "7.0       1\n",
      "127.0     1\n",
      "174.0     1\n",
      "56.0      1\n",
      "Name: Company_age, dtype: int64, 'Taiwan': Series([], Name: Company_age, dtype: int64), 'Turkey': 13.0     3\n",
      "41.0     2\n",
      "18.0     2\n",
      "176.0    2\n",
      "15.0     1\n",
      "25.0     1\n",
      "19.0     1\n",
      "63.0     1\n",
      "5.0      1\n",
      "Name: Company_age, dtype: int64, 'United_Kingdom': 13.0     6\n",
      "15.0     6\n",
      "34.0     5\n",
      "20.0     5\n",
      "8.0      5\n",
      "14.0     4\n",
      "22.0     4\n",
      "23.0     3\n",
      "9.0      3\n",
      "328.0    3\n",
      "296.0    3\n",
      "28.0     2\n",
      "24.0     2\n",
      "30.0     2\n",
      "333.0    2\n",
      "176.0    2\n",
      "42.0     2\n",
      "10.0     2\n",
      "19.0     2\n",
      "173.0    1\n",
      "110.0    1\n",
      "58.0     1\n",
      "122.0    1\n",
      "193.0    1\n",
      "38.0     1\n",
      "26.0     1\n",
      "27.0     1\n",
      "21.0     1\n",
      "72.0     1\n",
      "7.0      1\n",
      "2.0      1\n",
      "37.0     1\n",
      "74.0     1\n",
      "49.0     1\n",
      "117.0    1\n",
      "33.0     1\n",
      "115.0    1\n",
      "32.0     1\n",
      "182.0    1\n",
      "54.0     1\n",
      "138.0    1\n",
      "39.0     1\n",
      "77.0     1\n",
      "43.0     1\n",
      "56.0     1\n",
      "Name: Company_age, dtype: int64, 'United_States': 5.0      6\n",
      "29.0     6\n",
      "10.0     6\n",
      "18.0     5\n",
      "20.0     5\n",
      "6.0      5\n",
      "24.0     5\n",
      "7.0      5\n",
      "8.0      4\n",
      "9.0      4\n",
      "11.0     4\n",
      "12.0     4\n",
      "13.0     4\n",
      "17.0     4\n",
      "23.0     4\n",
      "15.0     4\n",
      "14.0     3\n",
      "22.0     3\n",
      "2.0      3\n",
      "19.0     3\n",
      "16.0     3\n",
      "27.0     2\n",
      "4.0      2\n",
      "25.0     2\n",
      "121.0    2\n",
      "52.0     2\n",
      "36.0     2\n",
      "111.0    1\n",
      "97.0     1\n",
      "47.0     1\n",
      "42.0     1\n",
      "54.0     1\n",
      "30.0     1\n",
      "120.0    1\n",
      "41.0     1\n",
      "147.0    1\n",
      "170.0    1\n",
      "39.0     1\n",
      "162.0    1\n",
      "34.0     1\n",
      "69.0     1\n",
      "74.0     1\n",
      "85.0     1\n",
      "3.0      1\n",
      "65.0     1\n",
      "56.0     1\n",
      "26.0     1\n",
      "75.0     1\n",
      "38.0     1\n",
      "84.0     1\n",
      "32.0     1\n",
      "100.0    1\n",
      "28.0     1\n",
      "99.0     1\n",
      "78.0     1\n",
      "Name: Company_age, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "year = datetime.date.today().year\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Company_age'] = df['Founded'].apply(lambda x: x if np.isnan(x) else int(year - x))\n",
    "    del df['Founded']\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "df['Company_age'] = df['Company_age']\n",
    "\n",
    "del year\n",
    "\n",
    "show_results('Company_age', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Job age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 day ago', '10d', '12d', '13d', '14d', '15d', '16d', '17d',\n",
       "       '19d', '20d', '21d', '22d', '23d', '24h', '26d', '27d', '28d',\n",
       "       '29d', '2d', '30d', '30d+', '3d', '5d', '6d', '7d', '8d', '9d'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(dfs['United_States']['Job_age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': 31    27\n",
      "1     11\n",
      "7      4\n",
      "2      4\n",
      "24     4\n",
      "17     3\n",
      "14     2\n",
      "15     2\n",
      "22     2\n",
      "21     2\n",
      "13     1\n",
      "16     1\n",
      "29     1\n",
      "28     1\n",
      "20     1\n",
      "8      1\n",
      "Name: Job_age, dtype: int64, 'Austria': 31    92\n",
      "11     8\n",
      "6      6\n",
      "4      5\n",
      "5      5\n",
      "14     3\n",
      "27     2\n",
      "10     2\n",
      "20     2\n",
      "18     2\n",
      "13     2\n",
      "26     1\n",
      "3      1\n",
      "21     1\n",
      "8      1\n",
      "22     1\n",
      "25     1\n",
      "24     1\n",
      "2      1\n",
      "1      1\n",
      "17     1\n",
      "Name: Job_age, dtype: int64, 'Belgium': 31    33\n",
      "18     8\n",
      "5      5\n",
      "12     4\n",
      "10     4\n",
      "7      2\n",
      "19     2\n",
      "6      2\n",
      "11     2\n",
      "25     1\n",
      "27     1\n",
      "Name: Job_age, dtype: int64, 'Canada': 31    4\n",
      "Name: Job_age, dtype: int64, 'Czech_Republic': 31    49\n",
      "14     2\n",
      "11     2\n",
      "7      2\n",
      "21     2\n",
      "20     1\n",
      "8      1\n",
      "12     1\n",
      "28     1\n",
      "13     1\n",
      "1      1\n",
      "27     1\n",
      "22     1\n",
      "15     1\n",
      "26     1\n",
      "4      1\n",
      "Name: Job_age, dtype: int64, 'Denmark': 31    45\n",
      "18     7\n",
      "6      6\n",
      "8      4\n",
      "11     4\n",
      "1      3\n",
      "27     3\n",
      "26     3\n",
      "13     3\n",
      "19     2\n",
      "20     2\n",
      "7      2\n",
      "12     1\n",
      "22     1\n",
      "10     1\n",
      "15     1\n",
      "25     1\n",
      "17     1\n",
      "21     1\n",
      "Name: Job_age, dtype: int64, 'Finland': 31    33\n",
      "19     4\n",
      "8      3\n",
      "11     2\n",
      "20     2\n",
      "21     1\n",
      "28     1\n",
      "18     1\n",
      "12     1\n",
      "25     1\n",
      "26     1\n",
      "1      1\n",
      "10     1\n",
      "15     1\n",
      "22     1\n",
      "13     1\n",
      "5      1\n",
      "14     1\n",
      "27     1\n",
      "Name: Job_age, dtype: int64, 'France': 31    94\n",
      "28    18\n",
      "7     12\n",
      "13    11\n",
      "5      8\n",
      "20     7\n",
      "14     7\n",
      "21     7\n",
      "15     6\n",
      "1      5\n",
      "27     5\n",
      "19     5\n",
      "22     4\n",
      "11     4\n",
      "4      4\n",
      "12     4\n",
      "10     4\n",
      "6      4\n",
      "26     3\n",
      "25     3\n",
      "18     3\n",
      "2      2\n",
      "8      2\n",
      "17     2\n",
      "29     1\n",
      "24     1\n",
      "3      1\n",
      "Name: Job_age, dtype: int64, 'Germany': 31    80\n",
      "1     10\n",
      "26     9\n",
      "11     8\n",
      "15     8\n",
      "8      7\n",
      "7      7\n",
      "13     5\n",
      "12     5\n",
      "10     4\n",
      "22     4\n",
      "5      4\n",
      "21     3\n",
      "29     3\n",
      "6      3\n",
      "20     2\n",
      "28     2\n",
      "Name: Job_age, dtype: int64, 'Greece': 31    32\n",
      "1      5\n",
      "14     4\n",
      "15     2\n",
      "22     2\n",
      "8      2\n",
      "3      2\n",
      "6      1\n",
      "11     1\n",
      "12     1\n",
      "20     1\n",
      "18     1\n",
      "4      1\n",
      "29     1\n",
      "Name: Job_age, dtype: int64, 'Hong_Kong': 31    64\n",
      "1      7\n",
      "17     3\n",
      "11     3\n",
      "7      3\n",
      "27     2\n",
      "20     2\n",
      "14     2\n",
      "21     2\n",
      "10     2\n",
      "15     2\n",
      "25     2\n",
      "24     2\n",
      "6      1\n",
      "13     1\n",
      "18     1\n",
      "9      1\n",
      "30     1\n",
      "29     1\n",
      "28     1\n",
      "12     1\n",
      "2      1\n",
      "Name: Job_age, dtype: int64, 'Hungary': 31    53\n",
      "6      4\n",
      "8      4\n",
      "19     3\n",
      "15     2\n",
      "1      2\n",
      "25     2\n",
      "29     2\n",
      "20     2\n",
      "7      2\n",
      "13     1\n",
      "9      1\n",
      "14     1\n",
      "18     1\n",
      "12     1\n",
      "23     1\n",
      "24     1\n",
      "28     1\n",
      "10     1\n",
      "26     1\n",
      "Name: Job_age, dtype: int64, 'Ireland': 31    39\n",
      "14     6\n",
      "1      5\n",
      "5      4\n",
      "6      4\n",
      "11     3\n",
      "13     3\n",
      "4      2\n",
      "7      2\n",
      "21     2\n",
      "2      1\n",
      "8      1\n",
      "18     1\n",
      "28     1\n",
      "26     1\n",
      "Name: Job_age, dtype: int64, 'Israel': 31    162\n",
      "13     72\n",
      "27      3\n",
      "17      3\n",
      "3       2\n",
      "9       2\n",
      "14      2\n",
      "15      1\n",
      "20      1\n",
      "24      1\n",
      "23      1\n",
      "4       1\n",
      "30      1\n",
      "Name: Job_age, dtype: int64, 'Italy': 31    44\n",
      "1      7\n",
      "5      4\n",
      "19     3\n",
      "8      2\n",
      "26     2\n",
      "9      2\n",
      "22     2\n",
      "23     2\n",
      "15     2\n",
      "3      1\n",
      "11     1\n",
      "30     1\n",
      "20     1\n",
      "29     1\n",
      "28     1\n",
      "14     1\n",
      "6      1\n",
      "7      1\n",
      "13     1\n",
      "Name: Job_age, dtype: int64, 'Japan': 31    30\n",
      "20     2\n",
      "8      2\n",
      "7      1\n",
      "14     1\n",
      "24     1\n",
      "15     1\n",
      "1      1\n",
      "6      1\n",
      "30     1\n",
      "26     1\n",
      "Name: Job_age, dtype: int64, 'Luxembourg': 31    28\n",
      "6      2\n",
      "14     2\n",
      "12     1\n",
      "28     1\n",
      "27     1\n",
      "26     1\n",
      "15     1\n",
      "7      1\n",
      "11     1\n",
      "19     1\n",
      "Name: Job_age, dtype: int64, 'Netherlands': 31    33\n",
      "7      2\n",
      "11     2\n",
      "1      1\n",
      "22     1\n",
      "20     1\n",
      "Name: Job_age, dtype: int64, 'New_Zealand': 31    20\n",
      "1      6\n",
      "28     3\n",
      "3      2\n",
      "7      2\n",
      "8      2\n",
      "2      2\n",
      "9      2\n",
      "13     2\n",
      "29     2\n",
      "24     2\n",
      "22     1\n",
      "16     1\n",
      "30     1\n",
      "17     1\n",
      "10     1\n",
      "14     1\n",
      "23     1\n",
      "Name: Job_age, dtype: int64, 'Norway': 31    17\n",
      "8      3\n",
      "12     3\n",
      "21     2\n",
      "1      2\n",
      "20     1\n",
      "15     1\n",
      "18     1\n",
      "4      1\n",
      "Name: Job_age, dtype: int64, 'Poland': 31    64\n",
      "6      5\n",
      "20     4\n",
      "5      4\n",
      "14     4\n",
      "22     3\n",
      "12     3\n",
      "27     2\n",
      "21     2\n",
      "26     2\n",
      "16     2\n",
      "25     2\n",
      "4      2\n",
      "18     2\n",
      "1      2\n",
      "8      2\n",
      "2      1\n",
      "19     1\n",
      "13     1\n",
      "7      1\n",
      "Name: Job_age, dtype: int64, 'Portugal': 31    82\n",
      "1     10\n",
      "22     9\n",
      "6      8\n",
      "12     7\n",
      "19     6\n",
      "7      6\n",
      "5      6\n",
      "18     5\n",
      "14     5\n",
      "21     5\n",
      "20     4\n",
      "11     4\n",
      "27     3\n",
      "30     3\n",
      "25     3\n",
      "9      3\n",
      "15     3\n",
      "13     3\n",
      "2      3\n",
      "8      2\n",
      "26     2\n",
      "29     2\n",
      "28     2\n",
      "23     1\n",
      "17     1\n",
      "4      1\n",
      "Name: Job_age, dtype: int64, 'Romania': 31    64\n",
      "11     5\n",
      "5      4\n",
      "6      4\n",
      "19     4\n",
      "21     4\n",
      "1      4\n",
      "20     3\n",
      "8      3\n",
      "25     3\n",
      "7      3\n",
      "15     3\n",
      "12     3\n",
      "23     2\n",
      "14     2\n",
      "27     2\n",
      "22     2\n",
      "4      2\n",
      "26     1\n",
      "29     1\n",
      "13     1\n",
      "2      1\n",
      "10     1\n",
      "9      1\n",
      "Name: Job_age, dtype: int64, 'Singapore': 31    68\n",
      "1     14\n",
      "10     6\n",
      "3      6\n",
      "2      5\n",
      "4      5\n",
      "8      4\n",
      "7      3\n",
      "9      3\n",
      "14     3\n",
      "21     3\n",
      "30     2\n",
      "28     2\n",
      "13     2\n",
      "6      2\n",
      "19     1\n",
      "17     1\n",
      "16     1\n",
      "25     1\n",
      "23     1\n",
      "5      1\n",
      "Name: Job_age, dtype: int64, 'South_Korea': 31    24\n",
      "4      4\n",
      "3      3\n",
      "8      3\n",
      "2      3\n",
      "9      3\n",
      "1      2\n",
      "15     2\n",
      "18     1\n",
      "23     1\n",
      "Name: Job_age, dtype: int64, 'Spain': 31    78\n",
      "14     6\n",
      "6      6\n",
      "19     6\n",
      "29     4\n",
      "8      4\n",
      "1      3\n",
      "11     3\n",
      "16     2\n",
      "15     2\n",
      "28     2\n",
      "27     2\n",
      "13     2\n",
      "12     2\n",
      "18     1\n",
      "7      1\n",
      "26     1\n",
      "25     1\n",
      "Name: Job_age, dtype: int64, 'Sweden': 31    90\n",
      "14     8\n",
      "26     4\n",
      "7      4\n",
      "5      4\n",
      "6      3\n",
      "21     3\n",
      "8      3\n",
      "13     3\n",
      "1      2\n",
      "27     2\n",
      "19     2\n",
      "25     2\n",
      "11     2\n",
      "16     2\n",
      "18     2\n",
      "15     2\n",
      "20     2\n",
      "9      1\n",
      "12     1\n",
      "10     1\n",
      "29     1\n",
      "30     1\n",
      "Name: Job_age, dtype: int64, 'Switzerland': 31    33\n",
      "19     5\n",
      "30     4\n",
      "14     3\n",
      "6      2\n",
      "12     2\n",
      "28     2\n",
      "1      2\n",
      "7      2\n",
      "21     1\n",
      "10     1\n",
      "18     1\n",
      "26     1\n",
      "24     1\n",
      "Name: Job_age, dtype: int64, 'Taiwan': 31    32\n",
      "10     1\n",
      "2      1\n",
      "9      1\n",
      "7      1\n",
      "4      1\n",
      "11     1\n",
      "17     1\n",
      "25     1\n",
      "Name: Job_age, dtype: int64, 'Turkey': 31    12\n",
      "1      3\n",
      "15     2\n",
      "3      1\n",
      "7      1\n",
      "8      1\n",
      "5      1\n",
      "16     1\n",
      "Name: Job_age, dtype: int64, 'United_Kingdom': 31    45\n",
      "1     12\n",
      "8      9\n",
      "14     5\n",
      "9      5\n",
      "22     5\n",
      "3      4\n",
      "7      4\n",
      "28     4\n",
      "26     3\n",
      "16     3\n",
      "13     3\n",
      "21     3\n",
      "6      2\n",
      "2      2\n",
      "12     2\n",
      "15     2\n",
      "19     1\n",
      "30     1\n",
      "10     1\n",
      "29     1\n",
      "17     1\n",
      "Name: Job_age, dtype: int64, 'United_States': 31    91\n",
      "1     51\n",
      "2     17\n",
      "20     6\n",
      "7      6\n",
      "8      6\n",
      "13     6\n",
      "16     6\n",
      "14     5\n",
      "5      5\n",
      "23     5\n",
      "17     4\n",
      "6      4\n",
      "9      4\n",
      "27     3\n",
      "10     3\n",
      "3      3\n",
      "12     3\n",
      "19     2\n",
      "28     2\n",
      "22     2\n",
      "15     2\n",
      "29     2\n",
      "26     1\n",
      "30     1\n",
      "21     1\n",
      "Name: Job_age, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def clean_job_age(job_age):\n",
    "\n",
    "    if job_age == \"24h\" or job_age == \"1 day ago\":\n",
    "        job_age = \"1d\"\n",
    "    elif job_age == \"30d+\":\n",
    "        job_age = \"31d\"\n",
    "\n",
    "    if isinstance(job_age, str):\n",
    "        return int(job_age.replace(\"d\", \"\"))\n",
    "    else:\n",
    "        return job_age\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    df['Job_age'] = df['Job_age'].apply(clean_job_age)\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "del clean_job_age\n",
    "\n",
    "show_results('Job_age', dfs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': $5 to $10 billion             10\n",
      "$10+ billion                   9\n",
      "$1 to $5 billion               6\n",
      "$25 to $100 million            2\n",
      "$500 million to $1 billion     1\n",
      "Name: Revenue_USD, dtype: int64, 'Austria': $1 to $5 billion              21\n",
      "$10+ billion                  15\n",
      "$5 to $25 million             10\n",
      "$5 to $10 billion             10\n",
      "$100 to $500 million          10\n",
      "$25 to $100 million            5\n",
      "$1 to $5 million               4\n",
      "Less than $1 million           1\n",
      "$500 million to $1 billion     1\n",
      "Name: Revenue_USD, dtype: int64, 'Belgium': $100 to $500 million    16\n",
      "$1 to $5 billion         8\n",
      "$10+ billion             3\n",
      "$25 to $100 million      3\n",
      "$1 to $5 million         1\n",
      "$5 to $25 million        1\n",
      "Name: Revenue_USD, dtype: int64, 'Canada': $100 to $500 million    1\n",
      "$10+ billion            1\n",
      "Name: Revenue_USD, dtype: int64, 'Czech_Republic': $10+ billion            11\n",
      "$100 to $500 million     6\n",
      "$1 to $5 billion         6\n",
      "$5 to $10 billion        5\n",
      "$25 to $100 million      5\n",
      "Name: Revenue_USD, dtype: int64, 'Denmark': $10+ billion                  15\n",
      "$5 to $10 billion              7\n",
      "$1 to $5 billion               3\n",
      "$100 to $500 million           3\n",
      "Less than $1 million           2\n",
      "$25 to $100 million            1\n",
      "$500 million to $1 billion     1\n",
      "Name: Revenue_USD, dtype: int64, 'Finland': $25 to $100 million     6\n",
      "$100 to $500 million    5\n",
      "$10+ billion            5\n",
      "$1 to $5 billion        2\n",
      "$1 to $5 million        2\n",
      "$5 to $25 million       1\n",
      "$5 to $10 billion       1\n",
      "Name: Revenue_USD, dtype: int64, 'France': $10+ billion                  25\n",
      "$25 to $100 million           22\n",
      "$1 to $5 billion              11\n",
      "$100 to $500 million          10\n",
      "$5 to $25 million              9\n",
      "$500 million to $1 billion     5\n",
      "$1 to $5 million               5\n",
      "$5 to $10 billion              5\n",
      "Less than $1 million           2\n",
      "Name: Revenue_USD, dtype: int64, 'Germany': $100 to $500 million          25\n",
      "$10+ billion                  23\n",
      "$1 to $5 billion              12\n",
      "$5 to $25 million             11\n",
      "$25 to $100 million           10\n",
      "$5 to $10 billion              3\n",
      "$1 to $5 million               3\n",
      "$500 million to $1 billion     1\n",
      "Name: Revenue_USD, dtype: int64, 'Greece': $10+ billion            19\n",
      "$25 to $100 million      5\n",
      "$100 to $500 million     2\n",
      "$1 to $5 billion         2\n",
      "$5 to $25 million        1\n",
      "Name: Revenue_USD, dtype: int64, 'Hong_Kong': $10+ billion                  11\n",
      "$1 to $5 billion               8\n",
      "$100 to $500 million           7\n",
      "$5 to $25 million              6\n",
      "$5 to $10 billion              3\n",
      "$500 million to $1 billion     3\n",
      "$25 to $100 million            3\n",
      "$1 to $5 million               3\n",
      "Less than $1 million           2\n",
      "Name: Revenue_USD, dtype: int64, 'Hungary': $10+ billion                  25\n",
      "$1 to $5 billion              15\n",
      "$5 to $10 billion              5\n",
      "$25 to $100 million            5\n",
      "$100 to $500 million           4\n",
      "$5 to $25 million              2\n",
      "$500 million to $1 billion     1\n",
      "Name: Revenue_USD, dtype: int64, 'Ireland': $10+ billion                  15\n",
      "$1 to $5 billion               8\n",
      "$5 to $10 billion              7\n",
      "$500 million to $1 billion     5\n",
      "$1 to $5 million               4\n",
      "$100 to $500 million           3\n",
      "$5 to $25 million              2\n",
      "$25 to $100 million            2\n",
      "Name: Revenue_USD, dtype: int64, 'Israel': $10+ billion                  3\n",
      "$5 to $25 million             3\n",
      "$1 to $5 billion              2\n",
      "Less than $1 million          1\n",
      "$500 million to $1 billion    1\n",
      "Name: Revenue_USD, dtype: int64, 'Italy': $100 to $500 million          9\n",
      "$10+ billion                  8\n",
      "$25 to $100 million           7\n",
      "$1 to $5 billion              5\n",
      "$5 to $10 billion             3\n",
      "$500 million to $1 billion    2\n",
      "$5 to $25 million             2\n",
      "Name: Revenue_USD, dtype: int64, 'Japan': Series([], Name: Revenue_USD, dtype: int64), 'Luxembourg': $10+ billion                  5\n",
      "$500 million to $1 billion    3\n",
      "$5 to $10 billion             3\n",
      "Less than $1 million          2\n",
      "$100 to $500 million          2\n",
      "$1 to $5 billion              2\n",
      "$25 to $100 million           1\n",
      "$1 to $5 million              1\n",
      "$5 to $25 million             1\n",
      "Name: Revenue_USD, dtype: int64, 'Netherlands': $1 to $5 billion              9\n",
      "$25 to $100 million           6\n",
      "$500 million to $1 billion    3\n",
      "$100 to $500 million          3\n",
      "$10+ billion                  1\n",
      "Less than $1 million          1\n",
      "Name: Revenue_USD, dtype: int64, 'New_Zealand': $1 to $5 billion        5\n",
      "$10+ billion            4\n",
      "$25 to $100 million     4\n",
      "$5 to $10 billion       2\n",
      "$1 to $5 million        2\n",
      "$5 to $25 million       1\n",
      "$100 to $500 million    1\n",
      "Less than $1 million    1\n",
      "Name: Revenue_USD, dtype: int64, 'Norway': $10+ billion            4\n",
      "$100 to $500 million    2\n",
      "$5 to $10 billion       2\n",
      "$1 to $5 billion        1\n",
      "$1 to $5 million        1\n",
      "Name: Revenue_USD, dtype: int64, 'Poland': $10+ billion                  24\n",
      "$100 to $500 million           9\n",
      "$5 to $10 billion              8\n",
      "$5 to $25 million              6\n",
      "$1 to $5 billion               3\n",
      "$500 million to $1 billion     3\n",
      "$25 to $100 million            2\n",
      "$1 to $5 million               1\n",
      "Less than $1 million           1\n",
      "Name: Revenue_USD, dtype: int64, 'Portugal': $10+ billion                  31\n",
      "$1 to $5 billion              16\n",
      "$100 to $500 million          11\n",
      "$25 to $100 million           11\n",
      "$5 to $10 billion              5\n",
      "$500 million to $1 billion     5\n",
      "$5 to $25 million              4\n",
      "$1 to $5 million               3\n",
      "Less than $1 million           1\n",
      "Name: Revenue_USD, dtype: int64, 'Romania': $10+ billion                  31\n",
      "$1 to $5 billion              18\n",
      "$5 to $25 million             12\n",
      "$25 to $100 million            7\n",
      "$100 to $500 million           6\n",
      "$500 million to $1 billion     5\n",
      "$1 to $5 million               4\n",
      "$5 to $10 billion              3\n",
      "Less than $1 million           1\n",
      "Name: Revenue_USD, dtype: int64, 'Singapore': $10+ billion                  19\n",
      "$1 to $5 billion              13\n",
      "$5 to $10 billion              6\n",
      "$5 to $25 million              6\n",
      "$25 to $100 million            5\n",
      "Less than $1 million           3\n",
      "$100 to $500 million           3\n",
      "$500 million to $1 billion     1\n",
      "$1 to $5 million               1\n",
      "Name: Revenue_USD, dtype: int64, 'South_Korea': Series([], Name: Revenue_USD, dtype: int64), 'Spain': $10+ billion                  22\n",
      "$1 to $5 billion              16\n",
      "$5 to $10 billion              5\n",
      "$500 million to $1 billion     4\n",
      "$5 to $25 million              4\n",
      "$25 to $100 million            4\n",
      "$1 to $5 million               1\n",
      "Name: Revenue_USD, dtype: int64, 'Sweden': $10+ billion                  7\n",
      "$25 to $100 million           7\n",
      "$100 to $500 million          6\n",
      "$500 million to $1 billion    5\n",
      "$5 to $25 million             4\n",
      "$1 to $5 billion              3\n",
      "$5 to $10 billion             3\n",
      "$1 to $5 million              1\n",
      "Less than $1 million          1\n",
      "Name: Revenue_USD, dtype: int64, 'Switzerland': $10+ billion            5\n",
      "$100 to $500 million    3\n",
      "$1 to $5 billion        1\n",
      "$5 to $10 billion       1\n",
      "Name: Revenue_USD, dtype: int64, 'Taiwan': Series([], Name: Revenue_USD, dtype: int64), 'Turkey': $10+ billion           6\n",
      "$25 to $100 million    4\n",
      "$1 to $5 billion       1\n",
      "Name: Revenue_USD, dtype: int64, 'United_Kingdom': $10+ billion                  16\n",
      "$25 to $100 million           11\n",
      "$500 million to $1 billion     8\n",
      "$5 to $25 million              8\n",
      "$1 to $5 billion               5\n",
      "$1 to $5 million               5\n",
      "$100 to $500 million           4\n",
      "$5 to $10 billion              1\n",
      "Less than $1 million           1\n",
      "Name: Revenue_USD, dtype: int64, 'United_States': $25 to $100 million           25\n",
      "$100 to $500 million          19\n",
      "$5 to $25 million             18\n",
      "$1 to $5 million              14\n",
      "$10+ billion                  11\n",
      "Less than $1 million           8\n",
      "$1 to $5 billion               7\n",
      "$5 to $10 billion              6\n",
      "$500 million to $1 billion     4\n",
      "Name: Revenue_USD, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "show_results('Revenue_USD', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Preview columns so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                       float64\n",
       "Country                      object\n",
       "Region                       object\n",
       "Seniority                    object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Salary_currency              object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Is_salary                      bool\n",
       "Salary_avg                  float64\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Change columns order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_column_to_index(df: pd.DataFrame, column_name: str, index: int):\n",
    "\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def move_columns_to_index(df: pd.DataFrame, column_names: list[str], index: int):\n",
    "\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.1 move salary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Is_salary                      bool\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                       float64\n",
       "Country                      object\n",
       "Region                       object\n",
       "Seniority                    object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    dfs[country] = move_columns_to_index(\n",
    "            df, \n",
    "            [\n",
    "                'Salary_min', \n",
    "                'Salary_max', \n",
    "                'Salary_avg', \n",
    "                'Salary_currency',\n",
    "                'Salary_employer_provided', \n",
    "                'Salary_hourly',\n",
    "                'Is_salary'\n",
    "            ], 3\n",
    "        )\n",
    "\n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.2 Move Seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Is_salary                      bool\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "City                         object\n",
       "State                       float64\n",
       "Country                      object\n",
       "Region                       object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "\n",
    "    try:\n",
    "        df = move_column_to_index(df, 'Seniority', 3)\n",
    "    except:\n",
    "        print(f\"{country}\\n{df.dtypes}\")\n",
    "\n",
    "    dfs[country] = df\n",
    "    \n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.3 Move City, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Is_salary                      bool\n",
       "City                         object\n",
       "State                       float64\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "Country                      object\n",
       "Region                       object\n",
       "Company_age                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    try:\n",
    "        df = move_columns_to_index(df, ['City', 'State'], 11)\n",
    "    except:\n",
    "        print(f\"{country}\\n{df.dtypes}\")\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.4 Move Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Is_salary                      bool\n",
       "City                         object\n",
       "State                       float64\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Company_age                 float64\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "Country                      object\n",
       "Region                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    try:\n",
    "        df = move_column_to_index(df, 'Company_age', 19)\n",
    "    except:\n",
    "        print(f\"{country}\\n{df.dtypes}\")\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.5 Move Work/Life_balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company_name                 object\n",
       "Rating                      float64\n",
       "Job_title                    object\n",
       "Seniority                    object\n",
       "Salary_min                  float64\n",
       "Salary_max                  float64\n",
       "Salary_avg                  float64\n",
       "Salary_currency              object\n",
       "Salary_employer_provided       bool\n",
       "Salary_hourly                object\n",
       "Is_salary                      bool\n",
       "City                         object\n",
       "State                       float64\n",
       "Description                  object\n",
       "Job_age                       int64\n",
       "Easy_apply                     bool\n",
       "Employees                    object\n",
       "Type_of_ownership            object\n",
       "Sector                       object\n",
       "Company_age                 float64\n",
       "Industry                     object\n",
       "Revenue_USD                  object\n",
       "Friend_recommend            float64\n",
       "CEO_approval                float64\n",
       "Career_opportunities        float64\n",
       "Senior_management           float64\n",
       "Work/Life_balance           float64\n",
       "Comp_&_benefits             float64\n",
       "Culture_&_values            float64\n",
       "Pros                         object\n",
       "Cons                         object\n",
       "Benefits_rating             float64\n",
       "Benefits_reviews             object\n",
       "Country                      object\n",
       "Region                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "\n",
    "    try:\n",
    "        df = move_columns_to_index(df, ['Senior_management', 'Work/Life_balance'], 25)\n",
    "    except:\n",
    "        print(f\"{country}\\n{df.dtypes}\")\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Technology requirements - parsing the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_tech(cloud_names: list[str]):\n",
    "\n",
    "    def is_tech(job_description: str):\n",
    "        \n",
    "        for cloud in cloud_names:\n",
    "            if re.search((r\"\\b\" + cloud + r\"\\b\"), job_description, re.IGNORECASE) or \\\n",
    "               re.search((r\"\\b\" + cloud + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    return is_tech\n",
    "\n",
    "def add_is_needed_column_to_df(df: pd.DataFrame, column_name: str, tech_names: list[str]):\n",
    "\n",
    "    df[column_name] = df['Description'].apply(make_is_tech(tech_names))\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_tech_to_dfs(dfs, cloud_names, column_name):\n",
    "\n",
    "    for country, df in dfs.items():\n",
    "        add_is_needed_column_to_df(df, column_name, cloud_names)\n",
    "\n",
    "        dfs[country] = df\n",
    "\n",
    "    show_results(column_name, dfs)\n",
    "    return dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Git and code repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': Series([], Name: Git, dtype: int64), 'Austria': Git    3\n",
      "Name: Git, dtype: int64, 'Belgium': GitLab    1\n",
      "Git       1\n",
      "Name: Git, dtype: int64, 'Canada': Series([], Name: Git, dtype: int64), 'Czech_Republic': Series([], Name: Git, dtype: int64), 'Denmark': Git    1\n",
      "Name: Git, dtype: int64, 'Finland': Series([], Name: Git, dtype: int64), 'France': Git       11\n",
      "GitLab    10\n",
      "Github     4\n",
      "Name: Git, dtype: int64, 'Germany': GitLab    2\n",
      "Name: Git, dtype: int64, 'Greece': Series([], Name: Git, dtype: int64), 'Hong_Kong': GitLab    1\n",
      "Git       1\n",
      "Name: Git, dtype: int64, 'Hungary': Series([], Name: Git, dtype: int64), 'Ireland': Series([], Name: Git, dtype: int64), 'Israel': Series([], Name: Git, dtype: int64), 'Italy': Series([], Name: Git, dtype: int64), 'Japan': Series([], Name: Git, dtype: int64), 'Luxembourg': Git    1\n",
      "Name: Git, dtype: int64, 'Netherlands': Series([], Name: Git, dtype: int64), 'New_Zealand': Series([], Name: Git, dtype: int64), 'Norway': Series([], Name: Git, dtype: int64), 'Poland': GitLab    1\n",
      "Name: Git, dtype: int64, 'Portugal': Git          4\n",
      "Bitbucket    1\n",
      "Name: Git, dtype: int64, 'Romania': Git    1\n",
      "Name: Git, dtype: int64, 'Singapore': Git       1\n",
      "Github    1\n",
      "Name: Git, dtype: int64, 'South_Korea': Git    1\n",
      "Name: Git, dtype: int64, 'Spain': Git    2\n",
      "Name: Git, dtype: int64, 'Sweden': Git       1\n",
      "Github    1\n",
      "Name: Git, dtype: int64, 'Switzerland': Series([], Name: Git, dtype: int64), 'Taiwan': Series([], Name: Git, dtype: int64), 'Turkey': Series([], Name: Git, dtype: int64), 'United_Kingdom': Github    1\n",
      "Git       1\n",
      "Name: Git, dtype: int64, 'United_States': Git          8\n",
      "GitLab       2\n",
      "Github       2\n",
      "Launchpad    1\n",
      "Name: Git, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "def check_repo(job_description: str):\n",
    "\n",
    "    git_platforms = [\n",
    "        r\"Github\", \n",
    "        r\"GitLab\", \n",
    "        r\"Bitbucket\", \n",
    "        r\"SourceForge\", \n",
    "        r\"Launchpad\", \n",
    "        r\"Google Cloud Source Repositories\",\n",
    "        r\"AWS CodeCommit\",\n",
    "        r\"GitBucket\",\n",
    "        r\"Gogs\",\n",
    "        r\"Gitea\",\n",
    "        r\"Apache Allura\",\n",
    "        r\"RhodeCode\",\n",
    "        r\"ONEDEV\",\n",
    "        r\"Codeberg\",\n",
    "        r\"Git\" # IMPORTANT, it has to be last!\n",
    "        ]\n",
    "    \n",
    "    for platform in git_platforms:\n",
    "        if re.search((r\"\\b\" + platform + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "            return platform\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "for country, df in dfs.items():\n",
    "        \n",
    "    df['Git'] = df['Description'].apply(check_repo)\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "del check_repo\n",
    "\n",
    "show_results('Git', dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Cloud Platforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.1 AWS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Often times, clients will use this in combination with autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    59\n",
      "True      8\n",
      "Name: AWS, dtype: int64, 'Austria': False    136\n",
      "True       3\n",
      "Name: AWS, dtype: int64, 'Belgium': False    60\n",
      "True      4\n",
      "Name: AWS, dtype: int64, 'Canada': False    4\n",
      "Name: AWS, dtype: int64, 'Czech_Republic': False    64\n",
      "True      4\n",
      "Name: AWS, dtype: int64, 'Denmark': False    88\n",
      "True      3\n",
      "Name: AWS, dtype: int64, 'Finland': False    52\n",
      "True      6\n",
      "Name: AWS, dtype: int64, 'France': False    192\n",
      "True      35\n",
      "Name: AWS, dtype: int64, 'Germany': False    158\n",
      "True       6\n",
      "Name: AWS, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: AWS, dtype: int64, 'Hong_Kong': False    93\n",
      "True     12\n",
      "Name: AWS, dtype: int64, 'Hungary': False    83\n",
      "True      3\n",
      "Name: AWS, dtype: int64, 'Ireland': False    67\n",
      "True      8\n",
      "Name: AWS, dtype: int64, 'Israel': False    198\n",
      "True      54\n",
      "Name: AWS, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: AWS, dtype: int64, 'Japan': False    42\n",
      "Name: AWS, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: AWS, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: AWS, dtype: int64, 'New_Zealand': False    45\n",
      "True      7\n",
      "Name: AWS, dtype: int64, 'Norway': False    28\n",
      "True      3\n",
      "Name: AWS, dtype: int64, 'Poland': False    96\n",
      "True     13\n",
      "Name: AWS, dtype: int64, 'Portugal': False    173\n",
      "True      16\n",
      "Name: AWS, dtype: int64, 'Romania': False    116\n",
      "True       7\n",
      "Name: AWS, dtype: int64, 'Singapore': False    125\n",
      "True       9\n",
      "Name: AWS, dtype: int64, 'South_Korea': False    43\n",
      "True      3\n",
      "Name: AWS, dtype: int64, 'Spain': False    115\n",
      "True      11\n",
      "Name: AWS, dtype: int64, 'Sweden': False    129\n",
      "True      16\n",
      "Name: AWS, dtype: int64, 'Switzerland': False    60\n",
      "Name: AWS, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: AWS, dtype: int64, 'Turkey': False    22\n",
      "Name: AWS, dtype: int64, 'United_Kingdom': False    113\n",
      "True       5\n",
      "Name: AWS, dtype: int64, 'United_States': False    213\n",
      "True      28\n",
      "Name: AWS, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Amazon Web Services\", \n",
    "    r\"AWS\",\n",
    "    ]\n",
    "\n",
    "column_name = 'AWS'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.2 Microsoft Azure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud computing platform operated by Microsoft that provides access, management, and development of applications and services via around the world-distributed data centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    60\n",
      "True      7\n",
      "Name: Microsoft_Azure, dtype: int64, 'Austria': False    133\n",
      "True       6\n",
      "Name: Microsoft_Azure, dtype: int64, 'Belgium': False    54\n",
      "True     10\n",
      "Name: Microsoft_Azure, dtype: int64, 'Canada': False    4\n",
      "Name: Microsoft_Azure, dtype: int64, 'Czech_Republic': False    61\n",
      "True      7\n",
      "Name: Microsoft_Azure, dtype: int64, 'Denmark': False    77\n",
      "True     14\n",
      "Name: Microsoft_Azure, dtype: int64, 'Finland': False    41\n",
      "True     17\n",
      "Name: Microsoft_Azure, dtype: int64, 'France': False    186\n",
      "True      41\n",
      "Name: Microsoft_Azure, dtype: int64, 'Germany': False    148\n",
      "True      16\n",
      "Name: Microsoft_Azure, dtype: int64, 'Greece': False    52\n",
      "True      4\n",
      "Name: Microsoft_Azure, dtype: int64, 'Hong_Kong': False    93\n",
      "True     12\n",
      "Name: Microsoft_Azure, dtype: int64, 'Hungary': False    78\n",
      "True      8\n",
      "Name: Microsoft_Azure, dtype: int64, 'Ireland': False    72\n",
      "True      3\n",
      "Name: Microsoft_Azure, dtype: int64, 'Israel': False    244\n",
      "True       8\n",
      "Name: Microsoft_Azure, dtype: int64, 'Italy': False    75\n",
      "True      5\n",
      "Name: Microsoft_Azure, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Microsoft_Azure, dtype: int64, 'Luxembourg': False    38\n",
      "True      2\n",
      "Name: Microsoft_Azure, dtype: int64, 'Netherlands': False    37\n",
      "True      3\n",
      "Name: Microsoft_Azure, dtype: int64, 'New_Zealand': False    49\n",
      "True      3\n",
      "Name: Microsoft_Azure, dtype: int64, 'Norway': False    26\n",
      "True      5\n",
      "Name: Microsoft_Azure, dtype: int64, 'Poland': False    100\n",
      "True       9\n",
      "Name: Microsoft_Azure, dtype: int64, 'Portugal': False    165\n",
      "True      24\n",
      "Name: Microsoft_Azure, dtype: int64, 'Romania': False    118\n",
      "True       5\n",
      "Name: Microsoft_Azure, dtype: int64, 'Singapore': False    130\n",
      "True       4\n",
      "Name: Microsoft_Azure, dtype: int64, 'South_Korea': False    44\n",
      "True      2\n",
      "Name: Microsoft_Azure, dtype: int64, 'Spain': False    115\n",
      "True      11\n",
      "Name: Microsoft_Azure, dtype: int64, 'Sweden': False    120\n",
      "True      25\n",
      "Name: Microsoft_Azure, dtype: int64, 'Switzerland': False    54\n",
      "True      6\n",
      "Name: Microsoft_Azure, dtype: int64, 'Taiwan': False    38\n",
      "True      2\n",
      "Name: Microsoft_Azure, dtype: int64, 'Turkey': False    22\n",
      "Name: Microsoft_Azure, dtype: int64, 'United_Kingdom': False    99\n",
      "True     19\n",
      "Name: Microsoft_Azure, dtype: int64, 'United_States': False    212\n",
      "True      29\n",
      "Name: Microsoft_Azure, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Microsoft Azure\", \n",
    "    r\"Azure\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_Azure'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.3 GCP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    62\n",
      "True      5\n",
      "Name: GPC, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: GPC, dtype: int64, 'Belgium': False    63\n",
      "True      1\n",
      "Name: GPC, dtype: int64, 'Canada': False    4\n",
      "Name: GPC, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: GPC, dtype: int64, 'Denmark': False    91\n",
      "Name: GPC, dtype: int64, 'Finland': False    53\n",
      "True      5\n",
      "Name: GPC, dtype: int64, 'France': False    192\n",
      "True      35\n",
      "Name: GPC, dtype: int64, 'Germany': False    164\n",
      "Name: GPC, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: GPC, dtype: int64, 'Hong_Kong': False    101\n",
      "True       4\n",
      "Name: GPC, dtype: int64, 'Hungary': False    86\n",
      "Name: GPC, dtype: int64, 'Ireland': False    70\n",
      "True      5\n",
      "Name: GPC, dtype: int64, 'Israel': False    243\n",
      "True       9\n",
      "Name: GPC, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: GPC, dtype: int64, 'Japan': False    39\n",
      "True      3\n",
      "Name: GPC, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: GPC, dtype: int64, 'Netherlands': False    40\n",
      "Name: GPC, dtype: int64, 'New_Zealand': False    52\n",
      "Name: GPC, dtype: int64, 'Norway': False    31\n",
      "Name: GPC, dtype: int64, 'Poland': False    103\n",
      "True       6\n",
      "Name: GPC, dtype: int64, 'Portugal': False    183\n",
      "True       6\n",
      "Name: GPC, dtype: int64, 'Romania': False    122\n",
      "True       1\n",
      "Name: GPC, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: GPC, dtype: int64, 'South_Korea': False    46\n",
      "Name: GPC, dtype: int64, 'Spain': False    121\n",
      "True       5\n",
      "Name: GPC, dtype: int64, 'Sweden': False    140\n",
      "True       5\n",
      "Name: GPC, dtype: int64, 'Switzerland': False    60\n",
      "Name: GPC, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: GPC, dtype: int64, 'Turkey': False    22\n",
      "Name: GPC, dtype: int64, 'United_Kingdom': False    114\n",
      "True       4\n",
      "Name: GPC, dtype: int64, 'United_States': False    229\n",
      "True      12\n",
      "Name: GPC, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Google Cloud Platform\", \n",
    "    r\"GCP\",\n",
    "    ]\n",
    "\n",
    "column_name = 'GPC'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Alibaba Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibaba Cloud provides cloud computing services to online businesses and Alibaba's own e-commerce ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Austria': False    139\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Canada': False    4\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Finland': False    58\n",
      "Name: Alibaba_Cloud, dtype: int64, 'France': False    227\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Germany': False    164\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Greece': False    56\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Israel': False    252\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Italy': False    80\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Japan': False    42\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: Alibaba_Cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Norway': False    31\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Poland': False    109\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Portugal': False    189\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Romania': False    123\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: Alibaba_Cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Spain': False    126\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: Alibaba_Cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: Alibaba_Cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Alibaba_Cloud, dtype: int64, 'United_States': False    241\n",
      "Name: Alibaba_Cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Alibaba Cloud\", \n",
    "    r\"Aliyun\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Alibaba_Cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Oracle Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Oracle_Cloud, dtype: int64, 'Austria': False    139\n",
      "Name: Oracle_Cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: Oracle_Cloud, dtype: int64, 'Canada': False    4\n",
      "Name: Oracle_Cloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Oracle_Cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: Oracle_Cloud, dtype: int64, 'Finland': False    58\n",
      "Name: Oracle_Cloud, dtype: int64, 'France': False    227\n",
      "Name: Oracle_Cloud, dtype: int64, 'Germany': False    164\n",
      "Name: Oracle_Cloud, dtype: int64, 'Greece': False    56\n",
      "Name: Oracle_Cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Oracle_Cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: Oracle_Cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: Oracle_Cloud, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Oracle_Cloud, dtype: int64, 'Italy': False    80\n",
      "Name: Oracle_Cloud, dtype: int64, 'Japan': False    42\n",
      "Name: Oracle_Cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Oracle_Cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: Oracle_Cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Oracle_Cloud, dtype: int64, 'Norway': False    31\n",
      "Name: Oracle_Cloud, dtype: int64, 'Poland': False    109\n",
      "Name: Oracle_Cloud, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Oracle_Cloud, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Oracle_Cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: Oracle_Cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: Oracle_Cloud, dtype: int64, 'Spain': False    126\n",
      "Name: Oracle_Cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: Oracle_Cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: Oracle_Cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: Oracle_Cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: Oracle_Cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Oracle_Cloud, dtype: int64, 'United_States': False    241\n",
      "Name: Oracle_Cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Oracle Cloud\", \n",
    "    r\"OCI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.5 IBM Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of cloud computing services for business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: IBM_cloud, dtype: int64, 'Austria': False    139\n",
      "Name: IBM_cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: IBM_cloud, dtype: int64, 'Canada': False    4\n",
      "Name: IBM_cloud, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: IBM_cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: IBM_cloud, dtype: int64, 'Finland': False    58\n",
      "Name: IBM_cloud, dtype: int64, 'France': False    225\n",
      "True       2\n",
      "Name: IBM_cloud, dtype: int64, 'Germany': False    164\n",
      "Name: IBM_cloud, dtype: int64, 'Greece': False    56\n",
      "Name: IBM_cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: IBM_cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: IBM_cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: IBM_cloud, dtype: int64, 'Israel': False    252\n",
      "Name: IBM_cloud, dtype: int64, 'Italy': False    80\n",
      "Name: IBM_cloud, dtype: int64, 'Japan': False    42\n",
      "Name: IBM_cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: IBM_cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: IBM_cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: IBM_cloud, dtype: int64, 'Norway': False    31\n",
      "Name: IBM_cloud, dtype: int64, 'Poland': False    109\n",
      "Name: IBM_cloud, dtype: int64, 'Portugal': False    189\n",
      "Name: IBM_cloud, dtype: int64, 'Romania': False    123\n",
      "Name: IBM_cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: IBM_cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: IBM_cloud, dtype: int64, 'Spain': False    126\n",
      "Name: IBM_cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: IBM_cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: IBM_cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: IBM_cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: IBM_cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: IBM_cloud, dtype: int64, 'United_States': False    241\n",
      "Name: IBM_cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"IBM Cloud\", \n",
    "    r\"Kyndryl\",\n",
    "    r\"Bluemix\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.6 Tencent Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tencent Cloud provides businesses across the globe with stable and secure industry-leading cloud products and services, leveraging technological advancements such as cloud computing, Big Data, AI, IoT and network security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Tencent_cloud, dtype: int64, 'Austria': False    139\n",
      "Name: Tencent_cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: Tencent_cloud, dtype: int64, 'Canada': False    4\n",
      "Name: Tencent_cloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Tencent_cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: Tencent_cloud, dtype: int64, 'Finland': False    58\n",
      "Name: Tencent_cloud, dtype: int64, 'France': False    227\n",
      "Name: Tencent_cloud, dtype: int64, 'Germany': False    164\n",
      "Name: Tencent_cloud, dtype: int64, 'Greece': False    56\n",
      "Name: Tencent_cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Tencent_cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: Tencent_cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: Tencent_cloud, dtype: int64, 'Israel': False    252\n",
      "Name: Tencent_cloud, dtype: int64, 'Italy': False    80\n",
      "Name: Tencent_cloud, dtype: int64, 'Japan': False    42\n",
      "Name: Tencent_cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Tencent_cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: Tencent_cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Tencent_cloud, dtype: int64, 'Norway': False    31\n",
      "Name: Tencent_cloud, dtype: int64, 'Poland': False    109\n",
      "Name: Tencent_cloud, dtype: int64, 'Portugal': False    189\n",
      "Name: Tencent_cloud, dtype: int64, 'Romania': False    123\n",
      "Name: Tencent_cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: Tencent_cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: Tencent_cloud, dtype: int64, 'Spain': False    126\n",
      "Name: Tencent_cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: Tencent_cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: Tencent_cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: Tencent_cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: Tencent_cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Tencent_cloud, dtype: int64, 'United_States': False    241\n",
      "Name: Tencent_cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Tencent Cloud\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Tencent_cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.8 OVHcloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A French cloud computing company which offers VPS, dedicated servers and other web services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: OVHcloud, dtype: int64, 'Austria': False    139\n",
      "Name: OVHcloud, dtype: int64, 'Belgium': False    64\n",
      "Name: OVHcloud, dtype: int64, 'Canada': False    4\n",
      "Name: OVHcloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: OVHcloud, dtype: int64, 'Denmark': False    91\n",
      "Name: OVHcloud, dtype: int64, 'Finland': False    58\n",
      "Name: OVHcloud, dtype: int64, 'France': False    227\n",
      "Name: OVHcloud, dtype: int64, 'Germany': False    164\n",
      "Name: OVHcloud, dtype: int64, 'Greece': False    56\n",
      "Name: OVHcloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: OVHcloud, dtype: int64, 'Hungary': False    86\n",
      "Name: OVHcloud, dtype: int64, 'Ireland': False    75\n",
      "Name: OVHcloud, dtype: int64, 'Israel': False    252\n",
      "Name: OVHcloud, dtype: int64, 'Italy': False    80\n",
      "Name: OVHcloud, dtype: int64, 'Japan': False    42\n",
      "Name: OVHcloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: OVHcloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: OVHcloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: OVHcloud, dtype: int64, 'Norway': False    31\n",
      "Name: OVHcloud, dtype: int64, 'Poland': False    109\n",
      "Name: OVHcloud, dtype: int64, 'Portugal': False    189\n",
      "Name: OVHcloud, dtype: int64, 'Romania': False    123\n",
      "Name: OVHcloud, dtype: int64, 'Singapore': False    134\n",
      "Name: OVHcloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: OVHcloud, dtype: int64, 'Spain': False    126\n",
      "Name: OVHcloud, dtype: int64, 'Sweden': False    145\n",
      "Name: OVHcloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: OVHcloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: OVHcloud, dtype: int64, 'Turkey': False    22\n",
      "Name: OVHcloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: OVHcloud, dtype: int64, 'United_States': False    241\n",
      "Name: OVHcloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"OVHcloud\",\n",
    "    r\"OVH\"\n",
    "    ]\n",
    "\n",
    "column_name = 'OVHcloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.9 DigitalOcean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud hosting provider that offers cloud computing services and Infrastructure as a Service (IaaS). Known for pricing and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Austria': False    139\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Canada': False    4\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Finland': False    58\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'France': False    227\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Germany': False    164\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Greece': False    56\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Israel': False    252\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Italy': False    80\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Japan': False    42\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Norway': False    31\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Poland': False    109\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Portugal': False    189\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Romania': False    123\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Spain': False    126\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: DigitalOcean_cloud, dtype: int64, 'United_States': False    241\n",
      "Name: DigitalOcean_cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"DigitalOcean\"\n",
    "    ]\n",
    "\n",
    "column_name = 'DigitalOcean_cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.10 Linode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An American cloud hosting provider that focused on providing Linux-based virtual machines, cloud infrastructure, and managed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Lincode_cloud, dtype: int64, 'Austria': False    139\n",
      "Name: Lincode_cloud, dtype: int64, 'Belgium': False    64\n",
      "Name: Lincode_cloud, dtype: int64, 'Canada': False    4\n",
      "Name: Lincode_cloud, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Lincode_cloud, dtype: int64, 'Denmark': False    91\n",
      "Name: Lincode_cloud, dtype: int64, 'Finland': False    58\n",
      "Name: Lincode_cloud, dtype: int64, 'France': False    227\n",
      "Name: Lincode_cloud, dtype: int64, 'Germany': False    164\n",
      "Name: Lincode_cloud, dtype: int64, 'Greece': False    56\n",
      "Name: Lincode_cloud, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Lincode_cloud, dtype: int64, 'Hungary': False    86\n",
      "Name: Lincode_cloud, dtype: int64, 'Ireland': False    75\n",
      "Name: Lincode_cloud, dtype: int64, 'Israel': False    252\n",
      "Name: Lincode_cloud, dtype: int64, 'Italy': False    80\n",
      "Name: Lincode_cloud, dtype: int64, 'Japan': False    42\n",
      "Name: Lincode_cloud, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Lincode_cloud, dtype: int64, 'Netherlands': False    40\n",
      "Name: Lincode_cloud, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Lincode_cloud, dtype: int64, 'Norway': False    31\n",
      "Name: Lincode_cloud, dtype: int64, 'Poland': False    109\n",
      "Name: Lincode_cloud, dtype: int64, 'Portugal': False    189\n",
      "Name: Lincode_cloud, dtype: int64, 'Romania': False    123\n",
      "Name: Lincode_cloud, dtype: int64, 'Singapore': False    134\n",
      "Name: Lincode_cloud, dtype: int64, 'South_Korea': False    46\n",
      "Name: Lincode_cloud, dtype: int64, 'Spain': False    126\n",
      "Name: Lincode_cloud, dtype: int64, 'Sweden': False    145\n",
      "Name: Lincode_cloud, dtype: int64, 'Switzerland': False    60\n",
      "Name: Lincode_cloud, dtype: int64, 'Taiwan': False    40\n",
      "Name: Lincode_cloud, dtype: int64, 'Turkey': False    22\n",
      "Name: Lincode_cloud, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Lincode_cloud, dtype: int64, 'United_States': False    241\n",
      "Name: Lincode_cloud, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "cloud_names = [\n",
    "    r\"Linode\",\n",
    "    r\"Akamai\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Lincode_cloud'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, cloud_names, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cloud_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Relational Database Management Systems (RDBMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.1 PostgreSQL\n",
    "Can be used as a data store for big data solutions.\n",
    "Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. <br>\n",
    "PostgreSQL features transactions with Atomicity, Consistency, Isolation, Durability (ACID) properties, automatically updatable views, materialized views, triggers, foreign keys, and stored procedures. <br> It is designed to handle a range of workloads, from single machines to data warehouses or Web services with many concurrent users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: PostgreSQL, dtype: int64, 'Austria': False    136\n",
      "True       3\n",
      "Name: PostgreSQL, dtype: int64, 'Belgium': False    63\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Canada': False    4\n",
      "Name: PostgreSQL, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: PostgreSQL, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'France': False    215\n",
      "True      12\n",
      "Name: PostgreSQL, dtype: int64, 'Germany': False    164\n",
      "Name: PostgreSQL, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: PostgreSQL, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Israel': False    247\n",
      "True       5\n",
      "Name: PostgreSQL, dtype: int64, 'Italy': False    80\n",
      "Name: PostgreSQL, dtype: int64, 'Japan': False    42\n",
      "Name: PostgreSQL, dtype: int64, 'Luxembourg': False    40\n",
      "Name: PostgreSQL, dtype: int64, 'Netherlands': False    40\n",
      "Name: PostgreSQL, dtype: int64, 'New_Zealand': False    51\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Norway': False    31\n",
      "Name: PostgreSQL, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: PostgreSQL, dtype: int64, 'Portugal': False    185\n",
      "True       4\n",
      "Name: PostgreSQL, dtype: int64, 'Romania': False    119\n",
      "True       4\n",
      "Name: PostgreSQL, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: PostgreSQL, dtype: int64, 'South_Korea': False    46\n",
      "Name: PostgreSQL, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: PostgreSQL, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: PostgreSQL, dtype: int64, 'Switzerland': False    60\n",
      "Name: PostgreSQL, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'Turkey': False    21\n",
      "True      1\n",
      "Name: PostgreSQL, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: PostgreSQL, dtype: int64, 'United_States': False    237\n",
      "True       4\n",
      "Name: PostgreSQL, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PostgreSQL'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.2 Microsoft SQL Server\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A software product with the primary function of storing and retrieving data as requested by other software applications—which may run either on the same computer or on another computer across a network (including the Internet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Belgium': False    58\n",
      "True      6\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Canada': False    4\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Finland': False    58\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'France': False    221\n",
      "True       6\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Greece': False    56\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Hungary': False    86\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Ireland': False    75\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Israel': False    249\n",
      "True       3\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Italy': False    76\n",
      "True      4\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Netherlands': False    40\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Norway': False    31\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Poland': False    109\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'South_Korea': False    46\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Spain': False    126\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Sweden': False    142\n",
      "True       3\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Switzerland': False    60\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Taiwan': False    40\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'Turkey': False    22\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'United_Kingdom': False    116\n",
      "True       2\n",
      "Name: Microsoft_SQL_Server, dtype: int64, 'United_States': False    234\n",
      "True       7\n",
      "Name: Microsoft_SQL_Server, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Microsoft SQL\",\n",
    "    r\"SQL Server\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_SQL_Server'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.3 MySQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source relational database management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: MySQL, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: MySQL, dtype: int64, 'Belgium': False    61\n",
      "True      3\n",
      "Name: MySQL, dtype: int64, 'Canada': False    4\n",
      "Name: MySQL, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: MySQL, dtype: int64, 'Denmark': False    91\n",
      "Name: MySQL, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'France': False    222\n",
      "True       5\n",
      "Name: MySQL, dtype: int64, 'Germany': False    164\n",
      "Name: MySQL, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: MySQL, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'Ireland': False    75\n",
      "Name: MySQL, dtype: int64, 'Israel': False    247\n",
      "True       5\n",
      "Name: MySQL, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: MySQL, dtype: int64, 'Japan': False    39\n",
      "True      3\n",
      "Name: MySQL, dtype: int64, 'Luxembourg': False    40\n",
      "Name: MySQL, dtype: int64, 'Netherlands': False    40\n",
      "Name: MySQL, dtype: int64, 'New_Zealand': False    50\n",
      "True      2\n",
      "Name: MySQL, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'Poland': False    109\n",
      "Name: MySQL, dtype: int64, 'Portugal': False    185\n",
      "True       4\n",
      "Name: MySQL, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: MySQL, dtype: int64, 'Singapore': False    131\n",
      "True       3\n",
      "Name: MySQL, dtype: int64, 'South_Korea': False    46\n",
      "Name: MySQL, dtype: int64, 'Spain': False    123\n",
      "True       3\n",
      "Name: MySQL, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: MySQL, dtype: int64, 'Switzerland': False    60\n",
      "Name: MySQL, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'Turkey': False    21\n",
      "True      1\n",
      "Name: MySQL, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: MySQL, dtype: int64, 'United_States': False    237\n",
      "True       4\n",
      "Name: MySQL, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MySQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MySQL'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.4 IBM Db2 warehouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A family of data management products, including database servers, developed by IBM. It initially supported the relational model, but was extended to support object–relational features and non-relational structures like JSON and XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: IBM_Db2, dtype: int64, 'Austria': False    139\n",
      "Name: IBM_Db2, dtype: int64, 'Belgium': False    64\n",
      "Name: IBM_Db2, dtype: int64, 'Canada': False    4\n",
      "Name: IBM_Db2, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: IBM_Db2, dtype: int64, 'Denmark': False    91\n",
      "Name: IBM_Db2, dtype: int64, 'Finland': False    58\n",
      "Name: IBM_Db2, dtype: int64, 'France': False    227\n",
      "Name: IBM_Db2, dtype: int64, 'Germany': False    164\n",
      "Name: IBM_Db2, dtype: int64, 'Greece': False    56\n",
      "Name: IBM_Db2, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: IBM_Db2, dtype: int64, 'Hungary': False    86\n",
      "Name: IBM_Db2, dtype: int64, 'Ireland': False    75\n",
      "Name: IBM_Db2, dtype: int64, 'Israel': False    252\n",
      "Name: IBM_Db2, dtype: int64, 'Italy': False    80\n",
      "Name: IBM_Db2, dtype: int64, 'Japan': False    42\n",
      "Name: IBM_Db2, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: IBM_Db2, dtype: int64, 'Netherlands': False    40\n",
      "Name: IBM_Db2, dtype: int64, 'New_Zealand': False    52\n",
      "Name: IBM_Db2, dtype: int64, 'Norway': False    31\n",
      "Name: IBM_Db2, dtype: int64, 'Poland': False    109\n",
      "Name: IBM_Db2, dtype: int64, 'Portugal': False    189\n",
      "Name: IBM_Db2, dtype: int64, 'Romania': False    123\n",
      "Name: IBM_Db2, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: IBM_Db2, dtype: int64, 'South_Korea': False    46\n",
      "Name: IBM_Db2, dtype: int64, 'Spain': False    126\n",
      "Name: IBM_Db2, dtype: int64, 'Sweden': False    145\n",
      "Name: IBM_Db2, dtype: int64, 'Switzerland': False    60\n",
      "Name: IBM_Db2, dtype: int64, 'Taiwan': False    40\n",
      "Name: IBM_Db2, dtype: int64, 'Turkey': False    22\n",
      "Name: IBM_Db2, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: IBM_Db2, dtype: int64, 'United_States': False    241\n",
      "Name: IBM_Db2, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Db2\",\n",
    "    r\"IBMDb2\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_Db2'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.5. Oracle PL/SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A procedural language designed specifically to embrace SQL statements within its syntax. PL/SQL program units are compiled by the Oracle Database server and stored inside the database. And at run-time, both PL/SQL and SQL run within the same server process, bringing optimal efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Austria': False    139\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Belgium': False    64\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Canada': False    4\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Denmark': False    91\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Finland': False    58\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'France': False    226\n",
      "True       1\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Greece': False    56\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Hungary': False    86\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Ireland': False    75\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Israel': False    252\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Japan': False    42\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Luxembourg': False    38\n",
      "True      2\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Netherlands': False    40\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Norway': False    31\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Poland': False    109\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Portugal': False    189\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Romania': False    123\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'South_Korea': False    46\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Spain': False    126\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Sweden': False    145\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Switzerland': False    60\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'Turkey': False    22\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Oracle_PL_SQL, dtype: int64, 'United_States': False    237\n",
      "True       4\n",
      "Name: Oracle_PL_SQL, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PL/SQL\",\n",
    "    r\"PL / SQL\",\n",
    "    r\"Procedural Language for SQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_PL_SQL'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. NoSQL Database Management Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: MongoDB, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: MongoDB, dtype: int64, 'Belgium': False    64\n",
      "Name: MongoDB, dtype: int64, 'Canada': False    4\n",
      "Name: MongoDB, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: MongoDB, dtype: int64, 'Denmark': False    91\n",
      "Name: MongoDB, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: MongoDB, dtype: int64, 'France': False    218\n",
      "True       9\n",
      "Name: MongoDB, dtype: int64, 'Germany': False    164\n",
      "Name: MongoDB, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: MongoDB, dtype: int64, 'Hong_Kong': False    103\n",
      "True       2\n",
      "Name: MongoDB, dtype: int64, 'Hungary': False    86\n",
      "Name: MongoDB, dtype: int64, 'Ireland': False    75\n",
      "Name: MongoDB, dtype: int64, 'Israel': False    240\n",
      "True      12\n",
      "Name: MongoDB, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: MongoDB, dtype: int64, 'Japan': False    42\n",
      "Name: MongoDB, dtype: int64, 'Luxembourg': False    40\n",
      "Name: MongoDB, dtype: int64, 'Netherlands': False    40\n",
      "Name: MongoDB, dtype: int64, 'New_Zealand': False    51\n",
      "True      1\n",
      "Name: MongoDB, dtype: int64, 'Norway': False    31\n",
      "Name: MongoDB, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: MongoDB, dtype: int64, 'Portugal': False    185\n",
      "True       4\n",
      "Name: MongoDB, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: MongoDB, dtype: int64, 'Singapore': False    134\n",
      "Name: MongoDB, dtype: int64, 'South_Korea': False    46\n",
      "Name: MongoDB, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: MongoDB, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: MongoDB, dtype: int64, 'Switzerland': False    60\n",
      "Name: MongoDB, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: MongoDB, dtype: int64, 'Turkey': False    21\n",
      "True      1\n",
      "Name: MongoDB, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: MongoDB, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: MongoDB, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MongoDB\",\n",
    "    r\"Mongo DB\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MongoDB'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Cassandra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Cassandra, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Cassandra, dtype: int64, 'Belgium': False    64\n",
      "Name: Cassandra, dtype: int64, 'Canada': False    4\n",
      "Name: Cassandra, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Cassandra, dtype: int64, 'Denmark': False    91\n",
      "Name: Cassandra, dtype: int64, 'Finland': False    58\n",
      "Name: Cassandra, dtype: int64, 'France': False    222\n",
      "True       5\n",
      "Name: Cassandra, dtype: int64, 'Germany': False    164\n",
      "Name: Cassandra, dtype: int64, 'Greece': False    56\n",
      "Name: Cassandra, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Cassandra, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Cassandra, dtype: int64, 'Ireland': False    75\n",
      "Name: Cassandra, dtype: int64, 'Israel': False    246\n",
      "True       6\n",
      "Name: Cassandra, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Cassandra, dtype: int64, 'Japan': False    42\n",
      "Name: Cassandra, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Cassandra, dtype: int64, 'Netherlands': False    40\n",
      "Name: Cassandra, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Cassandra, dtype: int64, 'Norway': False    31\n",
      "Name: Cassandra, dtype: int64, 'Poland': False    109\n",
      "Name: Cassandra, dtype: int64, 'Portugal': False    189\n",
      "Name: Cassandra, dtype: int64, 'Romania': False    123\n",
      "Name: Cassandra, dtype: int64, 'Singapore': False    134\n",
      "Name: Cassandra, dtype: int64, 'South_Korea': False    46\n",
      "Name: Cassandra, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Cassandra, dtype: int64, 'Sweden': False    145\n",
      "Name: Cassandra, dtype: int64, 'Switzerland': False    60\n",
      "Name: Cassandra, dtype: int64, 'Taiwan': False    40\n",
      "Name: Cassandra, dtype: int64, 'Turkey': False    22\n",
      "Name: Cassandra, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Cassandra, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Cassandra, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Cassandra\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cassandra'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Amazon DynamoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proprietary NoSQL database service that supports key–value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Austria': False    139\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Belgium': False    64\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Canada': False    4\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Denmark': False    91\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Finland': False    58\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'France': False    225\n",
      "True       2\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Germany': False    164\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Greece': False    56\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Hungary': False    86\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Israel': False    250\n",
      "True       2\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Italy': False    80\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Netherlands': False    40\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Norway': False    31\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Poland': False    109\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Romania': False    123\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Singapore': False    134\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'South_Korea': False    46\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Spain': False    126\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Sweden': False    145\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Switzerland': False    60\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Taiwan': False    40\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'Turkey': False    22\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Amazon_DynamoDB, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Amazon_DynamoDB, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"DynamoDB\",\n",
    "    r\"Dynamo DB\",\n",
    "    r\"SimpleDB\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_DynamoDB'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22.4 Neo4j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Neo4j, dtype: int64, 'Austria': False    139\n",
      "Name: Neo4j, dtype: int64, 'Belgium': False    64\n",
      "Name: Neo4j, dtype: int64, 'Canada': False    4\n",
      "Name: Neo4j, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Neo4j, dtype: int64, 'Denmark': False    91\n",
      "Name: Neo4j, dtype: int64, 'Finland': False    58\n",
      "Name: Neo4j, dtype: int64, 'France': False    227\n",
      "Name: Neo4j, dtype: int64, 'Germany': False    164\n",
      "Name: Neo4j, dtype: int64, 'Greece': False    56\n",
      "Name: Neo4j, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Neo4j, dtype: int64, 'Hungary': False    86\n",
      "Name: Neo4j, dtype: int64, 'Ireland': False    75\n",
      "Name: Neo4j, dtype: int64, 'Israel': False    250\n",
      "True       2\n",
      "Name: Neo4j, dtype: int64, 'Italy': False    80\n",
      "Name: Neo4j, dtype: int64, 'Japan': False    42\n",
      "Name: Neo4j, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Neo4j, dtype: int64, 'Netherlands': False    40\n",
      "Name: Neo4j, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Neo4j, dtype: int64, 'Norway': False    31\n",
      "Name: Neo4j, dtype: int64, 'Poland': False    109\n",
      "Name: Neo4j, dtype: int64, 'Portugal': False    189\n",
      "Name: Neo4j, dtype: int64, 'Romania': False    123\n",
      "Name: Neo4j, dtype: int64, 'Singapore': False    134\n",
      "Name: Neo4j, dtype: int64, 'South_Korea': False    46\n",
      "Name: Neo4j, dtype: int64, 'Spain': False    126\n",
      "Name: Neo4j, dtype: int64, 'Sweden': False    145\n",
      "Name: Neo4j, dtype: int64, 'Switzerland': False    60\n",
      "Name: Neo4j, dtype: int64, 'Taiwan': False    40\n",
      "Name: Neo4j, dtype: int64, 'Turkey': False    22\n",
      "Name: Neo4j, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Neo4j, dtype: int64, 'United_States': False    241\n",
      "Name: Neo4j, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Neo4j\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Neo4j'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 Apache Solr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features[2] and rich document (e.g., Word, PDF) handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Apache_Solr, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Apache_Solr, dtype: int64, 'Belgium': False    64\n",
      "Name: Apache_Solr, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Solr, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Apache_Solr, dtype: int64, 'Denmark': False    91\n",
      "Name: Apache_Solr, dtype: int64, 'Finland': False    58\n",
      "Name: Apache_Solr, dtype: int64, 'France': False    227\n",
      "Name: Apache_Solr, dtype: int64, 'Germany': False    164\n",
      "Name: Apache_Solr, dtype: int64, 'Greece': False    56\n",
      "Name: Apache_Solr, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Apache_Solr, dtype: int64, 'Hungary': False    86\n",
      "Name: Apache_Solr, dtype: int64, 'Ireland': False    75\n",
      "Name: Apache_Solr, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Apache_Solr, dtype: int64, 'Italy': False    80\n",
      "Name: Apache_Solr, dtype: int64, 'Japan': False    42\n",
      "Name: Apache_Solr, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Apache_Solr, dtype: int64, 'Netherlands': False    40\n",
      "Name: Apache_Solr, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Solr, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Solr, dtype: int64, 'Poland': False    109\n",
      "Name: Apache_Solr, dtype: int64, 'Portugal': False    189\n",
      "Name: Apache_Solr, dtype: int64, 'Romania': False    123\n",
      "Name: Apache_Solr, dtype: int64, 'Singapore': False    134\n",
      "Name: Apache_Solr, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Solr, dtype: int64, 'Spain': False    126\n",
      "Name: Apache_Solr, dtype: int64, 'Sweden': False    145\n",
      "Name: Apache_Solr, dtype: int64, 'Switzerland': False    60\n",
      "Name: Apache_Solr, dtype: int64, 'Taiwan': False    40\n",
      "Name: Apache_Solr, dtype: int64, 'Turkey': False    22\n",
      "Name: Apache_Solr, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Apache_Solr, dtype: int64, 'United_States': False    241\n",
      "Name: Apache_Solr, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Solr\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Solr'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. Data warehousing and Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 Amazon Redshift"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Austria': False    139\n",
      "Name: Amazon_Redshift, dtype: int64, 'Belgium': False    64\n",
      "Name: Amazon_Redshift, dtype: int64, 'Canada': False    4\n",
      "Name: Amazon_Redshift, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Denmark': False    91\n",
      "Name: Amazon_Redshift, dtype: int64, 'Finland': False    58\n",
      "Name: Amazon_Redshift, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: Amazon_Redshift, dtype: int64, 'Germany': False    161\n",
      "True       3\n",
      "Name: Amazon_Redshift, dtype: int64, 'Greece': False    56\n",
      "Name: Amazon_Redshift, dtype: int64, 'Hong_Kong': False    103\n",
      "True       2\n",
      "Name: Amazon_Redshift, dtype: int64, 'Hungary': False    86\n",
      "Name: Amazon_Redshift, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Israel': False    244\n",
      "True       8\n",
      "Name: Amazon_Redshift, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Japan': False    42\n",
      "Name: Amazon_Redshift, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Amazon_Redshift, dtype: int64, 'Netherlands': False    40\n",
      "Name: Amazon_Redshift, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Amazon_Redshift, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Amazon_Redshift, dtype: int64, 'Romania': False    123\n",
      "Name: Amazon_Redshift, dtype: int64, 'Singapore': False    131\n",
      "True       3\n",
      "Name: Amazon_Redshift, dtype: int64, 'South_Korea': False    46\n",
      "Name: Amazon_Redshift, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Amazon_Redshift, dtype: int64, 'Switzerland': False    60\n",
      "Name: Amazon_Redshift, dtype: int64, 'Taiwan': False    40\n",
      "Name: Amazon_Redshift, dtype: int64, 'Turkey': False    22\n",
      "Name: Amazon_Redshift, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Amazon_Redshift, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Amazon_Redshift, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Redshift\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_Redshift'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Google BigQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Google_BigQuery, dtype: int64, 'Austria': False    139\n",
      "Name: Google_BigQuery, dtype: int64, 'Belgium': False    64\n",
      "Name: Google_BigQuery, dtype: int64, 'Canada': False    4\n",
      "Name: Google_BigQuery, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Google_BigQuery, dtype: int64, 'Denmark': False    91\n",
      "Name: Google_BigQuery, dtype: int64, 'Finland': False    58\n",
      "Name: Google_BigQuery, dtype: int64, 'France': False    213\n",
      "True      14\n",
      "Name: Google_BigQuery, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Google_BigQuery, dtype: int64, 'Greece': False    56\n",
      "Name: Google_BigQuery, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Google_BigQuery, dtype: int64, 'Hungary': False    86\n",
      "Name: Google_BigQuery, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Google_BigQuery, dtype: int64, 'Israel': False    245\n",
      "True       7\n",
      "Name: Google_BigQuery, dtype: int64, 'Italy': False    80\n",
      "Name: Google_BigQuery, dtype: int64, 'Japan': False    38\n",
      "True      4\n",
      "Name: Google_BigQuery, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Google_BigQuery, dtype: int64, 'Netherlands': False    40\n",
      "Name: Google_BigQuery, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Google_BigQuery, dtype: int64, 'Norway': False    31\n",
      "Name: Google_BigQuery, dtype: int64, 'Poland': False    107\n",
      "True       2\n",
      "Name: Google_BigQuery, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Google_BigQuery, dtype: int64, 'Romania': False    122\n",
      "True       1\n",
      "Name: Google_BigQuery, dtype: int64, 'Singapore': False    134\n",
      "Name: Google_BigQuery, dtype: int64, 'South_Korea': False    46\n",
      "Name: Google_BigQuery, dtype: int64, 'Spain': False    126\n",
      "Name: Google_BigQuery, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: Google_BigQuery, dtype: int64, 'Switzerland': False    60\n",
      "Name: Google_BigQuery, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Google_BigQuery, dtype: int64, 'Turkey': False    22\n",
      "Name: Google_BigQuery, dtype: int64, 'United_Kingdom': False    117\n",
      "True       1\n",
      "Name: Google_BigQuery, dtype: int64, 'United_States': False    239\n",
      "True       2\n",
      "Name: Google_BigQuery, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"BigQuery\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_BigQuery'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Snowflake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowflake enables data storage, processing, and analytic solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Austria': False    139\n",
      "Name: Snowflake, dtype: int64, 'Belgium': False    64\n",
      "Name: Snowflake, dtype: int64, 'Canada': False    4\n",
      "Name: Snowflake, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Snowflake, dtype: int64, 'Denmark': False    88\n",
      "True      3\n",
      "Name: Snowflake, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'France': False    215\n",
      "True      12\n",
      "Name: Snowflake, dtype: int64, 'Germany': False    164\n",
      "Name: Snowflake, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Snowflake, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Snowflake, dtype: int64, 'Israel': False    244\n",
      "True       8\n",
      "Name: Snowflake, dtype: int64, 'Italy': False    80\n",
      "Name: Snowflake, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'New_Zealand': False    51\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Norway': False    31\n",
      "Name: Snowflake, dtype: int64, 'Poland': False    102\n",
      "True       7\n",
      "Name: Snowflake, dtype: int64, 'Portugal': False    185\n",
      "True       4\n",
      "Name: Snowflake, dtype: int64, 'Romania': False    119\n",
      "True       4\n",
      "Name: Snowflake, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Snowflake, dtype: int64, 'South_Korea': False    45\n",
      "True      1\n",
      "Name: Snowflake, dtype: int64, 'Spain': False    124\n",
      "True       2\n",
      "Name: Snowflake, dtype: int64, 'Sweden': False    136\n",
      "True       9\n",
      "Name: Snowflake, dtype: int64, 'Switzerland': False    60\n",
      "Name: Snowflake, dtype: int64, 'Taiwan': False    40\n",
      "Name: Snowflake, dtype: int64, 'Turkey': False    22\n",
      "Name: Snowflake, dtype: int64, 'United_Kingdom': False    115\n",
      "True       3\n",
      "Name: Snowflake, dtype: int64, 'United_States': False    223\n",
      "True      18\n",
      "Name: Snowflake, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Snowflake\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Snowflake'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.4 Oracle Exadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed to run Oracle Database workloads, such as an OLTP application running simultaneously with Analytics processing. Historically, specialized database computing platforms were designed for a particular workload, such as Data Warehousing, and poor or unusable for other workloads, such as OLTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Oracle_Exadata, dtype: int64, 'Austria': False    139\n",
      "Name: Oracle_Exadata, dtype: int64, 'Belgium': False    64\n",
      "Name: Oracle_Exadata, dtype: int64, 'Canada': False    4\n",
      "Name: Oracle_Exadata, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Oracle_Exadata, dtype: int64, 'Denmark': False    91\n",
      "Name: Oracle_Exadata, dtype: int64, 'Finland': False    58\n",
      "Name: Oracle_Exadata, dtype: int64, 'France': False    227\n",
      "Name: Oracle_Exadata, dtype: int64, 'Germany': False    164\n",
      "Name: Oracle_Exadata, dtype: int64, 'Greece': False    56\n",
      "Name: Oracle_Exadata, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Oracle_Exadata, dtype: int64, 'Hungary': False    86\n",
      "Name: Oracle_Exadata, dtype: int64, 'Ireland': False    75\n",
      "Name: Oracle_Exadata, dtype: int64, 'Israel': False    252\n",
      "Name: Oracle_Exadata, dtype: int64, 'Italy': False    80\n",
      "Name: Oracle_Exadata, dtype: int64, 'Japan': False    42\n",
      "Name: Oracle_Exadata, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Oracle_Exadata, dtype: int64, 'Netherlands': False    40\n",
      "Name: Oracle_Exadata, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Oracle_Exadata, dtype: int64, 'Norway': False    31\n",
      "Name: Oracle_Exadata, dtype: int64, 'Poland': False    109\n",
      "Name: Oracle_Exadata, dtype: int64, 'Portugal': False    189\n",
      "Name: Oracle_Exadata, dtype: int64, 'Romania': False    123\n",
      "Name: Oracle_Exadata, dtype: int64, 'Singapore': False    134\n",
      "Name: Oracle_Exadata, dtype: int64, 'South_Korea': False    46\n",
      "Name: Oracle_Exadata, dtype: int64, 'Spain': False    126\n",
      "Name: Oracle_Exadata, dtype: int64, 'Sweden': False    145\n",
      "Name: Oracle_Exadata, dtype: int64, 'Switzerland': False    60\n",
      "Name: Oracle_Exadata, dtype: int64, 'Taiwan': False    40\n",
      "Name: Oracle_Exadata, dtype: int64, 'Turkey': False    22\n",
      "Name: Oracle_Exadata, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Oracle_Exadata, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Oracle_Exadata, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Exadata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Exadata'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 SAP HANA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-model database that stores data in its memory instead of keeping it on a disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: SAP_HANA, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: SAP_HANA, dtype: int64, 'Belgium': False    64\n",
      "Name: SAP_HANA, dtype: int64, 'Canada': False    4\n",
      "Name: SAP_HANA, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: SAP_HANA, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: SAP_HANA, dtype: int64, 'Finland': False    58\n",
      "Name: SAP_HANA, dtype: int64, 'France': False    227\n",
      "Name: SAP_HANA, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: SAP_HANA, dtype: int64, 'Greece': False    56\n",
      "Name: SAP_HANA, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: SAP_HANA, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: SAP_HANA, dtype: int64, 'Ireland': False    75\n",
      "Name: SAP_HANA, dtype: int64, 'Israel': False    252\n",
      "Name: SAP_HANA, dtype: int64, 'Italy': False    80\n",
      "Name: SAP_HANA, dtype: int64, 'Japan': False    42\n",
      "Name: SAP_HANA, dtype: int64, 'Luxembourg': False    40\n",
      "Name: SAP_HANA, dtype: int64, 'Netherlands': False    40\n",
      "Name: SAP_HANA, dtype: int64, 'New_Zealand': False    52\n",
      "Name: SAP_HANA, dtype: int64, 'Norway': False    31\n",
      "Name: SAP_HANA, dtype: int64, 'Poland': False    109\n",
      "Name: SAP_HANA, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: SAP_HANA, dtype: int64, 'Romania': False    123\n",
      "Name: SAP_HANA, dtype: int64, 'Singapore': False    134\n",
      "Name: SAP_HANA, dtype: int64, 'South_Korea': False    46\n",
      "Name: SAP_HANA, dtype: int64, 'Spain': False    126\n",
      "Name: SAP_HANA, dtype: int64, 'Sweden': False    145\n",
      "Name: SAP_HANA, dtype: int64, 'Switzerland': False    60\n",
      "Name: SAP_HANA, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: SAP_HANA, dtype: int64, 'Turkey': False    22\n",
      "Name: SAP_HANA, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: SAP_HANA, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: SAP_HANA, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"HANA\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SAP_HANA'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.6 Teradata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mainly suitable for building large scale data warehousing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    65\n",
      "True      2\n",
      "Name: Teradata, dtype: int64, 'Austria': False    139\n",
      "Name: Teradata, dtype: int64, 'Belgium': False    64\n",
      "Name: Teradata, dtype: int64, 'Canada': False    4\n",
      "Name: Teradata, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Teradata, dtype: int64, 'Denmark': False    91\n",
      "Name: Teradata, dtype: int64, 'Finland': False    58\n",
      "Name: Teradata, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: Teradata, dtype: int64, 'Germany': False    164\n",
      "Name: Teradata, dtype: int64, 'Greece': False    56\n",
      "Name: Teradata, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Teradata, dtype: int64, 'Hungary': False    86\n",
      "Name: Teradata, dtype: int64, 'Ireland': False    75\n",
      "Name: Teradata, dtype: int64, 'Israel': False    248\n",
      "True       4\n",
      "Name: Teradata, dtype: int64, 'Italy': False    80\n",
      "Name: Teradata, dtype: int64, 'Japan': False    42\n",
      "Name: Teradata, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Teradata, dtype: int64, 'Netherlands': False    38\n",
      "True      2\n",
      "Name: Teradata, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Teradata, dtype: int64, 'Norway': False    31\n",
      "Name: Teradata, dtype: int64, 'Poland': False    109\n",
      "Name: Teradata, dtype: int64, 'Portugal': False    189\n",
      "Name: Teradata, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Teradata, dtype: int64, 'Singapore': False    134\n",
      "Name: Teradata, dtype: int64, 'South_Korea': False    45\n",
      "True      1\n",
      "Name: Teradata, dtype: int64, 'Spain': False    126\n",
      "Name: Teradata, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Teradata, dtype: int64, 'Switzerland': False    60\n",
      "Name: Teradata, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Teradata, dtype: int64, 'Turkey': False    22\n",
      "Name: Teradata, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Teradata, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Teradata, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Teradata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Teradata'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23. Data Integration and Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.1 Informatica PowerCenter - Data integration tool\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used extensively for ETL operations, data quality, data masking, data replication, data virtualization, and master data management services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Austria': False    139\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Belgium': False    64\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Canada': False    4\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Denmark': False    91\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Finland': False    58\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'France': False    224\n",
      "True       3\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Germany': False    164\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Greece': False    56\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Hungary': False    86\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Ireland': False    75\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Israel': False    252\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Japan': False    42\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Netherlands': False    40\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Norway': False    31\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Poland': False    109\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Portugal': False    189\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Romania': False    123\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'South_Korea': False    46\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Spain': False    124\n",
      "True       2\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Sweden': False    145\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Switzerland': False    60\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Taiwan': False    40\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'Turkey': False    22\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Informatica_PowerCenter, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Informatica_PowerCenter, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PowerCenter\",\n",
    "    r\"Power Center\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Informatica_PowerCenter'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.2 DataBricks - Data processing and analytics platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    63\n",
      "True      4\n",
      "Name: Databricks, dtype: int64, 'Austria': False    139\n",
      "Name: Databricks, dtype: int64, 'Belgium': False    61\n",
      "True      3\n",
      "Name: Databricks, dtype: int64, 'Canada': False    4\n",
      "Name: Databricks, dtype: int64, 'Czech_Republic': False    65\n",
      "True      3\n",
      "Name: Databricks, dtype: int64, 'Denmark': False    87\n",
      "True      4\n",
      "Name: Databricks, dtype: int64, 'Finland': False    54\n",
      "True      4\n",
      "Name: Databricks, dtype: int64, 'France': False    208\n",
      "True      19\n",
      "Name: Databricks, dtype: int64, 'Germany': False    164\n",
      "Name: Databricks, dtype: int64, 'Greece': False    54\n",
      "True      2\n",
      "Name: Databricks, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: Databricks, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Databricks, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Databricks, dtype: int64, 'Israel': False    249\n",
      "True       3\n",
      "Name: Databricks, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Databricks, dtype: int64, 'Japan': False    40\n",
      "True      2\n",
      "Name: Databricks, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Databricks, dtype: int64, 'Netherlands': False    38\n",
      "True      2\n",
      "Name: Databricks, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Databricks, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Databricks, dtype: int64, 'Poland': False    103\n",
      "True       6\n",
      "Name: Databricks, dtype: int64, 'Portugal': False    177\n",
      "True      12\n",
      "Name: Databricks, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Databricks, dtype: int64, 'Singapore': False    134\n",
      "Name: Databricks, dtype: int64, 'South_Korea': False    45\n",
      "True      1\n",
      "Name: Databricks, dtype: int64, 'Spain': False    120\n",
      "True       6\n",
      "Name: Databricks, dtype: int64, 'Sweden': False    139\n",
      "True       6\n",
      "Name: Databricks, dtype: int64, 'Switzerland': False    60\n",
      "Name: Databricks, dtype: int64, 'Taiwan': False    40\n",
      "Name: Databricks, dtype: int64, 'Turkey': False    22\n",
      "Name: Databricks, dtype: int64, 'United_Kingdom': False    113\n",
      "True       5\n",
      "Name: Databricks, dtype: int64, 'United_States': False    223\n",
      "True      18\n",
      "Name: Databricks, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Data Bricks\",\n",
    "    r\"Databricks\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Databricks'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.3 Presto - Query engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A distributed query engine for big data using the SQL query language. Its architecture allows users to query data sources such as Hadoop, Cassandra, Kafka, AWS S3, Alluxio, MySQL, MongoDB and Teradata, and allows use of multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Presto, dtype: int64, 'Austria': False    139\n",
      "Name: Presto, dtype: int64, 'Belgium': False    64\n",
      "Name: Presto, dtype: int64, 'Canada': False    4\n",
      "Name: Presto, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Presto, dtype: int64, 'Denmark': False    91\n",
      "Name: Presto, dtype: int64, 'Finland': False    58\n",
      "Name: Presto, dtype: int64, 'France': False    227\n",
      "Name: Presto, dtype: int64, 'Germany': False    164\n",
      "Name: Presto, dtype: int64, 'Greece': False    56\n",
      "Name: Presto, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Presto, dtype: int64, 'Hungary': False    86\n",
      "Name: Presto, dtype: int64, 'Ireland': False    75\n",
      "Name: Presto, dtype: int64, 'Israel': False    247\n",
      "True       5\n",
      "Name: Presto, dtype: int64, 'Italy': False    80\n",
      "Name: Presto, dtype: int64, 'Japan': False    42\n",
      "Name: Presto, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Presto, dtype: int64, 'Netherlands': False    40\n",
      "Name: Presto, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Presto, dtype: int64, 'Norway': False    31\n",
      "Name: Presto, dtype: int64, 'Poland': False    109\n",
      "Name: Presto, dtype: int64, 'Portugal': False    189\n",
      "Name: Presto, dtype: int64, 'Romania': False    123\n",
      "Name: Presto, dtype: int64, 'Singapore': False    134\n",
      "Name: Presto, dtype: int64, 'South_Korea': False    46\n",
      "Name: Presto, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Presto, dtype: int64, 'Sweden': False    145\n",
      "Name: Presto, dtype: int64, 'Switzerland': False    60\n",
      "Name: Presto, dtype: int64, 'Taiwan': False    40\n",
      "Name: Presto, dtype: int64, 'Turkey': False    22\n",
      "Name: Presto, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Presto, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Presto, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Presto\",\n",
    "    r\"PrestoDB\",\n",
    "    r\"PrestoSQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Presto'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24. Stream processing tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Kafka"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source system, distributed event store and stream-processing platform. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Apache_Kafka, dtype: int64, 'Austria': False    136\n",
      "True       3\n",
      "Name: Apache_Kafka, dtype: int64, 'Belgium': False    64\n",
      "Name: Apache_Kafka, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Kafka, dtype: int64, 'Czech_Republic': False    62\n",
      "True      6\n",
      "Name: Apache_Kafka, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Apache_Kafka, dtype: int64, 'Finland': False    56\n",
      "True      2\n",
      "Name: Apache_Kafka, dtype: int64, 'France': False    204\n",
      "True      23\n",
      "Name: Apache_Kafka, dtype: int64, 'Germany': False    164\n",
      "Name: Apache_Kafka, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Apache_Kafka, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Apache_Kafka, dtype: int64, 'Hungary': False    84\n",
      "True      2\n",
      "Name: Apache_Kafka, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Apache_Kafka, dtype: int64, 'Israel': False    220\n",
      "True      32\n",
      "Name: Apache_Kafka, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: Apache_Kafka, dtype: int64, 'Japan': False    42\n",
      "Name: Apache_Kafka, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Apache_Kafka, dtype: int64, 'Netherlands': False    40\n",
      "Name: Apache_Kafka, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Kafka, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Kafka, dtype: int64, 'Poland': False    107\n",
      "True       2\n",
      "Name: Apache_Kafka, dtype: int64, 'Portugal': False    181\n",
      "True       8\n",
      "Name: Apache_Kafka, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Apache_Kafka, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Apache_Kafka, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Kafka, dtype: int64, 'Spain': False    122\n",
      "True       4\n",
      "Name: Apache_Kafka, dtype: int64, 'Sweden': False    142\n",
      "True       3\n",
      "Name: Apache_Kafka, dtype: int64, 'Switzerland': False    59\n",
      "True      1\n",
      "Name: Apache_Kafka, dtype: int64, 'Taiwan': False    40\n",
      "Name: Apache_Kafka, dtype: int64, 'Turkey': False    21\n",
      "True      1\n",
      "Name: Apache_Kafka, dtype: int64, 'United_Kingdom': False    117\n",
      "True       1\n",
      "Name: Apache_Kafka, dtype: int64, 'United_States': False    233\n",
      "True       8\n",
      "Name: Apache_Kafka, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Kafka\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Kafka'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Flink"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data streams at a large scale and to deliver real-time analytical insights about your processed data with your streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Apache_Flink, dtype: int64, 'Austria': False    139\n",
      "Name: Apache_Flink, dtype: int64, 'Belgium': False    64\n",
      "Name: Apache_Flink, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Flink, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Apache_Flink, dtype: int64, 'Denmark': False    91\n",
      "Name: Apache_Flink, dtype: int64, 'Finland': False    58\n",
      "Name: Apache_Flink, dtype: int64, 'France': False    226\n",
      "True       1\n",
      "Name: Apache_Flink, dtype: int64, 'Germany': False    164\n",
      "Name: Apache_Flink, dtype: int64, 'Greece': False    56\n",
      "Name: Apache_Flink, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Apache_Flink, dtype: int64, 'Hungary': False    86\n",
      "Name: Apache_Flink, dtype: int64, 'Ireland': False    75\n",
      "Name: Apache_Flink, dtype: int64, 'Israel': False    244\n",
      "True       8\n",
      "Name: Apache_Flink, dtype: int64, 'Italy': False    80\n",
      "Name: Apache_Flink, dtype: int64, 'Japan': False    42\n",
      "Name: Apache_Flink, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Apache_Flink, dtype: int64, 'Netherlands': False    40\n",
      "Name: Apache_Flink, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Flink, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Flink, dtype: int64, 'Poland': False    109\n",
      "Name: Apache_Flink, dtype: int64, 'Portugal': False    189\n",
      "Name: Apache_Flink, dtype: int64, 'Romania': False    123\n",
      "Name: Apache_Flink, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: Apache_Flink, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Flink, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Apache_Flink, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: Apache_Flink, dtype: int64, 'Switzerland': False    60\n",
      "Name: Apache_Flink, dtype: int64, 'Taiwan': False    40\n",
      "Name: Apache_Flink, dtype: int64, 'Turkey': False    22\n",
      "Name: Apache_Flink, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Apache_Flink, dtype: int64, 'United_States': False    239\n",
      "True       2\n",
      "Name: Apache_Flink, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Flink\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Flink'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Dataflow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow is a managed service provided by Google Cloud for building and executing data processing pipelines. It enables developers to create scalable and efficient batch and streaming data pipelines using a simple programming model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Dataflow, dtype: int64, 'Austria': False    139\n",
      "Name: Dataflow, dtype: int64, 'Belgium': False    64\n",
      "Name: Dataflow, dtype: int64, 'Canada': False    4\n",
      "Name: Dataflow, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Dataflow, dtype: int64, 'Denmark': False    91\n",
      "Name: Dataflow, dtype: int64, 'Finland': False    58\n",
      "Name: Dataflow, dtype: int64, 'France': False    225\n",
      "True       2\n",
      "Name: Dataflow, dtype: int64, 'Germany': False    164\n",
      "Name: Dataflow, dtype: int64, 'Greece': False    56\n",
      "Name: Dataflow, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Dataflow, dtype: int64, 'Hungary': False    86\n",
      "Name: Dataflow, dtype: int64, 'Ireland': False    75\n",
      "Name: Dataflow, dtype: int64, 'Israel': False    252\n",
      "Name: Dataflow, dtype: int64, 'Italy': False    80\n",
      "Name: Dataflow, dtype: int64, 'Japan': False    42\n",
      "Name: Dataflow, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Dataflow, dtype: int64, 'Netherlands': False    40\n",
      "Name: Dataflow, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Dataflow, dtype: int64, 'Norway': False    31\n",
      "Name: Dataflow, dtype: int64, 'Poland': False    109\n",
      "Name: Dataflow, dtype: int64, 'Portugal': False    189\n",
      "Name: Dataflow, dtype: int64, 'Romania': False    123\n",
      "Name: Dataflow, dtype: int64, 'Singapore': False    134\n",
      "Name: Dataflow, dtype: int64, 'South_Korea': False    46\n",
      "Name: Dataflow, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Dataflow, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Dataflow, dtype: int64, 'Switzerland': False    60\n",
      "Name: Dataflow, dtype: int64, 'Taiwan': False    40\n",
      "Name: Dataflow, dtype: int64, 'Turkey': False    22\n",
      "Name: Dataflow, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Dataflow, dtype: int64, 'United_States': False    239\n",
      "True       2\n",
      "Name: Dataflow, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Dataflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dataflow'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 Workflow orchestration tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.1 Apache Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is an open-source platform used for programmatically creating, scheduling, and monitoring complex workflows or data pipelines. It allows users to define and execute a sequence of tasks or operations, while providing tools for tracking and troubleshooting workflow executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Apache_Airflow, dtype: int64, 'Austria': False    136\n",
      "True       3\n",
      "Name: Apache_Airflow, dtype: int64, 'Belgium': False    62\n",
      "True      2\n",
      "Name: Apache_Airflow, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Airflow, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Apache_Airflow, dtype: int64, 'Denmark': False    89\n",
      "True      2\n",
      "Name: Apache_Airflow, dtype: int64, 'Finland': False    58\n",
      "Name: Apache_Airflow, dtype: int64, 'France': False    206\n",
      "True      21\n",
      "Name: Apache_Airflow, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Apache_Airflow, dtype: int64, 'Greece': False    56\n",
      "Name: Apache_Airflow, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Apache_Airflow, dtype: int64, 'Hungary': False    86\n",
      "Name: Apache_Airflow, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Apache_Airflow, dtype: int64, 'Israel': False    240\n",
      "True      12\n",
      "Name: Apache_Airflow, dtype: int64, 'Italy': False    80\n",
      "Name: Apache_Airflow, dtype: int64, 'Japan': False    42\n",
      "Name: Apache_Airflow, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Apache_Airflow, dtype: int64, 'Netherlands': False    38\n",
      "True      2\n",
      "Name: Apache_Airflow, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Airflow, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Airflow, dtype: int64, 'Poland': False    106\n",
      "True       3\n",
      "Name: Apache_Airflow, dtype: int64, 'Portugal': False    185\n",
      "True       4\n",
      "Name: Apache_Airflow, dtype: int64, 'Romania': False    123\n",
      "Name: Apache_Airflow, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Apache_Airflow, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Airflow, dtype: int64, 'Spain': False    123\n",
      "True       3\n",
      "Name: Apache_Airflow, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: Apache_Airflow, dtype: int64, 'Switzerland': False    60\n",
      "Name: Apache_Airflow, dtype: int64, 'Taiwan': False    40\n",
      "Name: Apache_Airflow, dtype: int64, 'Turkey': False    22\n",
      "Name: Apache_Airflow, dtype: int64, 'United_Kingdom': False    117\n",
      "True       1\n",
      "Name: Apache_Airflow, dtype: int64, 'United_States': False    237\n",
      "True       4\n",
      "Name: Apache_Airflow, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Airflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Airflow'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.2 Luigi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luigi is a Python-based open-source workflow management system that helps to build complex pipelines of batch jobs. It provides a flexible and extensible architecture to create and manage complex data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Luigi, dtype: int64, 'Austria': False    139\n",
      "Name: Luigi, dtype: int64, 'Belgium': False    64\n",
      "Name: Luigi, dtype: int64, 'Canada': False    4\n",
      "Name: Luigi, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Luigi, dtype: int64, 'Denmark': False    91\n",
      "Name: Luigi, dtype: int64, 'Finland': False    58\n",
      "Name: Luigi, dtype: int64, 'France': False    227\n",
      "Name: Luigi, dtype: int64, 'Germany': False    164\n",
      "Name: Luigi, dtype: int64, 'Greece': False    56\n",
      "Name: Luigi, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Luigi, dtype: int64, 'Hungary': False    86\n",
      "Name: Luigi, dtype: int64, 'Ireland': False    75\n",
      "Name: Luigi, dtype: int64, 'Israel': False    252\n",
      "Name: Luigi, dtype: int64, 'Italy': False    80\n",
      "Name: Luigi, dtype: int64, 'Japan': False    42\n",
      "Name: Luigi, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Luigi, dtype: int64, 'Netherlands': False    40\n",
      "Name: Luigi, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Luigi, dtype: int64, 'Norway': False    31\n",
      "Name: Luigi, dtype: int64, 'Poland': False    109\n",
      "Name: Luigi, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Luigi, dtype: int64, 'Romania': False    123\n",
      "Name: Luigi, dtype: int64, 'Singapore': False    134\n",
      "Name: Luigi, dtype: int64, 'South_Korea': False    46\n",
      "Name: Luigi, dtype: int64, 'Spain': False    126\n",
      "Name: Luigi, dtype: int64, 'Sweden': False    145\n",
      "Name: Luigi, dtype: int64, 'Switzerland': False    60\n",
      "Name: Luigi, dtype: int64, 'Taiwan': False    40\n",
      "Name: Luigi, dtype: int64, 'Turkey': False    22\n",
      "Name: Luigi, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Luigi, dtype: int64, 'United_States': False    241\n",
      "Name: Luigi, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Luigi\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Luigi'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.3 SSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Server Integration Services (SSIS) is a Microsoft tool used for building data integration and ETL (extract, transform, load) workflows. It allows users to perform a range of tasks such as data extraction, transformation, and loading from various sources to different destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: SSIS, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: SSIS, dtype: int64, 'Belgium': False    61\n",
      "True      3\n",
      "Name: SSIS, dtype: int64, 'Canada': False    4\n",
      "Name: SSIS, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: SSIS, dtype: int64, 'Denmark': False    91\n",
      "Name: SSIS, dtype: int64, 'Finland': False    58\n",
      "Name: SSIS, dtype: int64, 'France': False    222\n",
      "True       5\n",
      "Name: SSIS, dtype: int64, 'Germany': False    164\n",
      "Name: SSIS, dtype: int64, 'Greece': False    56\n",
      "Name: SSIS, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: SSIS, dtype: int64, 'Hungary': False    86\n",
      "Name: SSIS, dtype: int64, 'Ireland': False    75\n",
      "Name: SSIS, dtype: int64, 'Israel': False    249\n",
      "True       3\n",
      "Name: SSIS, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: SSIS, dtype: int64, 'Japan': False    42\n",
      "Name: SSIS, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: SSIS, dtype: int64, 'Netherlands': False    38\n",
      "True      2\n",
      "Name: SSIS, dtype: int64, 'New_Zealand': False    52\n",
      "Name: SSIS, dtype: int64, 'Norway': False    31\n",
      "Name: SSIS, dtype: int64, 'Poland': False    109\n",
      "Name: SSIS, dtype: int64, 'Portugal': False    189\n",
      "Name: SSIS, dtype: int64, 'Romania': False    123\n",
      "Name: SSIS, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: SSIS, dtype: int64, 'South_Korea': False    46\n",
      "Name: SSIS, dtype: int64, 'Spain': False    126\n",
      "Name: SSIS, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: SSIS, dtype: int64, 'Switzerland': False    60\n",
      "Name: SSIS, dtype: int64, 'Taiwan': False    40\n",
      "Name: SSIS, dtype: int64, 'Turkey': False    22\n",
      "Name: SSIS, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: SSIS, dtype: int64, 'United_States': False    238\n",
      "True       3\n",
      "Name: SSIS, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"SSIS\",\n",
    "    r\"SQL Server Integration Services\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SSIS'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Big Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Hadoop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hadoop is an open-source framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It provides a distributed file system and supports various distributed computing models, such as MapReduce and Spark, for processing and analyzing large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Belgium': False    64\n",
      "Name: Apache_Hadoop, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Hadoop, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Denmark': False    91\n",
      "Name: Apache_Hadoop, dtype: int64, 'Finland': False    58\n",
      "Name: Apache_Hadoop, dtype: int64, 'France': False    217\n",
      "True      10\n",
      "Name: Apache_Hadoop, dtype: int64, 'Germany': False    164\n",
      "Name: Apache_Hadoop, dtype: int64, 'Greece': False    56\n",
      "Name: Apache_Hadoop, dtype: int64, 'Hong_Kong': False    103\n",
      "True       2\n",
      "Name: Apache_Hadoop, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Apache_Hadoop, dtype: int64, 'Israel': False    234\n",
      "True      18\n",
      "Name: Apache_Hadoop, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: Apache_Hadoop, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Netherlands': False    40\n",
      "Name: Apache_Hadoop, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Hadoop, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Hadoop, dtype: int64, 'Poland': False    107\n",
      "True       2\n",
      "Name: Apache_Hadoop, dtype: int64, 'Portugal': False    184\n",
      "True       5\n",
      "Name: Apache_Hadoop, dtype: int64, 'Romania': False    120\n",
      "True       3\n",
      "Name: Apache_Hadoop, dtype: int64, 'Singapore': False    125\n",
      "True       9\n",
      "Name: Apache_Hadoop, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Hadoop, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Apache_Hadoop, dtype: int64, 'Switzerland': False    60\n",
      "Name: Apache_Hadoop, dtype: int64, 'Taiwan': False    38\n",
      "True      2\n",
      "Name: Apache_Hadoop, dtype: int64, 'Turkey': False    22\n",
      "Name: Apache_Hadoop, dtype: int64, 'United_Kingdom': False    114\n",
      "True       4\n",
      "Name: Apache_Hadoop, dtype: int64, 'United_States': False    228\n",
      "True      13\n",
      "Name: Apache_Hadoop, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Hadoop\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hadoop'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Hive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hive is a data warehouse software that facilitates querying and managing large datasets stored in Hadoop file systems using a SQL-like language called HiveQL. It provides a high-level interface for data analysts and developers to analyze, transform, and summarize data stored in Hadoop Distributed File System (HDFS) and other compatible storage systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Apache_Hive, dtype: int64, 'Austria': False    139\n",
      "Name: Apache_Hive, dtype: int64, 'Belgium': False    64\n",
      "Name: Apache_Hive, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Hive, dtype: int64, 'Czech_Republic': False    65\n",
      "True      3\n",
      "Name: Apache_Hive, dtype: int64, 'Denmark': False    91\n",
      "Name: Apache_Hive, dtype: int64, 'Finland': False    58\n",
      "Name: Apache_Hive, dtype: int64, 'France': False    218\n",
      "True       9\n",
      "Name: Apache_Hive, dtype: int64, 'Germany': False    164\n",
      "Name: Apache_Hive, dtype: int64, 'Greece': False    56\n",
      "Name: Apache_Hive, dtype: int64, 'Hong_Kong': False    103\n",
      "True       2\n",
      "Name: Apache_Hive, dtype: int64, 'Hungary': False    86\n",
      "Name: Apache_Hive, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Apache_Hive, dtype: int64, 'Israel': False    239\n",
      "True      13\n",
      "Name: Apache_Hive, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: Apache_Hive, dtype: int64, 'Japan': False    42\n",
      "Name: Apache_Hive, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Apache_Hive, dtype: int64, 'Netherlands': False    40\n",
      "Name: Apache_Hive, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Hive, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Hive, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: Apache_Hive, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Apache_Hive, dtype: int64, 'Romania': False    123\n",
      "Name: Apache_Hive, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: Apache_Hive, dtype: int64, 'South_Korea': False    46\n",
      "Name: Apache_Hive, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Apache_Hive, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Apache_Hive, dtype: int64, 'Switzerland': False    60\n",
      "Name: Apache_Hive, dtype: int64, 'Taiwan': False    40\n",
      "Name: Apache_Hive, dtype: int64, 'Turkey': False    22\n",
      "Name: Apache_Hive, dtype: int64, 'United_Kingdom': False    115\n",
      "True       3\n",
      "Name: Apache_Hive, dtype: int64, 'United_States': False    231\n",
      "True      10\n",
      "Name: Apache_Hive, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Hive\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hive'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a distributed computing framework designed to process large-scale data processing and analysis workloads in parallel. It can be used for batch processing, real-time stream processing, machine learning, and graph processing, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    63\n",
      "True      4\n",
      "Name: Apache_Spark, dtype: int64, 'Austria': False    134\n",
      "True       5\n",
      "Name: Apache_Spark, dtype: int64, 'Belgium': False    62\n",
      "True      2\n",
      "Name: Apache_Spark, dtype: int64, 'Canada': False    4\n",
      "Name: Apache_Spark, dtype: int64, 'Czech_Republic': False    60\n",
      "True      8\n",
      "Name: Apache_Spark, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Finland': False    54\n",
      "True      4\n",
      "Name: Apache_Spark, dtype: int64, 'France': False    174\n",
      "True      53\n",
      "Name: Apache_Spark, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Apache_Spark, dtype: int64, 'Greece': False    53\n",
      "True      3\n",
      "Name: Apache_Spark, dtype: int64, 'Hong_Kong': False    101\n",
      "True       4\n",
      "Name: Apache_Spark, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: Apache_Spark, dtype: int64, 'Israel': False    197\n",
      "True      55\n",
      "Name: Apache_Spark, dtype: int64, 'Italy': False    75\n",
      "True      5\n",
      "Name: Apache_Spark, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Netherlands': False    38\n",
      "True      2\n",
      "Name: Apache_Spark, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Apache_Spark, dtype: int64, 'Norway': False    31\n",
      "Name: Apache_Spark, dtype: int64, 'Poland': False    101\n",
      "True       8\n",
      "Name: Apache_Spark, dtype: int64, 'Portugal': False    167\n",
      "True      22\n",
      "Name: Apache_Spark, dtype: int64, 'Romania': False    115\n",
      "True       8\n",
      "Name: Apache_Spark, dtype: int64, 'Singapore': False    125\n",
      "True       9\n",
      "Name: Apache_Spark, dtype: int64, 'South_Korea': False    45\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Spain': False    109\n",
      "True      17\n",
      "Name: Apache_Spark, dtype: int64, 'Sweden': False    141\n",
      "True       4\n",
      "Name: Apache_Spark, dtype: int64, 'Switzerland': False    59\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'Taiwan': False    38\n",
      "True      2\n",
      "Name: Apache_Spark, dtype: int64, 'Turkey': False    21\n",
      "True      1\n",
      "Name: Apache_Spark, dtype: int64, 'United_Kingdom': False    108\n",
      "True      10\n",
      "Name: Apache_Spark, dtype: int64, 'United_States': False    206\n",
      "True      35\n",
      "Name: Apache_Spark, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Spark\",\n",
    "    r\"PySpark\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Spark'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25. OS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.1 Linux"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free and open-source operating system based on the Unix system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Linux, dtype: int64, 'Belgium': False    64\n",
      "Name: Linux, dtype: int64, 'Canada': False    4\n",
      "Name: Linux, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Denmark': False    89\n",
      "True      2\n",
      "Name: Linux, dtype: int64, 'Finland': False    58\n",
      "Name: Linux, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: Linux, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Linux, dtype: int64, 'Greece': False    54\n",
      "True      2\n",
      "Name: Linux, dtype: int64, 'Hong_Kong': False    100\n",
      "True       5\n",
      "Name: Linux, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Israel': False    237\n",
      "True      15\n",
      "Name: Linux, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Linux, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'New_Zealand': False    49\n",
      "True      3\n",
      "Name: Linux, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Linux, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: Linux, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Linux, dtype: int64, 'Romania': False    119\n",
      "True       4\n",
      "Name: Linux, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Linux, dtype: int64, 'South_Korea': False    44\n",
      "True      2\n",
      "Name: Linux, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Linux, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: Linux, dtype: int64, 'Switzerland': False    60\n",
      "Name: Linux, dtype: int64, 'Taiwan': False    36\n",
      "True      4\n",
      "Name: Linux, dtype: int64, 'Turkey': False    22\n",
      "Name: Linux, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Linux, dtype: int64, 'United_States': False    238\n",
      "True       3\n",
      "Name: Linux, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Linux\", r\"Ubuntu\", r\"CentOS\", r\"Red Hat\", r\"Debian\", r\"Fedora\", r\"openSUSE\", r\"RHEL\", r\"Gentoo\", r\"Kali\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Linux'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.2 Unix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Unix, dtype: int64, 'Austria': False    139\n",
      "Name: Unix, dtype: int64, 'Belgium': False    64\n",
      "Name: Unix, dtype: int64, 'Canada': False    4\n",
      "Name: Unix, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Unix, dtype: int64, 'Denmark': False    91\n",
      "Name: Unix, dtype: int64, 'Finland': False    58\n",
      "Name: Unix, dtype: int64, 'France': False    223\n",
      "True       4\n",
      "Name: Unix, dtype: int64, 'Germany': False    164\n",
      "Name: Unix, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Hong_Kong': False    103\n",
      "True       2\n",
      "Name: Unix, dtype: int64, 'Hungary': False    86\n",
      "Name: Unix, dtype: int64, 'Ireland': False    75\n",
      "Name: Unix, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Unix, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Japan': False    42\n",
      "Name: Unix, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Netherlands': False    40\n",
      "Name: Unix, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Unix, dtype: int64, 'Norway': False    31\n",
      "Name: Unix, dtype: int64, 'Poland': False    109\n",
      "Name: Unix, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Unix, dtype: int64, 'Romania': False    123\n",
      "Name: Unix, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: Unix, dtype: int64, 'South_Korea': False    46\n",
      "Name: Unix, dtype: int64, 'Spain': False    126\n",
      "Name: Unix, dtype: int64, 'Sweden': False    145\n",
      "Name: Unix, dtype: int64, 'Switzerland': False    60\n",
      "Name: Unix, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Turkey': False    22\n",
      "Name: Unix, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Unix, dtype: int64, 'United_States': False    236\n",
      "True       5\n",
      "Name: Unix, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Solaris\", r\"AIX\", r\"HP-UX\", r\"BSD\", r\"IRIX\", r\"SCO Unix\", r\"Xenix\", r\"OpenServer\", r\"Unix\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Unix'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Unix, dtype: int64, 'Austria': False    139\n",
      "Name: Unix, dtype: int64, 'Belgium': False    64\n",
      "Name: Unix, dtype: int64, 'Canada': False    4\n",
      "Name: Unix, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Unix, dtype: int64, 'Denmark': False    91\n",
      "Name: Unix, dtype: int64, 'Finland': False    58\n",
      "Name: Unix, dtype: int64, 'France': False    223\n",
      "True       4\n",
      "Name: Unix, dtype: int64, 'Germany': False    164\n",
      "Name: Unix, dtype: int64, 'Greece': False    56\n",
      "Name: Unix, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: Unix, dtype: int64, 'Hungary': False    86\n",
      "Name: Unix, dtype: int64, 'Ireland': False    75\n",
      "Name: Unix, dtype: int64, 'Israel': False    252\n",
      "Name: Unix, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Japan': False    42\n",
      "Name: Unix, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Unix, dtype: int64, 'Netherlands': False    40\n",
      "Name: Unix, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Unix, dtype: int64, 'Norway': False    31\n",
      "Name: Unix, dtype: int64, 'Poland': False    109\n",
      "Name: Unix, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Unix, dtype: int64, 'Romania': False    123\n",
      "Name: Unix, dtype: int64, 'Singapore': False    132\n",
      "True       2\n",
      "Name: Unix, dtype: int64, 'South_Korea': False    46\n",
      "Name: Unix, dtype: int64, 'Spain': False    126\n",
      "Name: Unix, dtype: int64, 'Sweden': False    145\n",
      "Name: Unix, dtype: int64, 'Switzerland': False    60\n",
      "Name: Unix, dtype: int64, 'Taiwan': False    40\n",
      "Name: Unix, dtype: int64, 'Turkey': False    22\n",
      "Name: Unix, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Unix, dtype: int64, 'United_States': False    237\n",
      "True       4\n",
      "Name: Unix, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "for country, df in dfs.items():\n",
    "    dfs[country]['Unix'] = df.apply(lambda row: False if row['Unix'] and row['Linux'] else row['Unix'], axis=1)\n",
    "\n",
    "show_results(column_name, dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.3 Windows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A family of operating systems developed by Microsoft Corporation primarily for personal computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Windows, dtype: int64, 'Austria': False    139\n",
      "Name: Windows, dtype: int64, 'Belgium': False    64\n",
      "Name: Windows, dtype: int64, 'Canada': False    4\n",
      "Name: Windows, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Windows, dtype: int64, 'Denmark': False    91\n",
      "Name: Windows, dtype: int64, 'Finland': False    58\n",
      "Name: Windows, dtype: int64, 'France': False    224\n",
      "True       3\n",
      "Name: Windows, dtype: int64, 'Germany': False    162\n",
      "True       2\n",
      "Name: Windows, dtype: int64, 'Greece': False    56\n",
      "Name: Windows, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: Windows, dtype: int64, 'Hungary': False    86\n",
      "Name: Windows, dtype: int64, 'Ireland': False    75\n",
      "Name: Windows, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Windows, dtype: int64, 'Italy': False    80\n",
      "Name: Windows, dtype: int64, 'Japan': False    42\n",
      "Name: Windows, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Windows, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: Windows, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Windows, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Windows, dtype: int64, 'Poland': False    109\n",
      "Name: Windows, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Windows, dtype: int64, 'Romania': False    123\n",
      "Name: Windows, dtype: int64, 'Singapore': False    134\n",
      "Name: Windows, dtype: int64, 'South_Korea': False    46\n",
      "Name: Windows, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Windows, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Windows, dtype: int64, 'Switzerland': False    60\n",
      "Name: Windows, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Windows, dtype: int64, 'Turkey': False    22\n",
      "Name: Windows, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Windows, dtype: int64, 'United_States': False    241\n",
      "Name: Windows, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# If you have some soft like \"Windows Remote Desktop\", it's logical that it has to run on Windows, and there the system is required.\n",
    "\n",
    "tool_names = [\n",
    "    r\"Windows\", r\"WinNT\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Windows'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.4 macOS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proprietary operating system developed by Apple Inc. for its Macintosh line of computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: macOS, dtype: int64, 'Austria': False    139\n",
      "Name: macOS, dtype: int64, 'Belgium': False    64\n",
      "Name: macOS, dtype: int64, 'Canada': False    4\n",
      "Name: macOS, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: macOS, dtype: int64, 'Denmark': False    91\n",
      "Name: macOS, dtype: int64, 'Finland': False    58\n",
      "Name: macOS, dtype: int64, 'France': False    227\n",
      "Name: macOS, dtype: int64, 'Germany': False    164\n",
      "Name: macOS, dtype: int64, 'Greece': False    56\n",
      "Name: macOS, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: macOS, dtype: int64, 'Hungary': False    86\n",
      "Name: macOS, dtype: int64, 'Ireland': False    75\n",
      "Name: macOS, dtype: int64, 'Israel': False    252\n",
      "Name: macOS, dtype: int64, 'Italy': False    80\n",
      "Name: macOS, dtype: int64, 'Japan': False    42\n",
      "Name: macOS, dtype: int64, 'Luxembourg': False    40\n",
      "Name: macOS, dtype: int64, 'Netherlands': False    40\n",
      "Name: macOS, dtype: int64, 'New_Zealand': False    52\n",
      "Name: macOS, dtype: int64, 'Norway': False    31\n",
      "Name: macOS, dtype: int64, 'Poland': False    109\n",
      "Name: macOS, dtype: int64, 'Portugal': False    189\n",
      "Name: macOS, dtype: int64, 'Romania': False    123\n",
      "Name: macOS, dtype: int64, 'Singapore': False    134\n",
      "Name: macOS, dtype: int64, 'South_Korea': False    46\n",
      "Name: macOS, dtype: int64, 'Spain': False    126\n",
      "Name: macOS, dtype: int64, 'Sweden': False    145\n",
      "Name: macOS, dtype: int64, 'Switzerland': False    60\n",
      "Name: macOS, dtype: int64, 'Taiwan': False    40\n",
      "Name: macOS, dtype: int64, 'Turkey': False    22\n",
      "Name: macOS, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: macOS, dtype: int64, 'United_States': False    241\n",
      "Name: macOS, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"macOS\"\n",
    "    ]\n",
    "\n",
    "column_name = 'macOS'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Programming languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.1 Python 🐍"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a high-level, interpreted programming language used for various purposes such as web development, data analysis, artificial intelligence, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    63\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'Austria': False    133\n",
      "True       6\n",
      "Name: Python, dtype: int64, 'Belgium': False    56\n",
      "True      8\n",
      "Name: Python, dtype: int64, 'Canada': False    4\n",
      "Name: Python, dtype: int64, 'Czech_Republic': False    62\n",
      "True      6\n",
      "Name: Python, dtype: int64, 'Denmark': False    88\n",
      "True      3\n",
      "Name: Python, dtype: int64, 'Finland': False    54\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'France': False    168\n",
      "True      59\n",
      "Name: Python, dtype: int64, 'Germany': False    153\n",
      "True      11\n",
      "Name: Python, dtype: int64, 'Greece': False    51\n",
      "True      5\n",
      "Name: Python, dtype: int64, 'Hong_Kong': False    87\n",
      "True     18\n",
      "Name: Python, dtype: int64, 'Hungary': False    82\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'Ireland': False    71\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'Israel': False    137\n",
      "True     115\n",
      "Name: Python, dtype: int64, 'Italy': False    75\n",
      "True      5\n",
      "Name: Python, dtype: int64, 'Japan': False    38\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'Luxembourg': False    37\n",
      "True      3\n",
      "Name: Python, dtype: int64, 'Netherlands': False    36\n",
      "True      4\n",
      "Name: Python, dtype: int64, 'New_Zealand': False    49\n",
      "True      3\n",
      "Name: Python, dtype: int64, 'Norway': False    31\n",
      "Name: Python, dtype: int64, 'Poland': False    89\n",
      "True     20\n",
      "Name: Python, dtype: int64, 'Portugal': False    172\n",
      "True      17\n",
      "Name: Python, dtype: int64, 'Romania': False    113\n",
      "True      10\n",
      "Name: Python, dtype: int64, 'Singapore': False    119\n",
      "True      15\n",
      "Name: Python, dtype: int64, 'South_Korea': False    43\n",
      "True      3\n",
      "Name: Python, dtype: int64, 'Spain': False    115\n",
      "True      11\n",
      "Name: Python, dtype: int64, 'Sweden': False    136\n",
      "True       9\n",
      "Name: Python, dtype: int64, 'Switzerland': False    60\n",
      "Name: Python, dtype: int64, 'Taiwan': False    35\n",
      "True      5\n",
      "Name: Python, dtype: int64, 'Turkey': False    20\n",
      "True      2\n",
      "Name: Python, dtype: int64, 'United_Kingdom': False    104\n",
      "True      14\n",
      "Name: Python, dtype: int64, 'United_States': False    194\n",
      "True      47\n",
      "Name: Python, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Python\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Python'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.2 R 👴🏻"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language and environment for statistical graphics and computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: R, dtype: int64, 'Austria': False    137\n",
      "True       2\n",
      "Name: R, dtype: int64, 'Belgium': False    60\n",
      "True      4\n",
      "Name: R, dtype: int64, 'Canada': False    4\n",
      "Name: R, dtype: int64, 'Czech_Republic': False    65\n",
      "True      3\n",
      "Name: R, dtype: int64, 'Denmark': False    87\n",
      "True      4\n",
      "Name: R, dtype: int64, 'Finland': False    58\n",
      "Name: R, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: R, dtype: int64, 'Germany': False    159\n",
      "True       5\n",
      "Name: R, dtype: int64, 'Greece': False    56\n",
      "Name: R, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: R, dtype: int64, 'Hungary': False    86\n",
      "Name: R, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: R, dtype: int64, 'Israel': False    239\n",
      "True      13\n",
      "Name: R, dtype: int64, 'Italy': False    70\n",
      "True     10\n",
      "Name: R, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: R, dtype: int64, 'Luxembourg': False    40\n",
      "Name: R, dtype: int64, 'Netherlands': False    40\n",
      "Name: R, dtype: int64, 'New_Zealand': False    50\n",
      "True      2\n",
      "Name: R, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: R, dtype: int64, 'Poland': False    107\n",
      "True       2\n",
      "Name: R, dtype: int64, 'Portugal': False    183\n",
      "True       6\n",
      "Name: R, dtype: int64, 'Romania': False    123\n",
      "Name: R, dtype: int64, 'Singapore': False    131\n",
      "True       3\n",
      "Name: R, dtype: int64, 'South_Korea': False    43\n",
      "True      3\n",
      "Name: R, dtype: int64, 'Spain': False    121\n",
      "True       5\n",
      "Name: R, dtype: int64, 'Sweden': False    143\n",
      "True       2\n",
      "Name: R, dtype: int64, 'Switzerland': False    60\n",
      "Name: R, dtype: int64, 'Taiwan': False    40\n",
      "Name: R, dtype: int64, 'Turkey': False    22\n",
      "Name: R, dtype: int64, 'United_Kingdom': False    116\n",
      "True       2\n",
      "Name: R, dtype: int64, 'United_States': False    235\n",
      "True       6\n",
      "Name: R, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    # Avoid They'r etc.\n",
    "    r\"(?<!')[rR]\",\n",
    "    r\"RStudio\"\n",
    "    ]\n",
    "\n",
    "column_name = 'R'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.3 Scala"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala is a high-level, statically typed programming language designed for functional programming and scalable, concurrent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    64\n",
      "True      3\n",
      "Name: Scala, dtype: int64, 'Austria': False    139\n",
      "Name: Scala, dtype: int64, 'Belgium': False    63\n",
      "True      1\n",
      "Name: Scala, dtype: int64, 'Canada': False    4\n",
      "Name: Scala, dtype: int64, 'Czech_Republic': False    67\n",
      "True      1\n",
      "Name: Scala, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Scala, dtype: int64, 'Finland': False    58\n",
      "Name: Scala, dtype: int64, 'France': False    206\n",
      "True      21\n",
      "Name: Scala, dtype: int64, 'Germany': False    164\n",
      "Name: Scala, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Scala, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: Scala, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Scala, dtype: int64, 'Ireland': False    75\n",
      "Name: Scala, dtype: int64, 'Israel': False    211\n",
      "True      41\n",
      "Name: Scala, dtype: int64, 'Italy': False    76\n",
      "True      4\n",
      "Name: Scala, dtype: int64, 'Japan': False    42\n",
      "Name: Scala, dtype: int64, 'Luxembourg': False    38\n",
      "True      2\n",
      "Name: Scala, dtype: int64, 'Netherlands': False    40\n",
      "Name: Scala, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Scala, dtype: int64, 'Norway': False    31\n",
      "Name: Scala, dtype: int64, 'Poland': False    107\n",
      "True       2\n",
      "Name: Scala, dtype: int64, 'Portugal': False    183\n",
      "True       6\n",
      "Name: Scala, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Scala, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Scala, dtype: int64, 'South_Korea': False    46\n",
      "Name: Scala, dtype: int64, 'Spain': False    121\n",
      "True       5\n",
      "Name: Scala, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: Scala, dtype: int64, 'Switzerland': False    60\n",
      "Name: Scala, dtype: int64, 'Taiwan': False    40\n",
      "Name: Scala, dtype: int64, 'Turkey': False    22\n",
      "Name: Scala, dtype: int64, 'United_Kingdom': False    115\n",
      "True       3\n",
      "Name: Scala, dtype: int64, 'United_States': False    227\n",
      "True      14\n",
      "Name: Scala, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Scala\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Scala'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.4 Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is a high-level, high-performance programming language that is particularly suited for scientific computing, numerical analysis, and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Julia, dtype: int64, 'Austria': False    139\n",
      "Name: Julia, dtype: int64, 'Belgium': False    64\n",
      "Name: Julia, dtype: int64, 'Canada': False    4\n",
      "Name: Julia, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Julia, dtype: int64, 'Denmark': False    91\n",
      "Name: Julia, dtype: int64, 'Finland': False    58\n",
      "Name: Julia, dtype: int64, 'France': False    227\n",
      "Name: Julia, dtype: int64, 'Germany': False    164\n",
      "Name: Julia, dtype: int64, 'Greece': False    56\n",
      "Name: Julia, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Julia, dtype: int64, 'Hungary': False    86\n",
      "Name: Julia, dtype: int64, 'Ireland': False    75\n",
      "Name: Julia, dtype: int64, 'Israel': False    252\n",
      "Name: Julia, dtype: int64, 'Italy': False    80\n",
      "Name: Julia, dtype: int64, 'Japan': False    42\n",
      "Name: Julia, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Julia, dtype: int64, 'Netherlands': False    40\n",
      "Name: Julia, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Julia, dtype: int64, 'Norway': False    31\n",
      "Name: Julia, dtype: int64, 'Poland': False    109\n",
      "Name: Julia, dtype: int64, 'Portugal': False    189\n",
      "Name: Julia, dtype: int64, 'Romania': False    123\n",
      "Name: Julia, dtype: int64, 'Singapore': False    134\n",
      "Name: Julia, dtype: int64, 'South_Korea': False    46\n",
      "Name: Julia, dtype: int64, 'Spain': False    126\n",
      "Name: Julia, dtype: int64, 'Sweden': False    145\n",
      "Name: Julia, dtype: int64, 'Switzerland': False    60\n",
      "Name: Julia, dtype: int64, 'Taiwan': False    40\n",
      "Name: Julia, dtype: int64, 'Turkey': False    22\n",
      "Name: Julia, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Julia, dtype: int64, 'United_States': False    241\n",
      "Name: Julia, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Julia\",\n",
    "    r\"JuliaLang \",\n",
    "    ]\n",
    "\n",
    "column_name = 'Julia'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.4 SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language used to manage and manipulate relational databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    63\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'Austria': False    125\n",
      "True      14\n",
      "Name: SQL, dtype: int64, 'Belgium': False    52\n",
      "True     12\n",
      "Name: SQL, dtype: int64, 'Canada': False    4\n",
      "Name: SQL, dtype: int64, 'Czech_Republic': False    62\n",
      "True      6\n",
      "Name: SQL, dtype: int64, 'Denmark': False    86\n",
      "True      5\n",
      "Name: SQL, dtype: int64, 'Finland': False    54\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'France': False    173\n",
      "True      54\n",
      "Name: SQL, dtype: int64, 'Germany': False    154\n",
      "True      10\n",
      "Name: SQL, dtype: int64, 'Greece': False    52\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'Hong_Kong': False    91\n",
      "True     14\n",
      "Name: SQL, dtype: int64, 'Hungary': False    81\n",
      "True      5\n",
      "Name: SQL, dtype: int64, 'Ireland': False    70\n",
      "True      5\n",
      "Name: SQL, dtype: int64, 'Israel': False    187\n",
      "True      65\n",
      "Name: SQL, dtype: int64, 'Italy': False    71\n",
      "True      9\n",
      "Name: SQL, dtype: int64, 'Japan': False    38\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'Luxembourg': False    34\n",
      "True      6\n",
      "Name: SQL, dtype: int64, 'Netherlands': False    36\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'New_Zealand': False    48\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: SQL, dtype: int64, 'Poland': False    95\n",
      "True     14\n",
      "Name: SQL, dtype: int64, 'Portugal': False    160\n",
      "True      29\n",
      "Name: SQL, dtype: int64, 'Romania': False    110\n",
      "True      13\n",
      "Name: SQL, dtype: int64, 'Singapore': False    119\n",
      "True      15\n",
      "Name: SQL, dtype: int64, 'South_Korea': False    43\n",
      "True      3\n",
      "Name: SQL, dtype: int64, 'Spain': False    113\n",
      "True      13\n",
      "Name: SQL, dtype: int64, 'Sweden': False    131\n",
      "True      14\n",
      "Name: SQL, dtype: int64, 'Switzerland': False    57\n",
      "True      3\n",
      "Name: SQL, dtype: int64, 'Taiwan': False    36\n",
      "True      4\n",
      "Name: SQL, dtype: int64, 'Turkey': False    19\n",
      "True      3\n",
      "Name: SQL, dtype: int64, 'United_Kingdom': False    98\n",
      "True     20\n",
      "Name: SQL, dtype: int64, 'United_States': False    181\n",
      "True      60\n",
      "Name: SQL, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# You should know SQL and its flavors if anyone below is required\n",
    "tool_names = [\n",
    "    r\"SQL\",\n",
    "    r\"MySQL\",\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\",\n",
    "    r\"SQLite\",\n",
    "    r\"MariaDB\",\n",
    "    r\"IBM DB2\",\n",
    "    r\"Oracle Database\",\n",
    "    r\"Db2\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SQL'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.5 Java"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Java is a high-level, object-oriented programming language widely used for developing robust and scalable enterprise applications.\n",
    "\n",
    "In Data Science, Java can be used for developing machine learning models, data analysis, and data processing applications, as well as for building large-scale distributed systems for big data processing and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    65\n",
      "True      2\n",
      "Name: Java, dtype: int64, 'Austria': False    135\n",
      "True       4\n",
      "Name: Java, dtype: int64, 'Belgium': False    64\n",
      "Name: Java, dtype: int64, 'Canada': False    4\n",
      "Name: Java, dtype: int64, 'Czech_Republic': False    65\n",
      "True      3\n",
      "Name: Java, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Finland': False    58\n",
      "Name: Java, dtype: int64, 'France': False    206\n",
      "True      21\n",
      "Name: Java, dtype: int64, 'Germany': False    160\n",
      "True       4\n",
      "Name: Java, dtype: int64, 'Greece': False    54\n",
      "True      2\n",
      "Name: Java, dtype: int64, 'Hong_Kong': False    100\n",
      "True       5\n",
      "Name: Java, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Ireland': False    75\n",
      "Name: Java, dtype: int64, 'Israel': False    200\n",
      "True      52\n",
      "Name: Java, dtype: int64, 'Italy': False    76\n",
      "True      4\n",
      "Name: Java, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Netherlands': False    40\n",
      "Name: Java, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Java, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Poland': False    105\n",
      "True       4\n",
      "Name: Java, dtype: int64, 'Portugal': False    178\n",
      "True      11\n",
      "Name: Java, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Java, dtype: int64, 'Singapore': False    128\n",
      "True       6\n",
      "Name: Java, dtype: int64, 'South_Korea': False    45\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Spain': False    126\n",
      "Name: Java, dtype: int64, 'Sweden': False    142\n",
      "True       3\n",
      "Name: Java, dtype: int64, 'Switzerland': False    60\n",
      "Name: Java, dtype: int64, 'Taiwan': False    39\n",
      "True      1\n",
      "Name: Java, dtype: int64, 'Turkey': False    22\n",
      "Name: Java, dtype: int64, 'United_Kingdom': False    117\n",
      "True       1\n",
      "Name: Java, dtype: int64, 'United_States': False    229\n",
      "True      12\n",
      "Name: Java, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Java\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Java'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.6 C++"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general-purpose programming language designed for systems and application programming, and it is used in Data Science for building high-performance libraries and applications that require intensive computational tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: C++, dtype: int64, 'Austria': False    139\n",
      "Name: C++, dtype: int64, 'Belgium': False    64\n",
      "Name: C++, dtype: int64, 'Canada': False    4\n",
      "Name: C++, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: C++, dtype: int64, 'Denmark': False    91\n",
      "Name: C++, dtype: int64, 'Finland': False    58\n",
      "Name: C++, dtype: int64, 'France': False    227\n",
      "Name: C++, dtype: int64, 'Germany': False    164\n",
      "Name: C++, dtype: int64, 'Greece': False    56\n",
      "Name: C++, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: C++, dtype: int64, 'Hungary': False    86\n",
      "Name: C++, dtype: int64, 'Ireland': False    75\n",
      "Name: C++, dtype: int64, 'Israel': False    252\n",
      "Name: C++, dtype: int64, 'Italy': False    80\n",
      "Name: C++, dtype: int64, 'Japan': False    42\n",
      "Name: C++, dtype: int64, 'Luxembourg': False    40\n",
      "Name: C++, dtype: int64, 'Netherlands': False    40\n",
      "Name: C++, dtype: int64, 'New_Zealand': False    52\n",
      "Name: C++, dtype: int64, 'Norway': False    31\n",
      "Name: C++, dtype: int64, 'Poland': False    109\n",
      "Name: C++, dtype: int64, 'Portugal': False    189\n",
      "Name: C++, dtype: int64, 'Romania': False    123\n",
      "Name: C++, dtype: int64, 'Singapore': False    134\n",
      "Name: C++, dtype: int64, 'South_Korea': False    46\n",
      "Name: C++, dtype: int64, 'Spain': False    126\n",
      "Name: C++, dtype: int64, 'Sweden': False    145\n",
      "Name: C++, dtype: int64, 'Switzerland': False    60\n",
      "Name: C++, dtype: int64, 'Taiwan': False    40\n",
      "Name: C++, dtype: int64, 'Turkey': False    22\n",
      "Name: C++, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: C++, dtype: int64, 'United_States': False    241\n",
      "Name: C++, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"C\\+\\+\",\n",
    "    ]\n",
    "\n",
    "column_name = 'C++'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.7 Go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statically typed programming language designed for building simple, efficient, and reliable software, and it can be used in data engineering for building scalable, distributed systems for data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Go, dtype: int64, 'Austria': False    139\n",
      "Name: Go, dtype: int64, 'Belgium': False    64\n",
      "Name: Go, dtype: int64, 'Canada': False    4\n",
      "Name: Go, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Go, dtype: int64, 'Denmark': False    91\n",
      "Name: Go, dtype: int64, 'Finland': False    58\n",
      "Name: Go, dtype: int64, 'France': False    227\n",
      "Name: Go, dtype: int64, 'Germany': False    164\n",
      "Name: Go, dtype: int64, 'Greece': False    56\n",
      "Name: Go, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Go, dtype: int64, 'Hungary': False    86\n",
      "Name: Go, dtype: int64, 'Ireland': False    75\n",
      "Name: Go, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Go, dtype: int64, 'Italy': False    80\n",
      "Name: Go, dtype: int64, 'Japan': False    42\n",
      "Name: Go, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Go, dtype: int64, 'Netherlands': False    40\n",
      "Name: Go, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Go, dtype: int64, 'Norway': False    31\n",
      "Name: Go, dtype: int64, 'Poland': False    109\n",
      "Name: Go, dtype: int64, 'Portugal': False    184\n",
      "True       5\n",
      "Name: Go, dtype: int64, 'Romania': False    123\n",
      "Name: Go, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Go, dtype: int64, 'South_Korea': False    46\n",
      "Name: Go, dtype: int64, 'Spain': False    126\n",
      "Name: Go, dtype: int64, 'Sweden': False    145\n",
      "Name: Go, dtype: int64, 'Switzerland': False    60\n",
      "Name: Go, dtype: int64, 'Taiwan': False    40\n",
      "Name: Go, dtype: int64, 'Turkey': False    22\n",
      "Name: Go, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Go, dtype: int64, 'United_States': False    241\n",
      "Name: Go, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Go language\", # Go as separate word is too common in English\n",
    "    r\"Golang\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Go'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.8 Rust 🦀"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Rust is not yet as widely used as Python or R for data science, it is gaining popularity due to its ability to handle large-scale, computationally intensive tasks with high efficiency and safety, making it a promising language for data scientists and researchers alike. Additionally, Rust's rich set of libraries and tools, such as ndarray and RustDataScience, provide a solid foundation for building data-driven applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Rust, dtype: int64, 'Austria': False    139\n",
      "Name: Rust, dtype: int64, 'Belgium': False    64\n",
      "Name: Rust, dtype: int64, 'Canada': False    4\n",
      "Name: Rust, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Rust, dtype: int64, 'Denmark': False    91\n",
      "Name: Rust, dtype: int64, 'Finland': False    58\n",
      "Name: Rust, dtype: int64, 'France': False    227\n",
      "Name: Rust, dtype: int64, 'Germany': False    164\n",
      "Name: Rust, dtype: int64, 'Greece': False    56\n",
      "Name: Rust, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Rust, dtype: int64, 'Hungary': False    86\n",
      "Name: Rust, dtype: int64, 'Ireland': False    75\n",
      "Name: Rust, dtype: int64, 'Israel': False    252\n",
      "Name: Rust, dtype: int64, 'Italy': False    80\n",
      "Name: Rust, dtype: int64, 'Japan': False    42\n",
      "Name: Rust, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Rust, dtype: int64, 'Netherlands': False    40\n",
      "Name: Rust, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Rust, dtype: int64, 'Norway': False    31\n",
      "Name: Rust, dtype: int64, 'Poland': False    109\n",
      "Name: Rust, dtype: int64, 'Portugal': False    189\n",
      "Name: Rust, dtype: int64, 'Romania': False    123\n",
      "Name: Rust, dtype: int64, 'Singapore': False    134\n",
      "Name: Rust, dtype: int64, 'South_Korea': False    46\n",
      "Name: Rust, dtype: int64, 'Spain': False    126\n",
      "Name: Rust, dtype: int64, 'Sweden': False    145\n",
      "Name: Rust, dtype: int64, 'Switzerland': False    60\n",
      "Name: Rust, dtype: int64, 'Taiwan': False    40\n",
      "Name: Rust, dtype: int64, 'Turkey': False    22\n",
      "Name: Rust, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Rust, dtype: int64, 'United_States': False    241\n",
      "Name: Rust, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Rust\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Rust'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.8 Bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shell scripting language used for automating repetitive tasks and managing the operating system, including data processing tasks, in the command-line interface (CLI) on Unix and Unix-like systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Bash, dtype: int64, 'Austria': False    139\n",
      "Name: Bash, dtype: int64, 'Belgium': False    64\n",
      "Name: Bash, dtype: int64, 'Canada': False    4\n",
      "Name: Bash, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Bash, dtype: int64, 'Denmark': False    91\n",
      "Name: Bash, dtype: int64, 'Finland': False    58\n",
      "Name: Bash, dtype: int64, 'France': False    227\n",
      "Name: Bash, dtype: int64, 'Germany': False    164\n",
      "Name: Bash, dtype: int64, 'Greece': False    56\n",
      "Name: Bash, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: Bash, dtype: int64, 'Hungary': False    86\n",
      "Name: Bash, dtype: int64, 'Ireland': False    75\n",
      "Name: Bash, dtype: int64, 'Israel': False    250\n",
      "True       2\n",
      "Name: Bash, dtype: int64, 'Italy': False    80\n",
      "Name: Bash, dtype: int64, 'Japan': False    42\n",
      "Name: Bash, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Bash, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: Bash, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Bash, dtype: int64, 'Norway': False    31\n",
      "Name: Bash, dtype: int64, 'Poland': False    106\n",
      "True       3\n",
      "Name: Bash, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Bash, dtype: int64, 'Romania': False    123\n",
      "Name: Bash, dtype: int64, 'Singapore': False    134\n",
      "Name: Bash, dtype: int64, 'South_Korea': False    46\n",
      "Name: Bash, dtype: int64, 'Spain': False    126\n",
      "Name: Bash, dtype: int64, 'Sweden': False    145\n",
      "Name: Bash, dtype: int64, 'Switzerland': False    60\n",
      "Name: Bash, dtype: int64, 'Taiwan': False    40\n",
      "Name: Bash, dtype: int64, 'Turkey': False    22\n",
      "Name: Bash, dtype: int64, 'United_Kingdom': False    117\n",
      "True       1\n",
      "Name: Bash, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: Bash, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Bash\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Bash'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.9 Powershell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task automation and configuration management framework from Microsoft, which can be used in Data Science for automating various data processing tasks on Windows machines in the command-line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: PowerShell, dtype: int64, 'Austria': False    139\n",
      "Name: PowerShell, dtype: int64, 'Belgium': False    63\n",
      "True      1\n",
      "Name: PowerShell, dtype: int64, 'Canada': False    4\n",
      "Name: PowerShell, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: PowerShell, dtype: int64, 'Denmark': False    91\n",
      "Name: PowerShell, dtype: int64, 'Finland': False    58\n",
      "Name: PowerShell, dtype: int64, 'France': False    226\n",
      "True       1\n",
      "Name: PowerShell, dtype: int64, 'Germany': False    164\n",
      "Name: PowerShell, dtype: int64, 'Greece': False    56\n",
      "Name: PowerShell, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: PowerShell, dtype: int64, 'Hungary': False    86\n",
      "Name: PowerShell, dtype: int64, 'Ireland': False    75\n",
      "Name: PowerShell, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: PowerShell, dtype: int64, 'Italy': False    80\n",
      "Name: PowerShell, dtype: int64, 'Japan': False    42\n",
      "Name: PowerShell, dtype: int64, 'Luxembourg': False    40\n",
      "Name: PowerShell, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: PowerShell, dtype: int64, 'New_Zealand': False    52\n",
      "Name: PowerShell, dtype: int64, 'Norway': False    31\n",
      "Name: PowerShell, dtype: int64, 'Poland': False    109\n",
      "Name: PowerShell, dtype: int64, 'Portugal': False    189\n",
      "Name: PowerShell, dtype: int64, 'Romania': False    123\n",
      "Name: PowerShell, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: PowerShell, dtype: int64, 'South_Korea': False    46\n",
      "Name: PowerShell, dtype: int64, 'Spain': False    126\n",
      "Name: PowerShell, dtype: int64, 'Sweden': False    145\n",
      "Name: PowerShell, dtype: int64, 'Switzerland': False    60\n",
      "Name: PowerShell, dtype: int64, 'Taiwan': False    40\n",
      "Name: PowerShell, dtype: int64, 'Turkey': False    22\n",
      "Name: PowerShell, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: PowerShell, dtype: int64, 'United_States': False    241\n",
      "Name: PowerShell, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"PowerShell\",\n",
    "    r\"DOS Shell\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PowerShell'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.10 CLI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLI stands for Command Line Interface, which is a way to interact with a computer program through text commands, and it is commonly used in Data Science for running scripts, automating tasks, and managing software packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: CLI, dtype: int64, 'Austria': False    139\n",
      "Name: CLI, dtype: int64, 'Belgium': False    64\n",
      "Name: CLI, dtype: int64, 'Canada': False    4\n",
      "Name: CLI, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: CLI, dtype: int64, 'Denmark': False    91\n",
      "Name: CLI, dtype: int64, 'Finland': False    58\n",
      "Name: CLI, dtype: int64, 'France': False    227\n",
      "Name: CLI, dtype: int64, 'Germany': False    164\n",
      "Name: CLI, dtype: int64, 'Greece': False    56\n",
      "Name: CLI, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: CLI, dtype: int64, 'Hungary': False    86\n",
      "Name: CLI, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: CLI, dtype: int64, 'Israel': False    252\n",
      "Name: CLI, dtype: int64, 'Italy': False    80\n",
      "Name: CLI, dtype: int64, 'Japan': False    42\n",
      "Name: CLI, dtype: int64, 'Luxembourg': False    40\n",
      "Name: CLI, dtype: int64, 'Netherlands': False    40\n",
      "Name: CLI, dtype: int64, 'New_Zealand': False    52\n",
      "Name: CLI, dtype: int64, 'Norway': False    31\n",
      "Name: CLI, dtype: int64, 'Poland': False    109\n",
      "Name: CLI, dtype: int64, 'Portugal': False    189\n",
      "Name: CLI, dtype: int64, 'Romania': False    123\n",
      "Name: CLI, dtype: int64, 'Singapore': False    134\n",
      "Name: CLI, dtype: int64, 'South_Korea': False    46\n",
      "Name: CLI, dtype: int64, 'Spain': False    126\n",
      "Name: CLI, dtype: int64, 'Sweden': False    145\n",
      "Name: CLI, dtype: int64, 'Switzerland': False    60\n",
      "Name: CLI, dtype: int64, 'Taiwan': False    40\n",
      "Name: CLI, dtype: int64, 'Turkey': False    22\n",
      "Name: CLI, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: CLI, dtype: int64, 'United_States': False    240\n",
      "True       1\n",
      "Name: CLI, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"CLI\",\n",
    "    r\"Command Line Interface\"\n",
    "    ]\n",
    "\n",
    "column_name = 'CLI'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27. Virtualization Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business intelligence and data visualization tools used for analyzing and visualizing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.1 Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Tableau, dtype: int64, 'Austria': False    139\n",
      "Name: Tableau, dtype: int64, 'Belgium': False    64\n",
      "Name: Tableau, dtype: int64, 'Canada': False    4\n",
      "Name: Tableau, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Tableau, dtype: int64, 'Denmark': False    91\n",
      "Name: Tableau, dtype: int64, 'Finland': False    58\n",
      "Name: Tableau, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: Tableau, dtype: int64, 'Germany': False    163\n",
      "True       1\n",
      "Name: Tableau, dtype: int64, 'Greece': False    56\n",
      "Name: Tableau, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: Tableau, dtype: int64, 'Hungary': False    86\n",
      "Name: Tableau, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Tableau, dtype: int64, 'Israel': False    247\n",
      "True       5\n",
      "Name: Tableau, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Tableau, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Tableau, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Tableau, dtype: int64, 'Netherlands': False    40\n",
      "Name: Tableau, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Tableau, dtype: int64, 'Norway': False    31\n",
      "Name: Tableau, dtype: int64, 'Poland': False    109\n",
      "Name: Tableau, dtype: int64, 'Portugal': False    186\n",
      "True       3\n",
      "Name: Tableau, dtype: int64, 'Romania': False    119\n",
      "True       4\n",
      "Name: Tableau, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Tableau, dtype: int64, 'South_Korea': False    46\n",
      "Name: Tableau, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: Tableau, dtype: int64, 'Sweden': False    145\n",
      "Name: Tableau, dtype: int64, 'Switzerland': False    60\n",
      "Name: Tableau, dtype: int64, 'Taiwan': False    40\n",
      "Name: Tableau, dtype: int64, 'Turkey': False    22\n",
      "Name: Tableau, dtype: int64, 'United_Kingdom': False    116\n",
      "True       2\n",
      "Name: Tableau, dtype: int64, 'United_States': False    235\n",
      "True       6\n",
      "Name: Tableau, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Tableau\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Tableau'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.2 Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    65\n",
      "True      2\n",
      "Name: Power_BI, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Power_BI, dtype: int64, 'Belgium': False    58\n",
      "True      6\n",
      "Name: Power_BI, dtype: int64, 'Canada': False    4\n",
      "Name: Power_BI, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Power_BI, dtype: int64, 'Denmark': False    88\n",
      "True      3\n",
      "Name: Power_BI, dtype: int64, 'Finland': False    58\n",
      "Name: Power_BI, dtype: int64, 'France': False    220\n",
      "True       7\n",
      "Name: Power_BI, dtype: int64, 'Germany': False    157\n",
      "True       7\n",
      "Name: Power_BI, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Hong_Kong': False    102\n",
      "True       3\n",
      "Name: Power_BI, dtype: int64, 'Hungary': False    85\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Israel': False    250\n",
      "True       2\n",
      "Name: Power_BI, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: Power_BI, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Luxembourg': False    38\n",
      "True      2\n",
      "Name: Power_BI, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Power_BI, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: Power_BI, dtype: int64, 'Portugal': False    186\n",
      "True       3\n",
      "Name: Power_BI, dtype: int64, 'Romania': False    121\n",
      "True       2\n",
      "Name: Power_BI, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Power_BI, dtype: int64, 'South_Korea': False    46\n",
      "Name: Power_BI, dtype: int64, 'Spain': False    124\n",
      "True       2\n",
      "Name: Power_BI, dtype: int64, 'Sweden': False    139\n",
      "True       6\n",
      "Name: Power_BI, dtype: int64, 'Switzerland': False    59\n",
      "True      1\n",
      "Name: Power_BI, dtype: int64, 'Taiwan': False    40\n",
      "Name: Power_BI, dtype: int64, 'Turkey': False    22\n",
      "Name: Power_BI, dtype: int64, 'United_Kingdom': False    113\n",
      "True       5\n",
      "Name: Power_BI, dtype: int64, 'United_States': False    238\n",
      "True       3\n",
      "Name: Power_BI, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Power BI\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Power_BI'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.3 Google Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Google_Analytics, dtype: int64, 'Austria': False    138\n",
      "True       1\n",
      "Name: Google_Analytics, dtype: int64, 'Belgium': False    64\n",
      "Name: Google_Analytics, dtype: int64, 'Canada': False    4\n",
      "Name: Google_Analytics, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Google_Analytics, dtype: int64, 'Denmark': False    91\n",
      "Name: Google_Analytics, dtype: int64, 'Finland': False    58\n",
      "Name: Google_Analytics, dtype: int64, 'France': False    227\n",
      "Name: Google_Analytics, dtype: int64, 'Germany': False    164\n",
      "Name: Google_Analytics, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: Google_Analytics, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Google_Analytics, dtype: int64, 'Hungary': False    86\n",
      "Name: Google_Analytics, dtype: int64, 'Ireland': False    75\n",
      "Name: Google_Analytics, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: Google_Analytics, dtype: int64, 'Italy': False    80\n",
      "Name: Google_Analytics, dtype: int64, 'Japan': False    42\n",
      "Name: Google_Analytics, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Google_Analytics, dtype: int64, 'Netherlands': False    40\n",
      "Name: Google_Analytics, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Google_Analytics, dtype: int64, 'Norway': False    31\n",
      "Name: Google_Analytics, dtype: int64, 'Poland': False    108\n",
      "True       1\n",
      "Name: Google_Analytics, dtype: int64, 'Portugal': False    189\n",
      "Name: Google_Analytics, dtype: int64, 'Romania': False    123\n",
      "Name: Google_Analytics, dtype: int64, 'Singapore': False    134\n",
      "Name: Google_Analytics, dtype: int64, 'South_Korea': False    46\n",
      "Name: Google_Analytics, dtype: int64, 'Spain': False    126\n",
      "Name: Google_Analytics, dtype: int64, 'Sweden': False    145\n",
      "Name: Google_Analytics, dtype: int64, 'Switzerland': False    60\n",
      "Name: Google_Analytics, dtype: int64, 'Taiwan': False    40\n",
      "Name: Google_Analytics, dtype: int64, 'Turkey': False    22\n",
      "Name: Google_Analytics, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Google_Analytics, dtype: int64, 'United_States': False    241\n",
      "Name: Google_Analytics, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Google Analytics\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_Analytics'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.4 QlikView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: QlikView, dtype: int64, 'Austria': False    139\n",
      "Name: QlikView, dtype: int64, 'Belgium': False    64\n",
      "Name: QlikView, dtype: int64, 'Canada': False    4\n",
      "Name: QlikView, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: QlikView, dtype: int64, 'Denmark': False    91\n",
      "Name: QlikView, dtype: int64, 'Finland': False    58\n",
      "Name: QlikView, dtype: int64, 'France': False    225\n",
      "True       2\n",
      "Name: QlikView, dtype: int64, 'Germany': False    164\n",
      "Name: QlikView, dtype: int64, 'Greece': False    56\n",
      "Name: QlikView, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'Hungary': False    86\n",
      "Name: QlikView, dtype: int64, 'Ireland': False    75\n",
      "Name: QlikView, dtype: int64, 'Israel': False    251\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'Italy': False    78\n",
      "True      2\n",
      "Name: QlikView, dtype: int64, 'Japan': False    42\n",
      "Name: QlikView, dtype: int64, 'Luxembourg': False    40\n",
      "Name: QlikView, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: QlikView, dtype: int64, 'New_Zealand': False    52\n",
      "Name: QlikView, dtype: int64, 'Norway': False    31\n",
      "Name: QlikView, dtype: int64, 'Poland': False    109\n",
      "Name: QlikView, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'Romania': False    123\n",
      "Name: QlikView, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'South_Korea': False    46\n",
      "Name: QlikView, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: QlikView, dtype: int64, 'Switzerland': False    60\n",
      "Name: QlikView, dtype: int64, 'Taiwan': False    40\n",
      "Name: QlikView, dtype: int64, 'Turkey': False    22\n",
      "Name: QlikView, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: QlikView, dtype: int64, 'United_States': False    241\n",
      "Name: QlikView, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"QlikView\",\n",
    "    r\"Qlik\"\n",
    "    ]\n",
    "\n",
    "column_name = 'QlikView'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.5 Oracle BI server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Oracle_BI_server, dtype: int64, 'Austria': False    139\n",
      "Name: Oracle_BI_server, dtype: int64, 'Belgium': False    64\n",
      "Name: Oracle_BI_server, dtype: int64, 'Canada': False    4\n",
      "Name: Oracle_BI_server, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Oracle_BI_server, dtype: int64, 'Denmark': False    91\n",
      "Name: Oracle_BI_server, dtype: int64, 'Finland': False    58\n",
      "Name: Oracle_BI_server, dtype: int64, 'France': False    227\n",
      "Name: Oracle_BI_server, dtype: int64, 'Germany': False    164\n",
      "Name: Oracle_BI_server, dtype: int64, 'Greece': False    56\n",
      "Name: Oracle_BI_server, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Oracle_BI_server, dtype: int64, 'Hungary': False    86\n",
      "Name: Oracle_BI_server, dtype: int64, 'Ireland': False    75\n",
      "Name: Oracle_BI_server, dtype: int64, 'Israel': False    252\n",
      "Name: Oracle_BI_server, dtype: int64, 'Italy': False    80\n",
      "Name: Oracle_BI_server, dtype: int64, 'Japan': False    42\n",
      "Name: Oracle_BI_server, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Oracle_BI_server, dtype: int64, 'Netherlands': False    40\n",
      "Name: Oracle_BI_server, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Oracle_BI_server, dtype: int64, 'Norway': False    31\n",
      "Name: Oracle_BI_server, dtype: int64, 'Poland': False    109\n",
      "Name: Oracle_BI_server, dtype: int64, 'Portugal': False    189\n",
      "Name: Oracle_BI_server, dtype: int64, 'Romania': False    123\n",
      "Name: Oracle_BI_server, dtype: int64, 'Singapore': False    134\n",
      "Name: Oracle_BI_server, dtype: int64, 'South_Korea': False    46\n",
      "Name: Oracle_BI_server, dtype: int64, 'Spain': False    126\n",
      "Name: Oracle_BI_server, dtype: int64, 'Sweden': False    145\n",
      "Name: Oracle_BI_server, dtype: int64, 'Switzerland': False    60\n",
      "Name: Oracle_BI_server, dtype: int64, 'Taiwan': False    40\n",
      "Name: Oracle_BI_server, dtype: int64, 'Turkey': False    22\n",
      "Name: Oracle_BI_server, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Oracle_BI_server, dtype: int64, 'United_States': False    241\n",
      "Name: Oracle_BI_server, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Oracle Business Intelligence Enterprise Edition\",\n",
    "    r\"OBIEE\",\n",
    "    r\"Oracle BI server\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_BI_server'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.6 SAS Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: SAS_Analytics, dtype: int64, 'Austria': False    139\n",
      "Name: SAS_Analytics, dtype: int64, 'Belgium': False    64\n",
      "Name: SAS_Analytics, dtype: int64, 'Canada': False    4\n",
      "Name: SAS_Analytics, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: SAS_Analytics, dtype: int64, 'Denmark': False    91\n",
      "Name: SAS_Analytics, dtype: int64, 'Finland': False    58\n",
      "Name: SAS_Analytics, dtype: int64, 'France': False    227\n",
      "Name: SAS_Analytics, dtype: int64, 'Germany': False    164\n",
      "Name: SAS_Analytics, dtype: int64, 'Greece': False    56\n",
      "Name: SAS_Analytics, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: SAS_Analytics, dtype: int64, 'Hungary': False    86\n",
      "Name: SAS_Analytics, dtype: int64, 'Ireland': False    75\n",
      "Name: SAS_Analytics, dtype: int64, 'Israel': False    252\n",
      "Name: SAS_Analytics, dtype: int64, 'Italy': False    80\n",
      "Name: SAS_Analytics, dtype: int64, 'Japan': False    42\n",
      "Name: SAS_Analytics, dtype: int64, 'Luxembourg': False    40\n",
      "Name: SAS_Analytics, dtype: int64, 'Netherlands': False    40\n",
      "Name: SAS_Analytics, dtype: int64, 'New_Zealand': False    52\n",
      "Name: SAS_Analytics, dtype: int64, 'Norway': False    31\n",
      "Name: SAS_Analytics, dtype: int64, 'Poland': False    109\n",
      "Name: SAS_Analytics, dtype: int64, 'Portugal': False    189\n",
      "Name: SAS_Analytics, dtype: int64, 'Romania': False    123\n",
      "Name: SAS_Analytics, dtype: int64, 'Singapore': False    134\n",
      "Name: SAS_Analytics, dtype: int64, 'South_Korea': False    46\n",
      "Name: SAS_Analytics, dtype: int64, 'Spain': False    126\n",
      "Name: SAS_Analytics, dtype: int64, 'Sweden': False    145\n",
      "Name: SAS_Analytics, dtype: int64, 'Switzerland': False    60\n",
      "Name: SAS_Analytics, dtype: int64, 'Taiwan': False    40\n",
      "Name: SAS_Analytics, dtype: int64, 'Turkey': False    22\n",
      "Name: SAS_Analytics, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: SAS_Analytics, dtype: int64, 'United_States': False    241\n",
      "Name: SAS_Analytics, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"SAS Analytics\",\n",
    "    r\"Statistical Analysis System\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SAS_Analytics'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.7 Lumira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Lumira, dtype: int64, 'Austria': False    139\n",
      "Name: Lumira, dtype: int64, 'Belgium': False    64\n",
      "Name: Lumira, dtype: int64, 'Canada': False    4\n",
      "Name: Lumira, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Lumira, dtype: int64, 'Denmark': False    91\n",
      "Name: Lumira, dtype: int64, 'Finland': False    58\n",
      "Name: Lumira, dtype: int64, 'France': False    227\n",
      "Name: Lumira, dtype: int64, 'Germany': False    164\n",
      "Name: Lumira, dtype: int64, 'Greece': False    56\n",
      "Name: Lumira, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Lumira, dtype: int64, 'Hungary': False    86\n",
      "Name: Lumira, dtype: int64, 'Ireland': False    75\n",
      "Name: Lumira, dtype: int64, 'Israel': False    252\n",
      "Name: Lumira, dtype: int64, 'Italy': False    80\n",
      "Name: Lumira, dtype: int64, 'Japan': False    42\n",
      "Name: Lumira, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Lumira, dtype: int64, 'Netherlands': False    40\n",
      "Name: Lumira, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Lumira, dtype: int64, 'Norway': False    31\n",
      "Name: Lumira, dtype: int64, 'Poland': False    109\n",
      "Name: Lumira, dtype: int64, 'Portugal': False    189\n",
      "Name: Lumira, dtype: int64, 'Romania': False    123\n",
      "Name: Lumira, dtype: int64, 'Singapore': False    134\n",
      "Name: Lumira, dtype: int64, 'South_Korea': False    46\n",
      "Name: Lumira, dtype: int64, 'Spain': False    126\n",
      "Name: Lumira, dtype: int64, 'Sweden': False    145\n",
      "Name: Lumira, dtype: int64, 'Switzerland': False    60\n",
      "Name: Lumira, dtype: int64, 'Taiwan': False    40\n",
      "Name: Lumira, dtype: int64, 'Turkey': False    22\n",
      "Name: Lumira, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Lumira, dtype: int64, 'United_States': False    241\n",
      "Name: Lumira, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Lumira\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Lumira'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.8 IBM Cognos Impromptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Austria': False    139\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Belgium': False    64\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Canada': False    4\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Denmark': False    91\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Finland': False    58\n",
      "Name: Cognos_Impromptu, dtype: int64, 'France': False    227\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Germany': False    164\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Greece': False    56\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Hungary': False    86\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Ireland': False    75\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Israel': False    252\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Italy': False    80\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Japan': False    42\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Netherlands': False    40\n",
      "Name: Cognos_Impromptu, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Norway': False    31\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Poland': False    109\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Portugal': False    189\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Romania': False    123\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Singapore': False    134\n",
      "Name: Cognos_Impromptu, dtype: int64, 'South_Korea': False    46\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Spain': False    126\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Sweden': False    145\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Switzerland': False    60\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Taiwan': False    40\n",
      "Name: Cognos_Impromptu, dtype: int64, 'Turkey': False    22\n",
      "Name: Cognos_Impromptu, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Cognos_Impromptu, dtype: int64, 'United_States': False    241\n",
      "Name: Cognos_Impromptu, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Cognos Impromptu\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cognos_Impromptu'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.9 MicroStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: MicroStrategy, dtype: int64, 'Austria': False    139\n",
      "Name: MicroStrategy, dtype: int64, 'Belgium': False    64\n",
      "Name: MicroStrategy, dtype: int64, 'Canada': False    4\n",
      "Name: MicroStrategy, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: MicroStrategy, dtype: int64, 'Denmark': False    91\n",
      "Name: MicroStrategy, dtype: int64, 'Finland': False    58\n",
      "Name: MicroStrategy, dtype: int64, 'France': False    226\n",
      "True       1\n",
      "Name: MicroStrategy, dtype: int64, 'Germany': False    164\n",
      "Name: MicroStrategy, dtype: int64, 'Greece': False    56\n",
      "Name: MicroStrategy, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: MicroStrategy, dtype: int64, 'Hungary': False    86\n",
      "Name: MicroStrategy, dtype: int64, 'Ireland': False    75\n",
      "Name: MicroStrategy, dtype: int64, 'Israel': False    252\n",
      "Name: MicroStrategy, dtype: int64, 'Italy': False    80\n",
      "Name: MicroStrategy, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: MicroStrategy, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: MicroStrategy, dtype: int64, 'Netherlands': False    40\n",
      "Name: MicroStrategy, dtype: int64, 'New_Zealand': False    52\n",
      "Name: MicroStrategy, dtype: int64, 'Norway': False    31\n",
      "Name: MicroStrategy, dtype: int64, 'Poland': False    109\n",
      "Name: MicroStrategy, dtype: int64, 'Portugal': False    189\n",
      "Name: MicroStrategy, dtype: int64, 'Romania': False    123\n",
      "Name: MicroStrategy, dtype: int64, 'Singapore': False    134\n",
      "Name: MicroStrategy, dtype: int64, 'South_Korea': False    46\n",
      "Name: MicroStrategy, dtype: int64, 'Spain': False    125\n",
      "True       1\n",
      "Name: MicroStrategy, dtype: int64, 'Sweden': False    145\n",
      "Name: MicroStrategy, dtype: int64, 'Switzerland': False    60\n",
      "Name: MicroStrategy, dtype: int64, 'Taiwan': False    40\n",
      "Name: MicroStrategy, dtype: int64, 'Turkey': False    22\n",
      "Name: MicroStrategy, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: MicroStrategy, dtype: int64, 'United_States': False    241\n",
      "Name: MicroStrategy, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"MicroStrategy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MicroStrategy'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.10 InsightSquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: InsightSquared, dtype: int64, 'Austria': False    139\n",
      "Name: InsightSquared, dtype: int64, 'Belgium': False    64\n",
      "Name: InsightSquared, dtype: int64, 'Canada': False    4\n",
      "Name: InsightSquared, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: InsightSquared, dtype: int64, 'Denmark': False    91\n",
      "Name: InsightSquared, dtype: int64, 'Finland': False    58\n",
      "Name: InsightSquared, dtype: int64, 'France': False    227\n",
      "Name: InsightSquared, dtype: int64, 'Germany': False    164\n",
      "Name: InsightSquared, dtype: int64, 'Greece': False    56\n",
      "Name: InsightSquared, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: InsightSquared, dtype: int64, 'Hungary': False    86\n",
      "Name: InsightSquared, dtype: int64, 'Ireland': False    75\n",
      "Name: InsightSquared, dtype: int64, 'Israel': False    252\n",
      "Name: InsightSquared, dtype: int64, 'Italy': False    80\n",
      "Name: InsightSquared, dtype: int64, 'Japan': False    42\n",
      "Name: InsightSquared, dtype: int64, 'Luxembourg': False    40\n",
      "Name: InsightSquared, dtype: int64, 'Netherlands': False    40\n",
      "Name: InsightSquared, dtype: int64, 'New_Zealand': False    52\n",
      "Name: InsightSquared, dtype: int64, 'Norway': False    31\n",
      "Name: InsightSquared, dtype: int64, 'Poland': False    109\n",
      "Name: InsightSquared, dtype: int64, 'Portugal': False    189\n",
      "Name: InsightSquared, dtype: int64, 'Romania': False    123\n",
      "Name: InsightSquared, dtype: int64, 'Singapore': False    134\n",
      "Name: InsightSquared, dtype: int64, 'South_Korea': False    46\n",
      "Name: InsightSquared, dtype: int64, 'Spain': False    126\n",
      "Name: InsightSquared, dtype: int64, 'Sweden': False    145\n",
      "Name: InsightSquared, dtype: int64, 'Switzerland': False    60\n",
      "Name: InsightSquared, dtype: int64, 'Taiwan': False    40\n",
      "Name: InsightSquared, dtype: int64, 'Turkey': False    22\n",
      "Name: InsightSquared, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: InsightSquared, dtype: int64, 'United_States': False    241\n",
      "Name: InsightSquared, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"InsightSquared\",\n",
    "    ]\n",
    "\n",
    "column_name = 'InsightSquared'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.11 Sisense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Sisense, dtype: int64, 'Austria': False    139\n",
      "Name: Sisense, dtype: int64, 'Belgium': False    64\n",
      "Name: Sisense, dtype: int64, 'Canada': False    4\n",
      "Name: Sisense, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Sisense, dtype: int64, 'Denmark': False    91\n",
      "Name: Sisense, dtype: int64, 'Finland': False    58\n",
      "Name: Sisense, dtype: int64, 'France': False    227\n",
      "Name: Sisense, dtype: int64, 'Germany': False    164\n",
      "Name: Sisense, dtype: int64, 'Greece': False    56\n",
      "Name: Sisense, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Sisense, dtype: int64, 'Hungary': False    86\n",
      "Name: Sisense, dtype: int64, 'Ireland': False    75\n",
      "Name: Sisense, dtype: int64, 'Israel': False    252\n",
      "Name: Sisense, dtype: int64, 'Italy': False    80\n",
      "Name: Sisense, dtype: int64, 'Japan': False    42\n",
      "Name: Sisense, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Sisense, dtype: int64, 'Netherlands': False    40\n",
      "Name: Sisense, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Sisense, dtype: int64, 'Norway': False    31\n",
      "Name: Sisense, dtype: int64, 'Poland': False    109\n",
      "Name: Sisense, dtype: int64, 'Portugal': False    189\n",
      "Name: Sisense, dtype: int64, 'Romania': False    123\n",
      "Name: Sisense, dtype: int64, 'Singapore': False    134\n",
      "Name: Sisense, dtype: int64, 'South_Korea': False    46\n",
      "Name: Sisense, dtype: int64, 'Spain': False    126\n",
      "Name: Sisense, dtype: int64, 'Sweden': False    145\n",
      "Name: Sisense, dtype: int64, 'Switzerland': False    60\n",
      "Name: Sisense, dtype: int64, 'Taiwan': False    40\n",
      "Name: Sisense, dtype: int64, 'Turkey': False    22\n",
      "Name: Sisense, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Sisense, dtype: int64, 'United_States': False    241\n",
      "Name: Sisense, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Sisense\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Sisense'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.12 Dundas BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Dundas_BI, dtype: int64, 'Austria': False    139\n",
      "Name: Dundas_BI, dtype: int64, 'Belgium': False    64\n",
      "Name: Dundas_BI, dtype: int64, 'Canada': False    4\n",
      "Name: Dundas_BI, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Dundas_BI, dtype: int64, 'Denmark': False    91\n",
      "Name: Dundas_BI, dtype: int64, 'Finland': False    58\n",
      "Name: Dundas_BI, dtype: int64, 'France': False    227\n",
      "Name: Dundas_BI, dtype: int64, 'Germany': False    164\n",
      "Name: Dundas_BI, dtype: int64, 'Greece': False    56\n",
      "Name: Dundas_BI, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Dundas_BI, dtype: int64, 'Hungary': False    86\n",
      "Name: Dundas_BI, dtype: int64, 'Ireland': False    75\n",
      "Name: Dundas_BI, dtype: int64, 'Israel': False    252\n",
      "Name: Dundas_BI, dtype: int64, 'Italy': False    80\n",
      "Name: Dundas_BI, dtype: int64, 'Japan': False    42\n",
      "Name: Dundas_BI, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Dundas_BI, dtype: int64, 'Netherlands': False    40\n",
      "Name: Dundas_BI, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Dundas_BI, dtype: int64, 'Norway': False    31\n",
      "Name: Dundas_BI, dtype: int64, 'Poland': False    109\n",
      "Name: Dundas_BI, dtype: int64, 'Portugal': False    189\n",
      "Name: Dundas_BI, dtype: int64, 'Romania': False    123\n",
      "Name: Dundas_BI, dtype: int64, 'Singapore': False    134\n",
      "Name: Dundas_BI, dtype: int64, 'South_Korea': False    46\n",
      "Name: Dundas_BI, dtype: int64, 'Spain': False    126\n",
      "Name: Dundas_BI, dtype: int64, 'Sweden': False    145\n",
      "Name: Dundas_BI, dtype: int64, 'Switzerland': False    60\n",
      "Name: Dundas_BI, dtype: int64, 'Taiwan': False    40\n",
      "Name: Dundas_BI, dtype: int64, 'Turkey': False    22\n",
      "Name: Dundas_BI, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Dundas_BI, dtype: int64, 'United_States': False    241\n",
      "Name: Dundas_BI, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Dundas BI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dundas_BI'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.13 Domo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Domo, dtype: int64, 'Austria': False    139\n",
      "Name: Domo, dtype: int64, 'Belgium': False    64\n",
      "Name: Domo, dtype: int64, 'Canada': False    4\n",
      "Name: Domo, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Domo, dtype: int64, 'Denmark': False    91\n",
      "Name: Domo, dtype: int64, 'Finland': False    58\n",
      "Name: Domo, dtype: int64, 'France': False    227\n",
      "Name: Domo, dtype: int64, 'Germany': False    164\n",
      "Name: Domo, dtype: int64, 'Greece': False    56\n",
      "Name: Domo, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Domo, dtype: int64, 'Hungary': False    86\n",
      "Name: Domo, dtype: int64, 'Ireland': False    75\n",
      "Name: Domo, dtype: int64, 'Israel': False    252\n",
      "Name: Domo, dtype: int64, 'Italy': False    80\n",
      "Name: Domo, dtype: int64, 'Japan': False    42\n",
      "Name: Domo, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Domo, dtype: int64, 'Netherlands': False    40\n",
      "Name: Domo, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Domo, dtype: int64, 'Norway': False    31\n",
      "Name: Domo, dtype: int64, 'Poland': False    109\n",
      "Name: Domo, dtype: int64, 'Portugal': False    189\n",
      "Name: Domo, dtype: int64, 'Romania': False    123\n",
      "Name: Domo, dtype: int64, 'Singapore': False    134\n",
      "Name: Domo, dtype: int64, 'South_Korea': False    46\n",
      "Name: Domo, dtype: int64, 'Spain': False    126\n",
      "Name: Domo, dtype: int64, 'Sweden': False    145\n",
      "Name: Domo, dtype: int64, 'Switzerland': False    60\n",
      "Name: Domo, dtype: int64, 'Taiwan': False    40\n",
      "Name: Domo, dtype: int64, 'Turkey': False    22\n",
      "Name: Domo, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Domo, dtype: int64, 'United_States': False    241\n",
      "Name: Domo, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Domo\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Domo'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.14 Looker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Looker, dtype: int64, 'Austria': False    139\n",
      "Name: Looker, dtype: int64, 'Belgium': False    64\n",
      "Name: Looker, dtype: int64, 'Canada': False    4\n",
      "Name: Looker, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Looker, dtype: int64, 'Denmark': False    91\n",
      "Name: Looker, dtype: int64, 'Finland': False    58\n",
      "Name: Looker, dtype: int64, 'France': False    223\n",
      "True       4\n",
      "Name: Looker, dtype: int64, 'Germany': False    164\n",
      "Name: Looker, dtype: int64, 'Greece': False    56\n",
      "Name: Looker, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Looker, dtype: int64, 'Hungary': False    86\n",
      "Name: Looker, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Looker, dtype: int64, 'Israel': False    249\n",
      "True       3\n",
      "Name: Looker, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Looker, dtype: int64, 'Japan': False    42\n",
      "Name: Looker, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Looker, dtype: int64, 'Netherlands': False    40\n",
      "Name: Looker, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Looker, dtype: int64, 'Norway': False    31\n",
      "Name: Looker, dtype: int64, 'Poland': False    109\n",
      "Name: Looker, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Looker, dtype: int64, 'Romania': False    123\n",
      "Name: Looker, dtype: int64, 'Singapore': False    134\n",
      "Name: Looker, dtype: int64, 'South_Korea': False    46\n",
      "Name: Looker, dtype: int64, 'Spain': False    126\n",
      "Name: Looker, dtype: int64, 'Sweden': False    145\n",
      "Name: Looker, dtype: int64, 'Switzerland': False    60\n",
      "Name: Looker, dtype: int64, 'Taiwan': False    40\n",
      "Name: Looker, dtype: int64, 'Turkey': False    22\n",
      "Name: Looker, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Looker, dtype: int64, 'United_States': False    241\n",
      "Name: Looker, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Looker\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Looker'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28. Microsoft Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    66\n",
      "True      1\n",
      "Name: Excel, dtype: int64, 'Austria': False    139\n",
      "Name: Excel, dtype: int64, 'Belgium': False    62\n",
      "True      2\n",
      "Name: Excel, dtype: int64, 'Canada': False    4\n",
      "Name: Excel, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Excel, dtype: int64, 'Denmark': False    91\n",
      "Name: Excel, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: Excel, dtype: int64, 'France': False    227\n",
      "Name: Excel, dtype: int64, 'Germany': False    164\n",
      "Name: Excel, dtype: int64, 'Greece': False    56\n",
      "Name: Excel, dtype: int64, 'Hong_Kong': False    105\n",
      "Name: Excel, dtype: int64, 'Hungary': False    86\n",
      "Name: Excel, dtype: int64, 'Ireland': False    74\n",
      "True      1\n",
      "Name: Excel, dtype: int64, 'Israel': False    252\n",
      "Name: Excel, dtype: int64, 'Italy': False    80\n",
      "Name: Excel, dtype: int64, 'Japan': False    42\n",
      "Name: Excel, dtype: int64, 'Luxembourg': False    39\n",
      "True      1\n",
      "Name: Excel, dtype: int64, 'Netherlands': False    40\n",
      "Name: Excel, dtype: int64, 'New_Zealand': False    51\n",
      "True      1\n",
      "Name: Excel, dtype: int64, 'Norway': False    31\n",
      "Name: Excel, dtype: int64, 'Poland': False    109\n",
      "Name: Excel, dtype: int64, 'Portugal': False    188\n",
      "True       1\n",
      "Name: Excel, dtype: int64, 'Romania': False    123\n",
      "Name: Excel, dtype: int64, 'Singapore': False    134\n",
      "Name: Excel, dtype: int64, 'South_Korea': False    46\n",
      "Name: Excel, dtype: int64, 'Spain': False    123\n",
      "True       3\n",
      "Name: Excel, dtype: int64, 'Sweden': False    145\n",
      "Name: Excel, dtype: int64, 'Switzerland': False    60\n",
      "Name: Excel, dtype: int64, 'Taiwan': False    40\n",
      "Name: Excel, dtype: int64, 'Turkey': False    22\n",
      "Name: Excel, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Excel, dtype: int64, 'United_States': False    241\n",
      "Name: Excel, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "tool_names = [\n",
    "    r\"Excel\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Excel'\n",
    "\n",
    "dfs = add_tech_to_dfs(dfs, tool_names, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 29. Certifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edu_to_dfs(dfs, education_names, education_non_eng, column_name):\n",
    "\n",
    "    for country, df in dfs.items():\n",
    "\n",
    "        country_languages = countries_languages[country]\n",
    "\n",
    "        for language in country_languages:\n",
    "\n",
    "            names_to_add = education_non_eng[language]\n",
    "\n",
    "            education_names.extend(names_to_add)\n",
    "\n",
    "        add_is_needed_column_to_df(df, column_name, education_names)\n",
    "\n",
    "        dfs[country] = df\n",
    "\n",
    "    show_results(column_name, dfs)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is a need for any certification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "certificates_non_eng = {\n",
    "    'Arabic': [r\"الشهادات\", r\"شهادة\"],\n",
    "    'Basque': [r\"ziurtagiriak\", r\"ziurtagiri\"],\n",
    "    'Catalan': [r\"certificats\", r\"certificat\"],\n",
    "    'Czech': [r\"certifikáty\", r\"certifikát\"],\n",
    "    'German': [r\"Zertifikate\", r\"Zertifikat\"],\n",
    "    'Danish': [r\"certifikater\", r\"certifikat\"],\n",
    "    'Spanish': [r\"certificados\", r\"certificado\"],\n",
    "    'Finnish': [r\"todistukset\", r\"todistus\"],\n",
    "    'French': [r\"certificats\", r\"certificat\"],\n",
    "    'Frisian': [r\"sertifikaten\", r\"sertifikaat\"],\n",
    "    'Galician': [r\"certificados\", r\"certificado\"],\n",
    "    'Greek': [r\"πιστοποιητικά\", r\"πιστοποιητικό\"],\n",
    "    'Hebrew': [r\"תעודות\", r\"תעודה\"],\n",
    "    'Hungarian': [r\"tanúsítványok\", r\"tanúsítvány\"],\n",
    "    'Italian': [r\"certificati\", r\"certificato\"],\n",
    "    'Kurdish': [r\"belge\", r\"bername\"],\n",
    "    'Dutch': [r\"certificaten\", r\"certificaat\"],\n",
    "    'Norwegian': [r\"sertifikater\", r\"sertifikat\"],\n",
    "    'Polish': [r\"certyfikat\", r\"certyfikaty\"],\n",
    "    'Portuguese': [r\"certificados\", r\"certificado\"],\n",
    "    'Romanian': [r\"certificate\", r\"certificat\"],\n",
    "    'Slovakian': [r\"certifikáty\", r\"certifikát\"],\n",
    "    'Slovenian': [r\"certifikati\", r\"certifikat\"],\n",
    "    'Swedish': [r\"certifikat\", r\"certifikat\"],\n",
    "    'Turkish': [r\"sertifikalar\", r\"sertifika\"],\n",
    "    'Japanese': [r\"証明書\", r\"資格証明書\"],\n",
    "    'Korean': [r\"증명서\", r\"자격증\"],\n",
    "    'Chinese_TR': [r\"證書\", r\"資格證書\"],\n",
    "    'Chinese_SP': [r\"证书\", r\"资格证书\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': Series([], Name: Certificate, dtype: int64), 'Austria': Other    1\n",
      "Name: Certificate, dtype: int64, 'Belgium': Other    2\n",
      "Name: Certificate, dtype: int64, 'Canada': Series([], Name: Certificate, dtype: int64), 'Czech_Republic': Series([], Name: Certificate, dtype: int64), 'Denmark': Series([], Name: Certificate, dtype: int64), 'Finland': Series([], Name: Certificate, dtype: int64), 'France': Other    1\n",
      "Name: Certificate, dtype: int64, 'Germany': Series([], Name: Certificate, dtype: int64), 'Greece': Series([], Name: Certificate, dtype: int64), 'Hong_Kong': Other    4\n",
      "Name: Certificate, dtype: int64, 'Hungary': Series([], Name: Certificate, dtype: int64), 'Ireland': Other    1\n",
      "Name: Certificate, dtype: int64, 'Israel': Series([], Name: Certificate, dtype: int64), 'Italy': Other    1\n",
      "Name: Certificate, dtype: int64, 'Japan': Series([], Name: Certificate, dtype: int64), 'Luxembourg': Series([], Name: Certificate, dtype: int64), 'Netherlands': Series([], Name: Certificate, dtype: int64), 'New_Zealand': Series([], Name: Certificate, dtype: int64), 'Norway': Series([], Name: Certificate, dtype: int64), 'Poland': Series([], Name: Certificate, dtype: int64), 'Portugal': Series([], Name: Certificate, dtype: int64), 'Romania': Series([], Name: Certificate, dtype: int64), 'Singapore': Other    1\n",
      "Name: Certificate, dtype: int64, 'South_Korea': Other    1\n",
      "Name: Certificate, dtype: int64, 'Spain': Other    1\n",
      "Name: Certificate, dtype: int64, 'Sweden': Series([], Name: Certificate, dtype: int64), 'Switzerland': Series([], Name: Certificate, dtype: int64), 'Taiwan': Series([], Name: Certificate, dtype: int64), 'Turkey': Series([], Name: Certificate, dtype: int64), 'United_Kingdom': DataCamp    1\n",
      "Name: Certificate, dtype: int64, 'United_States': Series([], Name: Certificate, dtype: int64)}\n"
     ]
    }
   ],
   "source": [
    "def make_check_certificate(country: str):\n",
    "\n",
    "    def check_certificate(job_description: str):\n",
    "\n",
    "        # Coursera, Udemy, Datacamp etc. list\n",
    "        certifications = [\n",
    "            r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "            r\"Google Professional Data Engineer\",\n",
    "            r\"Microsoft Azure Data Engineering\",\n",
    "            r\"Nanodegree\",\n",
    "            r\"DataCamp\",\n",
    "            r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "            r\"Python, Bash and SQL Essentials for Data Engineering Specialization\",\n",
    "            r\"Data Engineering ETL, Web Scraping, and Automation\",\n",
    "            r\"Big Data Engineering with Hadoop and Spark\",\n",
    "            r\"Certificate\",\n",
    "            r\"Certificates\",\n",
    "            ]\n",
    "\n",
    "        country_languages = countries_languages[country]\n",
    "        certificate_eng = certifications[-2:]\n",
    "\n",
    "        for language in country_languages:\n",
    "\n",
    "            certs_to_add = certificates_non_eng[language]\n",
    "\n",
    "            certifications.extend(certs_to_add)\n",
    "\n",
    "\n",
    "        for certificate in certifications:\n",
    "            if re.search((r\"\\b\" + certificate + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "\n",
    "                for certificate_non_eng in certificates_non_eng.values():\n",
    "\n",
    "                    if any(certificate.lower() in cert.lower() for cert in certificate_non_eng) or \\\n",
    "                    any(certificate.lower() in cert.lower() for cert in certificate_eng):\n",
    "                        return \"Other\"\n",
    "\n",
    "                return certificate\n",
    "            \n",
    "        return np.nan\n",
    "    \n",
    "    return check_certificate\n",
    "\n",
    "\n",
    "column_name = 'Certificate'\n",
    "\n",
    "for country, df in dfs.items():\n",
    "\n",
    "    check_certificate = make_check_certificate(country)\n",
    "        \n",
    "    df[column_name] = df['Description'].apply(check_certificate)\n",
    "\n",
    "    dfs[country] = df\n",
    "\n",
    "del check_certificate\n",
    "\n",
    "show_results(column_name, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "del add_tech_to_dfs, certificates_non_eng, make_check_certificate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. Needed education level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that not all countries' educational levels are equal or the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.1 BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_non_eng = {\n",
    "    'Arabic': [r\"بكالوريوس\", r\"العلوم الأساسية\"],\n",
    "    'Basque': [ r\"Lizentziatura\",  r\"Lizentziadun\"],\n",
    "    'Catalan': [r\"Llicenciatura\", r\"Llicenciat\"],\n",
    "    'Czech': [r\"Bakalář\", r\"Bakalářský\"],\n",
    "    'German': [r\"Bakkalaureatsabschluss\", r\"Bakkalaureat\", r\"Bakkalaureus\"],\n",
    "    'Danish': [r\"Kandidat\"],\n",
    "    'Spanish': [r\"Grado\", r\"Licenciatura\"],\n",
    "    'Finnish': [ r\"Kandidaatti\", r\"Luonnontieteiden kandidaatti\"],\n",
    "    'French': [r\"licence\", r\"licence universitaire\"],\n",
    "    'Frisian': [],\n",
    "    'Galician': [r\"Grao\", r\"licenciatura\", r\"licenciado\"],\n",
    "    'Greek': [r\"Πτυχίο\", r\"Πτυχιακός\"],\n",
    "    'Hebrew': [r\"תואר ראשון\", r\"בצלמל\"],\n",
    "    'Hungarian': [r\"Alapképzés\", r\"diplomás\"],\n",
    "    'Italian': [ r\"Laurea\", r\"Triennale\"],\n",
    "    'Kurdish': [ r\"Zanist\"],\n",
    "    'Dutch': [r\"Bachelordiploma\"],\n",
    "    'Norwegian': [r\"Bachelorgrad\"],\n",
    "    'Polish': [r\"Licencjat\", r\"Inżynier\"],\n",
    "    'Portuguese': [r\"Bacharelado\", r\"diploma de bacharel\", r\"solteiro\", r\"celibatário\"],\n",
    "    'Romanian': [r\"burlac\", r\"licenţiat\"],\n",
    "    'Slovakian': [r\"Bakalár\"],\n",
    "    'Slovenian': [r\"Samec\"],\n",
    "    'Swedish': [r\"Ungkarl\", r\"Kandidat\"],\n",
    "    'Turkish': [r\"Lisans\"],\n",
    "    'Japanese': [r\"学士号\", r\"学士\", r\"理学士\", r\"学士課程\"],\n",
    "    'Korean': [r\"학사 학위\", r\"학사\", r\"이학사\", r\"학사 학위과정\"],\n",
    "    'Chinese_TR': [r\"學士學位\", r\"學士\", r\"理學士\", r\"學士學位課程\"],\n",
    "    'Chinese_SP': [r\"学士学位\", r\"学士\", r\"理学士\", r\"学士学位课程\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: BA, dtype: int64, 'Austria': False    134\n",
      "True       5\n",
      "Name: BA, dtype: int64, 'Belgium': False    60\n",
      "True      4\n",
      "Name: BA, dtype: int64, 'Canada': False    4\n",
      "Name: BA, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: BA, dtype: int64, 'Denmark': False    90\n",
      "True      1\n",
      "Name: BA, dtype: int64, 'Finland': False    58\n",
      "Name: BA, dtype: int64, 'France': False    225\n",
      "True       2\n",
      "Name: BA, dtype: int64, 'Germany': False    162\n",
      "True       2\n",
      "Name: BA, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: BA, dtype: int64, 'Hong_Kong': False    95\n",
      "True     10\n",
      "Name: BA, dtype: int64, 'Hungary': False    86\n",
      "Name: BA, dtype: int64, 'Ireland': False    71\n",
      "True      4\n",
      "Name: BA, dtype: int64, 'Israel': False    229\n",
      "True      23\n",
      "Name: BA, dtype: int64, 'Italy': False    71\n",
      "True      9\n",
      "Name: BA, dtype: int64, 'Japan': False    42\n",
      "Name: BA, dtype: int64, 'Luxembourg': False    38\n",
      "True      2\n",
      "Name: BA, dtype: int64, 'Netherlands': False    40\n",
      "Name: BA, dtype: int64, 'New_Zealand': False    52\n",
      "Name: BA, dtype: int64, 'Norway': False    31\n",
      "Name: BA, dtype: int64, 'Poland': False    106\n",
      "True       3\n",
      "Name: BA, dtype: int64, 'Portugal': False    182\n",
      "True       7\n",
      "Name: BA, dtype: int64, 'Romania': False    114\n",
      "True       9\n",
      "Name: BA, dtype: int64, 'Singapore': False    129\n",
      "True       5\n",
      "Name: BA, dtype: int64, 'South_Korea': False    38\n",
      "True      8\n",
      "Name: BA, dtype: int64, 'Spain': False    124\n",
      "True       2\n",
      "Name: BA, dtype: int64, 'Sweden': False    144\n",
      "True       1\n",
      "Name: BA, dtype: int64, 'Switzerland': False    60\n",
      "Name: BA, dtype: int64, 'Taiwan': False    35\n",
      "True      5\n",
      "Name: BA, dtype: int64, 'Turkey': False    20\n",
      "True      2\n",
      "Name: BA, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: BA, dtype: int64, 'United_States': False    235\n",
      "True       6\n",
      "Name: BA, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "education_name = [\n",
    "    r\"BA\",\n",
    "    r\"Bachelor\",\n",
    "    r\"BSc\",\n",
    "    r\"Bachelors\"\n",
    "    ]\n",
    "\n",
    "column_name = 'BA'\n",
    "\n",
    "dfs = add_edu_to_dfs(dfs, education_name, education_non_eng, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.2 MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_non_eng = {\n",
    "    'Arabic':  [r\"الماجستير\", r\"الماجستيرات\"],\n",
    "    'Basque': [r\"Masterren\"],\n",
    "    'Catalan': [r\"Mestres\"],\n",
    "    'Czech': [r\"Magistr\", r\"Magisterský\", r\"magisterský\"],\n",
    "    'German': [r\"Meister\"],\n",
    "    'Danish': [r\"Kandidatuddannelse\"],\n",
    "    'Spanish': [r\"Máster\", r\"Maestría\"],\n",
    "    'Finnish': [r\"Maisteri\", r\"Luonnontieteiden maisteri\"],\n",
    "    'French': [r\"Maîtrise\", r\"Master universitaire\"],\n",
    "    'Frisian': [],\n",
    "    'Galician': [r\"Mestrado\", r\"mestrado universitario\"],\n",
    "    'Greek': [ r\"Μεταπτυχιακό\", r\"μεταπτυχιακός\", r\"Μεταπτυχιακή σπουδή\"],\n",
    "    'Hebrew': [r\"תואר שני\", r\"מגיסטר\"],\n",
    "    'Hungarian': [r\"Mesterképzés\", r\"mesterképző\"],\n",
    "    'Italian': [r\"Laurea magistrale\", r\"Magistrale\"],\n",
    "    'Kurdish': [ r\"Masterên\"],\n",
    "    'Dutch': [r\"Meesters\"],\n",
    "    'Norwegian': [r\"Mestere\"],\n",
    "    'Polish': [r\"Magister\", r\"magisterski\"],\n",
    "    'Portuguese': [r\"Mestras\", r\"Mestres\"],\n",
    "    'Romanian': [r\"Masterat\"],\n",
    "    'Slovakian': [r\"Majstri\"],\n",
    "    'Slovenian': [r\"Magistri\", r\"Mojstri\"],\n",
    "    'Swedish': [r\"Mästare\"],\n",
    "    'Turkish': [r\"Ustalar\", r\"ustaları\"],\n",
    "    'Japanese': [r\"学士\", r\"学士号\", r\"学士課程\", r\"バチェラー\"],\n",
    "    'Korean': [r\"학사\", r\"학사학위\", r\"배철러\"],\n",
    "    'Chinese_TR': [r\"學士\", r\"學士學位\", r\"學士學位課程\", r\"學士學位課程\"],\n",
    "    'Chinese_SP': [r\"学士\", r\"学士学位\", r\"学士学位课程\", r\"本科\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    64\n",
      "True      3\n",
      "Name: MS, dtype: int64, 'Austria': False    133\n",
      "True       6\n",
      "Name: MS, dtype: int64, 'Belgium': False    55\n",
      "True      9\n",
      "Name: MS, dtype: int64, 'Canada': False    4\n",
      "Name: MS, dtype: int64, 'Czech_Republic': False    66\n",
      "True      2\n",
      "Name: MS, dtype: int64, 'Denmark': False    87\n",
      "True      4\n",
      "Name: MS, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: MS, dtype: int64, 'France': False    200\n",
      "True      27\n",
      "Name: MS, dtype: int64, 'Germany': False    161\n",
      "True       3\n",
      "Name: MS, dtype: int64, 'Greece': False    55\n",
      "True      1\n",
      "Name: MS, dtype: int64, 'Hong_Kong': False    99\n",
      "True      6\n",
      "Name: MS, dtype: int64, 'Hungary': False    86\n",
      "Name: MS, dtype: int64, 'Ireland': False    73\n",
      "True      2\n",
      "Name: MS, dtype: int64, 'Israel': False    245\n",
      "True       7\n",
      "Name: MS, dtype: int64, 'Italy': False    76\n",
      "True      4\n",
      "Name: MS, dtype: int64, 'Japan': False    41\n",
      "True      1\n",
      "Name: MS, dtype: int64, 'Luxembourg': False    33\n",
      "True      7\n",
      "Name: MS, dtype: int64, 'Netherlands': False    39\n",
      "True      1\n",
      "Name: MS, dtype: int64, 'New_Zealand': False    52\n",
      "Name: MS, dtype: int64, 'Norway': False    27\n",
      "True      4\n",
      "Name: MS, dtype: int64, 'Poland': False    106\n",
      "True       3\n",
      "Name: MS, dtype: int64, 'Portugal': False    182\n",
      "True       7\n",
      "Name: MS, dtype: int64, 'Romania': False    122\n",
      "True       1\n",
      "Name: MS, dtype: int64, 'Singapore': False    130\n",
      "True       4\n",
      "Name: MS, dtype: int64, 'South_Korea': False    43\n",
      "True      3\n",
      "Name: MS, dtype: int64, 'Spain': False    122\n",
      "True       4\n",
      "Name: MS, dtype: int64, 'Sweden': False    141\n",
      "True       4\n",
      "Name: MS, dtype: int64, 'Switzerland': False    60\n",
      "Name: MS, dtype: int64, 'Taiwan': False    38\n",
      "True      2\n",
      "Name: MS, dtype: int64, 'Turkey': False    22\n",
      "Name: MS, dtype: int64, 'United_Kingdom': False    116\n",
      "True       2\n",
      "Name: MS, dtype: int64, 'United_States': False    233\n",
      "True       8\n",
      "Name: MS, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "education_name = [\n",
    "    r\"MS\",\n",
    "    r\"MSc\",\n",
    "    r\"Master\",\n",
    "    r\"Masters\",\n",
    "    r\"master\\'s\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MS'\n",
    "\n",
    "dfs = add_edu_to_dfs(dfs, education_name, education_non_eng, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.3 Phd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_non_eng = {\n",
    "    'Arabic':  [r\"دكتوراه في الفلسفة\", r\"دكتوراه\", r\"دكتوراة\"],\n",
    "    'Basque': [r\"Filosofia Doktore\", r\"doktoretza\", r\"Doktoregoa\"],\n",
    "    'Catalan': [r\"Mestres\", r\"Doctorat\"],\n",
    "    'Czech': [r\"Doktor\", r\"Doktorský\"],\n",
    "    'German': [r\"Doktorin\", r\"Doktor\"],\n",
    "    'Danish': [],\n",
    "    'Spanish': [r\"Doctor\", r\"Doctora\"],\n",
    "    'Finnish': [r\"Tohtori\"],\n",
    "    'French': [r\"Doctorat\"],\n",
    "    'Frisian': [],\n",
    "    'Galician': [r\"Doutorando\", r\"Doutoramento\"],\n",
    "    'Greek': [],\n",
    "    'Hebrew': [r\"דוקטורט\", r\"תואר שלישי\"],\n",
    "    'Hungarian': [r\"Dr\"],\n",
    "    'Italian': [r\"dottorato di ricerca\"],\n",
    "    'Kurdish': [ r\"Dr\"],\n",
    "    'Dutch': [r\"Doctoraat\"],\n",
    "    'Norwegian': [],\n",
    "    'Polish': [r\"Doktor\", r\"doktorski\"],\n",
    "    'Portuguese': [r\"doutorado\"],\n",
    "    'Romanian': [r\"doctorat\"],\n",
    "    'Slovakian': [r\"PhDr\"],\n",
    "    'Slovenian': [r\"doktorat znanosti\"],\n",
    "    'Swedish': [r\"doktorsexamen\"],\n",
    "    'Turkish': [r\"Doktora\"],\n",
    "    'Japanese': [r\"哲学博士\"],\n",
    "    'Korean': [r\"박사학위\"],\n",
    "    'Chinese_TR': [r\"博士\", r\"哲學博士\"],\n",
    "    'Chinese_SP': [r\"博士\", r\"哲学博士\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Australia': False    67\n",
      "Name: Phd, dtype: int64, 'Austria': False    139\n",
      "Name: Phd, dtype: int64, 'Belgium': False    63\n",
      "True      1\n",
      "Name: Phd, dtype: int64, 'Canada': False    4\n",
      "Name: Phd, dtype: int64, 'Czech_Republic': False    68\n",
      "Name: Phd, dtype: int64, 'Denmark': False    91\n",
      "Name: Phd, dtype: int64, 'Finland': False    57\n",
      "True      1\n",
      "Name: Phd, dtype: int64, 'France': False    227\n",
      "Name: Phd, dtype: int64, 'Germany': False    164\n",
      "Name: Phd, dtype: int64, 'Greece': False    56\n",
      "Name: Phd, dtype: int64, 'Hong_Kong': False    104\n",
      "True       1\n",
      "Name: Phd, dtype: int64, 'Hungary': False    86\n",
      "Name: Phd, dtype: int64, 'Ireland': False    75\n",
      "Name: Phd, dtype: int64, 'Israel': False    252\n",
      "Name: Phd, dtype: int64, 'Italy': False    79\n",
      "True      1\n",
      "Name: Phd, dtype: int64, 'Japan': False    42\n",
      "Name: Phd, dtype: int64, 'Luxembourg': False    40\n",
      "Name: Phd, dtype: int64, 'Netherlands': False    40\n",
      "Name: Phd, dtype: int64, 'New_Zealand': False    52\n",
      "Name: Phd, dtype: int64, 'Norway': False    30\n",
      "True      1\n",
      "Name: Phd, dtype: int64, 'Poland': False    109\n",
      "Name: Phd, dtype: int64, 'Portugal': False    187\n",
      "True       2\n",
      "Name: Phd, dtype: int64, 'Romania': False    123\n",
      "Name: Phd, dtype: int64, 'Singapore': False    133\n",
      "True       1\n",
      "Name: Phd, dtype: int64, 'South_Korea': False    46\n",
      "Name: Phd, dtype: int64, 'Spain': False    126\n",
      "Name: Phd, dtype: int64, 'Sweden': False    145\n",
      "Name: Phd, dtype: int64, 'Switzerland': False    60\n",
      "Name: Phd, dtype: int64, 'Taiwan': False    40\n",
      "Name: Phd, dtype: int64, 'Turkey': False    22\n",
      "Name: Phd, dtype: int64, 'United_Kingdom': False    118\n",
      "Name: Phd, dtype: int64, 'United_States': False    239\n",
      "True       2\n",
      "Name: Phd, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "education_name = [\n",
    "    r\"Phd\",\n",
    "    r\"Ph\\.D\",\n",
    "    r\"DPhil\",\n",
    "    r\"Doctor of Philosophy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Phd'\n",
    "\n",
    "dfs = add_edu_to_dfs(dfs, education_name, education_non_eng, column_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 109)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company_name', 'Rating', 'Job_title', 'Seniority', 'Salary_min',\n",
       "       'Salary_max', 'Salary_avg', 'Salary_currency',\n",
       "       'Salary_employer_provided', 'Salary_hourly',\n",
       "       ...\n",
       "       'InsightSquared', 'Sisense', 'Dundas_BI', 'Domo', 'Looker', 'Excel',\n",
       "       'Certificate', 'BA', 'MS', 'Phd'],\n",
       "      dtype='object', length=109)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "del column_name, tool_names, education_name, education_non_eng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Final cleanup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.1 Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, df in dfs.items():\n",
    "    dfs[country] = df.rename({\n",
    "        'Company_name': 'Name',\n",
    "        'Job_title': 'Title',\n",
    "        'Salary_min': 'Min',\n",
    "        'Salary_max': 'Max',\n",
    "        'Salary_avg': 'Avg',\n",
    "        'Salary_currency': 'Currency',\n",
    "        'Is_salary': 'Specified',\n",
    "        'Salary_employer_provided': 'Employer_provided',\n",
    "        'Salary_hourly': 'Is_hourly',\n",
    "        'Alibaba_Cloud': 'Alibaba',\n",
    "        'Oracle_Cloud': 'Oracle',\n",
    "        'IBM_cloud': 'IBM',\n",
    "        'Tencent_cloud': 'Tencent',\n",
    "        'DigitalOcean_cloud': 'DigitalOcean',\n",
    "        'Lincode_cloud': 'Lincode'\n",
    "        }, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    241\n",
       "Name: Alibaba, dtype: int64"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States']['Alibaba'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.2 Change columns order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_multiindex = [\n",
    "    ('Job_details', 'Title'),\n",
    "    ('Job_details', 'Description'),\n",
    "    ('Job_details', 'Seniority'),\n",
    "    ('Job_details', 'City'),\n",
    "    ('Job_details', 'State'),\n",
    "    ('Job_details', 'Country'),\n",
    "    ('Job_details', 'Region'),\n",
    "    ('Job_details', 'Job_age'),\n",
    "    ('Job_details', 'Easy_apply'),\n",
    "    ('Salary', 'Min'),\n",
    "    ('Salary', 'Max'),\n",
    "    ('Salary', 'Avg'),\n",
    "    ('Salary', 'Currency'),\n",
    "    ('Salary', 'Employer_provided'),\n",
    "    ('Salary', 'Is_hourly'),\n",
    "    ('Salary', 'Specified'),\n",
    "    ('Company_info', 'Name'),\n",
    "    ('Company_info', 'Rating'),\n",
    "    ('Company_info', 'Employees'),\n",
    "    ('Company_info', 'Type_of_ownership'),\n",
    "    ('Company_info', 'Sector'),\n",
    "    ('Company_info', 'Industry'),\n",
    "    ('Company_info', 'Company_age'),\n",
    "    ('Company_info', 'Revenue_USD'),\n",
    "    ('Company_info', 'Friend_recommend'),\n",
    "    ('Company_info', 'CEO_approval'),\n",
    "    ('Company_info', 'Career_opportunities'),\n",
    "    ('Company_info', 'Comp_&_benefits'),\n",
    "    ('Company_info', 'Senior_management'),\n",
    "    ('Company_info', 'Work/Life_balance'),\n",
    "    ('Company_info', 'Culture_&_values'),\n",
    "    ('Company_info', 'Pros'),\n",
    "    ('Company_info', 'Cons'),\n",
    "    ('Company_info', 'Benefits_rating'),\n",
    "    ('Company_info', 'Benefits_reviews'),\n",
    "    ('Education', 'BA'),\n",
    "    ('Education', 'MS'),\n",
    "    ('Education', 'Phd'),\n",
    "    ('Education', 'Certificate'),\n",
    "    ('Version_control', 'Git'),\n",
    "    ('Cloud_platforms', 'AWS'),\n",
    "    ('Cloud_platforms', 'Microsoft_Azure'),\n",
    "    ('Cloud_platforms', 'GPC'),\n",
    "    ('Cloud_platforms', 'Alibaba'),\n",
    "    ('Cloud_platforms', 'Oracle'),\n",
    "    ('Cloud_platforms', 'IBM'),\n",
    "    ('Cloud_platforms', 'Tencent'),\n",
    "    ('Cloud_platforms', 'OVHcloud'),\n",
    "    ('Cloud_platforms', 'DigitalOcean'),\n",
    "    ('Cloud_platforms', 'Lincode'),\n",
    "    ('RDBMS', 'PostgreSQL'),\n",
    "    ('RDBMS', 'Microsoft_SQL_Server'),\n",
    "    ('RDBMS', 'IBM_Db2'),\n",
    "    ('RDBMS', 'MySQL'),\n",
    "    ('RDBMS', 'Oracle_PL_SQL'),\n",
    "    ('NOSQL', 'MongoDB'),\n",
    "    ('NOSQL', 'Cassandra'),\n",
    "    ('NOSQL', 'Amazon_DynamoDB'),\n",
    "    ('NOSQL', 'Neo4j'),\n",
    "    ('Search_&_Analytics', 'Apache_Solr'),\n",
    "    ('Search_&_Analytics', 'Amazon_Redshift'),\n",
    "    ('Search_&_Analytics', 'Google_BigQuery'),\n",
    "    ('Search_&_Analytics', 'Snowflake'),\n",
    "    ('Search_&_Analytics', 'Oracle_Exadata'),\n",
    "    ('Search_&_Analytics', 'SAP_HANA'),\n",
    "    ('Search_&_Analytics', 'Teradata'),\n",
    "    ('Data_integration_and_processing', 'Informatica_PowerCenter'),\n",
    "    ('Data_integration_and_processing', 'Databricks'),\n",
    "    ('Data_integration_and_processing', 'Presto'),\n",
    "    ('Stream_processing_tools', 'Apache_Kafka'),\n",
    "    ('Stream_processing_tools', 'Apache_Flink'),\n",
    "    ('Stream_processing_tools', 'Dataflow'),\n",
    "    ('Workflow_orchestration_tools', 'Apache_Airflow'),\n",
    "    ('Workflow_orchestration_tools', 'Luigi'),\n",
    "    ('Workflow_orchestration_tools', 'SSIS'),\n",
    "    ('Big_Data_processing', 'Apache_Hadoop'),\n",
    "    ('Big_Data_processing', 'Apache_Hive'),\n",
    "    ('Big_Data_processing', 'Apache_Spark'),\n",
    "    ('OS', 'Linux'),\n",
    "    ('OS', 'Unix'),\n",
    "    ('OS', 'Windows'),\n",
    "    ('OS', 'macOS'),\n",
    "    ('Programming_languages', 'Python'),\n",
    "    ('Programming_languages', 'R'),\n",
    "    ('Programming_languages', 'Scala'),\n",
    "    ('Programming_languages', 'Julia'),\n",
    "    ('Programming_languages', 'SQL'),\n",
    "    ('Programming_languages', 'Java'),\n",
    "    ('Programming_languages', 'C++'),\n",
    "    ('Programming_languages', 'Go'),\n",
    "    ('Programming_languages', 'Rust'),\n",
    "    ('Programming_languages', 'Bash'),\n",
    "    ('Programming_languages', 'PowerShell'),\n",
    "    ('Programming_languages', 'CLI'),\n",
    "    ('Business_Intelligence_Tools', 'Tableau'),\n",
    "    ('Business_Intelligence_Tools', 'Power_BI'),\n",
    "    ('Business_Intelligence_Tools', 'Google_Analytics'),\n",
    "    ('Business_Intelligence_Tools', 'QlikView'),\n",
    "    ('Business_Intelligence_Tools', 'Oracle_BI_server'),\n",
    "    ('Business_Intelligence_Tools', 'SAS_Analytics'),\n",
    "    ('Business_Intelligence_Tools', 'Lumira'),\n",
    "    ('Business_Intelligence_Tools', 'Cognos_Impromptu'),\n",
    "    ('Business_Intelligence_Tools', 'MicroStrategy'),\n",
    "    ('Business_Intelligence_Tools', 'InsightSquared'), \n",
    "    ('Business_Intelligence_Tools', 'Sisense'), \n",
    "    ('Business_Intelligence_Tools', 'Dundas_BI'),\n",
    "    ('Business_Intelligence_Tools', 'Domo'), \n",
    "    ('Business_Intelligence_Tools', 'Looker'), \n",
    "    ('Business_Intelligence_Tools', 'Excel')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n",
      "C:\\Users\\karol\\AppData\\Local\\Temp\\ipykernel_38516\\2285460728.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(index, col, df.pop(col))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Title           object\n",
       "Description     object\n",
       "Seniority       object\n",
       "City            object\n",
       "State          float64\n",
       "                ...   \n",
       "Sisense           bool\n",
       "Dundas_BI         bool\n",
       "Domo              bool\n",
       "Looker            bool\n",
       "Excel             bool\n",
       "Length: 109, dtype: object"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_column__to_index(df: pd.DataFrame, column_name: str, index: int):\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "    return df\n",
    "\n",
    "\n",
    "def move_columns_to_index(df: pd.DataFrame, column_names: list[str], index: int):\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "new_columns_order = [t[1] for t in columns_multiindex]\n",
    "for country, df in dfs.items():\n",
    "    dfs[country] = move_columns_to_index(df, new_columns_order, 0)\n",
    "\n",
    "\n",
    "dfs['Austria'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excel'"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria'].columns[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.3 Add multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, df in dfs.items():\n",
    "    dfs[country].columns = pd.MultiIndex.from_tuples(columns_multiindex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9                Talentbase\n",
       "10           Ratbacher GmbH\n",
       "11    REWE International IT\n",
       "12      Erste Group Bank AG\n",
       "15               Talentbase\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria']['Company_info']['Name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "15    False\n",
       "Name: Excel, dtype: bool"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['Austria']['Business_Intelligence_Tools']['Excel'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Python</th>\n",
       "      <th>R</th>\n",
       "      <th>Scala</th>\n",
       "      <th>Julia</th>\n",
       "      <th>SQL</th>\n",
       "      <th>Java</th>\n",
       "      <th>C++</th>\n",
       "      <th>Go</th>\n",
       "      <th>Rust</th>\n",
       "      <th>Bash</th>\n",
       "      <th>PowerShell</th>\n",
       "      <th>CLI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Python      R  Scala  Julia    SQL   Java    C++     Go   Rust   Bash  \\\n",
       "0   False  False  False  False  False  False  False  False  False  False   \n",
       "1    True  False  False  False  False  False  False  False  False  False   \n",
       "2   False  False  False  False  False  False  False  False  False  False   \n",
       "3   False  False  False  False   True  False  False  False  False  False   \n",
       "4   False  False  False  False  False  False  False  False  False  False   \n",
       "\n",
       "   PowerShell    CLI  \n",
       "0       False  False  \n",
       "1       False  False  \n",
       "2       False  False  \n",
       "3       False  False  \n",
       "4       False  False  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['United_States']['Programming_languages'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. Concat countries to one dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3094, 109)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_countries = pd.concat(dfs.values())\n",
    "\n",
    "df_all_countries.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33. Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Job_details</th>\n",
       "      <th>Salary</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Business_Intelligence_Tools</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Min</th>\n",
       "      <th>...</th>\n",
       "      <th>SAS_Analytics</th>\n",
       "      <th>Lumira</th>\n",
       "      <th>Cognos_Impromptu</th>\n",
       "      <th>MicroStrategy</th>\n",
       "      <th>InsightSquared</th>\n",
       "      <th>Sisense</th>\n",
       "      <th>Dundas_BI</th>\n",
       "      <th>Domo</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Excel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>Data Warehouse Engineer</td>\n",
       "      <td>Decision Research Corporation (DRC), a softwar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Honolulu</td>\n",
       "      <td>HI</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>83000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Overview:\\r\\nAbout Wipro:\\r\\nWipro Limited (NY...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Warren</td>\n",
       "      <td>MI</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>: Data Engineer</td>\n",
       "      <td>Title: Data Engineer (NO C2C/C2H/OPT)\\r\\nLocat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saint Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>124800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Job Title: Data Engineer\\r\\nLocation: Atlanta,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>GA</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Position Details:\\r\\nTitle: Data Engineer\\r\\nI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Durham</td>\n",
       "      <td>NC</td>\n",
       "      <td>United States</td>\n",
       "      <td>North America</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>77000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Job_details  \\\n",
       "                        Title   \n",
       "3089  Data Warehouse Engineer   \n",
       "3090            Data Engineer   \n",
       "3091          : Data Engineer   \n",
       "3092            Data Engineer   \n",
       "3093            Data Engineer   \n",
       "\n",
       "                                                                   \\\n",
       "                                            Description Seniority   \n",
       "3089  Decision Research Corporation (DRC), a softwar...       NaN   \n",
       "3090  Overview:\\r\\nAbout Wipro:\\r\\nWipro Limited (NY...       NaN   \n",
       "3091  Title: Data Engineer (NO C2C/C2H/OPT)\\r\\nLocat...       NaN   \n",
       "3092  Job Title: Data Engineer\\r\\nLocation: Atlanta,...       NaN   \n",
       "3093  Position Details:\\r\\nTitle: Data Engineer\\r\\nI...       NaN   \n",
       "\n",
       "                                                                          \\\n",
       "             City State        Country         Region Job_age Easy_apply   \n",
       "3089     Honolulu    HI  United States  North America       2       True   \n",
       "3090       Warren    MI  United States  North America       1      False   \n",
       "3091  Saint Louis    MO  United States  North America      21       True   \n",
       "3092      Atlanta    GA  United States  North America       1       True   \n",
       "3093       Durham    NC  United States  North America       1       True   \n",
       "\n",
       "        Salary  ... Business_Intelligence_Tools                          \\\n",
       "           Min  ...               SAS_Analytics Lumira Cognos_Impromptu   \n",
       "3089   83000.0  ...                       False  False            False   \n",
       "3090   66000.0  ...                       False  False            False   \n",
       "3091  124800.0  ...                       False  False            False   \n",
       "3092  120000.0  ...                       False  False            False   \n",
       "3093   77000.0  ...                       False  False            False   \n",
       "\n",
       "                                                                          \n",
       "     MicroStrategy InsightSquared Sisense Dundas_BI   Domo Looker  Excel  \n",
       "3089         False          False   False     False  False  False  False  \n",
       "3090         False          False   False     False  False  False  False  \n",
       "3091         False          False   False     False  False  False  False  \n",
       "3092         False          False   False     False  False  False  False  \n",
       "3093         False          False   False     False  False  False  False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_countries.reset_index(inplace=True, drop=True)\n",
    "df_all_countries.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 34. Save CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34.1 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from scraper.config.get import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "local_path = os.path.join(\n",
    "    config['output_path']['main'],\n",
    "    config['output_path']['clean'],\n",
    "    \"Data_Engineer\"\n",
    "    )\n",
    "\n",
    "file_name = \"Data_Engineer_15-04-2023.csv\"\n",
    "file_path = Path(f\"{local_path}/{file_name}\")\n",
    "\n",
    "folder = os.path.dirname(file_path)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "\n",
    "df_all_countries.to_csv(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34.2 Check save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Job_details</th>\n",
       "      <th>Salary</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Business_Intelligence_Tools</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Min</th>\n",
       "      <th>...</th>\n",
       "      <th>SAS_Analytics</th>\n",
       "      <th>Lumira</th>\n",
       "      <th>Cognos_Impromptu</th>\n",
       "      <th>MicroStrategy</th>\n",
       "      <th>InsightSquared</th>\n",
       "      <th>Sisense</th>\n",
       "      <th>Dundas_BI</th>\n",
       "      <th>Domo</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Excel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Help us make a big green dent in the universe....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>We’ve only just begun, but what a beginning. I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>75932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expressions of Interest - Data Engineer, Data ...</td>\n",
       "      <td>At EY, you’ll have the chance to build a caree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>61017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Why 7-Eleven?\\r\\nWe're an agile, human centred...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Overview\\r\\nThe Data Engineer delivers enterpr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>67796.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Job_details  \\\n",
       "                                               Title   \n",
       "0                                      Data Engineer   \n",
       "1                                      Data Engineer   \n",
       "2  Expressions of Interest - Data Engineer, Data ...   \n",
       "3                                      Data Engineer   \n",
       "4                                      Data Engineer   \n",
       "\n",
       "                                                                \\\n",
       "                                         Description Seniority   \n",
       "0  Help us make a big green dent in the universe....       NaN   \n",
       "1  We’ve only just begun, but what a beginning. I...       NaN   \n",
       "2  At EY, you’ll have the chance to build a caree...       NaN   \n",
       "3  Why 7-Eleven?\\r\\nWe're an agile, human centred...       NaN   \n",
       "4  Overview\\r\\nThe Data Engineer delivers enterpr...       NaN   \n",
       "\n",
       "                                                                  Salary  ...  \\\n",
       "              City State    Country   Region Job_age Easy_apply      Min  ...   \n",
       "0        Melbourne   NaN  Australia  Oceania       7       True      NaN  ...   \n",
       "1     North Sydney   NaN  Australia  Oceania       2      False  75932.0  ...   \n",
       "2           Sydney   NaN  Australia  Oceania      14      False  61017.0  ...   \n",
       "3  New South Wales   NaN  Australia  Oceania       2      False      NaN  ...   \n",
       "4        Melbourne   NaN  Australia  Oceania      17      False  67796.0  ...   \n",
       "\n",
       "  Business_Intelligence_Tools                                        \\\n",
       "                SAS_Analytics Lumira Cognos_Impromptu MicroStrategy   \n",
       "0                       False  False            False         False   \n",
       "1                       False  False            False         False   \n",
       "2                       False  False            False         False   \n",
       "3                       False  False            False         False   \n",
       "4                       False  False            False         False   \n",
       "\n",
       "                                                         \n",
       "  InsightSquared Sisense Dundas_BI   Domo Looker  Excel  \n",
       "0          False   False     False  False  False  False  \n",
       "1          False   False     False  False  False  False  \n",
       "2          False   False     False  False  False  False  \n",
       "3          False   False     False  False  False  False  \n",
       "4          False   False     False  False  False  False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check = pd.read_csv(file_path, index_col=0, header=[0, 1])\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">Job_details</th>\n",
       "      <th>Salary</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Business_Intelligence_Tools</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Seniority</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Region</th>\n",
       "      <th>Job_age</th>\n",
       "      <th>Easy_apply</th>\n",
       "      <th>Min</th>\n",
       "      <th>...</th>\n",
       "      <th>SAS_Analytics</th>\n",
       "      <th>Lumira</th>\n",
       "      <th>Cognos_Impromptu</th>\n",
       "      <th>MicroStrategy</th>\n",
       "      <th>InsightSquared</th>\n",
       "      <th>Sisense</th>\n",
       "      <th>Dundas_BI</th>\n",
       "      <th>Domo</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Excel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Help us make a big green dent in the universe....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>We’ve only just begun, but what a beginning. I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>75932.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expressions of Interest - Data Engineer, Data ...</td>\n",
       "      <td>At EY, you’ll have the chance to build a caree...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>61017.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Why 7-Eleven?\\r\\nWe're an agile, human centred...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Overview\\r\\nThe Data Engineer delivers enterpr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>67796.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Job_details  \\\n",
       "                                               Title   \n",
       "0                                      Data Engineer   \n",
       "1                                      Data Engineer   \n",
       "2  Expressions of Interest - Data Engineer, Data ...   \n",
       "3                                      Data Engineer   \n",
       "4                                      Data Engineer   \n",
       "\n",
       "                                                                \\\n",
       "                                         Description Seniority   \n",
       "0  Help us make a big green dent in the universe....       NaN   \n",
       "1  We’ve only just begun, but what a beginning. I...       NaN   \n",
       "2  At EY, you’ll have the chance to build a caree...       NaN   \n",
       "3  Why 7-Eleven?\\r\\nWe're an agile, human centred...       NaN   \n",
       "4  Overview\\r\\nThe Data Engineer delivers enterpr...       NaN   \n",
       "\n",
       "                                                                  Salary  ...  \\\n",
       "              City State    Country   Region Job_age Easy_apply      Min  ...   \n",
       "0        Melbourne   NaN  Australia  Oceania       7       True      NaN  ...   \n",
       "1     North Sydney   NaN  Australia  Oceania       2      False  75932.0  ...   \n",
       "2           Sydney   NaN  Australia  Oceania      14      False  61017.0  ...   \n",
       "3  New South Wales   NaN  Australia  Oceania       2      False      NaN  ...   \n",
       "4        Melbourne   NaN  Australia  Oceania      17      False  67796.0  ...   \n",
       "\n",
       "  Business_Intelligence_Tools                                        \\\n",
       "                SAS_Analytics Lumira Cognos_Impromptu MicroStrategy   \n",
       "0                       False  False            False         False   \n",
       "1                       False  False            False         False   \n",
       "2                       False  False            False         False   \n",
       "3                       False  False            False         False   \n",
       "4                       False  False            False         False   \n",
       "\n",
       "                                                         \n",
       "  InsightSquared Sisense Dundas_BI   Domo Looker  Excel  \n",
       "0          False   False     False  False  False  False  \n",
       "1          False   False     False  False  False  False  \n",
       "2          False   False     False  False  False  False  \n",
       "3          False   False     False  False  False  False  \n",
       "4          False   False     False  False  False  False  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.shape == df_all_countries.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
