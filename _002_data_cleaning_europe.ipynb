{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Importing  a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/RAW/Data Engineer/Data_Engineer_United_States_06-03-2023_23-41.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove rows only with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no empty rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is huge amount of duplicates. But this is the feature of glassdoor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1, how='all')\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no empty columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Now we will split `Location` column into `State` and `City`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'] = df['Location'].apply(lambda x: x.split(',')[0] if x is not np.nan and \",\" in x else x)\n",
    "df['City'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State'] = df['Location'].apply(lambda x: x.split(',')[1] if x is not np.nan and \",\" in x else x)\n",
    "df['State'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del df['Location']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Add job title seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Job_title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seniority(job_title:str):\n",
    "\n",
    "    seniority = {\n",
    "        'Junior' : [\"Jr.\", \"Junior\"],\n",
    "        'Mid' : [\"Mid\", \"Middle\"],\n",
    "        'Senior': [\"Sr.\", \"Senior\"],\n",
    "        'Lead': \"Lead\",\n",
    "        'Principle' : \"Principle\"\n",
    "    }\n",
    "    \n",
    "    if seniority['Junior'][0] in job_title or seniority['Junior'][1] in job_title :\n",
    "        return \"Junior\"\n",
    "    elif seniority['Mid'][0] in job_title or seniority['Mid'][1] in job_title :\n",
    "        return \"Mid\"\n",
    "    elif seniority['Senior'][0] in job_title or seniority['Senior'][1] in job_title :\n",
    "        return \"Senior\"\n",
    "    elif seniority['Lead'] in job_title:\n",
    "        return \"Lead\"\n",
    "    elif seniority['Principle'] in job_title:\n",
    "        return \"Principle\"\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "df['Seniority'] = df['Job_title'].apply(get_seniority)\n",
    "\n",
    "del get_seniority\n",
    "\n",
    "df['Seniority'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add non-standard seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_seniority_level(df, job_title, company_name, seniority_level):\n",
    "    df['Seniority'] = df.apply(\n",
    "        lambda row: seniority_level if row['Job_title'] == job_title and row['Company_name'] == company_name else row['Seniority'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "apply_seniority_level(df, \"Data Engineer (L5)\", \"Netflix\", \"Senior\")\n",
    "apply_seniority_level(df, \"Technical Support Engineer (L5) - Data Platform, Big Data / Analytics\", \"Netflix\", \"Senior\")\n",
    "apply_seniority_level(df, \"Data Engineer Level 3\", \"Infoorigin Inc\", \"Mid\")\n",
    "apply_seniority_level(df, \"Data Engineer IC4 - US ONLY\", \"Braintrust\", \"Lead\")\n",
    "apply_seniority_level(df, \"ETL Engineer/ Data Analyst - Software Engineer III\", \"JPMorgan Chase Bank, N.A.\", \"Senior\")\n",
    "apply_seniority_level(df, \"Software Engineer III (AI, Data, Python)\", \"JPMorgan Chase Bank, N.A.\", \"Senior\")\n",
    "apply_seniority_level(df, \"Data Engineer 925\", \"Certec Consulting\", \"Senior\")\n",
    "\n",
    "del apply_seniority_level\n",
    "\n",
    "df['Seniority'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Parse salary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Employer provided salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_employer_provided'] = df['Salary'].apply(lambda salary : True if isinstance(salary, str) and \"Employer Provided Salary\" in salary else False)\n",
    "df['Salary_employer_provided'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Salary per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_hourly'] = df['Salary'].apply(lambda salary : True if isinstance(salary, str) and \"Per Hour\" in salary else False)\n",
    "df['Salary_hourly'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Salary min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_min(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?K?)\"\n",
    "        match_min: str = re.findall(pattern_salary, salary)[0][0]\n",
    "\n",
    "        if \"K\" in match_min:\n",
    "            match_min = float(match_min.replace(\"K\", \"\"))\n",
    "            match_min *= 1000\n",
    "\n",
    "        return float(match_min)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "    \n",
    "def calculate_yearly_income(hourly_rate):\n",
    "\n",
    "    hours_per_week = 40\n",
    "    WEEKS_PER_YEAR = 52\n",
    "    HOURS_PER_YEAR = WEEKS_PER_YEAR * hours_per_week\n",
    "    gross_income = hourly_rate * HOURS_PER_YEAR\n",
    "    return gross_income\n",
    "\n",
    "df['Salary_min'] = df['Salary'].apply(get_salary_min)\n",
    "df['Salary_min'] = df.apply(\n",
    "        lambda row: calculate_yearly_income(row['Salary_min']) if row['Salary_hourly'] == True else row['Salary_min'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "del get_salary_min\n",
    "\n",
    "df['Salary_min']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Salary max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_max(salary):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_salary = r\"(\\d+(\\.\\d+)?K?)\"\n",
    "        match_max: str = re.findall(pattern_salary, salary)[-1][0]\n",
    "\n",
    "        if \"K\" in match_max:\n",
    "            match_max = float(match_max.replace(\"K\", \"\"))\n",
    "            match_max *= 1000\n",
    "\n",
    "        return float(match_max)\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "\n",
    "df['Salary_max'] = df['Salary'].apply(get_salary_max)\n",
    "df['Salary_max'] = df.apply(\n",
    "        lambda row: calculate_yearly_income(row['Salary_max']) if row['Salary_hourly'] == True else row['Salary_max'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "del get_salary_max\n",
    "\n",
    "df['Salary_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "\n",
    "del calculate_yearly_income"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 Salary currency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_currency(salary: str):\n",
    "\n",
    "    if isinstance(salary, str):\n",
    "\n",
    "        pattern_currency = r\"(.+?(?=\\d))\"\n",
    "\n",
    "        if \"Employer Provided Salary\" in salary:\n",
    "            pattern_currency = r\"(\\:.+?(?=\\d))\"\n",
    "\n",
    "        matched = re.search(pattern_currency, salary)\n",
    "\n",
    "        currency = matched.group(1).strip().replace(\":\", \"\")\n",
    "\n",
    "        return currency\n",
    "\n",
    "    else:\n",
    "\n",
    "        return salary\n",
    "    \n",
    "df['Salary_currency'] = df['Salary'].apply(get_currency)\n",
    "    \n",
    "del get_currency\n",
    "    \n",
    "df['Salary_currency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Salary']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Salary average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary_avg'] = (df['Salary_max']+df['Salary_min'])/2\n",
    "df['Salary_avg']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Employees'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Type of ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type_of_ownership'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "year = datetime.date.today().year\n",
    "\n",
    "df['Company_age'] = df['Founded'].apply(lambda x: x if np.isnan(x) else int(year - x))\n",
    "df['Company_age'] = df['Company_age']\n",
    "\n",
    "del df['Founded'], year\n",
    "\n",
    "df['Company_age'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Job age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(df['Job_age'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_job_age(job_age):\n",
    "\n",
    "    if job_age == \"24h\":\n",
    "        job_age = \"1d\"\n",
    "    elif job_age == \"30d+\":\n",
    "        job_age = \"31d\"\n",
    "\n",
    "    return int(job_age.replace(\"d\", \"\"))\n",
    "\n",
    "df['Job_age'] = df['Job_age'].apply(clean_job_age)\n",
    "\n",
    "del clean_job_age\n",
    "df['Job_age'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Revenue_USD'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Preview columns so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Change columns order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.1 move salary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_column__to_index(column_name: str, index: int):\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "\n",
    "\n",
    "def move_columns_to_index(column_names: list[str], index: int):\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "move_columns_to_index([\n",
    "    'Salary_min', \n",
    "    'Salary_max', \n",
    "    'Salary_avg', \n",
    "    'Salary_currency',\n",
    "    'Salary_employer_provided', \n",
    "    'Salary_hourly'\n",
    "    ], 3\n",
    "    )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.2 Move Seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_column__to_index('Seniority', 3)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.3 Move City, State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_columns_to_index(['City', 'State'], 11)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.4 Move Company age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_column__to_index('Company_age', 19)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17.5 Move Work/Life_balance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_columns_to_index(['Senior_management', 'Work/Life_balance'], 25)\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Technology requirements - parsing the job description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19 Git and code repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repo(job_description: str):\n",
    "\n",
    "    git_platforms = [\n",
    "        r\"Github\", \n",
    "        r\"GitLab\", \n",
    "        r\"Bitbucket\", \n",
    "        r\"SourceForge\", \n",
    "        r\"Launchpad\", \n",
    "        r\"Google Cloud Source Repositories\",\n",
    "        r\"AWS CodeCommit\",\n",
    "        r\"GitBucket\",\n",
    "        r\"Gogs\",\n",
    "        r\"Gitea\",\n",
    "        r\"Apache Allura\",\n",
    "        r\"RhodeCode\",\n",
    "        r\"ONEDEV\",\n",
    "        r\"Codeberg\",\n",
    "        r\"Git\" # IMPORTANT, it has to be last!\n",
    "        ]\n",
    "    \n",
    "    for platform in git_platforms:\n",
    "        if re.search((r\"\\b\" + platform + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "            return platform\n",
    "        \n",
    "    return np.nan\n",
    "        \n",
    "df['Git'] = df['Description'].apply(check_repo)\n",
    "\n",
    "del check_repo\n",
    "\n",
    "df['Git'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_tech(cloud_names: list[str]):\n",
    "\n",
    "    def is_tech(job_description: str):\n",
    "\n",
    "        \n",
    "        for cloud in cloud_names:\n",
    "            if re.search((r\"\\b\" + cloud + r\"\\b\"), job_description, re.IGNORECASE):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    return is_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_needed_column_to_df(column_name: str, tech_names: list[str]):\n",
    "\n",
    "    df[column_name] = df['Description'].apply(make_is_tech(tech_names))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. Cloud Platforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.1 AWS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Often times, clients will use this in combination with autoscaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Amazon Web Services\", \n",
    "    r\"AWS\",\n",
    "    ]\n",
    "\n",
    "column_name = 'AWS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.2 Microsoft Azure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud computing platform operated by Microsoft that provides access, management, and development of applications and services via around the world-distributed data centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Microsoft Azure\", \n",
    "    r\"Azure\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_Azure'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.3 GCP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search, Gmail, Google Drive, and YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Google Cloud Platform\", \n",
    "    r\"GCP\",\n",
    "    ]\n",
    "\n",
    "column_name = 'GPC'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Alibaba Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibaba Cloud provides cloud computing services to online businesses and Alibaba's own e-commerce ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Alibaba Cloud\", \n",
    "    r\"Aliyun\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Alibaba_Cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.4 Oracle Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Oracle Cloud\", \n",
    "    r\"OCI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.5 IBM Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of cloud computing services for business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"IBM Cloud\", \n",
    "    r\"Kyndryl\",\n",
    "    r\"Bluemix\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.6 Tencent Cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tencent Cloud provides businesses across the globe with stable and secure industry-leading cloud products and services, leveraging technological advancements such as cloud computing, Big Data, AI, IoT and network security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Tencent Cloud\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Tencent_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.8 OVHcloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A French cloud computing company which offers VPS, dedicated servers and other web services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"OVHcloud\",\n",
    "    r\"OVH\"\n",
    "    ]\n",
    "\n",
    "column_name = 'OVHcloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.9 DigitalOcean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cloud hosting provider that offers cloud computing services and Infrastructure as a Service (IaaS). Known for pricing and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"DigitalOcean\"\n",
    "    ]\n",
    "\n",
    "column_name = 'DigitalOcean_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20.10 Linode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An American cloud hosting provider that focused on providing Linux-based virtual machines, cloud infrastructure, and managed services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_names = [\n",
    "    r\"Linode\",\n",
    "    r\"Akamai\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Lincode_cloud'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, cloud_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cloud_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. Relational Database Management Systems (RDBMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.1 PostgreSQL\n",
    "Can be used as a data store for big data solutions.\n",
    "Postgres, is a free and open-source relational database management system (RDBMS) emphasizing extensibility and SQL compliance. <br>\n",
    "PostgreSQL features transactions with Atomicity, Consistency, Isolation, Durability (ACID) properties, automatically updatable views, materialized views, triggers, foreign keys, and stored procedures. <br> It is designed to handle a range of workloads, from single machines to data warehouses or Web services with many concurrent users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PostgreSQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.2 Microsoft SQL Server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A software product with the primary function of storing and retrieving data as requested by other software applications—which may run either on the same computer or on another computer across a network (including the Internet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Microsoft SQL\",\n",
    "    r\"SQL Server\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Microsoft_SQL_Server'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.3 MySQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source relational database management system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"MySQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MySQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.4 IBM Db2 warehouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A family of data management products, including database servers, developed by IBM. It initially supported the relational model, but was extended to support object–relational features and non-relational structures like JSON and XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Db2\",\n",
    "    r\"IBMDb2\"\n",
    "    ]\n",
    "\n",
    "column_name = 'IBM_Db2'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.5. Oracle PL/SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A procedural language designed specifically to embrace SQL statements within its syntax. PL/SQL program units are compiled by the Oracle Database server and stored inside the database. And at run-time, both PL/SQL and SQL run within the same server process, bringing optimal efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"PL/SQL\",\n",
    "    r\"PL / SQL\",\n",
    "    r\"Procedural Language for SQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_PL_SQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. NoSQL Database Management Systems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A source-available cross-platform document-oriented database program. Classified as a NoSQL database program, MongoDB uses JSON-like documents with optional schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"MongoDB\",\n",
    "    r\"Mongo DB\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MongoDB'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Cassandra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Cassandra\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cassandra'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Amazon DynamoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proprietary NoSQL database service that supports key–value and document data structures and is offered by Amazon.com as part of the Amazon Web Services portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"DynamoDB\",\n",
    "    r\"Dynamo DB\",\n",
    "    r\"SimpleDB\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_DynamoDB'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22.4 Neo4j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph database management system developed by Neo4j, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Neo4j\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Neo4j'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 Apache Solr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source enterprise-search platform, written in Java. Its major features include full-text search, hit highlighting, faceted search, real-time indexing, dynamic clustering, database integration, NoSQL features[2] and rich document (e.g., Word, PDF) handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Solr\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Solr'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. Data warehousing and Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.1 Amazon Redshift"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Redshift\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Amazon_Redshift'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.2 Google BigQuery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A serverless data warehouse that enables scalable analysis over petabytes of data. It is a Platform as a Service that supports querying using ANSI SQL. It also has built-in machine learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"BigQuery\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_BigQuery'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.3 Snowflake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowflake enables data storage, processing, and analytic solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Snowflake\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Snowflake'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.4 Oracle Exadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designed to run Oracle Database workloads, such as an OLTP application running simultaneously with Analytics processing. Historically, specialized database computing platforms were designed for a particular workload, such as Data Warehousing, and poor or unusable for other workloads, such as OLTP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Exadata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_Exadata'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.5 SAP HANA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-model database that stores data in its memory instead of keeping it on a disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"HANA\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SAP_HANA'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.6 Teradata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mainly suitable for building large scale data warehousing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Teradata\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Teradata'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 23. Data Integration and Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.1 Informatica PowerCenter - Data integration tool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used extensively for ETL operations, data quality, data masking, data replication, data virtualization, and master data management services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"PowerCenter\",\n",
    "    r\"Power Center\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Informatica_PowerCenter'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.2 DataBricks - Data processing and analytics platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unified set of tools for building, deploying, sharing, and maintaining enterprise-grade data solutions at scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Data Bricks\",\n",
    "    r\"Databricks\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Databricks'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23.3 Presto - Query engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A distributed query engine for big data using the SQL query language. Its architecture allows users to query data sources such as Hadoop, Cassandra, Kafka, AWS S3, Alluxio, MySQL, MongoDB and Teradata, and allows use of multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Presto\",\n",
    "    r\"PrestoDB\",\n",
    "    r\"PrestoSQL\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Presto'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24. Stream processing tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Kafka"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An open-source system, distributed event store and stream-processing platform. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Kafka\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Kafka'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data streams at a large scale and to deliver real-time analytical insights about your processed data with your streaming application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Flink\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Flink'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Dataflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow is a managed service provided by Google Cloud for building and executing data processing pipelines. It enables developers to create scalable and efficient batch and streaming data pipelines using a simple programming model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Dataflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dataflow'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25 Workflow orchestration tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.1 Apache Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Airflow is an open-source platform used for programmatically creating, scheduling, and monitoring complex workflows or data pipelines. It allows users to define and execute a sequence of tasks or operations, while providing tools for tracking and troubleshooting workflow executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Airflow\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Airflow'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.2 Luigi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luigi is a Python-based open-source workflow management system that helps to build complex pipelines of batch jobs. It provides a flexible and extensible architecture to create and manage complex data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Luigi\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Luigi'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25.3 SSIS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Server Integration Services (SSIS) is a Microsoft tool used for building data integration and ETL (extract, transform, load) workflows. It allows users to perform a range of tasks such as data extraction, transformation, and loading from various sources to different destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"SSIS\",\n",
    "    r\"SQL Server Integration Services\"\n",
    "    ]\n",
    "\n",
    "column_name = 'SSIS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Big Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.1 Apache Hadoop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hadoop is an open-source framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It provides a distributed file system and supports various distributed computing models, such as MapReduce and Spark, for processing and analyzing large data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Hadoop\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hadoop'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.2 Apache Hive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Hive is a data warehouse software that facilitates querying and managing large datasets stored in Hadoop file systems using a SQL-like language called HiveQL. It provides a high-level interface for data analysts and developers to analyze, transform, and summarize data stored in Hadoop Distributed File System (HDFS) and other compatible storage systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Hive\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Hive'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.3 Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a distributed computing framework designed to process large-scale data processing and analysis workloads in parallel. It can be used for batch processing, real-time stream processing, machine learning, and graph processing, among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Spark\",\n",
    "    r\"PySpark\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Apache_Spark'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25. Linux"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Family of Unix-like operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Linux\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Linux'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Programming languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.1 Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is a high-level, interpreted programming language used for various purposes such as web development, data analysis, artificial intelligence, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Python\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Python'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.2 R"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language and environment for statistical graphics and computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"R\",\n",
    "    r\"RStudio\"\n",
    "    ]\n",
    "\n",
    "column_name = 'R'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.3 Scala"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scala is a high-level, statically typed programming language designed for functional programming and scalable, concurrent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Scala\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Scala'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.4 SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A programming language used to manage and manipulate relational databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"SQL\",\n",
    "    r\"MySQL\",\n",
    "    r\"PostgreSQL\",\n",
    "    r\"Postgres\",\n",
    "    r\"SQLite\",\n",
    "    r\"MariaDB\",\n",
    "    r\"IBM DB2\",\n",
    "    r\"Oracle Database\",\n",
    "    r\"Db2\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SQL'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.5 Java"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Java is a high-level, object-oriented programming language widely used for developing robust and scalable enterprise applications.\n",
    "\n",
    "In Data Science, Java can be used for developing machine learning models, data analysis, and data processing applications, as well as for building large-scale distributed systems for big data processing and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Java\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Java'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.6 C++"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general-purpose programming language designed for systems and application programming, and it is used in Data Science for building high-performance libraries and applications that require intensive computational tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"C\\+\\+\",\n",
    "    ]\n",
    "\n",
    "column_name = 'C++'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.7 Go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statically typed programming language designed for building simple, efficient, and reliable software, and it can be used in data engineering for building scalable, distributed systems for data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Go language\", # Go as separate word is too common in English\n",
    "    r\"Golang\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Go'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.8 Bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shell scripting language used for automating repetitive tasks and managing the operating system, including data processing tasks, in the command-line interface (CLI) on Unix and Unix-like systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Bash\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Bash'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.9 Powershell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task automation and configuration management framework from Microsoft, which can be used in Data Science for automating various data processing tasks on Windows machines in the command-line interface (CLI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"PowerShell\",\n",
    "    r\"DOS Shell\"\n",
    "    ]\n",
    "\n",
    "column_name = 'PowerShell'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26.10 CLI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLI stands for Command Line Interface, which is a way to interact with a computer program through text commands, and it is commonly used in Data Science for running scripts, automating tasks, and managing software packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"CLI\",\n",
    "    r\"Command Line Interface\"\n",
    "    ]\n",
    "\n",
    "column_name = 'CLI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 27. Virtualization Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business intelligence and data visualization tools used for analyzing and visualizing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.1 Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Tableau\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Tableau'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.2 Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Power BI\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Power_BI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.3 Google Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Google Analytics\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Google_Analytics'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.4 QlikView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"QlikView\",\n",
    "    r\"Qlik\"\n",
    "    ]\n",
    "\n",
    "column_name = 'QlikView'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.5 Oracle BI server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Oracle Business Intelligence Enterprise Edition\",\n",
    "    r\"OBIEE\",\n",
    "    r\"Oracle BI server\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Oracle_BI_server'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.6 SAS Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"SAS Analytics\",\n",
    "    r\"Statistical Analysis System\",\n",
    "    ]\n",
    "\n",
    "column_name = 'SAS_Analytics'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.7 Lumira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Lumira\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Lumira'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.8 IBM Cognos Impromptu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Cognos Impromptu\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Cognos_Impromptu'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.9 MicroStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"MicroStrategy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'MicroStrategy'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.10 InsightSquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"InsightSquared\",\n",
    "    ]\n",
    "\n",
    "column_name = 'InsightSquared'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.11 Sisense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Sisense\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Sisense'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.12 Dundas BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Dundas BI\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Dundas_BI'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.13 Domo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Domo\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Domo'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27.14 Looker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Looker\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Looker'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28. Microsoft Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Excel\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Excel'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 29. Certifications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there is a need for any certification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coursera, Udemy, Datacamp etc. list\n",
    "tool_names = [\n",
    "    r\"Certificates\",\n",
    "    r\"Certificate\",\n",
    "    r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "    r\"Google Professional Data Engineer\",\n",
    "    r\"Microsoft Azure Data Engineering\",\n",
    "    r\"Data Engineer.+Nanodegree\",\n",
    "    r\"DataCamp\",\n",
    "    r\"Data Engineering, Big Data, and Machine Learning on GCP\",\n",
    "    r\"Python, Bash and SQL Essentials for Data Engineering Specialization\",\n",
    "    r\"Data Engineering ETL, Web Scraping, and Automation\",\n",
    "    r\"Big Data Engineering with Hadoop and Spark\"\n",
    "    ]\n",
    "\n",
    "column_name = 'Is_certificate'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. Needed education level"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.1 BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"BA\",\n",
    "    r\"Bachelor\",\n",
    "    r\"BSc\",\n",
    "    r\"Bachelors\"\n",
    "    ]\n",
    "\n",
    "column_name = 'BA'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.2 MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"MS\",\n",
    "    r\"MSc\",\n",
    "    r\"Master\",\n",
    "    r\"Masters\",\n",
    "    r\"master\\'s\"\n",
    "    ]\n",
    "\n",
    "column_name = 'MS'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30.3 Phd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [\n",
    "    r\"Phd\",\n",
    "    r\"Ph\\.D\",\n",
    "    r\"DPhil\",\n",
    "    r\"Doctor of Philosophy\",\n",
    "    ]\n",
    "\n",
    "column_name = 'Phd'\n",
    "\n",
    "add_is_needed_column_to_df(column_name, tool_names)\n",
    "\n",
    "df[column_name].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = df.columns\n",
    "columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del columns_names, column_name, tool_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Final cleanup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.1 Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\n",
    "    'Company_name': 'Name',\n",
    "    'Job_title': 'Title',\n",
    "    'Salary_min': 'Min',\n",
    "    'Salary_max': 'Max',\n",
    "    'Salary_avg': 'Avg',\n",
    "    'Salary_currency': 'Currency',\n",
    "    'Salary_employer_provided': 'Employer_provided',\n",
    "    'Salary_hourly': 'Is_hourly',\n",
    "    'Alibaba_Cloud': 'Alibaba',\n",
    "    'Oracle_Cloud': 'Oracle',\n",
    "    'IBM_cloud': 'IBM',\n",
    "    'Tencent_cloud': 'Tencent',\n",
    "    'DigitalOcean_cloud': 'DigitalOcean',\n",
    "    'Lincode_cloud': 'Lincode'\n",
    "    }, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.2 Change columns order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_column__to_index(column_name: str, index: int):\n",
    "    df.insert(index, column_name, df.pop(column_name))\n",
    "\n",
    "\n",
    "def move_columns_to_index(column_names: list[str], index: int):\n",
    "    for col in column_names:\n",
    "        df.insert(index, col, df.pop(col))\n",
    "        index += 1\n",
    "\n",
    "move_columns_to_index([                       \n",
    "                    'Title',\n",
    "                    'Description',\n",
    "                    'Seniority',\n",
    "                    'City',\n",
    "                    'State',\n",
    "                    'Job_age',\n",
    "                    'Easy_apply',\n",
    "                    'Min',\n",
    "                    'Max',\n",
    "                    'Avg',\n",
    "                    'Currency',\n",
    "                    'Employer_provided',\n",
    "                    'Is_hourly',\n",
    "                    'Name',\n",
    "                    'Rating',\n",
    "                    'Employees',\n",
    "                    'Type_of_ownership',\n",
    "                    'Sector',\n",
    "                    'Industry',\n",
    "                    'Company_age',\n",
    "                    'Revenue_USD',\n",
    "                    'Friend_recommend',\n",
    "                    'CEO_approval',\n",
    "                    'Career_opportunities',\n",
    "                    'Comp_&_benefits',\n",
    "                    'Senior_management',\n",
    "                    'Work/Life_balance',\n",
    "                    'Culture_&_values',\n",
    "                    'Pros',\n",
    "                    'Cons',\n",
    "                    'Benefits_rating',\n",
    "                    'Benefits_reviews',\n",
    "                    'BA',\n",
    "                    'MS',\n",
    "                    'Phd',\n",
    "                    'Is_certificate',\n",
    "                    'Git',\n",
    "                    'AWS',\n",
    "                    'Microsoft_Azure',\n",
    "                    'GPC',\n",
    "                    'Alibaba',\n",
    "                    'Oracle',\n",
    "                    'IBM',\n",
    "                    'Tencent',\n",
    "                    'OVHcloud',\n",
    "                    'DigitalOcean',\n",
    "                    'Lincode',\n",
    "                    'PostgreSQL',\n",
    "                    'Microsoft_SQL_Server',\n",
    "                    'IBM_Db2',\n",
    "                    'MySQL',\n",
    "                    'Oracle_PL_SQL',\n",
    "                    'MongoDB',\n",
    "                    'Cassandra',\n",
    "                    'Amazon_DynamoDB',\n",
    "                    'Neo4j',\n",
    "                    'Apache_Solr',\n",
    "                    'Amazon_Redshift',\n",
    "                    'Google_BigQuery',\n",
    "                    'Snowflake',\n",
    "                    'Oracle_Exadata',\n",
    "                    'SAP_HANA',\n",
    "                    'Teradata',\n",
    "                    'Informatica_PowerCenter',\n",
    "                    'Databricks',\n",
    "                    'Presto',\n",
    "                    'Apache_Kafka',\n",
    "                    'Apache_Flink',\n",
    "                    'Dataflow',\n",
    "                    'Apache_Airflow',\n",
    "                    'Luigi',\n",
    "                    'SSIS',\n",
    "                    'Apache_Hadoop',\n",
    "                    'Apache_Hive',\n",
    "                    'Apache_Spark',\n",
    "                    'Linux',\n",
    "                    'Python',\n",
    "                    'R',\n",
    "                    'Scala',\n",
    "                    'SQL',\n",
    "                    'Java',\n",
    "                    'C++',\n",
    "                    'Go',\n",
    "                    'Bash',\n",
    "                    'PowerShell',\n",
    "                    'CLI',\n",
    "                    'Tableau',\n",
    "                    'Power_BI',\n",
    "                    'Google_Analytics',\n",
    "                    'QlikView',\n",
    "                    'Oracle_BI_server',\n",
    "                    'SAS_Analytics',\n",
    "                    'Lumira',\n",
    "                    'Cognos_Impromptu',\n",
    "                    'MicroStrategy',\n",
    "                    'InsightSquared', \n",
    "                    'Sisense', \n",
    "                    'Dundas_BI',\n",
    "                    'Domo', \n",
    "                    'Looker', \n",
    "                    'Excel'\n",
    "                    ],0\n",
    "    )\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31.3 Add multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = pd.MultiIndex.from_tuples([\n",
    "                                        ('Job_details', 'Title'),\n",
    "                                        ('Job_details', 'Description'),\n",
    "                                        ('Job_details', 'Seniority'),\n",
    "                                        ('Job_details', 'City'),\n",
    "                                        ('Job_details', 'State'),\n",
    "                                        ('Job_details', 'Job_age'),\n",
    "                                        ('Job_details', 'Easy_apply'),\n",
    "                                        ('Salary', 'Min'),\n",
    "                                        ('Salary', 'Max'),\n",
    "                                        ('Salary', 'Avg'),\n",
    "                                        ('Salary', 'Currency'),\n",
    "                                        ('Salary', 'Employer_provided'),\n",
    "                                        ('Salary', 'Is_hourly'),\n",
    "                                        ('Company_info', 'Name'),\n",
    "                                        ('Company_info', 'Rating'),\n",
    "                                        ('Company_info', 'Employees'),\n",
    "                                        ('Company_info', 'Type_of_ownership'),\n",
    "                                        ('Company_info', 'Sector'),\n",
    "                                        ('Company_info', 'Industry'),\n",
    "                                        ('Company_info', 'Company_age'),\n",
    "                                        ('Company_info', 'Revenue_USD'),\n",
    "                                        ('Company_info', 'Friend_recommend'),\n",
    "                                        ('Company_info', 'CEO_approval'),\n",
    "                                        ('Company_info', 'Career_opportunities'),\n",
    "                                        ('Company_info', 'Comp_&_benefits'),\n",
    "                                        ('Company_info', 'Senior_management'),\n",
    "                                        ('Company_info', 'Work/Life_balance'),\n",
    "                                        ('Company_info', 'Culture_&_values'),\n",
    "                                        ('Company_info', 'Pros'),\n",
    "                                        ('Company_info', 'Cons'),\n",
    "                                        ('Company_info', 'Benefits_rating'),\n",
    "                                        ('Company_info', 'Benefits_reviews'),\n",
    "                                        ('Education', 'BA'),\n",
    "                                        ('Education', 'MS'),\n",
    "                                        ('Education', 'Phd'),\n",
    "                                        ('Education', 'Is_certificate'),\n",
    "                                        ('Version_control', 'Git'),\n",
    "                                        ('Cloud_platforms', 'AWS'),\n",
    "                                        ('Cloud_platforms', 'Microsoft_Azure'),\n",
    "                                        ('Cloud_platforms', 'GPC'),\n",
    "                                        ('Cloud_platforms', 'Alibaba'),\n",
    "                                        ('Cloud_platforms', 'Oracle'),\n",
    "                                        ('Cloud_platforms', 'IBM'),\n",
    "                                        ('Cloud_platforms', 'Tencent'),\n",
    "                                        ('Cloud_platforms', 'OVHcloud'),\n",
    "                                        ('Cloud_platforms', 'DigitalOcean'),\n",
    "                                        ('Cloud_platforms', 'Lincode'),\n",
    "                                        ('RDBMS', 'PostgreSQL'),\n",
    "                                        ('RDBMS', 'Microsoft_SQL_Server'),\n",
    "                                        ('RDBMS', 'IBM_Db2'),\n",
    "                                        ('RDBMS', 'MySQL'),\n",
    "                                        ('RDBMS', 'Oracle_PL_SQL'),\n",
    "                                        ('NOSQL', 'MongoDB'),\n",
    "                                        ('NOSQL', 'Cassandra'),\n",
    "                                        ('NOSQL', 'Amazon_DynamoDB'),\n",
    "                                        ('NOSQL', 'Neo4j'),\n",
    "                                        ('Search_&_Analytics', 'Apache_Solr'),\n",
    "                                        ('Search_&_Analytics', 'Amazon_Redshift'),\n",
    "                                        ('Search_&_Analytics', 'Google_BigQuery'),\n",
    "                                        ('Search_&_Analytics', 'Snowflake'),\n",
    "                                        ('Search_&_Analytics', 'Oracle_Exadata'),\n",
    "                                        ('Search_&_Analytics', 'SAP_HANA'),\n",
    "                                        ('Search_&_Analytics', 'Teradata'),\n",
    "                                        ('Data_integration_and_processing', 'Informatica_PowerCenter'),\n",
    "                                        ('Data_integration_and_processing', 'Databricks'),\n",
    "                                        ('Data_integration_and_processing', 'Presto'),\n",
    "                                        ('Stream_processing_tools', 'Apache_Kafka'),\n",
    "                                        ('Stream_processing_tools', 'Apache_Flink'),\n",
    "                                        ('Stream_processing_tools', 'Dataflow'),\n",
    "                                        ('Workflow_orchestration_tools', 'Apache_Airflow'),\n",
    "                                        ('Workflow_orchestration_tools', 'Luigi'),\n",
    "                                        ('Workflow_orchestration_tools', 'SSIS'),\n",
    "                                        ('Big_Data_processing', 'Apache_Hadoop'),\n",
    "                                        ('Big_Data_processing', 'Apache_Hive'),\n",
    "                                        ('Big_Data_processing', 'Apache_Spark'),\n",
    "                                        ('OS', 'Linux'),\n",
    "                                        ('Programming_languages', 'Python'),\n",
    "                                        ('Programming_languages', 'R'),\n",
    "                                        ('Programming_languages', 'Scala'),\n",
    "                                        ('Programming_languages', 'SQL'),\n",
    "                                        ('Programming_languages', 'Java'),\n",
    "                                        ('Programming_languages', 'C++'),\n",
    "                                        ('Programming_languages', 'Go'),\n",
    "                                        ('Programming_languages', 'Bash'),\n",
    "                                        ('Programming_languages', 'PowerShell'),\n",
    "                                        ('Programming_languages', 'CLI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Tableau'),\n",
    "                                        ('Business_Intelligence_Tools', 'Power_BI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Google_Analytics'),\n",
    "                                        ('Business_Intelligence_Tools', 'QlikView'),\n",
    "                                        ('Business_Intelligence_Tools', 'Oracle_BI_server'),\n",
    "                                        ('Business_Intelligence_Tools', 'SAS_Analytics'),\n",
    "                                        ('Business_Intelligence_Tools', 'Lumira'),\n",
    "                                        ('Business_Intelligence_Tools', 'Cognos_Impromptu'),\n",
    "                                        ('Business_Intelligence_Tools', 'MicroStrategy'),\n",
    "                                        ('Business_Intelligence_Tools', 'InsightSquared'), \n",
    "                                        ('Business_Intelligence_Tools', 'Sisense'), \n",
    "                                        ('Business_Intelligence_Tools', 'Dundas_BI'),\n",
    "                                        ('Business_Intelligence_Tools', 'Domo'), \n",
    "                                        ('Business_Intelligence_Tools', 'Looker'), \n",
    "                                        ('Business_Intelligence_Tools', 'Excel'),                   \n",
    "                                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Company_info']['Name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Business_Intelligence_Tools']['Excel'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. Save CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32.1 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from scraper.config.get import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "local_path = os.path.join(\n",
    "    config['output_path']['main'],\n",
    "    config['output_path']['clean'],\n",
    "    \"Data_Engineer\"\n",
    "    )\n",
    "\n",
    "file_name = \"Data_Engineer_United_States_06-03-2023_23-41.csv\"\n",
    "file_path = Path(f\"{local_path}/{file_name}\")\n",
    "\n",
    "folder = os.path.dirname(file_path)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "\n",
    "df.to_csv(file_path, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32.2 Check save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.read_csv(file_path, index_col=0, header=[0, 1])\n",
    "df_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check.shape == df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
